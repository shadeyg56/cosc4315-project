[
    {
        "title": "Our security auditor is an idiot. How do I give him the information he wants?",
        "question": "A security auditor for our servers has demanded the following within two weeks:\n\nA list of current usernames and plain-text passwords for all user accounts on all servers\nA list of all password changes for the past six months, again in plain-text\nA list of \"every file added to the server from remote devices\" in the past six months\nThe public and private keys of any SSH keys\nAn email sent to him every time a user changes their password, containing the plain text password\n\nWe're running Red Hat Linux 5/6 and CentOS 5 boxes with LDAP authentication.\nAs far as I'm aware, everything on that list is either impossible or incredibly difficult to get, but if I don't provide this information we face losing access to our payments platform and losing income during a transition period as we move to a new service. Any suggestions for how I can solve or fake this information?\nThe only way I can think to get all the plain text passwords, is to get everyone to reset their password and make a note of what they set it to. That doesn't solve the problem of the past six months of password changes, because I can't retroactively log that sort of stuff, the same goes for logging all the remote files.\nGetting all of the public and private SSH keys is possible (though annoying), since we have just a few users and computers. Unless I've missed an easier way to do this?\nI have explained to him many times that the things he's asking for are impossible. In response to my concerns, he responded with the following email:\n\nI have over 10 years experience in security auditing and a full\n  understanding of the redhat security methods, so I suggest you check\n  your facts about what is and isn't possible. You say no company could\n  possibly have this information but I have performed hundreds of audits\n  where this information has been readily available. All [generic credit\n  card processing provider] clients are required to conform with our new\n  security policies and this audit is intended to ensure those policies\n  have been implemented* correctly.\n\n*The \"new security policies\" were introduced two weeks before our audit, and the six months historical logging was not required before the policy changes.\nIn short, I need;\n\nA way to \"fake\" six months worth of password changes and make it look valid\nA way to \"fake\" six months of inbound file transfers\nAn easy way to collect all the SSH public and private keys being used\n\nIf we fail the security audit we lose access to our card processing platform (a critical part of our system) and it would take a good two weeks to move somewhere else. How screwed am I?\nUpdate 1 (Sat 23rd)\nThanks for all your responses, It gives me great relief to know this isn't standard practice.\nI'm currently planning out my email response to him explaining the situation. As many of you pointed out, we have to comply with PCI which explicitly states we shouldn't have any way to access plain-text passwords. I'll post the email when I've finished writing it. Unfortunately I don't think he's just testing us; these things are in the company's official security policy now. I have, however, set the wheels in motion to move away from them and onto PayPal for the time being.\nUpdate 2 (Sat 23rd)\nThis is the email I've drafted out, any suggestions for stuff to add/remove/change?\n\nHi [name],\nUnfortunately there is no way for us to provide you with some\n  of the information requested, mainly plain-text passwords, password\n  history, SSH keys and remote file logs. Not only are these things\n  technically impossible, but also being able to provide this\n  information would be both a against PCI Standards, and a breach of the\n  data protection act.\n  To quote the PCI requirements,\n8.4 Render all passwords unreadable during transmission and storage on\n  all system components using strong cryptography.\nI can provide you\n  with a list of usernames and hashed passwords used on our system,\n  copies of the SSH public keys and authorized hosts file (This will\n  give you enough information to determine the number of unique users\n  can connect to our servers, and the encryption methods used),\n  information about our password security requirements and our LDAP\n  server but this information may not be taken off site. I strongly\n  suggest you review your audit requirements as there is currently no way\n  for us to pass this audit while remaining in compliance of PCI and the\n  Data Protection act.\nRegards,\n  [me]\n\nI will be CC'ing in the company's CTO and our account manager, and I'm hoping the CTO can confirm this information is not available. I will also be contacting the PCI Security Standards Council to explain what he's requiring from us.\nUpdate 3 (26th)\nHere are some emails we exchanged;\nRE: my first email;\n\nAs explained, this information should be easily available on any well\n  maintained system to any competent administrator. Your failure to be\n  able to provide this information leads me to believe you are aware of\n  security flaws in your system and are not prepared to reveal them. Our\n  requests line up with the PCI guidelines and both can be met. Strong\n  cryptography only means the passwords must be encrypted while the user\n  is inputting them but then they should be moved to a recoverable format\n  for later use.\nI see no data protection issues for these requests, data protection only\n  applies to consumers not businesses so there should be no issues with this\n  information.\n\nJust, what, I, can't, even...\n\n\"Strong cryptography only means the passwords must be encrypted while\n  the user is inputting them but then they should be moved to a\n  recoverable format for later use.\"\n\nI'm going to frame that and put it on my wall.\nI got fed up being diplomatic and directed him to this thread to show him the response I got:\n\nProviding this information DIRECTLY contradicts several requirements\n  of the PCI guidelines. The section I quoted even says storage\n  (Implying to where we store the data on the disk). I started a\n  discussion on ServerFault.com (An on-line community for sys-admin\n  professionals) which has created a huge response, all suggesting this\n  information cannot be provided. Feel free to read through yourself\nhttps&colon;//serverfault.com/questions/293217/\nWe have finished moving over our system to a new platform and will be\n  cancelling our account with you within the next day or so but I want\n  you to realize how ridiculous these requests are, and no company\n  correctly implementing the PCI guidelines will, or should, be able to\n  provide this information. I strongly suggest you re-think your\n  security requirements as none of your customers should be able to\n  conform to this.\n\n(I'd actually forgotten I'd called him an idiot in the title, but as mentioned we'd already moved away from their platform so no real loss.)  \nAnd in his response, he states that apparently none of you know what you're talking about:\n\nI read in detail through those responses and your original post, the\n  responders all need to get their facts right. I have been in this\n  industry longer than anyone on that site, getting a list of user\n  account passwords is incredibly basic, it should be one of the first\n  things you do when learning how to secure your system and is essential\n  to the operation of any secure server. If you genuinely lack the\n  skills to do something this simple I'm going to assume you do not have\n  PCI installed on your servers as being able to recover this\n  information is a basic requirement of the software. When dealing with\n  something such as security you should not be asking these questions on\n  a public forum if you have no basic knowledge of how it works.\nI would also like to suggest that any attempt to reveal me, or\n  [company name] will be considered libel and appropriate legal action\n  will be taken\n\nKey idiotic points if you missed them:\n\nHe's been a security auditor longer than anyone else on here has (He's either guessing, or stalking you)\nBeing able to get a list of passwords on a UNIX system is 'basic'\nPCI is now software\nPeople shouldn't use forums when they're not sure of security\nPosing factual information (to which I have email proof) online is libel\n\nExcellent.\nPCI SSC have responded and are investigating him and the company. Our software has now moved onto PayPal so we know it's safe. I'm going to wait for PCI to get back to me first but I'm getting a little worried that they might have been using these security practices internally. If so, I think it is a major concern for us as all our card processing ran through them. If they were doing this internally I think the only responsible thing to do would be to inform our customers. \nI'm hoping when PCI realize how bad it is they will investigate the entire company and system but I'm not sure.\nSo now we've moved away from their platform, and assuming it will be at least a few days before PCI get back to me, any inventive suggestions for how to troll him a bit? =)\nOnce I've got clearance from my legal guy (I highly doubt any of this is actually libel but I wanted to double check) I'll publish the company name, his name and email, and if you wish you can contact him and explain why you don't understand the basics of Linux security like how to get a list of all the LDAP users passwords.\nLittle update:\nMy \"legal guy\" has suggested revealing the company would probably cause more problems than needed. I can say though, this is not a major provider, they have less 100 clients using this service. We originally started using them when the site was tiny and running on a little VPS, and we didn't want to go through all the effort of getting PCI (We used to redirect to their frontend, like PayPal Standard). But when we moved to directly processing cards (including getting PCI, and common sense), the devs decided to keep using the same company just a different API. The company is based in the Birmingham, UK area so I'd highly doubt anyone here will be affected.",
        "top_answer": "First, DON'T capitulate. He is not only an idiot but DANGEROUSLY wrong. In fact, releasing this information would violate the PCI standard  (which is what I'm assuming the audit is for since it's a payment processor) along with every other standard out there and just plain common sense. It would also expose your company to all sorts of liabilities. \nThe next thing I would do is send an email to your boss saying he needs to get corporate counsel involved to determine the legal exposure the company would be facing by proceeding with this action. \nThis last bit is up to you, but I would contact VISA with this information and get his PCI auditor status pulled. ",
        "url": "https://serverfault.com/questions/293217/our-security-auditor-is-an-idiot-how-do-i-give-him-the-information-he-wants"
    },
    {
        "title": "What is a Pem file and how does it differ from other OpenSSL Generated Key File Formats?",
        "question": "I am responsible for maintaining two Debian servers. Every time I have to do anything with security certificates, I Google for tutorials and beat away until it finally works.\nHowever, in my searches I often come across different file formats (.key, .csr, .pem) but I've never been able to find a good explanation of what each file format's purpose is.\nI was wondering if the good folks here at ServerFault could provide some clarification on this matter?",
        "top_answer": "SSL has been around for long enough you'd think that there would be agreed upon container formats. And you're right, there are. Too many standards as it happens. In the end, all of these are different ways to encode Abstract Syntax Notation 1 (ASN.1) formatted data \u2014 which happens to be the format x509 certificates are defined in \u2014 in machine-readable ways.\n\n.csr - This is a Certificate Signing Request. Some applications can generate these for submission to certificate-authorities. The actual format is PKCS10 which is defined in RFC 2986. It includes some/all of the key details of the requested certificate such as subject, organization, state, whatnot, as well as the public key of the certificate to get signed. These get signed by the CA and a certificate is returned. The returned certificate is the public certificate (which includes the public key but not the private key), which itself can be in a couple of formats.\n.pem - Defined in RFC 1422 (part of a series from 1421 through 1424) this is a container format that may include just the public certificate (such as with Apache installs, and CA certificate files /etc/ssl/certs), or may include an entire certificate chain including public key, private key, and root certificates. Confusingly, it may also encode a CSR (e.g. as used here) as the PKCS10 format can be translated into PEM. The name is from Privacy Enhanced Mail (PEM), a failed method for secure email but the container format it used lives on, and is a base64 translation of the x509 ASN.1 keys.\n.key - This is a (usually) PEM formatted file containing just the private-key of a specific certificate and is merely a conventional name and not a standardized one. In Apache installs, this frequently resides in /etc/ssl/private. The rights on these files are very important, and some programs will refuse to load these certificates if they are set wrong.\n.pkcs12 .pfx .p12 - Originally defined by RSA in the Public-Key Cryptography Standards (abbreviated PKCS), the \"12\" variant was originally enhanced by Microsoft, and later submitted as RFC 7292. This is a password-protected container format that contains both public and private certificate pairs. Unlike .pem files, this container is fully encrypted. Openssl can turn this into a .pem file with both public and private keys: openssl pkcs12 -in file-to-convert.p12 -out converted-file.pem -nodes\n\nA few other formats that show up from time to time:\n\n.der - A way to encode ASN.1 syntax in binary, a .pem file is just a Base64 encoded .der file. OpenSSL can convert these to .pem (openssl x509 -inform der -in to-convert.der -out converted.pem). Windows sees these as Certificate files. By default, Windows will export certificates as .DER formatted files with a different extension. Like...\n.cert .cer .crt - A .pem (or rarely .der) formatted file with a different extension, one that is recognized by Windows Explorer as a certificate, which .pem is not.\n.p7b .keystore - Defined in RFC 2315 as PKCS number 7, this is a format used by Windows for certificate interchange. Java understands these natively, and often uses .keystore as an extension instead. Unlike .pem style certificates, this format has a defined way to include certification-path certificates.\n.crl - A certificate revocation list. Certificate Authorities produce these as a way to de-authorize certificates before expiration. You can sometimes download them from CA websites.\n\n\nIn summary, there are four different ways to present certificates and their components:\n\nPEM - Governed by RFCs, used preferentially by open-source software because it is text-based and therefore less prone to translation/transmission errors. It can have a variety of extensions (.pem, .key, .cer, .cert, more)\nPKCS7 - An open standard used by Java and supported by Windows. Does not contain private key material.\nPKCS12 - A Microsoft private standard that was later defined in an RFC that provides enhanced security versus the plain-text PEM format. This can contain private key and certificate chain material. Its used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl.\nDER - The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used very much outside of Windows.\n\nI hope this helps.",
        "url": "https://serverfault.com/questions/9708/what-is-a-pem-file-and-how-does-it-differ-from-other-openssl-generated-key-file"
    },
    {
        "title": "How can I sort du -h output by size",
        "question": "I need to get a list of human readable du output. \nHowever, du does not have a \"sort by size\" option, and piping to sort doesn't work with the human readable flag.\nFor example, running:\ndu | sort -n -r \n\nOutputs a sorted disk usage by size (descending):\ndu |sort -n -r\n65108   .\n61508   ./dir3\n2056    ./dir4\n1032    ./dir1\n508     ./dir2\n\nHowever, running it with the human readable flag, does not sort properly:\ndu -h | sort -n -r\n\n508K    ./dir2\n64M     .\n61M     ./dir3\n2.1M    ./dir4\n1.1M    ./dir1\n\nDoes anyone know of a way to sort du -h  by size?",
        "top_answer": "As of GNU coreutils 7.5 released in August 2009, sort allows a -h parameter, which allows numeric suffixes of the kind produced by du -h:\ndu -hs * | sort -h\n\nIf you are using a sort that does not support -h, you can install GNU Coreutils. E.g. on an older Mac OS X:\nbrew install coreutils\ndu -hs * | gsort -h\n\nFrom sort manual:\n-h, --human-numeric-sort    compare human readable numbers (e.g., 2K 1G)",
        "url": "https://serverfault.com/questions/62411/how-can-i-sort-du-h-output-by-size"
    },
    {
        "title": "How to determine if a bash variable is empty?",
        "question": "What is the best way to determine if a variable in bash is empty (\"\")?\nI have heard that it is recommended that I do if [ \"x$variable\" = \"x\" ]\nIs that the correct way?  (there must be something more straightforward)",
        "top_answer": "This will return true if a variable is unset or set to the empty string (\"\").\nif [ -z \"${VAR}\" ];\n",
        "url": "https://serverfault.com/questions/7503/how-to-determine-if-a-bash-variable-is-empty"
    },
    {
        "title": "Ping a Specific Port",
        "question": "Just a quick sanity check here.\nCan you ping a specific port of a machine, and if so, can you provide an example?\nI'm looking for something like ping ip address portNum.",
        "top_answer": "You can't ping ports, as Ping is using ICMP which is an internet layer protocol that doesn't have ports. Ports belong to the transport layer protocols like TCP and UDP.\nHowever, you could use nmap to see whether ports are open or not\nnmap -p 80 example.com\n\nEdit:\nAs flokra mentioned, nmap is more than just a ping-for-ports-thingy. It's the security auditers and hackers best friend and comes with tons of cool options. Check the doc for all possible flags.",
        "url": "https://serverfault.com/questions/309357/ping-a-specific-port"
    },
    {
        "title": "Can scp copy directories recursively?",
        "question": "Currently I can only copy a single .tar file. But how can I copy directories recursively with scp?",
        "top_answer": "Yup, use -r:\nscp -rp sourcedirectory user@dest:/path\n\n\n-r means recursive\n-p preserves modification times, access times, and modes from the original file. \n\nNote: This creates the sourcedirectory inside /path thus the files will be in /path/sourcedirectory",
        "url": "https://serverfault.com/questions/264595/can-scp-copy-directories-recursively"
    },
    {
        "title": "How to automate SSH login with password?",
        "question": "How can I automate SSH login using a password?\nI'm configuring my test VM, so heavy security is not a consideration. SSH chosen for acceptable security with minimal configuration.\nex)\necho password | ssh id@server\n\nThis doesn't work.\nI realize I did this with some tricks somebody guided me through, but I can't remember now the trick I used.",
        "top_answer": "$ sudo apt-get install sshpass\n$ sshpass -p your_password ssh user@hostname\n",
        "url": "https://serverfault.com/questions/241588/how-to-automate-ssh-login-with-password"
    },
    {
        "title": "How do you find what process is holding a file open in Windows?",
        "question": "One thing that annoys me no end about Windows is the old sharing violation error. Often you can't identify what's holding it open. Usually it's just an editor or explorer just pointing to a relevant directory but sometimes I've had to resort to rebooting my machine.\nAny suggestions on how to find the culprit?",
        "top_answer": "I've had success with Sysinternals Process Explorer.  With this, you can search to find what process(es) have a file open, and you can use it to close the handle(s) if you want. Of course, it is safer to close the whole process.  Exercise caution and judgement.\nTo find a specific file, use the menu option Find->Find Handle or DLL...  Type in part of the path to the file.  The list of processes will appear below.\nIf you prefer command line, Sysinternals suite includes command line tool Handle, that lists open handles.\nExamples\n\nc:\\Program Files\\SysinternalsSuite>handle.exe |findstr /i \"e:\\\" (finds all files opened from drive e:\\\"\nc:\\Program Files\\SysinternalsSuite>handle.exe |findstr /i \"file-or-path-in-question\"\n",
        "url": "https://serverfault.com/questions/1966/how-do-you-find-what-process-is-holding-a-file-open-in-windows"
    },
    {
        "title": "How do I deal with a compromised server?",
        "question": "\nThis is a Canonical Question about Server Security - Responding to Breach Events (Hacking)\n  See Also:\n\nTips for Securing a LAMP Server\nReinstall after a Root Compromise?\n\n\nCanonical Version\nI suspect that one or more of my servers is compromised by a hacker, virus, or other mechanism:\n\nWhat are my first steps? When I arrive on site should I disconnect the server, preserve \"evidence\", are there other initial considerations?\nHow do I go about getting services back online?\nHow do I prevent the same thing from happening immediately again?\nAre there best practices or methodologies for learning from this incident?\nIf I wanted to put a Incident Response Plan together, where would I start? Should this be part of my Disaster Recovery or Business Continuity Planning?\n\nOriginal Version \n\n2011.01.02 - I'm on my way into work at 9.30 p.m. on a Sunday because our server has been compromised somehow and was resulting in a\n  DOS attack on our provider. The servers access to the Internet\n  has been shut down which means over 5-600 of our clients sites are now\n  down. Now this could be an FTP hack, or some weakness in code\n  somewhere. I'm not sure till I get there.\nHow can I track this down quickly? We're in for a whole lot of\n  litigation if I don't get the server back up ASAP. Any help is\n  appreciated. We are running Open SUSE 11.0.\n\n2011.01.03 - Thanks to everyone for your help. Luckily I WASN'T the only person responsible for this server, just the nearest. We managed\n  to resolve this problem, although it may not apply to many others in a\n  different situation. I'll detail what we did.\nWe unplugged the server from the net. It was performing (attempting to\n  perform) a Denial Of Service attack on another server in Indonesia,\n  and the guilty party was also based there.\nWe firstly tried to identify where on the server this was coming from,\n  considering we have over 500 sites on the server, we expected to be\n  moonlighting for some time. However, with SSH access still, we ran a\n  command to find all files edited or created in the time the attacks\n  started. Luckily, the offending file was created over the winter\n  holidays which meant that not many other files were created on the\n  server at that time.\nWe were then able to identify the offending file which was inside the\n  uploaded images folder within a ZenCart website.\nAfter a short cigarette break we concluded that, due to the files\n  location, it must have been uploaded via a file upload facility that\n  was inadequetly secured. After some googling, we found that there was\n  a security vulnerability that allowed files to be uploaded, within the\n  ZenCart admin panel, for a picture for a record company. (The section\n  that it never really even used), posting this form just uploaded any\n  file, it did not check the extension of the file, and didn't even\n  check to see if the user was logged in.\nThis meant that any files could be uploaded, including a PHP file for\n  the attack. We secured the vulnerability with ZenCart on the infected\n  site, and removed the offending files.\nThe job was done, and I was home for 2 a.m.\n\nThe Moral\n  - Always apply security patches for ZenCart, or any other CMS system for that matter. As when security updates are released, the whole\n  world is made aware of the vulnerability.\n  - Always do backups, and backup your backups.\n  - Employ or arrange for someone that will be there in times like these. To prevent anyone from relying on a panicy post on Server\n  Fault.\n",
        "top_answer": "It's hard to give specific advice from what you've posted here but I do have some generic advice based on a post I wrote ages ago back when I could still be bothered to blog.\nDon't Panic\nFirst things first, there are no \"quick fixes\" other than restoring your system from a backup taken prior to the intrusion, and this has at least two problems.\n\nIt's difficult to pinpoint when the intrusion happened.\nIt doesn't help you close the \"hole\" that allowed them to break in last time, nor deal with the consequences of any \"data theft\" that may also have taken place.\n\nThis question keeps being asked repeatedly by the victims of hackers breaking into their web server. The answers very rarely change, but people keep asking the question. I'm not sure why. Perhaps people just don't like the answers they've seen when searching for help, or they can't find someone they trust to give them advice. Or perhaps people read an answer to this question and focus too much on the 5% of why their case is special and different from the answers they can find online and miss the 95% of the question and answer where their case is near enough the same as the one they read online.\nThat brings me to the first important nugget of information. I really do appreciate that you are a special unique snowflake. I appreciate that your website is too, as it's a reflection of you and your business or at the very least, your hard work on behalf of an employer. But to someone on the outside looking in, whether a computer security person looking at the problem to try and help you or even the attacker himself, it is very likely that your problem will be at least 95% identical to every other case they've ever looked at.\nDon't take the attack personally, and don't take the recommendations that follow here or that you get from other people personally. If you are reading this after just becoming the victim of a website hack then I really am sorry, and I really hope you can find something helpful here, but this is not the time to let your ego get in the way of what you need to do.\nYou have just found out that your server(s) got hacked. Now what?\nDo not panic. Absolutely do not act in haste, and absolutely do not try and pretend things never happened and not act at all.\nFirst: understand that the disaster has already happened. This is not the time for denial; it is the time to accept what has happened, to be realistic about it, and to take steps to manage the consequences of the impact.\nSome of these steps are going to hurt, and (unless your website holds a copy of my details) I really don't care if you ignore all or some of these steps, that's up to you. But following them properly will make things better in the end. The medicine might taste awful but sometimes you have to overlook that if you really want the cure to work.\nStop the problem from becoming worse than it already is:\n\nThe first thing you should do is disconnect the affected systems from the Internet. Whatever other problems you have, leaving the system connected to the web will only allow the attack to continue. I mean this quite literally; get someone to physically visit the server and unplug network cables if that is what it takes, but disconnect the victim from its muggers before you try to do anything else.\nChange all your passwords for all accounts on all computers that are on the same network as the compromised systems. No really. All accounts. All computers. Yes, you're right, this might be overkill; on the other hand, it might not. You don't know either way, do you?\nCheck your other systems. Pay special attention to other Internet facing services, and to those that hold financial or other commercially sensitive data.\nIf the system holds anyone's personal data,  immediately inform the person responsible for data protection (if that's not you) and URGE a full disclosure. I know this one is tough. I know this one is going to hurt. I know that many businesses want to sweep this kind of problem under the carpet but the business is going to have to deal with it - and needs to do so with an eye on any and all relevant privacy laws.\n\nHowever annoyed your customers might be to have you tell them about a problem, they'll be far more annoyed if you don't tell them, and they only find out for themselves after someone charges $8,000 worth of goods using the credit card details they stole from your site.\nRemember what I said previously? The bad thing has already happened. The only question now is how well you deal with it.\nUnderstand the problem fully:\n\nDo NOT put the affected systems back online until this stage is fully complete, unless you want to be the person whose post was the tipping point for me actually deciding to write this article. I'm not going to link to that post so that people can get a cheap laugh, but the real tragedy is when people fail to learn from their mistakes.\nExamine the 'attacked' systems to understand how the attacks succeeded in compromising your security. Make every effort to find out where the attacks \"came from\", so that you understand what problems you have and need to address to make your system safe in the future.\nExamine the 'attacked' systems again, this time to understand where the attacks went, so that you understand what systems were compromised in the attack. Ensure you follow up any pointers that suggest compromised systems could become a springboard to attack your systems further.\nEnsure the \"gateways\" used in any and all attacks are fully understood, so that you may begin to close them properly. (e.g. if your systems were compromised by a SQL injection attack, then not only do you need to close the particular flawed line of code that they broke in by, you would want to audit all of your code to see if the same type of mistake was made elsewhere).\nUnderstand that attacks might succeed because of more than one flaw. Often, attacks succeed not through finding one major bug in a system but by stringing together several issues (sometimes minor and trivial by themselves) to compromise a system. For example, using SQL injection attacks to send commands to a database server, discovering the website/application you're attacking is running in the context of an administrative user and using the rights of that account as a stepping-stone to compromise other parts of a system. Or as hackers like to call it: \"another day in the office taking advantage of common mistakes people make\".\n\nWhy not just \"repair\" the exploit or rootkit you've detected and put the system back online?\nIn situations like this the problem is that you don't have control of that system any more. It's not your computer any more.\nThe only way to be certain that you've got control of the system is to rebuild the system. While there's a lot of value in finding and fixing the exploit used to break into the system, you can't be sure about what else has been done to the system once the intruders gained control (indeed, its not unheard of for hackers that recruit systems into a botnet to patch the exploits they used themselves, to safeguard \"their\" new computer from other hackers, as well as installing their rootkit).\nMake a plan for recovery and to bring your website back online and stick to it:\nNobody wants to be offline for longer than they have to be. That's a given. If this website is a revenue generating mechanism then the pressure to bring it back online quickly will be intense. Even if the only thing at stake is your / your company's reputation, this is still going generate a lot of pressure to put things back up quickly.\nHowever, don't give in to the temptation to go back online too quickly. Instead move with as fast as possible to understand what caused the problem and to solve it before you go back online or else you will almost certainly fall victim to an intrusion once again, and remember, \"to get hacked once can be classed as misfortune; to get hacked again straight afterward looks like carelessness\" (with apologies to Oscar Wilde).\n\nI'm assuming you've understood all the issues that led to the successful intrusion in the first place before you even start this section. I don't want to overstate the case but if you haven't done that first then you really do need to. Sorry.\nNever pay blackmail / protection money. This is the sign of an easy mark and you don't want that phrase ever used to describe you.\nDon't be tempted to put the same server(s) back online without a full rebuild. It should be far quicker to build a new box or \"nuke the server from orbit and do a clean install\" on the old hardware than it would be to audit every single corner of the old system to make sure it is clean before putting it back online again. If you disagree with that then you probably don't know what it really means to ensure a system is fully cleaned, or your website deployment procedures are an unholy mess. You presumably have backups and test deployments of your site that you can just use to build the live site, and if you don't then being hacked is not your biggest problem.\nBe very careful about re-using data that was \"live\" on the system at the time of the hack. I won't say \"never ever do it\" because you'll just ignore me, but frankly I think you do need to consider the consequences of keeping data around when you know you cannot guarantee its integrity. Ideally, you should restore this from a backup made prior to the intrusion. If you cannot or will not do that, you should be very careful with that data because it's tainted. You should especially be aware of the consequences to others if this data belongs to customers or site visitors rather than directly to you.\nMonitor the system(s) carefully. You should resolve to do this as an ongoing process in the future (more below) but you take extra pains to be vigilant during the period immediately following your site coming back online. The intruders will almost certainly be back, and if you can spot them trying to break in again you will certainly be able to see quickly if you really have closed all the holes they used before plus any they made for themselves, and you might gather useful information you can pass on to your local law enforcement.\n\n\nReducing the risk in the future.\nThe first thing you need to understand is that security is a process that you have to apply throughout the entire life-cycle of designing, deploying and maintaining an Internet-facing system, not something you can slap a few layers over your code afterwards like cheap paint. To be properly secure, a service and an application need to be designed from the start with this in mind as one of the major goals of the project. I realise that's boring and you've heard it all before and that I \"just don't realise the pressure man\" of getting your beta web2.0 (beta) service into beta status on the web, but the fact is that this keeps getting repeated because it was true the first time it was said and it hasn't yet become a lie.\nYou can't eliminate risk. You shouldn't even try to do that. What you should do however is to understand which security risks are important to you, and understand how to manage and reduce both the impact of the risk and the probability that the risk will occur.\nWhat steps can you take to reduce the probability of an attack being successful?\nFor example:\n\nWas the flaw that allowed people to break into your site a known bug in vendor code, for which a patch was available? If so, do you need to re-think your approach to how you patch applications on your Internet-facing servers?\nWas the flaw that allowed people to break into your site an unknown bug in vendor code, for which a patch was not available? I most certainly do not advocate changing suppliers whenever something like this bites you because they all have their problems and you'll run out of platforms in a year at the most if you take this approach. However, if a system constantly lets you down then you should either migrate to something more robust or at the very least, re-architect your system so that vulnerable components stay wrapped up in cotton wool and as far away as possible from hostile eyes.\nWas the flaw a bug in code developed by you (or a contractor working for you)? If so, do you need to re-think your approach to how you approve code for deployment to your live site? Could the bug have been caught with an improved test system, or with changes to your coding \"standard\" (for example, while technology is not a panacea, you can reduce the probability of a successful SQL injection attack by using well-documented coding techniques).\nWas the flaw due to a problem with how the server or application software was deployed? If so, are you using automated procedures to build and deploy servers where possible? These are a great help in maintaining a consistent \"baseline\" state on all your servers, minimising the amount of custom work that has to be done on each one and hence hopefully minimising the opportunity for a mistake to be made. Same goes with code deployment - if you require something \"special\" to be done to deploy the latest version of your web app then try hard to automate it and ensure it always is done in a consistent manner.\nCould the intrusion have been caught earlier with better monitoring of your systems? Of course, 24-hour monitoring or an \"on call\" system for your staff might not be cost effective, but there are companies out there who can monitor your web facing services for you and alert you in the event of a problem. You might decide you can't afford this or don't need it and that's just fine... just take it into consideration.\nUse tools such as tripwire and nessus where appropriate - but don't just use them blindly because I said so. Take the time to learn how to use a few good security tools that are appropriate to your environment, keep these tools updated and use them on a regular basis.\nConsider hiring security experts to 'audit' your website security on a regular basis. Again, you might decide you can't afford this or don't need it and that's just fine... just take it into consideration.\n\nWhat steps can you take to reduce the consequences of a successful attack?\nIf you decide that the \"risk\" of the lower floor of your home flooding is high, but not high enough to warrant moving, you should at least move the irreplaceable family heirlooms upstairs. Right?\n\nCan you reduce the amount of services directly exposed to the Internet? Can you maintain some kind of gap between your internal services and your Internet-facing services? This ensures that even if your external systems are compromised the chances of using this as a springboard to attack your internal systems are limited.\nAre you storing information you don't need to store? Are you storing such information \"online\" when it could be archived somewhere else. There are two points to this part; the obvious one is that people cannot steal information from you that you don't have, and the second point is that the less you store, the less you need to maintain and code for, and so there are fewer chances for bugs to slip into your code or systems design.\nAre you using \"least access\" principles for your web app? If users only need to read from a database, then make sure the account the web app uses to service this only has read access, don't allow it write access and certainly not system-level access.\nIf you're not very experienced at something and it is not central to your business, consider outsourcing it. In other words, if you run a small website talking about writing desktop application code and decide to start selling small desktop applications from the site then consider \"outsourcing\" your credit card order system to someone like Paypal.\nIf at all possible, make practicing recovery from compromised systems part of your Disaster Recovery plan. This is arguably just another \"disaster scenario\" that you could encounter, simply one with its own set of problems and issues that are distinct from the usual 'server room caught fire'/'was invaded by giant server eating furbies' kind of thing.\n\n... And finally\nI've probably left out no end of stuff that others consider important, but the steps above should at least help you start sorting things out if you are unlucky enough to fall victim to hackers.\nAbove all: Don't panic. Think before you act. Act firmly once you've made a decision, and leave a comment below if you have something to add to my list of steps.",
        "url": "https://serverfault.com/questions/218005/how-do-i-deal-with-a-compromised-server"
    },
    {
        "title": "What exactly do the colors in htop status bars mean?",
        "question": "By default, htop shows colored status bars for processors, memory, and swap. From left to right, the bars are colored green, blue, yellow, and red depending on some thresholds.\nWhat does it mean when the Memory bar has a small level of green and blue, and almost all the remainder is yellow? The swap bar is empty. The color settings for htop are \"default\".\n",
        "top_answer": "Hitting F1 or h will show you the key. But for reference, the default colors are:\nCPU:\n\nBlue = Low priority threads\nGreen = Normal priority threads\nRed = Kernel threads\n\nMemory:\n\nGreen = Used memory\nBlue = Buffers\nYellow/Orange = Cache\n\nThere are a couple of different color-schemes available, you can see them through hitting F2.",
        "url": "https://serverfault.com/questions/180711/what-exactly-do-the-colors-in-htop-status-bars-mean"
    },
    {
        "title": "ssh returns \"Bad owner or permissions on ~/.ssh/config\"",
        "question": "When I try to ssh to another box, I get this strange error\n$ ssh hostname\nBad owner or permissions on ~/.ssh/config\n\nBut I made sure that I own and have rw permissions on the file:\nls -la ~/.ssh/\ntotal 40K\ndrwx------ 2 robert robert 4.0K Mar 29 11:04 ./\ndrwx------ 7 robert robert 4.0K Mar 29 11:04 ../\n-rw-r--r-- 1 robert robert 2.0K Mar 17 20:47 authorized_keys\n-rw-rw-r-- 1 robert robert   31 Mar 29 11:04 config\n-rw------- 1 robert robert 1.7K Aug  4  2010 id_rsa\n-rw-r--r-- 1 robert robert  406 Aug  4  2010 id_rsa.pub\n-rw-r--r-- 1 robert robert 6.1K Mar 29 11:03 known_hosts\n",
        "top_answer": "I needed to have rw for user only permissions on config.  This fixed it.\nchmod 600 ~/.ssh/config\n\nAs others have noted below, it could be the file owner.  (upvote them!)\nchown $USER ~/.ssh/config\n\nIf your whole folder has invalid permissions here's a table of possible permissions:\n\n\n\n\nPath\nPermission\n\n\n\n\n.ssh directory (code)\n0700 (drwx------)\n\n\nprivate keys (ex: id_rsa) (code)\n0600 (-rw-------)\n\n\nconfig\n0600 (-rw-------)\n\n\npublic keys (*.pub ex: id_rsa.pub)\n0644 (-rw-r--r--)\n\n\nauthorized_keys (code)\n0644 (-rw-r--r--)\n\n\nknown_hosts\n0644 (-rw-r--r--)\n\n\n\n\nSources:\n\nopenssh check-perm.c\nopenssh readconf.c\nopenssh ssh_user_config fix_authorized_keys_perms\n",
        "url": "https://serverfault.com/questions/253313/ssh-returns-bad-owner-or-permissions-on-ssh-config"
    },
    {
        "title": "How do I tell Git for Windows where to find my private RSA key?",
        "question": "My Git setup runs fine on Linux, but when I try to set things up under Windows (using Git for Windows and TortoiseGit), I don't know where to put my private SSH key (or, better still, how to tell ssh where it's located). I'm using the standard ssh.exe option during installation of Git for Windows. The setup runs fine if I allow password authentication (in lieu of RSA) on the server.  ",
        "top_answer": "For Git Bash\nIf you are running msysgit (I am assuming you are) and are looking to run Git Bash (I recommend it over TortoiseGit, but I lean to the CLI more than GUI now), you need to figure out what your home directory is for Git Bash by starting it then type pwd (On Windows\u00a07, it will be something like C:\\Users\\phsr I think). While you're in Git Bash, you should mkdir .ssh.\nAfter you have the home directory, and a .ssh folder under that, you want to open PuTTYgen and open the key (.ppk file) you have previously created. Once your key is open, you want to select Conversions -> Export OpenSSH key and save it to HOME\\.ssh\\id_rsa. After you have the key at that location, Git Bash will recognize the key and use it.\nNote: Comments indicate that this doesn't work in all cases. You may need to copy the OpenSSH key to Program Files\\Git\\.ssh\\id_rsa (or Program Files (x86)\\Git\\.ssh\\id_rsa).\nFor TortoiseGit\nWhen using TortoiseGit, you need to set the SSH key via pacey's directions.  You need to do that for every repository you are using TortoiseGit with.",
        "url": "https://serverfault.com/questions/194567/how-do-i-tell-git-for-windows-where-to-find-my-private-rsa-key"
    },
    {
        "title": "In Nginx, how can I rewrite all http requests to https while maintaining sub-domain?",
        "question": "I want to rewrite all http requests on my web server to be https requests, I started with the following:\n\nserver {\n    listen      80;\n\n    location / {\n      rewrite     ^(.*)   https://mysite.com$1 permanent;\n    }\n...\n\n\nOne Problem is that this strips away any subdomain information (e.g., node1.mysite.com/folder), how could I rewrite the above to reroute everything to https and maintain the sub-domain?",
        "top_answer": "Correct way in new versions of nginx\nTurn out my first answer to this question was correct at certain time, but it turned into another pitfall - to stay up to date please check Taxing rewrite pitfalls\n## Redirect all HTTP traffic to HTTPS\nserver {\n  listen          [::]:80 default_server;\n  listen               80 default_server;\n  server_name     _;\n  return          307 https://$host$request_uri;\n}\n\nserver {\n  listen         443 ssl http2 quic;\n  server_name    my.domain.com;\n\n  # add Strict-Transport-Security to prevent man in the middle attacks\n  add_header Strict-Transport-Security \"max-age=31536000\" always; \n\n  ...\n}\n",
        "url": "https://serverfault.com/questions/67316/in-nginx-how-can-i-rewrite-all-http-requests-to-https-while-maintaining-sub-dom"
    },
    {
        "title": "What is the difference between double and single square brackets in bash?",
        "question": "I just wondered what exactly the difference between\n[[ $STRING != foo ]]\n\nand\n[ $STRING != foo ]\n\nis, apart from that the latter is POSIX-compliant, found in sh and the former is an extension found in bash.",
        "top_answer": "There are several differences.  In my opinion, a few of the most important are:\n\n[ is a builtin in Bash and many other modern shells. The builtin [ is similar to test with the additional requirement of a closing ]. The builtins [ and test imitate the functionality /bin/[ and /bin/test along with their limitations so that scripts would be backwards compatible. The original executables still exist mostly for POSIX compliance and backwards compatibility. Running the command type [ in Bash indicates that [ is interpreted as a builtin by default. (Note: which [ only looks for executables on the PATH and is equivalent to type -P [. You can execute type --help for details)\n[[ is not as compatible, it won't necessarily work with whatever /bin/sh points to.  So [[ is the more modern Bash / Zsh / Ksh option.\nBecause [[ is built into the shell and does not have legacy requirements, you don't need to worry about word splitting based on the IFS variable to mess up on variables that evaluate to a string with spaces.  Therefore, you don't really need to put the variable in double quotes.\n\nFor the most part, the rest is just some nicer syntax.  To see more differences, I recommend this link to an FAQ answer: What is the difference between test, [ and [[ ?.  In fact, if you are serious about bash scripting, I recommend reading the entire wiki, including the FAQ, Pitfalls, and Guide.  The test section from the guide section explains these differences as well, and why the author(s) think [[ is a better choice if you don't need to worry about being as portable.  The main reasons are:\n\nYou don't have to worry about quoting the left-hand side of the test so that it actually gets read as a variable.\nYou don't have to escape less than and greater than < > with backslashes in order for them not to get evaluated as input redirection, which can really mess some stuff up by overwriting files.  This again goes back to [[ being a builtin. If [ (test) is an external program the shell would have to make an exception in the way it evaluates < and > only if /bin/test is being called, which wouldn't really make sense.\n",
        "url": "https://serverfault.com/questions/52034/what-is-the-difference-between-double-and-single-square-brackets-in-bash"
    },
    {
        "title": "mysqldump throws: Unknown table 'COLUMN_STATISTICS' in information_schema (1109)",
        "question": "Every time I try to make a mysqldump I get the following error:\n$> mysqldump --single-transaction --host host -u user -p db > db.sql\nmysqldump: Couldn't execute 'SELECT COLUMN_NAME, JSON_EXTRACT(HISTOGRAM,\n'$.\"number-of-buckets-specified\"') FROM\ninformation_schema.COLUMN_STATISTICS WHERE SCHEMA_NAME = 'db' AND\nTABLE_NAME = 'Absence';':\nUnknown table 'COLUMN_STATISTICS' in information_schema (1109)\n\nThe result is a dump which is not complete. The strange thing is that the same command, executed from another host, works without throwing any errors. Did someone experienced the same problem?\nI'm using mysql-client 8.0 and try to access a mysql 5-7 server - maybe that is the reason?",
        "top_answer": "This is due to a new flag that is enabled by default in mysqldump 8.\nYou can disable it by adding --column-statistics=0. The command will be something like:\nmysqldump --column-statistics=0 --host=<server> --user=<user> --password=<password> \n\nCheck this link for more information. To disable column statistics by default, you can add\n[mysqldump]\ncolumn-statistics=0\n\nto a MySQL config file, go to /etc/my.cnf, ~/.my.cnf, or directly to /etc/mysql/mysql.cnf.",
        "url": "https://serverfault.com/questions/912162/mysqldump-throws-unknown-table-column-statistics-in-information-schema-1109"
    },
    {
        "title": "What's the default superuser username/password for postgres after a new install?",
        "question": "I have just installed postgres 8.4 on Ubuntu 9.10 and it has never asked me to create a superuser. Is there a default superuser and its password? If not, how do I create a new one?",
        "top_answer": "CAUTION The answer about changing the UNIX password for \"postgres\" through \"$ sudo passwd postgres\" is not preferred, and can even be DANGEROUS!\nThis is why: By default, the UNIX account \"postgres\" is locked, which means it cannot be logged in using a password. If you use \"sudo passwd postgres\", the account is immediately unlocked. Worse, if you set the password to something weak, like \"postgres\", then you are exposed to a great security danger. For example, there are a number of bots out there trying the username/password combo \"postgres/postgres\" to log into your UNIX system.\nWhat you should do is follow Chris James's answer:\nsudo -u postgres psql postgres\n\n# \\password postgres\n\nEnter new password: \n\nTo explain it a little bit.  There are usually two default ways to login to PostgreSQL server: \n\nBy running the \"psql\" command as a UNIX user (so-called IDENT/PEER authentication), e.g.:  sudo -u postgres psql. Note that sudo -u does NOT unlock the UNIX user.\nby TCP/IP connection using PostgreSQL's own managed username/password (so-called TCP authentication) (i.e., NOT the UNIX password). \n\nSo you never want to set the password for UNIX account \"postgres\".  Leave it locked as it is by default.\nOf course things can change if you configure it differently from the default setting. For example, one could sync the PostgreSQL password with UNIX password and only allow local logins.  That would be beyond the scope of this question.",
        "url": "https://serverfault.com/questions/110154/whats-the-default-superuser-username-password-for-postgres-after-a-new-install"
    },
    {
        "title": "How does IPv4 Subnetting Work?",
        "question": "\nThis is a Canonical Question about IPv4 Subnets.\nRelated:\n\nHow does IPv6 subnetting work and how does it differ from IPv4 subnetting?\n\n\nHow does Subnetting Work, and How do you do it by hand or in your head?  Can someone explain both conceptually and with several examples? Server Fault gets lots of subnetting homework questions, so we could use an answer to point them to on Server Fault itself.\n\nIf I have a network, how do I figure\nout how to split it up?\nIf I am given a netmask, how do I\nknow what the network Range is for\nit?\nSometimes there is a slash followed\nby a number, what is that number?\nSometimes there is a subnet mask, but also a wildcard mask, they seem like the same thing but they are different?\nSomeone mentioned something about knowing binary for this?\n",
        "top_answer": "IP subnets exist to allow routers to choose appropriate destinations for packets. You can use IP subnets to break up larger networks for logical reasons (firewalling, etc), or physical need (smaller broadcast domains, etc).\nSimply put, though, IP routers use your IP subnets to make routing decisions. Understand how those decisions work, and you can understand how to plan IP subnets.\nCounting to 1\nIf you are already fluent in binary (base 2) notation you can skip this section.\nFor those of you who are left: Shame on you for not being fluent in binary notation!\nYes, that may be a bit harsh. It's really, really easy to learn to count in binary, and to learn shortcuts to convert binary to decimal and back. You really should know how to do it.\nCounting in binary is so simple because you only have to know how to count to 1!\nThink of a car's \"odometer\", except that unlike a traditional odometer each digit can only count up to 1 from 0. When the car is fresh from the factory the odometer reads \"00000000\".\nWhen you've driven your first mile the odometer reads \"00000001\". So far, so good.\nWhen you've driven your second mile the first digit of the odometer rolls back over to \"0\" (since its maximum value is \"1\") and the second digit of the odometer rolls over to \"1\", making the odometer read \"00000010\". This looks like the number 10 in decimal notation, but it's actually 2 (the number of miles you've driven the car so far) in binary notation.\nWhen you've driven the third mile the odometer reads \"00000011\", since the first digit of the odometer turns again. The number \"11\", in binary notation, is the same as the decimal number 3.\nFinally, when you've driven your fourth mile both digits (which were reading \"1\" at the end of the third mile) roll back over to zero position, and the 3rd digit rolls up to the \"1\" position, giving us \"00000100\". That's the binary representation of the decimal number 4.\nYou can memorize all of that if you want, but you really only need to understand how the little odometer \"rolls over\" as the number it's counting gets bigger. It's exactly the same as a traditional decimal odometer's operation, except that each digit can only be \"0\" or \"1\" on our fictional \"binary odometer\".\nTo convert a decimal number to binary you could roll the odometer forward, tick by tick, counting aloud until you've rolled it a number of times equal to the decimal number you want to convert to binary. Whatever is displayed on the odometer after all that counting and rolling would be the binary representation of the decimal number you counted up to.\nSince you understand how the odometer rolls forward you'll also understand how it rolls backward, too. To convert a binary number displayed on the odometer back to decimal you could roll the odometer back one tick at a time, counting aloud until the odometer reads \"00000000\". When all that counting and rolling is done, the last number you say aloud would be the decimal representation of the binary number the odometer started with.\nConverting values between binary and decimal this way would be very tedious. You could do it, but it wouldn't be very efficient. It's easier to learn a little algorithm to do it faster.\nA quick aside: Each digit in a binary number is known as a \"bit\". That's \"b\" from \"binary\" and \"it\" from \"digit\". A bit is a bnary digit.\nConverting a binary number like, say, \"1101011\" to decimal is a simple process with a handy little algorithm.\nStart by counting the number of bits in the binary number. In this case, there are 7. Make 7 divisions on a sheet of paper (in your mind, in a text file, etc) and begin filling them in from right to left. In the rightmost slot, enter the number \"1\", because we'll always start with \"1\". In the next slot to the left enter double the value in the slot to the right (so, \"2\" in the next one, \"4\" in the next one) and continue until all the slots are full. (You'll end up memorizing these numbers, which are the powers of 2, as you do this more and more. I'm alright up to 131,072 in my head but I usually need a calculator or paper after that).\nSo, you should have the following on your paper in your little slots.\n 64    |    32    |    16    |    8    |    4    |    2    |    1    |\n\nTranscribe the bits from the binary number below the slots, like so:\n 64    |    32    |    16    |    8    |    4    |    2    |    1    |\n  1          1          0         1         0         1         1\n\nNow, add some symbols and compute the answer to the problem:\n 64    |    32    |    16    |    8    |    4    |    2    |    1    |\nx 1        x 1        x 0       x 1       x 0       x 1       x 1\n---        ---        ---       ---       ---       ---       ---\n       +          +          +         +         +         +         =\n\nDoing all the math, you should come up with:\n 64    |    32    |    16    |    8    |    4    |    2    |    1    |\nx 1        x 1        x 0       x 1       x 0       x 1       x 1\n---        ---        ---       ---       ---       ---       ---\n 64    +    32    +     0    +    8    +    0    +    2    +    1    =   107\n\nThat's got it. \"1101011\" in decimal is 107. It's just simple steps and easy math.\nConverting decimal to binary is just as easy and is the same basic algorithm, run in reverse.\nSay that we want to convert the number 218 to binary. Starting on the right of a sheet of paper, write the number \"1\". To the left, double that value (so, \"2\") and continue moving toward the left of the paper doubling the last value. If the number you are about to write is greater than the number being converted stop writing. otherwise, continue doubling the prior number and writing. (Converting a big number, like 34,157,216,092, to binary using this algorithm can be a bit tedious but it's certainly possible.)\nSo, you should have on your paper:\n 128    |    64    |    32    |    16    |    8    |    4    |    2    |    1    |\n\nYou stopped writing numbers at 128 because doubling 128, which would give you 256, would be large than the number being converted (218).\nBeginning from the leftmost number, write \"218\" above it (128) and ask yourself: \"Is 218 larger than or equal to 128?\" If the answer is yes, scratch a \"1\" below \"128\". Above \"64\", write the result of 218 minus 128 (90).\nLooking at \"64\", ask yourself: \"Is 90 larger than or equal to 64?\" It is, so you'd write a \"1\" below \"64\", then subtract 64 from 90 and write that above \"32\" (26).\nWhen you get to \"32\", though, you find that 32 is not greater than or equal to 26. In this case, write a \"0\" below \"32\", copy the number (26) from above 32\" to above \"16\" and then continue asking yourself the same question with the rest of the numbers.\nWhen you're all done, you should have:\n 218         90         26         26        10         2         2         0\n 128    |    64    |    32    |    16    |    8    |    4    |    2    |    1    |\n   1          1          0          1         1         0         1         0\n\nThe numbers at the top are just notes used in computation and don't mean much to us. At the bottom, though, you see a binary number \"11011010\". Sure enough, 218, converted to binary, is \"11011010\".\nFollowing these very simple procedures you can convert binary to decimal and back again w/o a calculator. The math is all very simple and the rules can be memorized with just a bit of practice.\nSplitting Up Addresses\nThink of IP routing like pizza delivery.\nWhen you're asked to deliver a pizza to \"123 Main Street\" it's very clear to you, as a human, that you want to go to the building numbered \"123\" on the street named \"Main Street\". It's easy to know that you need to go to the 100-block of Main Street because the building number is between 100 and 199 and most city blocks are numbered in hundreds. You \"just know\" how to split the address up.\nRouters deliver packets, not pizza. Their job is the same as a pizza driver: To get the cargo (packets) as close to the destination as possible. A router is connected to two or more IP subnets (to be at all useful). A router must examine destination IP addresses of packets and break those destination addresses up into their \"street name\" and \"building number\" components, just like the pizza driver, to make decisions about delivery.\nEach computer (or \"host\") on an IP network is configured with a unique IP address and subnet mask. That IP address can be divided up into a \"building number\" component (like \"123\" in the example above) called the \"host ID\" and a \"street name\" component (like \"Main Street\" in the example above) called the \"network ID\". For our human eyes, it's easy to see where the building number and the street name are in \"123 Main Street\", but harder to see that division in \"10.13.216.41 with a  subnet mask of 255.255.192.0\".\nIP routers \"just know\" how to split up IP addresses into these component parts to make routing decisions. Since understanding how IP packets are routed hinges on understanding this process, we need to know how to break up IP addresses, too. Fortunately, extracting the host ID and the network ID out of an IP address and subnet mask is actually pretty easy.\nStart by writing out the IP address in binary (use a calculator if you haven't learned to do this in your head just yet, but make a note learn how to do it-- it's really, really easy and impresses the opposite sex at parties):\n      10.      13.     216.      41\n00001010.00001101.11011000.00101001\n\nWrite out the subnet mask in binary, too:\n     255.     255.     192.       0\n11111111.11111111.11000000.00000000\n\nWritten side-by-side, you can see that the point in the subnet mask where the \"1s\" stop \"lines up\" to a point in the IP address. That's the point that the network ID and the host ID split. So, in this case:\n      10.      13.     216.      41\n00001010.00001101.11011000.00101001 - IP address\n11111111.11111111.11000000.00000000 - subnet mask\n00001010.00001101.11000000.00000000 - Portion of IP address covered by 1s in subnet mask, remaining bits set to 0\n00000000.00000000.00011000.00101001 - Portion of IP address covered by 0s in subnet mask, remaining bits set to 0\n\nRouters use the subnet mask to \"mask out\" the bits covered by 1s in the IP address (replacing the bits that are not \"masked out\" with 0s) to extract the network ID:\n      10.      13.     192.       0\n00001010.00001101.11000000.00000000 - Network ID\n\nLikewise, by using the subnet mask to \"mask out\" the bits covered by 0s in the IP address (replacing the bits that are not \"masked out\" with 0s again) a router can extract the host ID:\n       0.       0.      24.      41\n00000000.00000000.00011000.00101001 - Portion of IP address covered by 0s in subnet mask, remaining bits set to 0\n\nIt's not as easy for our human eyes to see the \"break\" between the network ID and the host ID as it is between the \"building number\" and the \"street name\" in physical addresses during pizza delivery, but the ultimate effect is the same.\nNow that you can split up IP addresses and subnet masks into host ID's and network ID's you can route IP just like a router does.\nMore Terminology\nYou're going to see subnet masks written all over the Internet and throughout the rest of this answer as (IP/number). This notation is known as \"Classless Inter-Domain Routing\" (CIDR) notation. \"255.255.255.0\" is made up of 24 bits of 1s at the beginning, and it's faster to write that as \"/24\" than as \"255.255.255.0\". To convert a CIDR number (like \"/16\") to a dotted-decimal subnet mask just write out that number of 1s, split it into groups of 8 bits, and convert it to decimal. (A \"/16\" is \"255.255.0.0\", for instance.)\nBack in the \"old days\", subnet masks weren't specified, but rather were derived by looking at certain bits of the IP address. An IP address starting with 0 - 127, for example, had an implied subnet mask of 255.0.0.0 (called a \"class A\" IP address).\nThese implied subnet masks aren't used today and I don't recommend learning about them anymore unless you have the misfortune of dealing with very old equipment or old protocols (like RIPv1) that don't support classless IP addressing. I'm not going to mention these \"classes\" of addresses further because it's inapplicable today and can be confusing.\nSome devices use a notation called \"wildcard masks\". A \"wildcard mask\" is nothing more than a subnet mask with all 0s where there would be 1s, and 1s where there would be 0s. The \"wildcard mask\" of a /26 is:\n 11111111.11111111.11111111.11000000 - /26 subnet mask\n 00000000.00000000.00000000.00111111 - /26 \"wildcard mask\"\n\nTypically you see \"wildcard masks\" used to match host IDs in access-control lists or firewall rules. We won't discuss them any further here.\nHow a Router Works\nAs I've said before, IP routers have a similar job to a pizza delivery driver in that they need to get their cargo (packets) to its destination. When presented with a packet bound for address 192.168.10.2, an IP router needs to determine which of its network interfaces will best get that packet closer to its destination.\nLet's say that you are an IP router, and you have interfaces connected to you numbered:\n\nEthernet0 - 192.168.20.1, subnet mask /24\nEthernet1 - 192.168.10.1, subnet mask /24\n\nIf you receive a packet to deliver with a destination address of \"192.168.10.2\", it's pretty easy to tell (with your human eyes) that the packet should be sent out the interface Ethernet1, because the Ethernet1 interface address corresponds to the packet's destination address. All the computers attached to the Ethernet1 interface will have IP addresses starting with \"192.168.10.\", because the network ID of the IP address assigned to your interface Ethernet1 is \"192.168.10.0\".\nFor a router, this route selection process is done by building a routing table and consulting the table each time a packet is to be delivered. A routing table contains network ID and destination interface names. You already know how to obtain a network ID from an IP address and subnet mask, so you're on your way to building a routing table. Here's our routing table for this router:\n\nNetwork ID: 192.168.20.0 (11000000.10101000.00010100.00000000) - 24 bit subnet mask - Interface Ethernet0\nNetwork ID: 192.168.10.0 (11000000.10101000.00001010.00000000) - 24 bit subnet mask - Interface Ethernet1\n\nFor our incoming packet bound for \"192.168.10.2\", we need only convert that packet's address to binary (as humans -- the router gets it as binary off the wire to begin with) and attempt to match it to each address in our routing table (up to the number of bits in the subnet mask) until we match an entry.\n\nIncoming packet destination: 11000000.10101000.00001010.00000010\n\nComparing that to the entries in our routing table:\n11000000.10101000.00001010.00000010 - Destination address for packet\n11000000.10101000.00010100.00000000 - Interface Ethernet0\n!!!!!!!!.!!!!!!!!.!!!????!.xxxxxxxx - ! indicates matched digits, ? indicates no match, x indicates not checked (beyond subnet mask)\n\n11000000.10101000.00001010.00000010 - Destination address for packet\n11000000.10101000.00001010.00000000 - Interface Ethernet1, 24 bit subnet mask\n!!!!!!!!.!!!!!!!!.!!!!!!!!.xxxxxxxx - ! indicates matched digits, ? indicates no match, x indicates not checked (beyond subnet mask)\n\nThe entry for Ethernet0 matches the first 19 bits fine, but then stops matching. That means it's not the proper destination interface. You can see that the interface Ethernet1 matches 24 bits of the destination address. Ah, ha! The packet is bound for interface Ethernet1.\nIn a real-life router, the routing table is sorted in such a manner that the longest subnet masks are checked for matches first (i.e. the most specific routes), and numerically so that as soon as a match is found the packet can be routed and no further matching attempts are necessary (meaning that 192.168.10.0 would be listed first and 192.168.20.0 would never have been checked). Here, we're simplifying that a bit. Fancy data structures and algorithms make faster IP routers, but simple algorithms will produce the same results.\nStatic Routes\nUp to this point, we've talked about our hypothetical router as having networks directly connected to it. That's not, obviously, how the world really works. In the pizza-driving analogy, sometimes the driver isn't allowed any further into the building than the front desk, and has to hand-off the pizza to somebody else for delivery to the final recipient (suspend your disbelief and bear with me while I stretch my analogy, please).\nLet's start by calling our router from the earlier examples \"Router A\". You already know RouterA's routing table as:\n\nNetwork ID: 192.168.20.0 (11000000.10101000.00010100.00000000) - subnet mask /24 - Interface RouterA-Ethernet0\nNetwork ID: 192.168.10.0 (11000000.10101000.00001010.00000000) - subnet mask /24 - Interface RouterA-Ethernet1\n\nSuppose that there's another router, \"Router B\", with the IP addresses 192.168.10.254/24 and 192.168.30.1/24 assigned to its Ethernet0 and Ethernet1 interfaces.  It has the following routing table:\n\nNetwork ID: 192.168.10.0 (11000000.10101000.00001010.00000000) - subnet mask /24 - Interface RouterB-Ethernet0\nNetwork ID: 192.168.30.0 (11000000.10101000.00011110.00000000) - subnet mask /24 - Interface RouterB-Ethernet1\n\nIn pretty ASCII art, the network looks like this:\n               Interface                      Interface\n               Ethernet1                      Ethernet1\n               192.168.10.1/24                192.168.30.254/24\n     __________  V                  __________  V\n    |          | V                 |          | V\n----| ROUTER A |------- /// -------| ROUTER B |----\n  ^ |__________|                 ^ |__________|\n  ^                              ^\nInterface                      Interface\nEthernet0                      Ethernet0\n192.168.20.1/24                192.168.10.254/24\n\nYou can see that Router B knows how to \"get to\" a network, 192.168.30.0/24, that Router A knows nothing about.\nSuppose that a PC with the IP address 192.168.20.13 attached to the network connected to router A's Ethernet0 interface sends a packet to Router A for delivery. Our hypothetical packet is destined for the IP address 192.168.30.46, which is a device attached to the network connected to the Ethernet1 interface of Router B.\nWith the routing table shown above, neither entry in Router A's routing table matches the destination 192.168.30.46, so Router A will return the packet to the sending PC with the message \"Destination network unreachable\".\nTo make Router A \"aware\" of the existence of the 192.168.30.0/24 network, we add the following entry to the routing table on Router A:\n\nNetwork ID: 192.168.30.0 (11000000.10101000.00011110.00000000) - subnet mask /24 - Accessible via 192.168.10.254\n\nIn this way, Router A has a routing table entry that matches the 192.168.30.46 destination of our example packet. This routing table entry effectively says \"If you get a packet bound for 192.168.30.0/24, send it on to 192.168.10.254 because he knows how to deal with it.\" This is the analogous \"hand-off the pizza at the front desk\" action that I mentioned earlier-- passing the packet on to somebody else who knows how to get it closer to its destination.\nAdding an entry to a routing table \"by hand\" is known as adding a \"static route\".\nIf Router B wants to deliver packets to the 192.168.20.0 subnet mask 255.255.255.0 network, it will need an entry in its routing table, too:\n\nNetwork ID: 192.168.20.0 (11000000.10101000.00010100.00000000) - subnet mask /24 - Accessible via: 192.168.10.1 (Router A's IP address in the 192.168.10.0 network)\n\nThis would create a path for delivery between the 192.168.30.0/24 network and the 192.168.20.0/24 network across the 192.168.10.0/24 network between these routers.\nYou always want to be sure that routers on both sides of such an \"interstitial network\" have a routing table entry for the \"far end\" network. If router B in our example didn't have a routing table entry for \"far end\" network 192.168.20.0/24 attached to router A our hypothetical packet from the PC at 192.168.20.13 would get to the destination device at 192.168.30.46, but any reply that 192.168.30.46 tried to send back would be returned by router B as \"Destination network unreachable.\" One-way communication is generally not desirable. Always be sure you think about traffic flowing in both directions when you think about communication in computer networks.\nYou can get a lot of mileage out of static routes. Dynamic routing protocols like EIGRP, RIP, etc, are really nothing more than a way for routers to exchange routing information between each other that could, in fact, be configured with static routes. One large advantage to using dynamic routing protocols over static routes, though, is that dynamic routing protocols can dynamically change the routing table based on network conditions (bandwidth utilization, an interface \"going down\", etc) and, as such, using a dynamic routing protocol can result in a configuration that \"routes around\" failures or bottlenecks in the network infrastructure. (Dynamic routing protocols are WAY outside the scope of this answer, though.)\nYou Can't Get There From Here\nIn the case of our example Router A, what happens when a packet bound for \"172.16.31.92\" comes in?\nLooking at the Router A routing table, neither destination interface or static route matches the first 24 bits of 172.18.31.92 (which is 10101100.00010010.00011111.01011100, by the way).\nAs we already know, Router A would return the packet to the sender via a \"Destination network unreachable\" message.\nSay that there's another router (Router C) sitting at the address \"192.168.20.254\". Router C has a connection to the Internet!\n                              Interface                      Interface                      Interface\n                              Ethernet1                      Ethernet1                      Ethernet1\n                              192.168.20.254/24              192.168.10.1/24                192.168.30.254/24\n                    __________  V                  __________  V                  __________  V\n((  heap o  ))     |          | V                 |          | V                 |          | V\n(( internet )) ----| ROUTER C |------- /// -------| ROUTER A |------- /// -------| ROUTER B |----\n((   w00t!  ))   ^ |__________|                 ^ |__________|                 ^ |__________|\n                 ^                              ^                              ^\n               Interface                      Interface                      Interface\n               Ethernet0                      Ethernet0                      Ethernet0\n               10.35.1.1/30                   192.168.20.1/24                192.168.10.254/24\n\nIt would be nice if Router A could route packets that do not match any local interface up to Router C such that Router C can send them on to the Internet. Enter the \"default gateway\" route.\nAdd an entry at the end of our routing table like this:\n\nNetwork ID: 0.0.0.0 (00000000.00000000.00000000.00000000) - subnet mask /0 - Destination router: 192.168.20.254\n\nWhen we attempt to match \"172.16.31.92\" to each entry in the routing table we end up hitting this new entry. It's a bit perplexing, at first. We're looking to match zero bits of the destination address with... wait... what? Matching zero bits? So, we're not looking for a match at all. This routing table entry is saying, basically, \"If you get here, rather than giving up on delivery, send the packet on to the router at 192.168.20.254 and let him handle it\".\n192.168.20.254 is a destination we DO know how to deliver a packet to. When confronted with a packet bound for a destination for which we have no specific routing table entry this \"default gateway\" entry will always match (since it matches zero bits of the destination address) and gives us a \"last resort\" place that we can send packets for delivery. You'll sometimes hear the default gateway called the \"gateway of last resort.\"\nIn order for a default gateway route to be effective it must refer to a router that is reachable using the other entries in the routing table. If you tried to specify a default gateway of 192.168.50.254 in Router A, for example, delivery to such a default gateway would fail. 192.168.50.254 isn't an address that Router A knows how to deliver packets to using any of the other routes in its routing table, so such an address would be ineffective as a default gateway. This can be stated concisely: The default gateway must be set to an address already reachable by using another route in the routing table.\nReal routers typically store the default gateway as the last route in their routing table such that it matches packets after they've failed to match all other entries in the table.\nUrban Planning and IP Routing\nBreaking up a IP subnet into smaller IP subnets is like urban planning. In urban planning, zoning is used to adapt to natural features of the landscape (rivers, lakes, etc), to influence traffic flows between different parts of the city, and to segregate different types of land-use (industrial, residential, etc). IP subnetting is really much the same.\nThere are three main reasons why you would subnet a network:\n\nYou may want to communicate across different unlike communication media. If you have a T1 WAN connection between two buildings IP routers could be placed on the ends of these connections to facilitate communication across the T1. The networks on each end (and possibly the \"interstitial\" network on the T1 itself) would be assigned to unique IP subnets so that the routers can make decisions about which traffic should be sent across the T1 line.\n\nIn an Ethernet network, you might use subnetting to limit the amount of broadcast traffic in a given portion of the network. Application-layer protocols use the broadcast capability of Ethernet for very useful purposes. As you get more and more hosts packed into the same Ethernet network, though, the percentage of broadcast traffic on the wire (or air, in wireless Ethernet) can increase to such a point as to create problems for delivery of non-broadcast traffic. (In the olden days, broadcast traffic could overwhelm the CPU of hosts by forcing them to examine each broadcast packet. That's less likely today.) Excessive traffic on switched Ethernet can also come in form of \"flooding of frames to unknown destinations\". This condition is caused by an Ethernet switch being unable to keep track of every destination on the network and is the reason why switched Ethernet networks can't scale to an infinite number of hosts. The effect of flooding of frames to unknown destinations is similar to the the effect of excess broadcast traffic, for the purposes of subnetting.\n\nYou may want to \"police\" the types of traffic flowing between different groups of hosts. Perhaps you have print server devices and you only want authorized print queuing server computers to send jobs to them. By limiting the traffic allowed to flow to the print server device subnet users can't configure their PCs to talk directly to the print server devices to bypass print accounting. You might put the print server devices into a subnet all to themselves and create a rule in the router or firewall attached to that subnet to control the list of hosts permitted to send traffic to the print server devices. (Both routers and firewalls can typically make decisions about how or whether to deliver a packet based on the source and destination addresses of the packet. Firewalls are typically a sub-species of router with an obsessive personality. They can be very, very concerned about the payload of packets, whereas routers typically disregard payloads and just deliver the packets.)\n\n\nIn planning a city, you can plan how streets intersect with each other, and can use turn-only, one-way, and dead-end streets to influence traffic flows. You might want Main Street to be 30 blocks long, with each block having up to 99 buildings each. It's pretty easy to plan your street numbering such that each block in Main Street has a range of street numbers increasing by 100 for each block. It's very easy to know what the \"starting number\" in each subsequent block should be.\nIn planning IP subnets, you're concerned with building the right number of subnets (streets) with the right number of available host ID's (building numbers), and using routers to connect the subnets to each other (intersections). Rules about allowed source and destination addresses specified in the routers can further control the flow of traffic. Firewalls can act like obsessive traffic cops.\nFor the purposes of this answer, building our subnets is our only major concern. Instead of working in decimal, as you would with urban planning, you work in binary to describe the bounds of each subnet.\nContinued on: How does IPv4 Subnetting Work?\n(Yes ... we reached the maximum size of an answer (30000 characters).)",
        "url": "https://serverfault.com/questions/49765/how-does-ipv4-subnetting-work"
    },
    {
        "title": "Something is burning in the server room; how can I quickly identify what it is?",
        "question": "The other day, we notice a terrible burning smell coming out of the server room. Long story short, it ended up being one of the battery modules that was burning up in the UPS unit, but it took a good couple of hours before we were able to figure it out. The main reason we were able to figure it out is that the UPS display finally showed that the module needed to be replaced.\nHere was the problem: the whole room was filled with the smell. Doing a sniff test was very difficult because the smell had infiltrated everything (not to mention it made us light headed). We almost mistakenly took our production database server down because it's where the smell was the strongest. The vitals appeared to be ok (CPU temps showed 60 degrees C, and fan speeds ok), but we weren't sure. It just so happened that the battery module that burnt up was about the same height as the server on the rack and only 3 ft away. Had this been a real emergency, we would have failed miserably.\nRealistically, the chances that actual server hardware is burning up is a fairly rare occurrence and most of the time we'll be looking at the UPS the culprit. But with several racks with several pieces of equipment, it can quickly become a guessing game. How does one quickly and accurately determine what piece of equipment is actually burning up? I realize this question is highly dependent on the environment variables such as room size, ventilation, location, etc, but any input would be appreciated.",
        "top_answer": "The general consensus seems to be that the answer to your question comes in two parts:\nHow do we find the source of the funny burning smell?\nYou've got the \"How\" pretty well nailed down:  \n\nThe \"Sniff Test\"\nLook for visible smoke/haze\nWalk the room with a thermal (IR) camera to find hot spots\nCheck monitoring and device panels for alerts\n\nYou can improve your chances of finding the problem quickly in a number of ways - improved monitoring is often the easiest. Some questions to ask:\n\nDo you get temperature and other health alerts from your equipment?\nAre your UPS systems reporting faults to your monitoring system?\nDo you get current-draw alarms from your power distribution equipment?\nAre the room smoke detectors reporting to the monitoring system? (and can they?)\n\n\nWhen should we troubleshoot versus hitting the Big Red Switch?\nThis is a more interesting question.\nHitting the big red switch can cost your company a huge amount of money in a hurry: Clean agent releases can be into the tens of thousands of dollars, and the outage / recovery costs after an emergency power off (EPO, \"dropping the room\") can be devastating.\nYou do not want to drop a datacenter because a capacitor in a power supply popped and made the room smell.\nConversely, a fire in a server room can cost your company its data/equipment, and more importantly your staff's lives.\nTroubleshooting \"that funny burning smell\" should never take precedence over safety, so it's important to have some clear rules about troubleshooting \"pre-fire\" conditions.\nThe guidelines that follow are my personal limitations that I apply in absence of (or in addition to) any other clearly defined procedure/rules - they've served me well and they may help you, but they could just as easily get me killed or fired tomorrow, so apply them at your own risk.\n\nIf you see smoke or fire, drop the room\nThis should go without saying but let's say it anyway: If there is an active fire (or smoke indicating that there soon will be) you evacuate the room, cut the power, and discharge the fire suppression system.\nExceptions may exist (exercise some common sense), but this is almost always the correct action.\nIf you're proceeding to troubleshoot, always have at least one other person involved\nThis is for two reasons.  First, you do not want to be wandering around in a datacenter and all of a sudden have a rack go up in the row you're walking down and nobody knows you're there.  Second, the other person is your sanity check on troubleshooting versus dropping the room, and should you make the call to hit the Big Red Switch you have the benefit of having a second person concur with the decision (helps to avoid the career-limiting aspects of such a decision if someone questions it later).\nExercise prudent safety measures while troubleshooting\nMake sure you always have an escape path (an open end of a row and a clear path to an exit).\nKeep someone stationed at the EPO / fire suppression release.\nCarry a fire extinguisher with you (Halon or other clean-agent, please).\nRemember rule #1 above.\nWhen in doubt, leave the room.\nTake care about your breathing: use a respirator or an oxygen mask. This might save your health in case of chemical fire.\nSet a limit and stick to it\nMore accurately, set two limits:\n\nCondition (\"How much worse will I let this get?\"), and\nTime (\"How long will I keep trying to find the problem before its too risky?\").\n\nThe limits you set can also be used to let your team begin an orderly shutdown of the affected area, so when you DO pull power you're not crashing a bunch of active machines, and your recovery time will be much shorter, but remember that if the orderly shutdown is taking too long you may have to let a few systems crash in the name of safety.\nTrust your gut\nIf you are concerned about safety at any time, call the troubleshooting off and clear the room.\nYou may or may not drop the room based on a gut feeling, but regrouping outside the room in (relative) safety is prudent.\n\nIf there isn't imminent danger you may elect bring in the local fire department before taking any drastic actions like an EPO or clean-agent release.  (They may tell you to do so anyway: Their mandate is to protect people, then property, but they're obviously the experts in dealing with fires so you should do what they say!)\n\nWe've addressed this in comments, but it may as well get summarized in an answer too -- @DeerHunter, @Chris, @Sirex, and many others contributed to the discussion\n",
        "url": "https://serverfault.com/questions/496139/something-is-burning-in-the-server-room-how-can-i-quickly-identify-what-it-is"
    },
    {
        "title": "Displaying a remote SSL certificate details using CLI tools",
        "question": "In Chrome, clicking on the green HTTPS lock icon opens a window with the certificate details:\n\nWhen I tried the same with cURL, I got only some of the information:\n$ curl -vvI https://gnupg.org\n* Rebuilt URL to: https://gnupg.org/\n* Hostname was NOT found in DNS cache\n*   Trying 217.69.76.60...\n* Connected to gnupg.org (217.69.76.60) port 443 (#0)\n* TLS 1.2 connection using TLS_DHE_RSA_WITH_AES_128_CBC_SHA\n* Server certificate: gnupg.org\n* Server certificate: Gandi Standard SSL CA\n* Server certificate: UTN-USERFirst-Hardware\n> HEAD / HTTP/1.1\n> User-Agent: curl/7.37.1\n> Host: gnupg.org\n> Accept: */*\n\nAny idea how to get the full certificate information form a command line tool (cURL or other)?",
        "top_answer": "You should be able to use OpenSSL for your purpose:\necho | openssl s_client -showcerts -servername gnupg.org -connect gnupg.org:443 2>/dev/null | openssl x509 -inform pem -noout -text\n\nThat command connects to the desired website and pipes the certificate in PEM format on to another openssl command that reads and parses the details.\n(Note that \"redundant\" -servername parameter is necessary to make openssl do a request with SNI support.)",
        "url": "https://serverfault.com/questions/661978/displaying-a-remote-ssl-certificate-details-using-cli-tools"
    },
    {
        "title": "How do I change my private key passphrase?",
        "question": "I have an existing public/private key pair. The private key is password protected, and the encryption may be either RSA or DSA. These keys are the kind you generate with ssh-keygen and generally store under ~/.ssh.\nI'd like to change the private key's password. How do I go about it, on a standard Unix shell?\nAlso, how do I simply remove the password? Just change it to empty?",
        "top_answer": "To change the passphrase on your default key:\n$ ssh-keygen -p\n\nIf you need to specify a key, pass the -f option:\n$ ssh-keygen -p -f ~/.ssh/id_rsa\n\nthen provide your old and new passphrase (twice) at the prompts. (Use ~/.ssh/id_rsa if you have an RSA key.)\nMore details from man ssh-keygen:\n[...]\nSYNOPSIS\n   ssh-keygen  [-q]  [-a  rounds]  [-b  bits] [-C comment] [-f output_keyfile] [-m format] [-N new_passphrase] [-O option]\n              [-t ecdsa | ecdsa-sk | ed25519 | ed25519-sk | rsa] [-w provider] [-Z cipher]\n   ssh-keygen -p [-a rounds] [-f keyfile] [-m format] [-N new_passphrase] [-P old_passphrase] [-Z cipher]\n[...]\n     -f filename\n             Specifies the filename of the key file.\n[...]\n     -N new_passphrase\n             Provides the new passphrase.\n\n     -P passphrase\n             Provides the (old) passphrase.\n\n     -p      Requests changing the passphrase of a private key file instead of\n             creating a new private key.  The program will prompt for the file\n             containing the private key, for the old passphrase, and twice for\n             the new passphrase.\n[...]\n",
        "url": "https://serverfault.com/questions/50775/how-do-i-change-my-private-key-passphrase"
    },
    {
        "title": "How to set environment variable in systemd service?",
        "question": "I have an Arch Linux system with systemd and I've created my own service. The configuration service at /etc/systemd/system/myservice.service looks like this:\n[Unit]\nDescription=My Daemon\n\n[Service]\nExecStart=/bin/myforegroundcmd\n\n[Install]\nWantedBy=multi-user.target\n\nNow I want to have an environment variable set for the /bin/myforegroundcmd. How do I do that?",
        "top_answer": "Times change and so do best practices.\nThe current best way to do this is to run systemctl edit myservice, which will create an override file for you or let you edit an existing one. \nIn normal installations this will create a directory /etc/systemd/system/myservice.service.d, and inside that directory create a file whose name ends in .conf (typically, override.conf), and in this file you can add to or override any part of the unit shipped by the distribution.\nFor instance, in a file /etc/systemd/system/myservice.service.d/myenv.conf:\n[Service]\nEnvironment=\"SECRET=pGNqduRFkB4K9C2vijOmUDa2kPtUhArN\"\nEnvironment=\"ANOTHER_SECRET=JP8YLOc2bsNlrGuD6LVTq7L36obpjzxd\"\n\nAlso note that if the directory exists and is empty, your service will be disabled! If you don't intend to put something in the directory, ensure that it does not exist.\n\nFor reference, the old way was:\nThe recommended way to do this is to create a file /etc/sysconfig/myservice which contains your variables, and then load them with EnvironmentFile.\nFor complete details, see Fedora's documentation on how to write a systemd script.",
        "url": "https://serverfault.com/questions/413397/how-to-set-environment-variable-in-systemd-service"
    },
    {
        "title": "How to run a server on port 80 as a normal user on Linux?",
        "question": "I have Googled about a solution for quite some time, but couldn't find an answer. \nI am on Ubuntu Linux and want to run a server on port 80, but due to security mechanism of Ubuntu, I get the following error: \n\njava.net.BindException: Permission denied:80\n\nI think it should be simple enough to either disable this security mechanism so that port 80 is available to all users or to assign required privileges to the current user to access port 80.",
        "top_answer": "Short answer: you can't. Ports below 1024 can be opened only by root. As per comment - well, you can, using CAP_NET_BIND_SERVICE, but that approach, applied to java bin will make any java program to be run with this setting, which is undesirable, if not a security risk.\nThe long answer: you can redirect connections on port 80 to some other port you can open as normal user.\nRun as root:\n# iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080\n\nAs loopback devices (like localhost) do not use the prerouting rules, if you need to use localhost, etc., add this rule as well (thanks @Francesco):\n# iptables -t nat -I OUTPUT -p tcp -d 127.0.0.1 --dport 80 -j REDIRECT --to-ports 8080\n\nNOTE: The above solution is not well suited for multi-user systems, as any user can open port 8080 (or any other high port you decide to use), thus intercepting the traffic. (Credits to CesarB).\nEDIT: as per comment question - to delete the above rule:\n# iptables -t nat --line-numbers -n -L\n\nThis will output something like:\nChain PREROUTING (policy ACCEPT)\nnum  target     prot opt source               destination         \n1    REDIRECT   tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:8080 redir ports 8088\n2    REDIRECT   tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:80 redir ports 8080\n\nThe rule you are interested in is nr. 2, so to delete it:\n# iptables -t nat -D PREROUTING 2\n",
        "url": "https://serverfault.com/questions/112795/how-to-run-a-server-on-port-80-as-a-normal-user-on-linux"
    },
    {
        "title": "Why is my crontab not working, and how can I troubleshoot it?",
        "question": "\nThis is a Canonical Question about using cron & crontab.\n\nYou have been directed here because the community is fairly sure that the answer to your question can be found below. If your question is not answered below then the answers will help you gather information that will help the community help you. This information should be edited into your original question.\nThe answer for 'Why is my crontab not working, and how can I troubleshoot it?' can be seen below.  This addresses the cron system with the crontab highlighted.",
        "top_answer": "How to fix all of your crontab related woes/problems (Linux)\n\n\nThis is a community wiki, if you notice anything incorrect with this answer or have additional information then please edit it.\n\n\nFirst, basic terminology:\n\ncron(8) is the daemon that executes scheduled commands.\ncrontab(1) is the program used to modify user crontab(5) files.\ncrontab(5) is a per user file that contains instructions for cron(8).\n\nNext, education about cron:\nEvery user on a system may have their own crontab file. The location of the root and user crontab files are system dependant but they are generally below /var/spool/cron.\nThere is a system-wide /etc/crontab file, the /etc/cron.d directory may contain crontab fragments which are also read and actioned by cron.  Some Linux distributions (eg, Red Hat) also have /etc/cron.{hourly,daily,weekly,monthly} which are directories, scripts inside which will be executed every hour/day/week/month, with root privilege.\nroot can always use the crontab command; regular users may or may not be granted access.  When you edit the crontab file with the command crontab -e and save it, crond checks it for basic validity but does not guarantee your crontab file is correctly formed.  There is a file called cron.deny which will specify which users cannot use cron.  The cron.deny file location is system dependent and can be deleted which will allow all users to use cron.\nIf the computer is not powered on or crond daemon is not running, and the date/time for a command to run has passed, crond will not catchup and run past queries.\ncrontab particulars, how to formulate a command:\nA crontab command is represented by a single line. You cannot use \\ to extend a command over multiple lines. The hash (#) sign represents a comment which means anything on that line is ignored by cron. Leading whitespace and blank lines are ignored.\nBe VERY careful when using the percent (%) sign in your command. Unless they are escaped \\% they are converted into newlines and everything after the first non-escaped % is passed to your command on stdin.\nThere are two formats for crontab files:\n\nUser crontabs\n # Example of job definition:\n # .---------------- minute (0 - 59)\n # |  .------------- hour (0 - 23)\n # |  |  .---------- day of month (1 - 31)\n # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7)\n # |  |  |  |  |\n # *  *  *  *  *   command to be executed\n\n\nSystem wide /etc/crontab and /etc/cron.d fragments\n # Example of job definition:\n # .---------------- minute (0 - 59)\n # |  .------------- hour (0 - 23)\n # |  |  .---------- day of month (1 - 31)\n # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7)\n # |  |  |  |  |\n # *  *  *  *  * user-name  command to be executed\n\n\n\nNotice that the latter requires a user-name. The command will be run as the named user.\nThe first 5 fields of the line represent the time(s) when the command should be run.\nYou can use numbers or where applicable day/month names in the time specification.\n\nThe fields are separated by spaces or tabs.\nA comma (,) is used to specify a list e.g 1,4,6,8 which means run at 1,4,6,8.\nRanges are specified with a dash (-) and may be combined with lists e.g. 1-3,9-12 which means between 1 and 3 then between 9 and 12.\nThe / character can be used to introduce a step e.g. 2/5 which means starting at 2 then every 5 (2,7,12,17,22...). They do not wrap past the end.\nAn  asterisk (*) in a field signifies the entire range for that field (e.g. 0-59 for the minute field).\nRanges and steps can be combined e.g. */2 signifies starting at the minimum for the relevant field then every 2 e.g. 0 for minutes( 0,2...58), 1 for months (1,3 ... 11)  etc.\n\nDebugging cron commands\nCheck the mail!\nBy default cron will mail any output from the command to the user it is running the command as. If there is no output there will be no mail. If you want cron to send mail to a different account then you can set the MAILTO environment variable in the crontab file e.g.\n[email\u00a0protected]\n1 2 * * * /path/to/your/command\n\nCapture the output yourself\nYou can redirect stdout and stderr to a file. The exact syntax for capturing output may vary depending on what shell cron is using. Here are two examples which save all output to a file at /tmp/mycommand.log:\n1 2 * * * /path/to/your/command &>/tmp/mycommand.log\n1 2 * * * /path/to/your/command >/tmp/mycommand.log 2>&1\n\nCheck cron user permission to write on /tmp/mycommand.log\nYour cron job runner user must have permission to write\necho \"test\" > /tmp/mycommand.log\n\nRun cron hello world\nRun crontab -e and add this line\n* * * * * echo \"$(date +\"%T\") Hello from crontab\" >> /tmp/mycommand.log\n\nThen monitor text file(tail -f /tmp/mycommand.log) to see somethings like\n\n01:30:50 Hello from crontab\n\nLeading * * * * * in crontab means run following command for every minutes.\nLook at the logs\nCron logs its actions via syslog, which (depending on your setup) often go to /var/log/cron or /var/log/syslog.\nIf required you can filter the cron statements with e.g.\ngrep CRON /var/log/syslog \n\nNow that we've gone over the basics of cron, where the files are and how to use them let's look at some common problems.\nCheck that cron is running\nIf cron isn't running then your commands won't be scheduled ...\nps -ef | grep cron | grep -v grep\n\nshould get you something like\nroot    1224   1  0 Nov16 ?    00:00:03 cron\n\nor\nroot    2018   1  0 Nov14 ?    00:00:06 crond\n\nIf not restart it\n/sbin/service cron start\n\nor\n/sbin/service crond start\n\nThere may be other methods; use what your distro provides.\ncron runs your command in a restricted environment.\nWhat environment variables are available is likely to be very limited.  Typically, you'll only get a few variables defined, such as $LOGNAME, $HOME, and $PATH.\nOf particular note is the PATH is restricted to /bin:/usr/bin. The vast majority of \"my cron script doesn't work\" problems are caused by this restrictive path.  If your command is in a different location you can solve this in a couple of ways:\n\nProvide the full path to your command.\n 1 2 * * * /path/to/your/command\n\n\nProvide a suitable PATH in the crontab file\n PATH=/bin:/usr/bin:/path/to/something/else\n 1 2 * * * command \n\n\n\nIf your command requires other environment variables you can define them in the crontab file too.\ncron runs your command with cwd == $HOME\nRegardless of where the program you execute resides on the filesystem, the current working directory of the program when cron runs it will be the user's home directory.  If you access files in your program, you'll need to take this into account if you use relative paths, or (preferably) just use fully-qualified paths everywhere, and save everyone a whole lot of confusion.\nThe last command in my crontab doesn't run\nCron generally requires that commands are terminated with a new line. Edit your crontab; go to the end of the line which contains the last command and insert a new line (press enter).\nCheck the crontab format\nYou can't use a user crontab formatted crontab for /etc/crontab or the fragments in /etc/cron.d and vice versa.  A user formatted crontab does not include a username in the 6th position of a row, while a system formatted crontab includes the username and runs the command as that user.\nI put a file in /etc/cron.{hourly,daily,weekly,monthly} and it doesn't run\n\nCheck that the filename doesn't have an extension see run-parts\nEnsure the file has execute permissions.\nTell the system what to use when executing your script (eg. put #!/bin/sh at top)\n\nCron date related bugs\nIf your date is recently changed by a user or system update, timezone or other, then crontab will start behaving erratically and exhibit bizarre bugs, sometimes working, sometimes not.  This is crontab's attempt to try to \"do what you want\" when the time changes out from underneath it.  The \"minute\" field will become ineffective after the hour is changed.  In this scenario, only asterisks would be accepted.  Restart cron and try it again without connecting to the internet (so the date doesn't have a chance to reset to one of the time servers).\nPercent signs, again\nTo emphasise the advice about percent signs, here's an example of what cron does with them:\n# cron entry\n* * * * * cat >$HOME/cron.out%foo%bar%baz\n\nwill create the ~/cron.out file containing the 3 lines\nfoo\nbar\nbaz\n\nThis is particularly intrusive when using the date command. Be sure to escape the percent signs\n* * * * * /path/to/command --day \"$(date \"+\\%Y\\%m\\%d\")\"\n\nHow to use sudo in cron jobs\nwhen running as a non-root user,\ncrontab -e\n\nwill open the user's crontab, while\nsudo crontab -e\n\nwill open the root user's crontab. It's not recommended to run sudo commands in a user cron job, so if you need root privileges in your cron job, you should run that job in root's crontab and remove sudo from the command.\n\nUse online services like crontab guru to check task scheduling.",
        "url": "https://serverfault.com/questions/449651/why-is-my-crontab-not-working-and-how-can-i-troubleshoot-it"
    },
    {
        "title": "How can I get the size of an Amazon S3 bucket?",
        "question": "I'd like to graph the size (in bytes, and # of items) of an Amazon S3 bucket and am looking for an efficient way to get the data.\nThe s3cmd tools provide a way to get the total file size using s3cmd du s3://bucket_name, but I'm worried about its ability to scale since it looks like it fetches data about every file and calculates its own sum. Since Amazon charges users in GB-Months it seems odd that they don't expose this value directly.\nAlthough Amazon's REST API returns the number of items in a bucket, s3cmd doesn't seem to expose it. I could do s3cmd ls -r s3://bucket_name | wc -l but that seems like a hack.\nThe Ruby AWS::S3 library looked promising, but only provides the # of bucket items, not the total bucket size.\nIs anyone aware of any other command line tools or libraries (prefer Perl, PHP, Python, or Ruby) which provide ways of getting this data?",
        "top_answer": "The AWS CLI now supports the --query parameter which takes a JMESPath expressions. \nThis means you can sum the size values given by list-objects using sum(Contents[].Size) and count like length(Contents[]).\nThis can be be run using the official AWS CLI as below and was introduced in Feb 2014\n aws s3api list-objects --bucket BUCKETNAME --output json --query \"[sum(Contents[].Size), length(Contents[])]\"\n",
        "url": "https://serverfault.com/questions/84815/how-can-i-get-the-size-of-an-amazon-s3-bucket"
    },
    {
        "title": "Showing total progress in rsync: is it possible?",
        "question": "I have searched for this option already, but have only found solutions that involve custom patching. The fact that it does not show in --help and no more info can be found probably indicates the answers is 'no', but I'd like to see this confirmed.\nIs it possible to show total file transfer progress with rsync?",
        "top_answer": "There is now an official way to do this in rsync (version 3.1.0 protocol version 31, tested with Ubuntu Trusty 14.04).\n#> ./rsync -a --info=progress2 /usr .\n    305,002,533  80%   65.69MB/s    0:00:01  xfr#1653, ir-chk=1593/3594)\n\nI tried with my /usr folder because I wanted this feature for transferring whole filesystems, and /usr seemed to be a good representative sample.\nThe --info=progress2 gives a nice overall percentage, even if it's just a partial value. In fact, my /usr folder is more than 6 gigs:\n#> du -sh /usr\n6,6G    /usr/\n\nand rsync took a lot of time to scan it all. So almost all the time the percentage I've seen was about 90% completed, but nonetheless it's comforting to see that something is being copied :)\nReferences:\n\nhttps://stackoverflow.com/a/7272339/1396334\nhttps://download.samba.org/pub/rsync/NEWS#3.1.0\n",
        "url": "https://serverfault.com/questions/219013/showing-total-progress-in-rsync-is-it-possible"
    },
    {
        "title": "How can I use environment variables in Nginx.conf",
        "question": "I have a docker container running Nginx, that links to another docker container. The host name and IP address of the second container is loaded into the Nginx container as environment variables on startup, but is not know before then (it's dynamic). I want my nginx.conf to use these values - e.g.\nupstream gunicorn {\n    server $APP_HOST_NAME:$APP_HOST_PORT;\n}\n\nHow can I get environment variables into the Nginx configuration on startup?\nEDIT 1\nThis is the entire file, after the suggested answer below:\nenv APP_WEB_1_PORT_5000_TCP_ADDR;\n# Nginx host configuration for django_app\n\n# Django app is served by Gunicorn, running under port 5000 (via Foreman)\nupstream gunicorn {\n    server $ENV{\"APP_WEB_1_PORT_5000_TCP_ADDR\"}:5000;\n}\n\nserver {\n    listen 80;\n\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    location /static/ {\n        alias /app/static/;\n    }\n    location /media/ {\n        alias /app/media/;\n    }\n    location / {\n        proxy_pass http://gunicorn;\n    }\n}\n\nReloading nginx then errors:\n$ nginx -s reload\nnginx: [emerg] unknown directive \"env\" in /etc/nginx/sites-enabled/default:1\n\nEDIT 2: more details\nCurrent environment variables\nroot@87ede56e0b11:/# env | grep APP_WEB_1\nAPP_WEB_1_NAME=/furious_turing/app_web_1\nAPP_WEB_1_PORT=tcp://172.17.0.63:5000\nAPP_WEB_1_PORT_5000_TCP=tcp://172.17.0.63:5000\nAPP_WEB_1_PORT_5000_TCP_PROTO=tcp\nAPP_WEB_1_PORT_5000_TCP_PORT=5000\nAPP_WEB_1_PORT_5000_TCP_ADDR=172.17.0.63\n\nRoot nginx.conf:\nroot@87ede56e0b11:/# head /etc/nginx/nginx.conf\nuser www-data;\nworker_processes 4;\npid /var/run/nginx.pid;\nenv APP_WEB_1_PORT_5000_TCP_ADDR;\n\nSite nginx configuration:\nroot@87ede56e0b11:/# head /etc/nginx/sites-available/default\n# Django app is served by Gunicorn, running under port 5000 (via Foreman)\nupstream gunicorn {\n    server $ENV{\"APP_WEB_1_PORT_5000_TCP_ADDR\"}:5000;\n}\n\nserver {\n    listen 80;\n\nReload nginx configuration:\nroot@87ede56e0b11:/# nginx -s reload\nnginx: [emerg] directive \"server\" is not terminated by \";\" in /etc/nginx/sites-enabled/default:3\n",
        "top_answer": "Here another one with envsubst.\nnginx docker image already runs envsubst through /docker-entrypoint.d/20-envsubst-on-templates.sh. You only need to place a template file in the right place: /etc/nginx/templates/my-file.conf.template (note: the file must have .template extension!).\nThen, you can create a dedicated template file only for variables with one or more map clauses that we will use as our custom variables. You must choose a name for the conf.d file so that it is the first one that nginx takes.\nThis is the basic concept explained with docker-compose and three files:\ndocker-compose.yml:\nversion: \"3.9\"\nservices:\n  nginx:\n    image: nginx:1.23\n    volumes:\n        - ./template-variables:/etc/nginx/templates/10-variables.conf.template:ro\n\n    environment:\n        EXTERNAL_IP: \"1.2.3.4\"\n\ntemplate-variables:\nmap $host $external_ip {\n  default \"$EXTERNAL_IP\";\n}\n\nnginx.conf:\nserver {\n     location / {\n\n          proxy_set_header X-Real-IP $external_ip;\n      }\n}\n",
        "url": "https://serverfault.com/questions/577370/how-can-i-use-environment-variables-in-nginx-conf"
    },
    {
        "title": "How do I get the current Unix time in milliseconds in Bash?",
        "question": "How do I get the current Unix time in milliseconds (i.e number of milliseconds since Unix epoch January 1 1970)?",
        "top_answer": "This:\ndate +%s \n\nwill return the number of seconds since the epoch.\nThis:\ndate +%s%N\n\nreturns the seconds and current nanoseconds.\nSo:\ndate +%s%N | cut -b1-13\n\nwill give you the number of milliseconds since the epoch - current seconds plus the left three of the nanoseconds.\n\nand from MikeyB - echo $(($(date +%s%N)/1000000)) (dividing by 1000 only brings to microseconds)",
        "url": "https://serverfault.com/questions/151109/how-do-i-get-the-current-unix-time-in-milliseconds-in-bash"
    },
    {
        "title": "How do you search for backdoors from the previous IT person?",
        "question": "We all know it happens. A bitter old IT guy leaves a backdoor into the system and network in order to have fun with the new guys and show the company how bad things are without him.\nI've never personally experienced this. The most I've experienced is somebody who broke and stole stuff right before leaving. I'm sure this happens, though.\nSo, when taking over a network that can't quite be trusted, what steps should be taken to ensure everything is safe and secure?",
        "top_answer": "It's really, really, really hard. It requires a very complete audit. If you're very sure the old person left something behind that'll go boom, or require their re-hire because they're the only one who can put a fire out, then it's time to assume you've been rooted by a hostile party. Treat it like a group of hackers came in and stole stuff, and you have to clean up after their mess. Because that's what it is.\n\nAudit every account on every system to ensure it is associated with a specific entity.\nAccounts that seem associated to systems but no one can account for are to be mistrusted.\nAccounts that aren't associated with anything need to be purged (this needs to be done anyway, but it is especially important in this case)\nChange any and all passwords they might conceivably have come into contact with.\nThis can be a real problem for utility accounts as those passwords tend to get hard-coded into things.\nIf they were a helpdesk type responding to end-user calls, assume they have the password of anyone they assisted.\nIf they had Enterprise Admin or Domain Admin to Active Directory, assume they grabbed a copy of the password hashes before they left. These can be cracked so fast now that a company-wide password change will need to be forced within days.\nIf they had root access to any *nix boxes assume they walked off with the password hashes.\nReview all public-key SSH key usage to ensure their keys are purged, and audit if any private keys were exposed while you're at it.\nIf they had access to any telecom gear, change any router/switch/gateway/PBX passwords. This can be a really royal pain as this can involve significant outages.\nFully audit your perimeter security arrangements.\nEnsure all firewall holes trace to known authorized devices and ports.\nEnsure all remote access methods (VPN, SSH, BlackBerry, ActiveSync, Citrix, SMTP, IMAP, WebMail, whatever) have no extra authentication tacked on, and fully vet them for unauthorized access methods.\nEnsure remote WAN links trace to fully employed people, and verify it. Especially wireless connections. You don't want them walking off with a company paid cell-modem or smart-phone. Contact all such users to ensure they have the right device.\nFully audit internal privileged-access arrangements. These are things like SSH/VNC/RDP/DRAC/iLO/IMPI access to servers that general users don't have, or any access to sensitive systems like payroll.\nWork with all external vendors and service providers to ensure contacts are correct.\nEnsure they are eliminated from all contact and service lists. This should be done anyway after any departure, but is extra-important now.\nValidate all contacts are legitimate and have correct contact information, this is to find ghosts that can be impersonated.\nStart hunting for logic bombs.\nCheck all automation (task schedulers, cron jobs, UPS call-out lists, or anything that runs on a schedule or is event-triggered) for signs of evil. By \"All\" I mean all. Check every single crontab. Check every single automated action in your monitoring system, including the probes themselves. Check every single Windows Task Scheduler; even workstations. Unless you work for the government in a highly sensitive area you won't be able to afford \"all\", do as much as you can.\nValidate key system binaries on every server to ensure they are what they should be. This is tricky, especially on Windows, and nearly impossible to do retroactively on one-off systems.\nStart hunting for rootkits. By definition they're hard to find, but there are scanners for this.\n\nThe decision to kick off an audit of this incredible scope needs to be made at a very high level. The decision to treat this as a potential criminal case will be made by your Legal team. If they elect to do some preliminary investigation first, go for it. Start looking.\nIf you find any evidence, stop immediately.\n\nNotify your legal team as soon as you find something likely.\nThe decision to treat it as a criminal case will be made at that time.\nFurther action by untrained hands (you) can spoil evidence and you don't want that, not unless you want the perp to walk free.\nIf outside security experts are retained, you are their local expert. Work with them, to their direction. They understand the legal requirements for evidence, you do not.\nThere will be a lot of negotiation between the security experts, your management, and legal counsel. That's expected, work with them.\n\n\nBut, really, how far do you have to go? This is where risk management comes into play. Simplistically, this is the method of balancing expected risk against loss. Sysadmins do this when we decide which off-site location we want to put backups; bank safety deposit box vs an out-of-region datacenter. Figuring out how much of this list needs following is an exercise in risk-management.\nIn this case the assessment will start with a few things:\n\nThe expected skill level of the departed\nThe access of the departed\nThe expectation that evil was done\nThe potential damage of any evil\nRegulatory requirements for reporting perpetrated evil vs preemptively found evil. Generally you have to report the former, but not the later.\n\nThe decision of how far down the above rabbit-hole to dive will depend on the answers to these questions. For routine admin departures where expectation of evil is very slight, the full circus is not required; changing admin-level passwords and re-keying any external-facing SSH hosts is probably sufficient. Again, corporate risk-management security posture determines this.\nFor admins who were terminated for cause, or evil cropped up after their otherwise normal departure, the circus becomes more needed. The worst-case scenario is a paranoid BOFH-type who has been notified that their position will be made redundant in 2 weeks, as that gives them plenty of time to get ready; in circumstances like these Kyle's idea of a generous severance package can mitigate all kind of problems. Even paranoids can forgive a lot of sins after a check containing 4 months pay arrives. That check will probably cost less than the cost of the security consultants needed to ferret out their evil.\nBut ultimately, it comes down to the cost of determining if evil was done versus the potential cost of any evil actually being done.",
        "url": "https://serverfault.com/questions/171893/how-do-you-search-for-backdoors-from-the-previous-it-person"
    },
    {
        "title": "What permissions should my website files/folders have on a Linux webserver?",
        "question": "\nThis is a Canonical Question about File Permissions on a Linux web server.\n\nI have a Linux web server running Apache2 that hosts several websites. Each website has its own folder in /var/www/.\n/var/www/contoso.com/\n/var/www/contoso.net/\n/var/www/fabrikam.com/\n\nThe base directory /var/www/ is owned by root:root. Apache is running as www-data:www-data. The Fabrikam website is maintained by two developers, Alice and Bob. Both Contoso websites are maintained by one developer, Eve. All websites allow users to upload images. If a website is compromised, the impact should be as limited as possible.\nI want to know the best way to set up permissions so that Apache can serve the content, the website is secure from attacks, and the developers can still make changes. One of the websites is structured like this:\n/var/www/fabrikam.com\n    /cache\n    /modules\n    /styles\n    /uploads\n    /index.php\n\nHow should the permissions be set on these directories and files? I read somewhere that you should never use 777 permissions on a website, but I don't understand what problems that could cause. During busy periods, the website automatically caches some pages and stores the results in the cache folder. All of the content submitted by website visitors is saved to the uploads folder.",
        "top_answer": "When deciding what permissions to use, you need to know exactly who your users are and what they need. A webserver interacts with two types of user.\nAuthenticated users have a user account on the server and can be provided with specific privileges. This usually includes system administrators, developers, and service accounts. They usually make changes to the system using SSH or SFTP.\nAnonymous users are the visitors to your website. Although they don't have permissions to access files directly, they can request a web page and the web server acts on their behalf. You can limit the access of anonymous users by being careful about what permissions the web server process has. On many Linux distributions, Apache runs as the www-data user but it can be different. Use ps aux | grep httpd or ps aux | grep apache to see what user Apache is using on your system.\n\nNotes on linux permissions\nLinux and other POSIX-compliant systems use traditional unix permissions. There is an excellent article on Wikipedia about Filesystem permissions so I won't repeat everything here. But there are a few things you should be aware of.\nThe execute bit\nInterpreted scripts (eg. Ruby, PHP) work just fine without the execute permission. Only binaries and shell scripts need the execute bit. In order to traverse (enter) a directory, you need to have execute permission on that directory. The webserver needs this permission to list a directory or serve any files inside of it.\nDefault new file permissions\nWhen a file is created, it normally inherits the group id of whoever created it. But sometimes you want new files to inherit the group id of the folder where they are created, so you would enable the SGID bit on the parent folder.\nDefault permission values depend on your umask. The umask subtracts permissions from newly created files, so the common value of 022 results in files being created with 755. When collaborating with a group, it's useful to change your umask to 002 so that files you create can be modified by group members. And if you want to customize the permissions of uploaded files, you either need to change the umask for apache or run chmod after the file has been uploaded.\n\nThe problem with 777\nWhen you chmod 777 your website, you have no security whatsoever. Any user on the system can change or delete any file in your website. But more seriously, remember that the web server acts on behalf of visitors to your website, and now the web server is able to change the same files that it's executing. If there are any programming vulnerabilities in your website, they can be exploited to deface your website, insert phishing attacks, or steal information from your server without you ever knowing.\nAdditionally, if your server runs on a well-known port (which it should to prevent non-root users from spawning listening services that are world-accessible), that means your server must be started by root (although any sane server will immediately drop to a less-privileged account once the port is bound). In other words, if you're running a webserver where the main executable is part of the version control (e.g. a CGI app), leaving its permissions (or, for that matter, the permissions of the containing directory, since the user could rename the executable) at 777 allows any user to run any executable as root.\n\nDefine the requirements\n\nDevelopers need read/write access to files so they can update the website\nDevelopers need read/write/execute on directories so they can browse around\nApache needs read access to files and interpreted scripts\nApache needs read/execute access to serveable directories\nApache needs read/write/execute access to directories for uploaded content\n\n\nMaintained by a single user\nIf only one user is responsible for maintaining the site, set them as the user owner on the website directory and give the user full rwx permissions. Apache still needs access so that it can serve the files, so set www-data as the group owner and give the group r-x permissions.\nIn your case, Eve, whose username might be eve, is the only user who maintains contoso.com :\nchown -R eve contoso.com/\nchgrp -R www-data contoso.com/\nchmod -R 750 contoso.com/\nchmod g+s contoso.com/\n\nls -l\ndrwxr-s--- 2 eve      www-data   4096 Feb  5 22:52 contoso.com\n\nIf you have folders that need to be writable by Apache, you can just modify the permission values for the group owner so that www-data has write access.\nchmod g+w uploads\n\nls -l\ndrwxrws--- 2 eve      www-data   4096 Feb  5 22:52 uploads\n\nThe benefit of this configuration is that it becomes harder (but not impossible*) for other users on the system to snoop around, since only the user and group owners can browse your website directory. This is useful if you have secret data in your configuration files. Be careful about your umask! If you create a new file here, the permission values will probably default to 755. You can run umask 027 so that new files default to 640 (rw- r-- ---).\n\nMaintained by a group of users\nIf more than one user is responsible for maintaining the site, you will need to create a group to use for assigning permissions. It's good practice to create a separate group for each website, and name the group after that website.\ngroupadd dev-fabrikam\nusermod -a -G dev-fabrikam alice\nusermod -a -G dev-fabrikam bob\n\nIn the previous example, we used the group owner to give privileges to Apache, but now that is used for the developers group. Since the user owner isn't useful to us any more, setting it to root is a simple way to ensure that no privileges are leaked. Apache still needs access, so we give read access to the rest of the world.\nchown -R root fabrikam.com\nchgrp -R dev-fabrikam fabrikam.com\nchmod -R 775 fabrikam.com\nchmod g+s fabrikam.com\n\nls -l\ndrwxrwsr-x 2 root     dev-fabrikam   4096 Feb  5 22:52 fabrikam.com\n\nIf you have folders that need to be writable by Apache, you can make Apache either the user owner or the group owner. Either way, it will have all the access it needs. Personally, I prefer to make it the user owner so that the developers can still browse and modify the contents of upload folders.\nchown -R www-data uploads\n\nls -l\ndrwxrwxr-x 2 www-data     dev-fabrikam   4096 Feb  5 22:52 uploads\n\nAlthough this is a common approach, there is a downside. Since every other user on the system has the same privileges to your website as Apache does, it's easy for other users to browse your site and read files that may contain secret data, such as your configuration files.\nYou can have your cake and eat it too\nThis can be futher improved upon. It's perfectly legal for the owner to have less privileges than the group, so instead of wasting the user owner by assigning it to root, we can make Apache the user owner on the directories and files in your website. This is a reversal of the single maintainer scenario, but it works equally well.\nchown -R www-data fabrikam.com\nchgrp -R dev-fabrikam fabrikam.com\nchmod -R 570 fabrikam.com\nchmod g+s fabrikam.com\n\nls -l\ndr-xrwx--- 2 www-data  dev-fabrikam   4096 Feb  5 22:52 fabrikam.com\n\nIf you have folders that need to be writable by Apache, you can just modify the permission values for the user owner so that www-data has write access.\nchmod u+w uploads\n\nls -l\ndrwxrwx--- 2 www-data  dev-fabrikam   4096 Feb  5 22:52 fabrikam.com\n\nOne thing to be careful about with this solution is that the user owner of new files will match the creator instead of being set to www-data. So any new files you create won't be readable by Apache until you chown them.\n\n*Apache privilege separation\nI mentioned earlier that it's actually possible for other users to snoop around your website no matter what kind of privileges you're using. By default, all Apache processes run as the same www-data user, so any Apache process can read files from all other websites configured on the same server, and sometimes even make changes. Any user who can get Apache to run a script can gain the same access that Apache itself has.\nTo combat this problem, there are various approaches to privilege separation in Apache. However, each approach comes with various performance and security drawbacks. In my opinion, any site with higher security requirements should be run on a dedicated server instead of using VirtualHosts on a shared server.\n\nAdditional considerations\nI didn't mention it before, but it's usually a bad practice to have developers editing the website directly. For larger sites, you're much better off having some kind of release system that updates the webserver from the contents of a version control system. The single maintainer approach is probably ideal, but instead of a person you have automated software.\nIf your website allows uploads that don't need to be served out, those uploads should be stored somewhere outside the web root. Otherwise, you might find that people are downloading files that were intended to be secret. For example, if you allow students to submit assignments, they should be saved into a directory that isn't served by Apache. This is also a good approach for configuration files that contain secrets.\nFor a website with more complex requirements, you may want to look into the use of Access Control Lists. These enable much more sophisticated control of privileges.\nIf your website has complex requirements, you may want to write a script that sets up all of the permissions. Test it thoroughly, then keep it safe. It could be worth its weight in gold if you ever find yourself needing to rebuild your website for some reason.",
        "url": "https://serverfault.com/questions/357108/what-permissions-should-my-website-files-folders-have-on-a-linux-webserver"
    },
    {
        "title": "Anyone else experiencing high rates of Linux server crashes during a leap second day?",
        "question": "*NOTE: if your server still has issues due to confused kernels, and you can't reboot - the simplest solution proposed with gnu date installed on your system is: date -s now.  This will reset the kernel's internal \"time_was_set\" variable and fix the CPU hogging futex loops in java and other userspace tools.  I have straced this command on my own system an confirmed it's doing what it says on the tin *\nPOSTMORTEM\nAnticlimax: only thing that died was my VPN (openvpn) link to the cluster, so there was an exciting few seconds while it re-established.  Everything else was fine, and starting up ntp went cleanly after the leap second had passed.\nI have written up my full experience of the day at http://blog.fastmail.fm/2012/07/03/a-story-of-leaping-seconds/\nIf you look at Marco's blog at http://my.opera.com/marcomarongiu/blog/2012/06/01/an-humble-attempt-to-work-around-the-leap-second - he has a solution for phasing the time change over 24 hours using ntpd -x to avoid the 1 second skip.  This is an alternative smearing method to running your own ntp infrastructure.\n\nJust today, Sat June 30th, 2012 - starting soon after the start of the day GMT.  We've had a handful of servers in different datacentres as managed by different teams all go dark - not responding to pings, screen blank.\nThey're all running Debian Squeeze - with everything from stock kernel to custom 3.2.21 builds.  Most are Dell M610 blades, but I've also just lost a Dell R510 and other departments have lost machines from other vendors too.  There was also an older IBM x3550 which crashed and which I thought might be unrelated, but now I'm wondering.\nThe one crash which I did get a screen dump from said:\n[3161000.864001] BUG: spinlock lockup on CPU#1, ntpd/3358\n[3161000.864001]  lock: ffff88083fc0d740, .magic: dead4ead, .owner: imapd/24737, .owner_cpu: 0\n\nUnfortunately the blades all supposedly had kdump configured, but they died so hard that kdump didn't trigger - and they had console blanking turned on.  I've disabled console blanking now, so fingers crossed I'll have more information after the next crash.\nJust want to know if it's a common thread or \"just us\".  It's really odd that they're different units in different datacentres bought at different times and run by different admins (I run the FastMail.FM ones)... and now even different vendor hardware.  Most of the machines which crashed had been up for weeks/months and were running 3.1 or 3.2 series kernels.\nThe most recent crash was a machine which had only been up about 6 hours running 3.2.21.\nTHE WORKAROUND\nOk people, here's how I worked around it.\n\ndisabled ntp: /etc/init.d/ntp stop\ncreated http://linux.brong.fastmail.fm/2012-06-30/fixtime.pl (code stolen from Marco, see blog posts in comments)\nran fixtime.pl without an argument to see that there was a leap second set\nran fixtime.pl with an argument to remove the leap second\n\nNOTE: depends on adjtimex.  I've put a copy of the squeeze adjtimex binary at http://linux.brong.fastmail.fm/2012-06-30/adjtimex \u2014 it will run without dependencies on a squeeze 64 bit system.  If you put it in the same directory as fixtime.pl, it will be used if the system one isn't present.  Obviously if you don't have squeeze 64-bit\u2026 find your own.\nI'm going to start ntp again tomorrow.\nAs an anonymous user suggested - an alternative to running adjtimex is to just set the time yourself, which will presumably also clear the leapsecond counter.",
        "top_answer": "This is caused by a livelock when ntpd calls adjtimex(2) to tell the kernel to insert a leap second. See lkml posting http://lkml.indiana.edu/hypermail/linux/kernel/1203.1/04598.html\nRed Hat should also be updating their KB article as well. https://access.redhat.com/knowledge/articles/15145\nUPDATE: Red Hat has a second KB article just for this issue here: https://access.redhat.com/knowledge/solutions/154713 - the previous article is for an earlier, unrelated problem\nThe work-around is to just turn off ntpd. If ntpd already issued the adjtimex(2) call, you may need to disable ntpd and reboot to be 100% safe.\nThis affects RHEL 6 and other distros running newer kernels (newer than approx 2.6.26), but not RHEL 5.\nThe reason this is occurring before the leap second is actually scheduled to occur is that ntpd lets the kernel handle the leap second at midnight, but needs to alert the kernel to insert the leap second before midnight. ntpd therefore calls adjtimex(2) sometime during the day of the leap second, at which point this bug is triggered.\nIf you have adjtimex(8) installed, you can use this script to determine if flag 16 is set. Flag 16 is \"inserting leap second\":\nadjtimex -p | perl -p -e 'undef $_, next unless m/status: (\\d+)/; (16 & $1) && print \"leap second flag is set:\\n\"'\n\nUPDATE:\nRed Hat has updated their KB article to note: \"RHEL 6 customers may be affected by a known issue that causes NMI Watchdog to detect a hang when receiving the NTP leapsecond announcement. This issue is being addressed in a timely manner. If your systems received the leapsecond announcement and did not experience this issue, then they are no longer affected.\"\nUPDATE: The above language was removed from the Red Hat article; and a second KB solution was added detailing the adjtimex(2) crash issue: https://access.redhat.com/knowledge/solutions/154713\nHowever, the code change in the LKML post by IBM Engineer John Stultz notes there may also be a deadlock when the leap second is actually applied, so you may want to disable the leap second by rebooting or using adjtimex(8) after disabling ntpd.\nFINAL UPDATE:\nWell, I'm no kernel dev, but I reviewed John Stultz's patch again here: https://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=6b43ae8a619d17c4935c3320d2ef9e92bdeed05d\nIf I'm reading it right this time, I was wrong about there being another deadlock when the leap second is applied. That seems to be Red Hat's opinion as well, based on their KB entry. However, if you have disabled ntpd, keep it disabled for another 10 minutes, so that you don't hit the deadlock when ntpd calls adjtimex(2).\nWe'll find out if there are any more bugs soon :)\nPOST-LEAP SECOND UPDATE:\nI spent the last few hours reading through the ntpd and pre-patch (buggy) kernel code, and while I may be very wrong here, I'll attempt to explain what I think was going on:\nFirst, ntpd calls adjtimex(2) all the time. It does this as part of its \"clock loop filter\", defined in local_clock in ntp_loopfilter.c. You can see that code here: http://www.opensource.apple.com/source/ntp/ntp-70/ntpd/ntp_loopfilter.c (from ntp version 4.2.6).\nThe clock loop filter runs quite often -- it runs every time ntpd polls its upstream servers, which by default is every 17 minutes or more. The relevant bit of the clock loop filter is:\nif (sys_leap == LEAP_ADDSECOND)\n    ntv.status |= STA_INS;\n\nAnd then:\nntp_adjtime(&ntv)\n\nIn other words, on days when there's a leap second, ntpd sets the \"STA_INS\" flag and calls adjtimex(2) (via its portability-wrapper).\nThat system call makes its way to the kernel. Here's the relevant kernel code: https://github.com/mirrors/linux/blob/a078c6d0e6288fad6d83fb6d5edd91ddb7b6ab33/kernel/time/ntp.c\nThe kernel codepath is roughly this:\n\nline 663 - start of do_adjtimex routine.\nline 691 - cancel any existing leap-second timer.\nline 709 - grab the ntp_lock spinlock (this lock is involved in the possible livelock crash)\nline 724 - call process_adjtimex_modes.\nline 616 - call process_adj_status.\nline 590 - set time_status global variable, based on flags set in adjtimex(2) call\nline 592 - check time_state global variable. in most cases, call ntp_start_leap_timer.\nline 554 - check time_status global variable. STA_INS will be set, so set time_state to TIME_INS and call hrtimer_start (another kernel function) to start the leap second timer. in the process of creating a timer, this code grabs the xtime_lock. if this happens while another CPU has already grabbed the xtime_lock and the ntp_lock, then the kernel livelocks. this is why John Stultz wrote the patch to avoid using hrtimers. This is what was causing everyone trouble today.\nline 598 - if ntp_start_leap_timer did not actually start a leap timer, set time_state to TIME_OK\nline 751 - assuming the kernel does not livelock, the stack is unwound and the ntp_lock spinlock is released.\n\nThere are a couple interesting things here.\nFirst, line 691 cancels the existing timer every time adjtimex(2) is called. Then, 554 re-creates that timer. This means each time ntpd ran its clock loop filter, the buggy code was invoked.\nTherefore I believe Red Hat was wrong when they said that once ntpd had set the leap-second flag, the system would not crash. I believe each system running ntpd had the potential to livelock every 17 minutes (or more) for the 24-hour period before the leap-second. I believe this may also explain why so many systems crashed; a one-time chance of crashing would be much less likely to hit as compared to 3 chances an hour.\nUPDATE: In Red Hat's KB solution at https://access.redhat.com/knowledge/solutions/154713 , Red Hat engineers did come to the same conclusion (that running ntpd would continuously hit the buggy code). And indeed they did so several hours before I did. This solution wasn't linked to the main article at https://access.redhat.com/knowledge/articles/15145 , so I didn't notice it until now.\nSecond, this explains why loaded systems were more likely to crash. Loaded systems will be handling more interrupts, causing the \"do_tick\" kernel function to be called more often, giving more of a chance for this code to run and grab the ntp_lock while the timer was being created. \nThird, is there a chance of the system crashing when the leap-second actually occurs? I don't know for sure, but possibly yes, because the timer that fires and actually executes the leap-second adjustment (ntp_leap_second, on line 388) also grabs the ntp_lock spinlock, and has a call to hrtimer_add_expires_ns. I don't know if that call might also be able to cause a livelock, but it doesn't seem impossible.\nFinally, what causes the leap-second flag to be disabled after the leap-second has run? The answer there is ntpd stops setting the leap-second flag at some point after midnight when it calls adjtimex(2). Since the flag isn't set, the check on line 554 will not be true, and no timer will be created, and line 598 will reset the time_state global variable to TIME_OK. This explains why if you checked the flag with adjtimex(8) just after the leap second, you would still see the leap-second flag set.\nIn short, the best advice for today seems to be the first I gave after all: disable ntpd, and disable the leap-second flag.\nAnd some final thoughts:\n\nnone of the Linux vendors noticed John Stultz's patch and applied it to their kernels :(\nwhy didn't John Stultz alert some of the vendors this was needed? perhaps the chance of the livelock seemed low enough making noise wasn't warranted.\nI've heard reports of Java processes locking up or spinning when the leap-second was applied. Perhaps we should follow Google's lead and rethink how we apply leap-seconds to our systems: http://googleblog.blogspot.com/2011/09/time-technology-and-leaping-seconds.html\n\n\n06/02 Update from John Stultz:\nhttps://lkml.org/lkml/2012/7/1/203\nThe post contained a step-by-step walk-through of why the leap second caused the futex timers to expire prematurely and continuously, spiking the CPU load. ",
        "url": "https://serverfault.com/questions/403732/anyone-else-experiencing-high-rates-of-linux-server-crashes-during-a-leap-second"
    },
    {
        "title": "Can I automatically add a new host to known_hosts?",
        "question": "Here's my situation: I'm setting up a test harness that will, from a central client, launch a number of virtual machine instances and then execute commands on them via ssh.  The virtual machines will have previously unused hostnames and IP addresses, so they won't be in the ~/.ssh/known_hosts file on the central client.\nThe problem I'm  having is that the first ssh command run against a new virtual instance always comes up with an interactive prompt:\nThe authenticity of host '[hostname] ([IP address])' can't be established.\nRSA key fingerprint is [key fingerprint].\nAre you sure you want to continue connecting (yes/no)?\n\nIs there a way that I can bypass this and get the new host to be already known to the client machine, maybe by using a public key that's already baked into the virtual machine image ?  I'd really like to avoid having to use Expect or whatever to answer the interactive prompt if I can. ",
        "top_answer": "Set the StrictHostKeyChecking option to no, either in the config file or via -o :\nssh -o StrictHostKeyChecking=no [email\u00a0protected]",
        "url": "https://serverfault.com/questions/132970/can-i-automatically-add-a-new-host-to-known-hosts"
    },
    {
        "title": "Create a public SSH key from the private key?",
        "question": "Let's suppose I have a SSH key, but I've deleted the public key part. I have the private key part. Is there some way I can regenerate the public key part?",
        "top_answer": "Use the -y option to ssh-keygen:\nssh-keygen -f ~/.ssh/id_rsa -y > ~/.ssh/id_rsa.pub\n\nFrom the 'man ssh-keygen'\n -y      This option will read a private OpenSSH format file and print an\n         OpenSSH public key to stdout.\n\nSpecify the private key with the -f option, yours might be dsa instead of rsa.  The name of your private key probably contains which you used.  The newly generated public key should be the same as the one you generated before.",
        "url": "https://serverfault.com/questions/52285/create-a-public-ssh-key-from-the-private-key"
    },
    {
        "title": "How to send ctrl+alt+del using Remote Desktop?",
        "question": "How can I send ctrl+alt+del to a remote computer over Remote Desktop?\nFor example, if I wanted to change the local admin password on a remote PC using a Remote Desktop connection, it would be helpful to be able to send the ctrl+alt+del key sequence to the remote computer. \nI would normally do this by pressing ctrl+alt+del and selecting the change password option. But I can't send ctrl+alt+del using Remote Desktop since this \"special\" key series is always handled by the local client.  ",
        "top_answer": "ctrl+alt+end is the prescribed way to do this.\nCoding Horror has some other shortcuts.",
        "url": "https://serverfault.com/questions/57222/how-to-send-ctrlaltdel-using-remote-desktop"
    },
    {
        "title": "What are the functional differences between .profile .bash_profile and .bashrc",
        "question": "What are the functional differences between the .profile, .bash_profile and .bashrc files? ",
        "top_answer": ".bash_profile and .bashrc are specific to bash, whereas .profile is read by many shells in the absence of their own shell-specific config files.  (.profile was used by the original Bourne shell.)  .bash_profile or .profile is read by login shells, along with .bashrc; subshells read only .bashrc.  (Between job control and modern windowing systems, .bashrc by itself doesn't get used much.  If you use screen or tmux, screens/windows usually run subshells instead of login shells.)\nThe idea behind this was that one-time setup was done by .profile (or shell-specific version thereof), and per-shell stuff by .bashrc.  For example, you generally only want to load environment variables once per session instead of getting them whacked any time you launch a subshell within a session, whereas you always want your aliases (which aren't propagated automatically like environment variables are).\nOther notable shell config files:\n/etc/bash_profile (fallback /etc/profile) is read before the user's .profile for system-wide configuration, and likewise /etc/bashrc in subshells (no fallback for this one).  Many systems including Ubuntu also use an /etc/profile.d directory containing shell scriptlets, which are . (source)-ed from /etc/profile; the fragments here are per-shell, with *.sh applying to all Bourne/POSIX compatible shells and other extensions applying to that particular shell.",
        "url": "https://serverfault.com/questions/261802/what-are-the-functional-differences-between-profile-bash-profile-and-bashrc"
    },
    {
        "title": "Copying a large directory tree locally? cp or rsync?",
        "question": "I have to copy a large directory tree, about 1.8 TB. It's all local. Out of habit I'd use rsync, however I wonder if there's much point, and if I should rather use cp.\nI'm worried about permissions and uid/gid, since they have to be preserved in the copy (I know rsync does this). As well as things like symlinks.\nThe destination is empty, so I don't have to worry about conditionally updating some files. It's all local disk, so I don't have to worry about ssh or network.\nThe reason I'd be tempted away from rsync, is because rsync might do more than I need. rsync checksums files. I don't need that, and am concerned that it might take longer than cp.\nSo what do you reckon, rsync or cp?",
        "top_answer": "I would use rsync as it means that if it is interrupted for any reason, then you can restart it easily with very little cost. And being rsync, it can even restart part way through a large file. As others mention, it can exclude files easily. The simplest way to preserve most things is to use the -a flag \u2013 \u2018archive.\u2019 So:\nrsync -a source dest\n\nAlthough UID/GID and symlinks are preserved by -a (see -lpgo), your question implies you might want a full copy of the filesystem information; and -a doesn't include hard-links, extended attributes, or ACLs (on Linux) or the above nor resource forks (on OS X.) Thus, for a robust copy of a filesystem, you'll need to include those flags:\nrsync -aHAX source dest # Linux\nrsync -aHE source dest  # OS X\n\nThe default cp will start again, though the -u flag will \"copy only when the SOURCE file is newer than the destination file or when the destination file is missing\". And the -a (archive) flag will be recursive, not recopy files if you have to restart and preserve permissions. So:\ncp -au source dest\n",
        "url": "https://serverfault.com/questions/43014/copying-a-large-directory-tree-locally-cp-or-rsync"
    },
    {
        "title": "Nginx reverse proxy + URL rewrite",
        "question": "Nginx is running on port 80, and I'm using it to reverse proxy URLs with path /foo to port 3200 this way:\nlocation /foo {\n                proxy_pass http://localhost:3200;\n                proxy_redirect     off;\n                proxy_set_header   Host $host;\n}\n\nThis works fine, but I have an application on port 3200, for which I don't want the initial /foo to be sent to. That is - when I access http://localhost/foo/bar, I want only /bar to be the path as received by the app. So I tried adding this line to the location block above:\nrewrite ^(.*)foo(.*)$ http://localhost:3200/$2 permanent;\n\nThis causes 302 redirect (change in URL), but I want 301. What should I do?",
        "top_answer": "Any redirect to localhost doesn't make sense from a remote system (e.g. client's Web browser). So the rewrite flags permanent (301) or redirect (302) are not usable in your case. \nPlease try following setup using a transparent rewrite rule:\nlocation  /foo {\n  rewrite /foo/(.*) /$1  break;\n  proxy_pass         http://localhost:3200;\n  proxy_redirect     off;\n  proxy_set_header   Host $host;\n}\n\nUse curl -i to test your rewrites. A very subtle change to the rule can cause nginx to perform a redirect.",
        "url": "https://serverfault.com/questions/379675/nginx-reverse-proxy-url-rewrite"
    },
    {
        "title": "Check if port is open or closed on a Linux server?",
        "question": "How can I check if a port is listening on a Linux server?",
        "top_answer": "You can check if a process listens on a TCP or UDP port with netstat -tuplen.\nTo check whether some ports are accessible from the outside (this is probably what you want) you can use a port scanner like Nmap from another system. Running Nmap on the same host you want to check is quite useless for your purpose.",
        "url": "https://serverfault.com/questions/309052/check-if-port-is-open-or-closed-on-a-linux-server"
    },
    {
        "title": "100% uptime for a web application",
        "question": "We received an interesting \"requirement\" from a client today.\nThey want 100% uptime with off-site failover on a web application. From our web application's viewpoint, this isn't an issue. It was designed to be able to scale out across multiple database servers, etc.\nHowever, from a networking issue I just can't seem to figure out how to make it work.\nIn a nutshell, the application will live on servers within the client's network. It is accessed by both internal and external people. They want us to maintain an off-site copy of the system that in the event of a serious failure at their premises would immediately pick up and take over.\nNow we know there is absolutely no way to resolve it for internal people (carrier pigeon?), but they want the external users to not even notice.\nQuite frankly, I haven't the foggiest idea of how this might be possible. It seems that if they lose Internet connectivity then we would have to do a DNS change to forward traffic to the external machines... Which, of course, takes time. \nIdeas?\nUPDATE\nI had a discussion with the client today and they clarified on the issue.  \nThey stuck by the 100% number, saying the application should stay active even in the event of a flood. However, that requirement only kicks in if we host it for them. They said they would handle the uptime requirement if the application lives entirely on their servers. You can guess my response.",
        "top_answer": "Here is Wikipedia's handy chart of the pursuit of nines:\n\nInterestingly, only 3 of the top 20 websites were able to achieve the mythical 5 nines or 99.999% uptime in 2007.  They were Yahoo, AOL, and Comcast.  In the first 4 months of 2008, some of the most popular social networks, didn't even come close to that.\nFrom the chart, it should be evident how ridiculous the pursuit of 100% uptime is...",
        "url": "https://serverfault.com/questions/316637/100-uptime-for-a-web-application"
    },
    {
        "title": "How to bind MySQL server to more than one IP address?",
        "question": "Is there a secret way to bind MySQL to more than one IP address?\nAs far as I can see the bind-address parameter in the my.cnf does not support more than one IP and you can't have it more than once.",
        "top_answer": "No, there isn't (I just checked 1 hour ago). You can comment the bind-address in my.cnf:\n\nNote: \u00ab 1 hour ago \u00bb is now more than 15 years ago.\n\n#skip-networking\n#bind-address                   = 127.0.0.1\n\nIf you want only 2 IPs, you will then have to use a firewall.\nFor MySql version 8.0.13 and above, and MariaDB version 10.11 and above, you can specify a list of comma-separated IP addresses.\nbind-address = 10.0.0.1,10.0.1.1,10.0.2.1\n\nRelevant MySql documentation.\nRelevant MariaDB documentation.\nThank you @KurtFitzner for the MariaDB info.\nRemember to restart your MySQL instance after changing the config file.",
        "url": "https://serverfault.com/questions/139323/how-to-bind-mysql-server-to-more-than-one-ip-address"
    },
    {
        "title": "How to Unban an IP properly with Fail2Ban",
        "question": "I'm using Fail2Ban on a server and I'm wondering how to unban an IP properly.\nI know I can work with IPTables directly: iptables -D fail2ban-ssh <number>\nBut is there not a way to do it with the fail2ban-client?\nIn the manuals it states something like: fail2ban-client get ssh actionunban <IP>. But that doesn't work.\nAlso, I don't want to /etc/init.d/fail2ban restart as that would lose all the bans in the list.",
        "top_answer": "First, find the right jail to unban for. Use:\nfail2ban-client status | grep \"Jail list\" | sed -E 's/^[^:]+:[ \\t]+//' | sed 's/,//g'\n\nto get the actual jail names.  Then, use this to find the iptables rule name:\niptables -L -n\n\nThe rule name and jail name may not be the same but it should be clear which one is related to which.\nThen, with Fail2Ban v0.8.8 and later:\nfail2ban-client set <jail-name> unbanip <ip-address>\n\nWhere <jail-name> is the jail name, and <ip-address> is the IP address to unban.\nWith Fail2Ban before v0.8.8, the syntax is different:\nfail2ban-client get <jail-name> actionunban <ip-address>\n",
        "url": "https://serverfault.com/questions/285256/how-to-unban-an-ip-properly-with-fail2ban"
    },
    {
        "title": "What's the command-line utility in Windows to do a reverse DNS look-up?",
        "question": "Is there a built-in command line tool that will do reverse DNS look-ups in Windows?  I.e., something like <toolname> w.x.y.z => mycomputername\nI've tried:\n\nnslookup: seems to be forward look-up only.\nhost: doesn't exist\ndig: also doesn't exist.  \n\nI found \"What's the reverse DNS command line utility?\" via a search, but this is specifically looking for a *nix utility, not a Windows one.",
        "top_answer": "ping -a w.x.y.z\n\nShould resolve the name from the IP address if the reverse lookup zone has been set up properly. If the reverse lookup zone does not have an entry for the record, the -a will just ping without a name.",
        "url": "https://serverfault.com/questions/41064/whats-the-command-line-utility-in-windows-to-do-a-reverse-dns-look-up"
    },
    {
        "title": "How do I select which Apache MPM to use?",
        "question": "\nThis is a Canonical Question about selecting the right Apache httpd MPM.\n\nI'm a little confused between the different MPMs offered by Apache - 'worker', 'event', 'prefork', etc.\nWhat are the major differences between them, and how can I decide which one will be best for a given deployment?",
        "top_answer": "There are a number of MPM modules (Multi-Processing Modules), but by far the most widely used (at least on *nix platforms) are the three main ones: prefork, worker, and event.  Essentially, they represent the evolution of the Apache web server, and the different ways that the server has been built to handle HTTP requests within the computing constraints of the time over its long (in software terms) history.\n\nprefork\nmpm_prefork is.. well.. it's compatible with everything.  It spins off a number of child processes for serving requests, and the child processes only serve one request at a time.  Because it's got the server process sitting there, ready for action, and not needing to deal with thread marshaling, it's actually faster than the more modern threaded MPMs when you're only dealing with a single request at a time - but concurrent requests suffer, since they're made to wait in line until a server process is free.  Additionally, attempting to scale up in the count of prefork child processes, you'll easily suck down some serious RAM.\nIt's probably not advisable to use prefork unless you need a module that's not thread safe.\nUse if: You need modules that break when threads are used, like mod_php.  Even then, consider using FastCGI and php-fpm.\nDon't use if: Your modules won't break in threading.\nworker\nmpm_worker uses threading - which is a big help for concurrency.  Worker spins off some child processes, which in turn spin off child threads; similar to prefork, some spare threads are kept ready if possible, to service incoming connections.  This approach is much kinder on RAM, since the thread count doesn't have a direct bearing on memory use like the server count does in prefork.  It also handles concurrency much more easily, since the connections just need to wait for a free thread (which is usually available) instead of a spare server in prefork.\nUse if: You're on Apache 2.2, or 2.4 and you're running primarily SSL.\nDon't use if: You really can't go wrong, unless you need prefork for compatibility.\nHowever, note that the treads are attached to connections and not requests - which means that a keep-alive connection always keeps a hold of a thread until it's closed (which can be a long time, depending on your configuration).  Which is why we have..\nevent\nmpm_event is very similar to worker, structurally; it's just been moved from 'experimental' to 'stable' status in Apache 2.4.  The big difference is that it uses a dedicated thread to deal with the kept-alive connections, and hands requests down to child threads only when a request has actually been made (allowing those threads to free back up immediately after the request is completed).  This is great for concurrency of clients that aren't necessarily all active at a time, but make occasional requests, and when the clients might have a long keep-alive timeout.\nThe exception here is with SSL connections; in that case, it behaves identically to worker (gluing a given connection to a given thread until the connection closes).\nUse if: You're on Apache 2.4 and like threads, but you don't like having threads waiting for idle connections.  Everyone likes threads!\nDon't use if: You're not on Apache 2.4, or you need prefork for compatibility.\n\nIn today's world of slowloris, AJAX, and browsers that like to multiplex 6 TCP connections (with keep-alive, of course) to your server, concurrency is an important factor in making your server scale and scale well.  Apache's history has tied it down in this regard, and while it's really still not up to par with the likes of nginx or lighttpd in terms of resource usage or scale, it's clear that the development team is working toward building a web server that's still relevant in today's high-request-concurrency world.",
        "url": "https://serverfault.com/questions/383526/how-do-i-select-which-apache-mpm-to-use"
    },
    {
        "title": "Why do I need Nginx and something like Gunicorn?",
        "question": "I'm looking for an overly simplified answer to the following question. I'm trying to build a foundational understanding of how Nginx works alongside something like Gunicorn.\nDo I need both Nginx and something like Gunicorn to deploy Django apps on Nginx?\nIf so, what actually handles the HTTP requests?\nPs. I don't want to use Apache and mod_wsgi!",
        "top_answer": "Overly simplified: You need something that executes Python but Python isn't the best at handling all types of requests.\n[disclaimer: I'm a Gunicorn developer]\nLess simplified: Regardless of what app server you use (Gunicorn, mod_wsgi, mod_uwsgi, cherrypy) any sort of non-trivial deployment will have something upstream that will handle the requests that your Django app should not be handling. Trivial examples of such requests are serving static assets (images/css/js).\nThis results in two first tiers of the classic \"three tier architecture\". Ie, the webserver (Nginx in your case) will handle many requests for images and static resources. Requests that need to be dynamically generated will then be passed on to the application server (Gunicorn in your example). (As an aside, the third of the three tiers is the database)\nHistorically speaking, each of these tiers would be hosted on separate machines (and there would most likely be multiple machines in the first two tiers, ie: 5 web servers dispatch requests to two app servers which in turn query a single database).\nIn the modern era we now have applications of all shapes and sizes. Not every weekend project or small business site actually needs the horsepower of multiple machines and will run quite happily on a single box. This has spawned new entries into the array of hosting solutions. Some solutions will marry the app server to the web server (Apache httpd + mod_wsgi, Nginx + mod_uwsgi, etc). And its not at all uncommon to host the database on the same machine as one of these web/app server combinations.\nNow in the case of Gunicorn, we made a specific decision (copying from Ruby's Unicorn) to keep things separate from Nginx while relying on Nginx's proxying behavior. Specifically, if we can assume that Gunicorn will never read connections directly from the internet, then we don't have to worry about clients that are slow. This means that the processing model for Gunicorn is embarrassingly simple.\nThe separation also allows Gunicorn to be written in pure Python which minimizes the cost of development while not significantly impacting performance. It also allows users the ability to use other proxies (assuming they buffer correctly).\nAs to your second question about what actually handles the HTTP request, the simple answer is Gunicorn. The complete answer is both Nginx and Gunicorn handle the request. Basically, Nginx will receive the request and if it's a dynamic request (generally based on URL patterns) then it will give that request to Gunicorn, which will process it, and then return a response to Nginx which then forwards the response back to the original client.\nSo in closing, yes. You need both Nginx and Gunicorn (or something similar) for a proper Django deployment. If you're specifically looking to host Django with Nginx, then I would investigate Gunicorn, mod_uwsgi, and maybe CherryPy as candidates for the Django side of things.",
        "url": "https://serverfault.com/questions/331256/why-do-i-need-nginx-and-something-like-gunicorn"
    },
    {
        "title": "command for checking Apache configuration",
        "question": "I'm looking for a command that checks the validity of the config files in Apache server on both Debian and RHEL distros.  I need to do this prior to restart, so there will be no downtime.",
        "top_answer": "Check: http://httpd.apache.org/docs/2.2/programs/apachectl.html\napachectl configtest\n",
        "url": "https://serverfault.com/questions/232145/command-for-checking-apache-configuration"
    },
    {
        "title": "How to remove an image tag in Docker without removing the image itself?",
        "question": "Let's say I want to tag a Docker image, and make a typo. How do I remove the tag without removing the image itself? Neither the manpages nor the Docker documentation mention removing tags.\ndocker tag 0e5574283393 my-imaj\ndocker tag 0e5574283393 my-image\n# docker untag my-imaj # There is no \"docker untag\"!\n",
        "top_answer": "If your image is tagged with more than one tag, then docker rmi will remove the tag, but not the image.\nSo in your example ...\n# docker rmi my-imaj\n\n... will remove that tag and leave the image present with the other correct tag.",
        "url": "https://serverfault.com/questions/703344/how-to-remove-an-image-tag-in-docker-without-removing-the-image-itself"
    },
    {
        "title": "Finding out what user Apache is running as?",
        "question": "I want to secure a file upload directory on my server as described beautifully here, but I have one problem before I can follow these instructions. I don't know what user Apache is running as.\nI've found a suggestion that you can look in httpd.conf and there will be a \"User\" line, but there is no such line in my httpd.conf file, so I guess Apache is running as the default user. I can't find out what that is, though.\nSo, my question is (are):\n\nhow do I find out what the default\nuser is\ndo I need to change the default user\nif the answer is yes and I change the\ndefault user by editing httpd.conf,\nis it likely to screw anything up?\n\nThanks!",
        "top_answer": "ps aux | egrep '(apache|httpd)' typically will show what apache is running as.\nUsually you do not need to change the default user, \"nobody\" or \"apache\" are typically fine users.  As long as its not \"root\" ;)\nedit: more accurate command for catching apache binaries too",
        "url": "https://serverfault.com/questions/125865/finding-out-what-user-apache-is-running-as"
    },
    {
        "title": "What's the difference between IP address 0.0.0.0 and 127.0.0.1?",
        "question": "I know that 127.0.0.1 ~ 127.255.255.254 are the loopback IP addresses for most modern operating systems, and these IP addresses can be used to refer to our own computer.\nBut what's 0.0.0.0? It seems it also refers to the local computer, so what's the difference?\nAnd, could you explain the following IP connections for me:\n",
        "top_answer": "The only thing is that you're not saying \"all addresses should have access\" -- that's done in your firewall(s) and/or the server software and/or other security layers like tcpwrappers.\n0.0.0.0, in this context, means \"all IP addresses on the local machine\" (in fact probably, \"all IPv4 addresses on the local machine\").  So, if your web server machine has two IP addresses, 192.168.1.1 and 10.1.2.1, and you allow a web server daemon like Apache to listen on 0.0.0.0, it will be reachable at both of those IP addresses.  But only to what can contact those IP addresses and the web port(s).\nNote that, in a different context (routing) 0.0.0.0 usually means the default route (the route to \"the rest of\" the internet, aside from routes in your local network etc.).",
        "url": "https://serverfault.com/questions/78048/whats-the-difference-between-ip-address-0-0-0-0-and-127-0-0-1"
    },
    {
        "title": "Moving an already-running process to Screen",
        "question": "My office job routinely sees me connected to a Linux box via VNC.\nSometimes I start a remote job on the console, and realize later that it runs much longer than expected. (Should have started that one under Screen in the first place...)\nI don't want to keep my workstation running overnight just to keep the VNC session open; I would like  to move that already-running remote job into a Screen session (on the remote box), so I can power down the workstation (and reconnect next morning).\nHow can this be done, if at all?",
        "top_answer": "Have a look at reptyr, which does exactly that.  The github page has all the information.\n\nreptyr - A tool for \"re-ptying\" programs.\nreptyr is a utility for taking an\n  existing running program and attaching\n  it to a new terminal. Started a\n  long-running process over ssh, but\n  have to leave and don't want to\n  interrupt it? Just start a screen, use\n  reptyr to grab it, and then kill the\n  ssh session and head on home.\nUSAGE\nreptyr PID\n\"reptyr PID\" will grab the process\n  with id PID and attach it to your\n  current terminal.\nAfter attaching, the process will take\n  input from and write output to the new\n  terminal, including ^C and ^Z.\n  (Unfortunately, if you background it,\n  you will still have to run \"bg\" or\n  \"fg\" in the old terminal. This is\n  likely impossible to fix in a\n  reasonable way without patching your\n  shell.)\n",
        "url": "https://serverfault.com/questions/55880/moving-an-already-running-process-to-screen"
    },
    {
        "title": "how to download the ssl certificate from a website?",
        "question": "I want to download the ssl certificate from, say https://www.google.com, using wget or any other commands. Any unix command line? wget or openssl?",
        "top_answer": "In order to download the certificate, you need to use the client built into openssl like so:\n</dev/null openssl s_client -connect $HOST:$PORTNUMBER -servername $SERVERNAME \\\n    | openssl x509 > /tmp/$SERVERNAME.cert\n\nThat will save the certificate to /tmp/$SERVERNAME.cert.\nThe -servername is used to select the correct certificate when multiple are presented, in the case of SNI.\nYou can use -showcerts if you want to download all the certificates in the chain. But if you just want to download the server certificate, there is no need to specify -showcerts.  The x509 at the end will strip out the intermediate certs, you will need to use sed -n '/-----BEGIN/,/-----END/p' instead of the x509 at the end.\n</dev/null indicates that nothing should be sent to the server, so that the connection is released.\nopenssl x509 removes information about the certificate chain and connection details. This is the preferred format to import the certificate into other keystores.",
        "url": "https://serverfault.com/questions/139728/how-to-download-the-ssl-certificate-from-a-website"
    },
    {
        "title": "Can I nohup/screen an already-started process?",
        "question": "I'm doing some test-runs of long-running data migration scripts, over SSH. Let's say I start running a script around 4 PM; now, 6 PM rolls around, and I'm cursing myself for not doing this all in screen.\nIs there any way to \"retroactively\" nohup a process, or do I need to leave my computer online all night? If it's not possible to attach screen to/nohup a process I've already started, then why? Something to do with how parent/child proceses interact? (I won't accept a \"no\" answer that doesn't at least address the question of 'why' -- sorry ;) )",
        "top_answer": "If you're using Bash, you can run disown -h job\n\ndisown\ndisown [-ar] [-h] [jobspec ...]\n\nWithout options, each jobspec is removed from the table of active jobs.\n  If the -h option is given, the job\n  is not removed from the table, but is\n  marked so that SIGHUP is not sent to\n  the job if the shell receives a\n  SIGHUP. If jobspec is not present, and\n  neither the -a nor -r option is\n  supplied, the current job is used. If\n  no jobspec is supplied, the -a\n  option means to remove or mark all\n  jobs; the -r option without a\n  jobspec argument restricts operation\n  to running jobs.\n",
        "url": "https://serverfault.com/questions/24425/can-i-nohup-screen-an-already-started-process"
    },
    {
        "title": "What is archive mode in rsync?",
        "question": "I know you can use -a or --archive to activate archive mode when using rsync. Unfortunately, I have no idea what archive mode is supposed to do, and the man page is not at all explicit about what this is:\n\nequals -rlptgoD (no -H,-A,-X)\n\nCan you explain what those options (rlptgoD) mean and what's the behaviour of rsync when I use them?",
        "top_answer": "It's all of these:\n\n-r, --recursive             recurse into directories\n-l, --links                 copy symlinks as symlinks\n-p, --perms                 preserve permissions\n-t, --times                 preserve modification times\n-g, --group                 preserve group\n-o, --owner                 preserve owner (super-user only)\n-D                          same as --devices --specials\n--devices               preserve device files (super-user only)\n--specials              preserve special files\n\nIt excludes:\n\n-H, --hard-links            preserve hard links\n-A, --acls                  preserve ACLs (implies -p)\n-X, --xattrs                preserve extended attributes\n\nIt's perfect for backups. My \"default\" set of switches is -avzP - archive mode, be verbose, use compression, preserve partial files, display progress.\nNote: Invariably when the descriptions say \"preserve\", it means make the destination be like the source.",
        "url": "https://serverfault.com/questions/141773/what-is-archive-mode-in-rsync"
    },
    {
        "title": "Do systemd unit files have to be reloaded when modified?",
        "question": "Let's say I write a mine.service file. Then I use systemctl enable mine.service.\nIf I later decide to edit mine.service, do I have to tell systemd that mine.service was changed? If so, how do I do that?",
        "top_answer": "After you make changes to your unit file, you should run systemctl daemon-reload, as outlined here.\n\ndaemon-reload\n  Reload systemd manager configuration. This will rerun all generators (see systemd.generator(7)), reload all unit files, and recreate the entire dependency tree. While the daemon is being reloaded, all sockets systemd listens on behalf of user configuration will stay accessible.\n\nYou can then restart (or reload) your service as you desire with \nsystemctl restart your-service-name\n\n(daemon-reload won't reload/restart the services themselves, just makes systemd aware of the new configuration)",
        "url": "https://serverfault.com/questions/700862/do-systemd-unit-files-have-to-be-reloaded-when-modified"
    },
    {
        "title": "Redirect, Change URLs or Redirect HTTP to HTTPS in Apache - Everything You Ever Wanted to Know About mod_rewrite Rules but Were Afraid to Ask",
        "question": "\nThis is a Canonical Question about Apache's mod_rewrite.\n\nChanging a request URL or redirecting users to a different URL than the one they originally requested is done using mod_rewrite. This includes such things as:\n\nChanging HTTP to HTTPS (or the other way around)\nChanging a request to a page which no longer exist to a new replacement.\nModifying a URL format (such as ?id=3433 to /id/3433 )\nPresenting a different page based on the browser, based on the referrer, based on anything possible under the moon and sun.\nAnything you want to mess around with URL\n\nEverything You Ever Wanted to Know about Mod_Rewrite Rules but Were Afraid to Ask!\nHow can I become an expert at writing mod_rewrite rules?\n\nWhat is the fundamental format and structure of mod_rewrite rules?\nWhat form/flavor of regular expressions do I need to have a solid grasp of?\nWhat are the most common mistakes/pitfalls when writing rewrite rules?\nWhat is a good method for testing and verifying mod_rewrite rules?\nAre there SEO or performance implications of mod_rewrite rules I should be aware of?\nAre there common situations where mod_rewrite might seem like the right tool for the job but isn't?\nWhat are some common examples?\n\nA place to test your rules\nThe htaccess tester web site is a great place to play around with your rules and test them. It even shows the debug output so you can see what matched and what did not.",
        "top_answer": "mod_rewrite syntax order\nmod_rewrite has some specific ordering rules that affect processing. Before anything gets done, the RewriteEngine On directive needs to be given as this turns on mod_rewrite processing. This should be before any other rewrite directives.\nRewriteCond preceding RewriteRule makes that ONE rule subject to the conditional. Any following RewriteRules will be processed as if they were not subject to conditionals.\nRewriteEngine On\nRewriteCond %{HTTP_REFERER}          ^https?://serverfault\\.com(/|$)\nRewriteRule $/blog/(.*)\\.html        $/blog/$1.sf.html\n\nIn this simple case, if the HTTP referrer is from serverfault.com, redirect blog requests to special serverfault pages (we're just that special). However, if the above block had an extra RewriteRule line:\nRewriteEngine On\nRewriteCond %{HTTP_REFERER}          ^https?://serverfault\\.com(/|$)\nRewriteRule $/blog/(.*)\\.html        $/blog/$1.sf.html\nRewriteRule $/blog/(.*)\\.jpg         $/blog/$1.sf.jpg\n\nAll .jpg files would go to the special serverfault pages, not just the ones with a referrer indicating it came from here. This is clearly not the intent of the how these rules are written. It could be done with multiple RewriteCond rules:\nRewriteEngine On\nRewriteCond %{HTTP_REFERER}          ^https?://serverfault\\.com(/|$)\nRewriteRule ^/blog/(.*)\\.html        /blog/$1.sf.html\nRewriteCond %{HTTP_REFERER}          ^https?://serverfault\\.com(/|$)\nRewriteRule ^/blog/(.*)\\.jpg         /blog/$1.sf.jpg\n\nBut probably should be done with some trickier replacement syntax.\nRewriteEngine On\nRewriteCond %{HTTP_REFERER}                ^https?://serverfault\\.com(/|$)\nRewriteRule ^/blog/(.*)\\.(html|jpg)        /blog/$1.sf.$2\n\nThe more complex RewriteRule contains the conditionals for processing. The last parenthetical, (html|jpg) tells RewriteRule to match for either html or jpg, and to represent the matched string as $2 in the rewritten string. This is logically identical to the previous block, with two RewriteCond/RewriteRule pairs, it just does it on two lines instead of four.\nMultiple RewriteCond lines are implicitly ANDed, and can be explicitly ORed. To handle referrers from both ServerFault and Super User (explicit OR):\nRewriteEngine On\nRewriteCond %{HTTP_REFERER}                ^https?://serverfault\\.com(/|$)    [OR]\nRewriteCond %{HTTP_REFERER}                ^https?://superuser\\.com(/|$)\nRewriteRule ^/blog/(.*)\\.(html|jpg)        /blog/$1.sf.$2\n\nTo serve ServerFault referred pages with Chrome browsers (implicit AND):\nRewriteEngine On\nRewriteCond %{HTTP_REFERER}                ^https?://serverfault\\.com(/|$)\nRewriteCond %{HTTP_USER_AGENT}             ^Mozilla.*Chrome.*$\nRewriteRule ^/blog/(.*)\\.(html|jpg)        /blog/$1.sf.$2\n\n\nRewriteBase is also order specific as it specifies how following RewriteRule directives handle their processing. It is very useful in .htaccess files. If used, it should be the first directive under \"RewriteEngine on\" in an .htaccess file. Take this example:\nRewriteEngine On\nRewriteBase /blog\nRewriteCond %{HTTP_REFERER}           ^https?://serverfault\\.com(/|$)\nRewriteRule ^(.*)\\.(html|jpg)         $1.sf.$2\n\nThis is telling mod_rewrite that this particular URL it is currently handling was arrived by way of http://example.com/blog/ instead of the physical directory path (/home/$Username/public_html/blog) and to treat it accordingly. Because of this, the RewriteRule considers it's string-start to be after the \"/blog\" in the URL. Here is the same thing written two different ways. One with RewriteBase, the other without:\nRewriteEngine On\n\n##Example 1: No RewriteBase##\nRewriteCond %{HTTP_REFERER}                                   ^https?://serverfault\\.com(/|$)\nRewriteRule /home/assdr/public_html/blog/(.*)\\.(html|jpg)     $1.sf.$2\n\n##Example 2: With RewriteBase##\nRewriteBase /blog\nRewriteCond %{HTTP_REFERER}           ^https?://serverfault\\.com(/|$)\nRewriteRule ^(.*)\\.(html|jpg)         $1.sf.$2\n\nAs you can see, RewriteBase allows rewrite rules to leverage the web-site path to content rather than the web-server, which can make them more intelligible to those who edit such files. Also, they can make the directives shorter, which has an aesthetic appeal.\n\nRewriteRule matching syntax\nRewriteRule itself has a complex syntax for matching strings. I'll cover the flags (things like [PT]) in another section. Because Sysadmins learn by example more often than by reading a man-page I'll give examples and explain what they do.\nRewriteRule ^/blog/(.*)$    /newblog/$1\n\nThe .* construct matches any single character (.) zero or more times (*). Enclosing it in parenthesis tells it to provide the string that was matched as the $1 variable.\nRewriteRule ^/blog/.*/(.*)$  /newblog/$1\n\nIn this case, the first .* was NOT enclosed in parens so isn't provided to the rewritten string. This rule removes a directory level on the new blog-site. (/blog/2009/sample.html becomes /newblog/sample.html).\nRewriteRule ^/blog/(2008|2009)/(.*)$   /newblog/$2\n\nIn this case, the first parenthesis expression sets up a matching group. This becomes $1, which is not needed and therefore not used in the rewritten string.\nRewriteRule ^/blog/(2008|2009)/(.*)$   /newblog/$1/$2\n\nIn this case, we use $1 in the rewritten string.\nRewriteRule ^/blog/(20[0-9][0-9])/(.*)$   /newblog/$1/$2\n\nThis rule uses a special bracket syntax that specifies a character range. [0-9] matches the numerals 0 through 9. This specific rule will handle years from 2000 to 2099.\nRewriteRule ^/blog/(20[0-9]{2})/(.*)$  /newblog/$1/$2\n\nThis does the same thing as the previous rule, but the {2} portion tells it to match the previous character (a bracket expression in this case) two times.\nRewriteRule ^/blog/([0-9]{4})/([a-z]*)\\.html   /newblog/$1/$2.shtml\n\nThis case will match any lower-case letter in the second matching expression, and do so for as many characters as it can. The \\. construct tells it to treat the period as an actual period, not the special character it is in previous examples. It will break if the file-name has dashes in it, though.\nRewriteRule ^/blog/([0-9]{4})/([-a-z]*)\\.html  /newblog/$1/$2.shtml\n\nThis traps file-names with dashes in them. However, as - is a special character in bracket expressions, it has to be the first character in the expression.\nRewriteRule ^/blog/([0-9]{4})/([-0-9a-zA-Z]*)\\.html   /newblog/$1/$2.shtml\n\nThis version traps any file name with letters, numbers or the - character in the file-name. This is how you specify multiple character sets in a bracket expression.\n\nRewriteRule flags\nThe flags on rewrite rules have a host of special meanings and usecases. \nRewriteRule ^/blog/([0-9]{4})/([-a-z]*).\\html  /newblog/$1/$2.shtml  [L]\n\nThe flag is the [L] at the end of the above expression. Multiple flags can be used, separated by a comma. The linked documentation describes each one, but here they are anyway:\nL = Last. Stop processing RewriteRules once this one matches. Order counts!\nC = Chain. Continue processing the next RewriteRule. If this rule doesn't match, then the next rule won't be executed. More on this later.\nE = Set environmental variable. Apache has various environmental variables that can affect web-server behavior. \nF = Forbidden. Returns a 403-Forbidden error if this rule matches.\nG = Gone. Returns a 410-Gone error if this rule matches.\nH = Handler. Forces the request to be handled as if it were the specified MIME-type.\nN = Next. Forces the rule to start over again and re-match. BE CAREFUL! Loops can result.\nNC = No case. Allows jpg to match both jpg and JPG. \nNE = No escape. Prevents the rewriting of special characters (. ? # & etc) into their hex-code equivalents.\nNS = No subrequests. If you're using server-side-includes, this will prevent matches to the included files.\nP = Proxy. Forces the rule to be handled by mod_proxy. Transparently provide content from other servers, because your web-server fetches it and re-serves it. This is a dangerous flag, as a poorly written one will turn your web-server into an open-proxy and That is Bad.\nPT = Pass Through. Take into account Alias statements in RewriteRule matching.\nQSA = QSAppend. When the original string contains a query (http://example.com/thing?asp=foo) append the original query string to the rewritten string. Normally it would be discarded. Important for dynamic content.\nR = Redirect. Provide an HTTP redirect to the specified URL. Can also provide exact redirect code [R=303]. Very similar to RedirectMatch, which is faster and should be used when possible.\nS = Skip. Skip this rule.\nT = Type. Specify the mime-type of the returned content. Very similar to the AddType directive.\nYou know how I said that RewriteCond applies to one and only one rule? Well, you can get around that by chaining.\nRewriteEngine On\nRewriteCond %{HTTP_REFERER}          ^https?://serverfault\\.com(/|$)\nRewriteRule ^/blog/(.*)\\.html        /blog/$1.sf.html     [C]\nRewriteRule ^/blog/(.*)\\.jpg         /blog/$1.sf.jpg\n\nBecause the first RewriteRule has the Chain flag, the second rewrite-rule will execute when the first does, which is when the previous RewriteCond rule is matched. Handy if Apache regular-expressions make your brain hurt. However, the all-in-one-line method I point to in the first section is faster from an optimization point of view.\nRewriteRule ^/blog/([0-9]{4})/([-0-9a-zA-Z]*)\\.html   /newblog/$1/$2.shtml\n\nThis can be made simpler through flags:\nRewriteRule ^/blog/([0-9]{4})/([-0-9a-z]*)\\.html   /newblog/$1/$2.shtml   [NC]\n\nAlso, some flags also apply to RewriteCond. Notably, NoCase.\nRewriteCond %{HTTP_REFERER}        ^https?://serverfault\\.com(/|$)     [NC]\n\nWill match \"ServerFault.com\"",
        "url": "https://serverfault.com/questions/214512/redirect-change-urls-or-redirect-http-to-https-in-apache-everything-you-ever"
    },
    {
        "title": "Linux command to inspect TXT records of a domain [closed]",
        "question": "Is there a linux shell command that I can use to inspect the TXT records of a domain?",
        "top_answer": "Dig will also do it quite nicely: dig -t txt example.com and if you add the +short option you get just the txt record in quote marks with no other cruft.",
        "url": "https://serverfault.com/questions/148721/linux-command-to-inspect-txt-records-of-a-domain"
    },
    {
        "title": "List all DNS records in a domain using dig?",
        "question": "My company runs an internal DNS for mycompany.example \nThere is a machine on the network that I need to find, but I\u2019ve forgotten its name. If I could see a list, it would probably jog my memory.\nHow can I list all of the domain records for mycompany.example?",
        "top_answer": "Answer\nThe short answer to your specific question of listing CNAMEs is that you cannot without permission to do zone transfers (see How to list all CNAME records for a given domain?).\nThat said, if your company's DNS server still supports the ANY query, you can use dig to list the other records by doing:\ndig +noall +answer +multiline yourdomain.yourtld any \n\nThese ... +noall +answer +multiline ... are strictly optional and are simply output formatting flags to make the output more easily human readable (see dig man page).\nExample\n$ dig +noall +answer +multiline bad.horse any\n\nReturns:\nbad.horse.              7200 IN A 162.252.205.157\nbad.horse.              7200 IN CAA 0 issue \"letsencrypt.org\"\nbad.horse.              7200 IN CAA 0 iodef \"mailto:[email\u00a0protected]\"\nbad.horse.              7200 IN MX 10 mx.sandwich.net.\nbad.horse.              7200 IN NS a.sn1.us.\nbad.horse.              7200 IN NS b.sn1.us.\nbad.horse.              7200 IN SOA a.sn1.us. n.sn1.us. (\n                                2017032202 ; serial\n                                1200       ; refresh (20 minutes)\n                                180        ; retry (3 minutes)\n                                1209600    ; expire (2 weeks)\n                                60         ; minimum (1 minute)\n                                )\n\nCaveats (RFC8482)\nNote that, since around 2019, most public DNS servers have stopped answering most DNS ANY queries usefully. For background on that, see: RFC8482 - Saying goodbye to ANY\nIf ANY queries do not enumerate multiple records, the only option is to request each record type (e.g. A, CNAME, or MX) individually.",
        "url": "https://serverfault.com/questions/138949/list-all-dns-records-in-a-domain-using-dig"
    },
    {
        "title": "How do I add Access-Control-Allow-Origin in NGINX?",
        "question": "How do I set the Access-Control-Allow-Origin header so I can use web-fonts from my subdomain on my main domain?\n\nNotes: \nYou'll find examples of this and other headers for most HTTP servers in the HTML5BP Server Configs projects https://github.com/h5bp/server-configs",
        "top_answer": "Nginx has to be compiled with http://wiki.nginx.org/NginxHttpHeadersModule (default on Ubuntu and some other Linux distros). Then you can do this\nlocation ~* \\.(eot|ttf|woff|woff2)$ {\n    add_header Access-Control-Allow-Origin *;\n}\n",
        "url": "https://serverfault.com/questions/162429/how-do-i-add-access-control-allow-origin-in-nginx"
    },
    {
        "title": "What are the obstacles to providing reliable Internet access and Wi-Fi at large tech conferences? [closed]",
        "question": "Every tech conference I've ever been to, and I've been to a lot, has had absolutely abysmal Wi-Fi and Internet access.\nSometimes it's the DHCP server running out of addresses. Sometimes the backhaul is clearly inadequate. Sometimes there's one router for a ballroom with 3000 people. But it's always SOMETHING. It never works.\nWhat are some of the best practices for conference organizers? What questions should they ask the conference venue or ISP to know, in advance, if the Wi-Fi is going to work? What are the most common causes of crappy Wi-Fi at conferences? Are they avoidable, or is Wi-Fi simply not an adequate technology for large conferences? ",
        "top_answer": "(For those that are interested, I have finally written up my 2009 report on the wireless at PyCon).\nI have done the wireless for the PyCon conference most of the years since we moved from George Washington University into hotels, so I have some ideas about this, which have been proven in battle -- though only with around a thousand users.\nOne thing I hear a lot of people talking about in this discussion is \"open air coverage in a ballroom\".  One theory I operate under is that the ballroom is NOT an open air environment.  Human bodies soak up 802.11b/g and 802.11a quite nicely.\nHere are some of my thoughts, but more details are available in my conference reports if you search google for \"pycon wireless\" -- the tummy.com links are what you want.\nI use just the non-overlapping channels, and spread the APs out.  For 802.11b/g, I run the radios at the lowest power settings.  For 802.11a I run it at the higest power setting because we have so many channels.\nI try to keep the APs fairly low, so the bodies can help reduce interference between APs on the same channel.\nI set all the APs to the same ESSID so that people can \"roam\" to different APs as loads (number of associated clients) go up or coverage goes down (more people coming in, etc).\nLots and lots of APs.  The first year we had the hotel do the networking, they eventually brought in 6 APs, but they had started with only a couple.  Despite that we had told them that we would be heavily using their wireless.  But we also had other problems like the DHCP server giving out leases with a gateway in a different network than the address.  (Calls to support resulted in \"I'll just reboot everything.\").\nWe are running relatively inexpensive D-Link dual-radio APs, costing around $100 or $200 each.  We just haven't really had the budget to buy 20 to 40 of the $600+ high end APs.  These D-Link APs have worked surprisingly well.\nIn 2009 we had a hell of a problem with netbooks.  Something about the radios in these just stinks for use at this sort of conference.  I've heard reports of people putting Intel wireless cards in the Netbooks and getting much better performance.  At PyCon 2009, my netbook couldn't get a reliable connection after the conference started, but my ThinkPad had no problems.  I heard similar reports from people with Mac and other \"real\" laptops, but the cheapest hardware just was not working.\nI have NOT done anything with directional antennas.  I was expecting to need them, but so far it has worked out well enough.\nMost hotels cannot provide enough bandwidth.  Don't worry though, there are lots of terrestrial wireless providers that can provide 100mbps.  I'm not talking about the places that run 802.11g from some tower, but people with real, serious radios and backhaul to cope with it.\nOver the last several years we haven't really had much in the way of wired ports, mostly because of budget and volunteer effort required to cable all those locations.  In 2010 we expect to have quite a few wired ports.  I like the idea of wiring every seat for wired, but I would doubt we'll be able to cover even 10% simply due to the effort required to wire and maintain such a network.  Getting people off the wireless is great.\nGetting people off the 802.11b frequencies is good as well.  Most people talking about since Joel has brought it up have been saying things like \"3 non-overlapping channels\", which is true for the 2.4GHz spectrum.  However, we have seen a HUGE move towards the 5.2GHz spectrum.  The first year I ran the network (2006?), we had around 25% usage.  In 2008 we had over 60% in 5.2GHz.\nSo, yes, running wireless with thousands of people requires some thought.  But, giving it some thought seems to have resulted in a fairly high level of satisfaction.\nSean",
        "url": "https://serverfault.com/questions/72767/what-are-the-obstacles-to-providing-reliable-internet-access-and-wi-fi-at-large"
    },
    {
        "title": "Useful Command-line Commands on Windows",
        "question": "The aim for this Wiki is to promote using a command to open up commonly used applications without having to go through many mouse clicks - thus saving time on monitoring and troubleshooting Windows machines.\nAnswer entries need to specify\n\nApplication name\nCommands\nScreenshot (Optional)\n\nShortcut to commands\n\n&& - Command Chaining\n%SYSTEMROOT%\\System32\\rcimlby.exe -LaunchRA - Remote Assistance (Windows XP)\nappwiz.cpl - Programs and Features (Formerly Known as \"Add or Remove Programs\")\nappwiz.cpl @,2 - Turn Windows Features On and Off (Add/Remove Windows Components pane)\narp - Displays and modifies the IP-to-Physical address translation tables used by address resolution protocol (ARP)\nat - Schedule tasks either locally or remotely without using Scheduled Tasks\nbootsect.exe - Updates the master boot code for hard disk partitions to switch between BOOTMGR and NTLDR\ncacls - Change Access Control List (ACL) permissions on a directory, its subcontents, or files\ncalc - Calculator\nchkdsk - Check/Fix the disk surface for physical errors or bad sectors\ncipher - Displays or alters the encryption of directories [files] on NTFS partitions\ncleanmgr.exe - Disk Cleanup\nclip - Redirects output of command line tools to the Windows clipboard\ncls - clear the command line screen\ncmd /k - Run command with command extensions enabled\ncolor - Sets the default console foreground and background colors in console\ncommand.com - Default Operating System Shell\ncompmgmt.msc - Computer Management\ncontrol.exe /name Microsoft.NetworkAndSharingCenter - Network and Sharing Center\ncontrol keyboard - Keyboard Properties\ncontrol mouse(or main.cpl) - Mouse Properties\ncontrol sysdm.cpl,@0,3 - Advanced Tab of the System Properties dialog\ncontrol userpasswords2 - Opens the classic User Accounts dialog\ndesk.cpl - opens the display properties\ndevmgmt.msc - Device Manager\ndiskmgmt.msc - Disk Management\ndiskpart - Disk management from the command line\ndsa.msc - Opens active directory users and computers\ndsquery - Finds any objects in the directory according to criteria\ndxdiag - DirectX Diagnostic Tool\neventvwr - Windows Event Log (Event Viewer)\nexplorer . - Open explorer with the current folder selected.\nexplorer /e, . - Open explorer, with folder tree, with current folder selected.\nF7 - View command history\nfind - Searches for a text string in a file or files\nfindstr - Find a string in a file\nfirewall.cpl - Opens the Windows Firewall settings\nfsmgmt.msc - Shared Folders\nfsutil - Perform tasks related to FAT and NTFS file systems\nftp - Transfers files to and from a computer running an FTP server service\ngetmac - Shows the mac address(es) of your network adapter(s)\ngpedit.msc - Group Policy Editor\ngpresult - Displays the Resultant Set of Policy (RSoP) information for a target user and computer\nhttpcfg.exe - HTTP Configuration Utility\niisreset - To restart IIS\nInetMgr.exe - Internet Information Services (IIS) Manager 7\nInetMgr6.exe - Internet Information Services (IIS) Manager 6\nintl.cpl - Regional and Language Options\nipconfig - Internet protocol configuration\nlusrmgr.msc - Local Users and Groups Administrator\nmsconfig - System Configuration\nnotepad - Notepad? ;)\nmmsys.cpl - Sound/Recording/Playback properties\nmode - Configure system devices\nmore - Displays one screen of output at a time\nmrt - Microsoft Windows Malicious Software Removal Tool\nmstsc.exe - Remote Desktop Connection\nnbstat - displays protocol statistics and current TCP/IP connections using NBT\nncpa.cpl - Network Connections\nnetsh - Display or modify the network configuration of a computer that is currently running\nnetstat - Network Statistics\nnet statistics - Check computer up time\nnet stop - Stops a running service.\nnet use - Connects a computer to or disconnects a computer from a shared resource,  displays information about computer connections, or mounts a local share with different privileges (documentation)\nodbcad32.exe - ODBC Data Source Administrator\npathping - A traceroute that collects detailed packet loss stats\nperfmon - Opens Reliability and Performance Monitor\nping - Determine whether a remote computer is accessible over the network\npowercfg.cpl - Power management control panel applet\nqfecheck - Shows installed Hotfixes applied to the server/workstation.\nquser - Display information about user sessions on a terminal server\nqwinsta - See disconnected remote desktop sessions\nreg.exe - Console Registry Tool for Windows\nregedit - Registry Editor\nrasdial - Connects to a VPN or a dialup network\nrobocopy - Backup/Restore/Copy large amounts of files reliably\nrsop.msc - Resultant Set of Policy (shows the combined effect of all group policies active on the current system/login)\nrunas - Run specific tools and programs with different permissions than the user's current logon provides\nsc - Manage anything you want to do with services.\nschtasks - Enables an administrator to create, delete, query, change, run and end scheduled tasks on a local or remote system.\nsecpol.msc - Local Security Settings\nservices.msc - Services control panel\nset - Displays, sets, or removes cmd.exe environment variables.\nset DIRCMD - Preset dir parameter in cmd.exe\nstart - Starts a separate window to run a specified program or command\nstart. - opens the current directory in the Windows Explorer.\nshutdown.exe - Shutdown or Reboot a local/remote machine\nsubst.exe - Associates a path with a drive letter, including local drives\nsysteminfo -Displays a comprehensive information about the system\ntaskkill - terminate tasks by process id (PID) or image name\ntasklist.exe - List Processes on local or a remote machine\ntaskmgr.exe - Task Manager\ntelephon.cpl - Telephone and Modem properties\ntimedate.cpl - Date and Time\ntitle - Change the title of the CMD window you have open\ntracert - Trace route\nwhoami /all - Display Current User/Group/Privilege Information\nwmic - Windows Management Instrumentation Command-line\nwinver.exe - Find Windows Version\nwscui.cpl - Windows Security Center\nwuauclt.exe - Windows Update AutoUpdate Client\n",
        "top_answer": "A little known one is\ngetmac\n\nIt shows the MAC address(es) of your network adapter(s).\n",
        "url": "https://serverfault.com/questions/3780/useful-command-line-commands-on-windows"
    },
    {
        "title": "How do I view the details of a digital certificate .cer file?",
        "question": "I am using Windows and have been given a .cer file. How can I view the details of it?",
        "top_answer": "OpenSSL will allow you to look at it if it is installed on your system, using the OpenSSL x509 tool.\nopenssl x509 -noout -text -in 'cerfile.cer';\n\nThe format of the .CER file might require that you specify a different encoding format to be explicitly called out.\nopenssl x509 -inform pem -noout -text -in 'cerfile.cer';\n\nor\nopenssl x509 -inform der -noout -text -in 'cerfile.cer';\n\nOn Windows systems you can right click the .cer file and select Open. That will then let you view most of the meta data.\nOn Windows you run Windows certificate manager program using certmgr.msc command in the run window. Then you can import your certificates and view details.",
        "url": "https://serverfault.com/questions/215606/how-do-i-view-the-details-of-a-digital-certificate-cer-file"
    },
    {
        "title": "How to assign permissions to ApplicationPoolIdentity account",
        "question": "In IIS 7 on Windows Server 2008, application pools can be run as the \"ApplicationPoolIdentity\" account instead of the NetworkService account.\nHow do I assign permissions to this \"ApplicationPoolIdentity\" account.  It does not appear as a local user on the machine.  It does not appear as a group anywhere.  Nothing remotely like it appears anywhere.  When I browse for local users, groups, and built-in accounts, it does not appear in the list, nor does anything similar appear in the list.  What is going on?\nI'm not the only one with this problem: see Trouble with ApplicationPoolIdentity in IIS 7.5 + Windows 7 for an example.\n\n\"This is unfortunately a limitation of the object picker on Windows Server 2008/Windows Vista - as several people have discovered it already, you can still manipulate the ACL for the app-pool identity using command line tools like icacls.\"",
        "top_answer": "Update: The original question was for Windows Server 2008, but the solution is easier for Windows Server 2008 R2 and Windows Server 2012 (and Windows 7 and 8). You can add the user through the NTFS UI by typing it in directly.  The name is in the format of IIS APPPOOL\\{app pool name}.  For example: IIS APPPOOL\\DefaultAppPool.\nIIS APPPOOL\\{app pool name}\n\nNote: Per comments below, there are two things to be aware of:\n\nEnter the string directly into the \"Select User or Group\" and not in the search field.\nIn a domain environment you need to set the Location to your local computer first.\n\nReference to Microsoft Docs article: Application Pool Identities > Securing Resources\nOriginal response: (for Windows Server 2008) This is a great feature, but as you mentioned it's not fully implemented yet.  You can add the app pool identity from the command prompt with something like icacls, then you can manage it from the GUI.  For example, run something like this from the command prompt: \nicacls c:\\inetpub\\wwwroot /grant \"IIS APPPOOL\\DefaultAppPool\":(OI)(CI)(RX)\n\nThen, in Windows Explorer, go to the wwwroot folder and edit the security permissions. You will see what looks like a group (the group icon) called DefaultAppPool.  You can now edit the permissions.  \nHowever, you don't need to use this at all.  It's a bonus that you can use if you want.  You can use the old way of creating a custom user per app pool and assigning the custom user to disk.  That has full UI support.\nThis SID injection method is nice because it allows you to use a single user but fully isolate each site from each other without having to create unique users for each app pool.  Pretty impressive, and it will be even better with UI support.\nNote: If you are unable to find the application pool user, check to see if the Windows service called Application Host Helper Service is running. It's the service that maps application pool users to Windows accounts.",
        "url": "https://serverfault.com/questions/81165/how-to-assign-permissions-to-applicationpoolidentity-account"
    },
    {
        "title": "How to reply with 200 from Nginx, without serving a file?",
        "question": "I have configured Apache to send back a 200 response without serving any file with this configuration line\nRedirect 200 /hello\n\nCan I do this with Nginx? I don't want to serve a file, I just want the server to respond with a 200 (I'm just logging the request).\nI know I can add an index file and achieve the same thing, but doing it in the config means there's one less thing that can go wrong.",
        "top_answer": "Yes, you can\nlocation / {\n    return 200 'gangnam style!';\n    # because default content-type is application/octet-stream,\n    # browser will offer to \"save the file\"...\n    # if you want to see reply in browser, uncomment next line \n    # add_header Content-Type text/plain;\n}\n",
        "url": "https://serverfault.com/questions/196929/how-to-reply-with-200-from-nginx-without-serving-a-file"
    },
    {
        "title": "How can I run Debian stable but install some packages from testing?",
        "question": "Say you're running a server and you don't want to upgrade to Testing (Squeeze) from Stable (Lenny) to just install a required package or two.\nWhat's the best way of installing only certain packages from Testing?",
        "top_answer": "Many people seem to be afraid of mixing stable with testing, but frankly, testing is fairly stable in its own right, and with proper preferences and solution checking, you can avoid the \"stability drift\" that puts your core packages on the unstable path.\n\"Testing is fairly stable??\", you ask.  Yes.  In order for a package to migrate from unstable to testing, it has to have zero open bugs for 10 consecutive days.  Chances are that, especially for the more popular packages, somebody is going to submit a bug report for an unstable version if something is wrong.\nEven if you don't want to mix the environments, it's still nice to have the option there in case you run into something that requires a newer version than what is in stable.  \nHere's what I recommend for setting this up:\nFirst, create the following files in /etc/apt/preferences.d:\nstable.pref:\n# 500 <= P < 990: causes a version to be installed unless there is a\n# version available belonging to the target release or the installed\n# version is more recent\n\nPackage: *\nPin: release a=stable\nPin-Priority: 900\n\ntesting.pref:\n# 100 <= P < 500: causes a version to be installed unless there is a\n# version available belonging to some other distribution or the installed\n# version is more recent\n\nPackage: *\nPin: release a=testing\nPin-Priority: 400\n\nunstable.pref:\n# 0 < P < 100: causes a version to be installed only if there is no\n# installed version of the package\n\nPackage: *\nPin: release a=unstable\nPin-Priority: 50\n\nexperimental.pref:\n# 0 < P < 100: causes a version to be installed only if there is no\n# installed version of the package\n\nPackage: *\nPin: release a=experimental\nPin-Priority: 1\n\n(Don't be afraid of the unstable/experimental stuff here.  The priorities are low enough that it's never going to automatically install any of that stuff.  Even the testing branch will behave, as it's only going to install the packages you want to be in testing.)\nNow, creating a matching set for /etc/apt/sources.list.d:\nstable.list: Copy from your original /etc/apt/sources.list.  Rename the old file to something like sources.list.orig.\ntesting.list: Same as stable.list, except with testing.\nunstable.list: Same as stable.list, except with unstable, and remove the security lists.\nexperimental.list: Same as unstable.list, except with experimental.\nYou can also add a oldstable in sources.lists.d and preferences.d (use a priority of 1), though this moniker will tend to expire and disappear before the next stable cycle.  In cases like that, you can use http://archive.debian.org/debian/ and \"hardcode\" the Debian version (etch, lenny, etc.).\nTo install the testing version of a package, simply use aptitude install lib-foobar-package/testing, or just jump into aptitude's GUI and select the version inside of the package details (hit enter on the package you're looking at).\nIf you get complaints of package conflicts, look at the solutions first.  In most cases, the first one is going to be \"don't install this version\".  Learn to use the per-package accept/reject resolver choices.  For example, if you're installing foobar-package/testing, and the first solution is \"don't install foobar-package/testing\", then mark that choice as rejected, and the other solutions will never veer to that path again.  In cases like these, you'll probably have to install a few other testing packages.\nIf it's getting too hairy (like it's trying to upgrade libc or the kernel or some other huge core system), then you can either reject those upgrade paths or just back out of the initial upgrade altogether.  Remember that it's only going to upgrade stuff to testing/unstable if you allow it to.\nEDIT: Fixed some priority pins, and updated the list.",
        "url": "https://serverfault.com/questions/22414/how-can-i-run-debian-stable-but-install-some-packages-from-testing"
    },
    {
        "title": "Why does my hostname appear with the address 127.0.1.1 rather than 127.0.0.1 in /etc/hosts?",
        "question": "This may be a bit of a noobish question, but I was taking a look at /etc/hosts on my new Xubuntu install and saw this:\n127.0.0.1 localhost\n127.0.1.1 myhostname\n\nOn most 'nixes I've used, the second line is omitted, and if I want to add my hostname to the hosts file, I'd just do this:\n127.0.0.1 localhost myhostname\n\nIs there a difference between these two files in any practical sense?",
        "top_answer": "There isn't a great deal of difference between the two; 127/8 (eg: 127.0.0.0 => 127.255.255.255) are all bound to the loopback interface.\nThe reason why is documented in the Debian manual in Ch. 5 Network Setup - 5.1.1. The hostname resolution. \nUltimately, it is a bug workaround; the original report is 316099.",
        "url": "https://serverfault.com/questions/363095/why-does-my-hostname-appear-with-the-address-127-0-1-1-rather-than-127-0-0-1-in"
    },
    {
        "title": "Why is \"chmod -R 777 /\" destructive?",
        "question": "\nThis is a Canonical Question about File Permission and Why 777 is \"destructive\".\n\nI'm not asking how to fix this problem, as there are a ton of references of that already on Server Fault (reinstall OS). Why does it do anything destructive at all?\nIf you've ever ran this command you pretty much immediately destroy your operating system. I'm not clear why removing restrictions has any impact on existing processes. For example, if I don't have read access to something and after a quick mistype in the terminal suddenly I now have access well... why does that cause Linux to break?",
        "top_answer": "First of all a minor terminology nitpick: chmod doesn't remove permissions. It CHANGES them.  \n\nNow the meat of the issue -- The mode 777 means \"Anyone can read, write or execute this file\" - You have given permission for anyone to do (effectively) whatever the heck they want.\nNow, why is this bad?\n\nYou've just let everyone read/modify every file on your system.\n\nKiss password security goodbye (anyone can read the shadow file and crack your passwords, but why bother? Just CHANGE the password! It's much easier!).\nKiss security for your binaries goodbye (someone can just write a new login program that lets them in every time).\nKiss your files goodbye: One user misdirects rm -r / and it's all over. The OS was told to let them do whatever they wanted!\n\nYou've pissed off every program that checks permissions on files before starting.\nsudo, sendmail, and a host of others simply will not start any more.  They will examine key file permissions, see they're not what they're supposed to be, and kick back an error message.\nSimilarly ssh will break horribly (key files must have specific permissions,  otherwise they're \"insecure\" and by default SSH will refuse to use them.)\nYou've wiped out the setuid / setgid bits on the programs that had them.\nThe mode 777 is actually 0777.  Among the things in that leading digit are the setuid and setgid bits.\nMost programs which are setuid/setgid have that bit set because they must run with certain privileges.  They're broken now.\nYou've broken /tmp and /var/tmp\nThe other thing in that leading octal digit that got zero'd is the sticky bit -- That which protects files in /tmp (and /var/tmp) from being deleted by people who don't own them.\nThere are (unfortunately) plenty of badly-behaved scripts out there that \"clean up\" by doing an rm -r /tmp/*, and without the sticky bit set on /tmp  you can kiss all the files in that directory goodbye.\nHaving scratch files disappear can really upset some badly-written programs...\nYou've caused havoc in /dev /proc and similar filesystems\nThis is more of an issue on older Unix systems where /dev is a real filesystem, and the stuff it contains are special files created with mknod, as the permissions change will be preserved across reboots, but on any system having your device permissions changing can cause substantial problems, from the obvious security risks (everyone can read every TTY) to the less-obvious potential causes of a kernel panic.\nCredit to @Tonny for pointing out this possibility\nSockets and Pipes may break, or have other problems\nSockets and pipes may break entirely, or be exposed to malicious injection as a result of being made world-writeable.\nCredit to @Tonny for pointing out this possibility\nYou've made every file on your system executable\nA lot of people have . in their PATH environment variable (you shouldn't!) - This could cause an unpleasant surprise as now anyone can drop a file conveniently named like a command (say make or ls, and have a shot at getting you to run their malicious code.\nCredit to @RichHomolka for pointing out this possibility\nOn some systems chmod will reset Access Control Lists (ACLs)\nThis means you may wind up having to re-create all your ACLs in addition to fixing permissions everywhere (and is an actual example of the command being destructive).\nCredit to @JamesYoungman for pointing out this possibility\n\n\nWill the parts of the system which are already running continue to run?  Probably, for a while at least.\nBut the next time you need to launch a program, or restart a service, or heaven forbid REBOOT the box you're in for a world of hurt as #2 and #3 above will rear their ugly heads.",
        "url": "https://serverfault.com/questions/364677/why-is-chmod-r-777-destructive"
    },
    {
        "title": "Can you pass user/pass for HTTP Basic Authentication in URL parameters?",
        "question": "I believe this is not possible, but someone I know insisted that it works. I don't even know what parameters to try, and I haven't found this documented anywhere.\nI tried http://myserver.com/~user=username&password=mypassword but it doesn't work.\nCan you confirm that it's not in fact possible to pass the user/pass via HTTP parameters (GET or POST)?",
        "top_answer": "It is indeed not possible to pass the username and password via query parameters in standard HTTP auth.  Instead, you use a special URL format, like this: http://username:[email\u00a0protected]/ -- this sends the credentials in the standard HTTP \"Authorization\" header.\nIt's possible that whoever you were speaking to was thinking of a custom module or code that looked at the query parameters and verified the credentials.  This isn't standard HTTP auth, though, it's an application-specific thing.",
        "url": "https://serverfault.com/questions/371907/can-you-pass-user-pass-for-http-basic-authentication-in-url-parameters"
    },
    {
        "title": "I've inherited a rat's nest of cabling. What now?",
        "question": "You know, you see pictures like below and sort of chuckle until you actually have to deal with it.\nI have just inherited something that looks like the picture below. The culture of the organization does not tolerate down time very well, yet I have been tasked to 'clean it up'.  The network functions as it is, and there doesn't seem to be rush to get it done, but I will have to tackle the bear at some point. I get the ugly eye when I mention anything about weekends.  \nSo my question goes, is there sort of a structured approach to this problem?\nMy Ideas thus far:\n\nLabel, Label, Label\nMake up my patch cables of desired length ahead of time\nDo each subnet at a time (appears that each subnet are for different physical locations)\nReplace one cable at a time for each subnet\nIt's easier to get forgiveness than permision?\n\n",
        "top_answer": "In no particular order here are some suggestions that have been helpful to me over the years-\n\nCan any of the equipment in those racks be eliminated, upgraded or consolidated?  It's hard to tell what's there, but in my experience these kinds of messes tend to be aggravated by gear that should have been pulled out years ago.\nOnce you've got some idea of the minimum set of equipment then consider how best to lay it out.  The criteria here may vary, but grouping by technology type or business function might make sense.  Clearly the proximity of high density devices (i.e. switches) and patch panels and such will immediately be apparent.\nUse cable management!!!  There are both horizontal and vertical cable management solutions.  Use both - horizontals around patch panels and other significant concentrations, verticals next to switches and to facilitate risers.\nIt's always surprising, but how power cables are routed should be considered.  UPS units in the bottom of racks, PDU selection and diversity all need to be considered before pulling a cable.\nKeep inventory of common cable lengths.  It's late at night and you want to go home.  A 3' cable is what's necessary but the closest you have handy is 5'.  This is how these kinds of messes develop.\nDocumenting is part of the game, but the importance of labeling cannot be overstated.  With clear labels and efficient/clean cabling the number of mistakes will be vastly decreased and troubleshooting simplified.\nLimit who can pull cables!!!  Differing styles and degrees of attention to detail can yield chaos pretty quickly.  \n",
        "url": "https://serverfault.com/questions/420877/ive-inherited-a-rats-nest-of-cabling-what-now"
    },
    {
        "title": "Kubernetes stuck on ContainerCreating",
        "question": "A pod in my Kubernetes cluster is stuck on \"ContainerCreating\" after running a create. How do I see logs for this operation in order to diagnose why it is stuck? kubectl logs doesn't seem to work since the container needs to be in a non-pending state.",
        "top_answer": "kubectl describe pods will list some (probably most but not all) of the events associated with the pod, including pulling of images, starting of containers.",
        "url": "https://serverfault.com/questions/728727/kubernetes-stuck-on-containercreating"
    },
    {
        "title": "How to see stdout of ansible commands?",
        "question": "How do I see stdout for ansible-playbook commands?  -v only shows ansible output, not the individual commands.  It would be great if I could figure out how to do this immediately, so if something fails or hangs I can see why.\ne.g.\n- name: print to stdout\n  action: command echo \"hello\"\n\nwould print\nTASK: [print variable] ******************************************************** \n\nhello\n",
        "top_answer": "I think you can register the result to a variable, then print with debug.\n- name: print to stdout\n  command: echo \"hello\"\n  register: hello\n\n- debug: msg=\"{{ hello.stdout }}\"\n\n- debug: msg=\"{{ hello.stderr }}\"\n",
        "url": "https://serverfault.com/questions/537060/how-to-see-stdout-of-ansible-commands"
    },
    {
        "title": "Can anyone explain precisely what IOWait is?",
        "question": "As much as I have read about iowait, it is still mystery to me.\nI know it's the time spent by the CPU waiting for a IO operations to complete, but what kind of IO operations precisely? What I am also not sure, is why it so important? Can't the CPU just do something else while the IO operation completes, and then get back to processing data?\nAlso what are the right tools to diagnose what process(es) did exactly wait for IO.\nAnd what are the ways to minimize IO wait time?",
        "top_answer": "\nI know it's the time spent by the CPU\n  waiting for a IO operations to\n  complete, but what kind of IO\n  operations precisely? What I am also\n  not sure, is why it so important?\n  Can't the CPU just do something else\n  while the IO operation completes, and\n  then get back to processing data?\n\nYes, the operating system will schedule other processes to run while one is blocked on IO. However inside that process, unless it's using asynchronous IO, it will not progress until whatever IO operation is complete.\n\nAlso what are the right tools to\n  diagnose what process(es) did exactly\n  wait for IO.\n\nSome tools you might find useful\n\niostat, to monitor the service times of your disks\niotop (if your kernel supports it), to monitor the breakdown of IO requests per process\nstrace, to look at the actual operations issued by a process\n\n\nAnd what are the ways to minimize IO\n  wait time?\n\n\nensure you have free physical memory so the OS can cache disk blocks in memory\nkeep your filesystem disk usage below 80% to avoid excessive fragmentation\ntune your filesystem\nuse a battery backed array controller\nchoose good buffer sizes when performing io operations\n",
        "url": "https://serverfault.com/questions/12679/can-anyone-explain-precisely-what-iowait-is"
    },
    {
        "title": "Docker Container time & timezone (will not reflect changes)",
        "question": "Where do Docker containers get their time information? I've created some containers from the basic ubuntu:trusty image, and when I run it and request 'date', I get UTC time. \nFor awhile I got around this by doing the following in my Dockerfile: \nRUN sudo echo \"America/Los_Angeles\" > /etc/timezone\n\nHowever, for some reason that stopped working. Searching online I saw the below suggested:\ndocker run -v /etc/timezone:/etc/timezone [image-name]\n\nBoth these methods correctly set the timezone though!\n$ cat /etc/timezone\nAmerica/Los_Angeles\n$ date\nTue Apr 14 23:46:51 UTC 2015\n\nAnyone know what gives? ",
        "top_answer": "The secret here is that dpkg-reconfigure tzdata simply creates /etc/localtime as a copy, hardlink or symlink (a symlink is preferred) to a file in /usr/share/zoneinfo. So it is possible to do this entirely from your Dockerfile. Consider:\nENV TZ=America/Los_Angeles\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n\nAnd as a bonus, TZ will be set correctly in the container as well.\nThis is also distribution-agnostic, so it works with pretty much any Linux.\nNote: if you are using an alpine based image you have to install the tzdata first. (see this issue here)\nLooks like this:\nRUN apk add --no-cache tzdata\nENV TZ America/Los_Angeles\n",
        "url": "https://serverfault.com/questions/683605/docker-container-time-timezone-will-not-reflect-changes"
    },
    {
        "title": "How to include multiple domains in an spf TXT Record",
        "question": "I am looking to setup a TXT spf record that has 2 included domains... individually:\nv=spf1 include:_spf.google.com ~all\n\nand \nv=spf1 include:otherdomain.com ~all\n\nWhat is the proper way of combining them into a single item?",
        "top_answer": "v=spf1 include:_spf.google.com include:otherdomain.com ~all\nThere's no restriction against including multiple names in a single entry; Hotmail, for instance, takes this to extremes.  Note that multiple includes, or nested ones, need to stay under the limit of 10 total DNS lookups for the whole SPF check.",
        "url": "https://serverfault.com/questions/283125/how-to-include-multiple-domains-in-an-spf-txt-record"
    },
    {
        "title": "What\u2019s the point in having \u201cwww\u201d in a URL?",
        "question": "Other than for historical reasons, is there is reason to have \u201cwww\u201d in a URL?\nShould I create a permanent redirect from www.xyz.com to xyz.com, or from xyz.com to www.xyz.com? Which one would you suggest and why?",
        "top_answer": "One of the reasons why you need www or some other subdomain has to do with a quirk of DNS and the CNAME record.\nSuppose for the purposes of this example that you are running a big site and contract out hosting to a CDN (Content Distribution Network) such as Akamai. What you typically do is set up the DNS record for your site as a CNAME to some akamai.com address. This gives the CDN the opportunity to supply an IP address that is close to the browser (in geographic or network terms). If you used an A record on your site, then you would not be able to offer this flexibility.\nThe quirk of the DNS is that if you have a CNAME record for a host name, you cannot have any other records for that same host. However, your top level domain example.com usually must have an NS and SOA record. Therefore, you cannot also add a CNAME record for example.com.\nThe use of www.example.com gives you the opportunity to use a CNAME for www that points to your CDN, while leaving the required NS and SOA records on example.com. The example.com record will usually also have an A record to point to a host that will redirect to www.example.com using an HTTP redirect.",
        "url": "https://serverfault.com/questions/145777/what-s-the-point-in-having-www-in-a-url"
    },
    {
        "title": "When does `cron.daily` run?",
        "question": "When do entries in cron.daily (and .weekly and .hourly) run, and is it configurable?\nI haven't found a definitive answer to this, and am hoping there is one.\nI'm running RHEL5 and CentOS 4, but for other distros/platforms would be great, too.",
        "top_answer": "For the distributions you mention:\nOn CentOS 5.4 (Should be same for RHEL5)\ngrep run-parts /etc/crontab\n\n01 * * * * root run-parts /etc/cron.hourly\n02 4 * * * root run-parts /etc/cron.daily\n22 4 * * 0 root run-parts /etc/cron.weekly\n42 4 1 * * root run-parts /etc/cron.monthly\n\nSo cron.daily runs at 04:02am.\nSame on CentOS 4.8",
        "url": "https://serverfault.com/questions/135906/when-does-cron-daily-run"
    },
    {
        "title": "df in linux not showing correct free space after file removal",
        "question": "I have file servers which are used to store files. Files might reside there for a week, or for a year. Unfortunately, when I remove files from the server, df command doesn't reflect the freed up space. So eventually, the server gets filled up (df shows 99%), and my script doesn't send any more files there, except there might be a few dozen GB of free space on there. \nI got noatime flag on the mounted partitions if that makes any difference. ",
        "top_answer": "Deleting the filename doesn't actually delete the file. Some other process is holding the file open, causing it to not be deleted; restart or kill that process to release the file.\nUse \nlsof +L1\n\nto find out which process is using a deleted (unlinked) file.",
        "url": "https://serverfault.com/questions/232525/df-in-linux-not-showing-correct-free-space-after-file-removal"
    },
    {
        "title": "How can I find out what Active Directory groups I'm a member of?",
        "question": "I'm running a Windows XP desktop in a corporate environment.  How can I find out which Active Directory groups I belong to?",
        "top_answer": "Try running gpresult /R for RSoP summary or gpresult /V for verbose output from the command line as an administrator on the computer. It should output something like this:\nC:\\Windows\\system32>gpresult /V\n\nMicrosoft (R) Windows (R) Operating System Group Policy Result tool v2.0\nCopyright (C) Microsoft Corp. 1981-2001\n\nCreated On 2/10/2010 at 10:27:41 AM\n\n\nRSOP data for OQMSupport01\\- on OQMSUPPORT01 : Logging Mode\n------------------------------------------------------------\n\nOS Configuration:            Standalone Workstation\nOS Version:                  6.1.7600\nSite Name:                   N/A\nRoaming Profile:             N/A\nLocal Profile:               C:\\Users\\-\nConnected over a slow link?: No\n\n\nCOMPUTER SETTINGS\n------------------\n\n    Last time Group Policy was applied: 2/10/2010 at 10:16:09 AM\n    Group Policy was applied from:      N/A\n    Group Policy slow link threshold:   500 kbps\n    Domain Name:                        OQMSUPPORT01\n    Domain Type:                        <Local Computer>\n\n    Applied Group Policy Objects\n    -----------------------------\n        N/A\n\n    The following GPOs were not applied because they were filtered out\n    -------------------------------------------------------------------\n        Local Group Policy\n            Filtering:  Not Applied (Empty)\n\n    The computer is a part of the following security groups\n    -------------------------------------------------------\n        System Mandatory Level\n        Everyone\n        Debugger Users\n        IIS_WPG\n        SQLServer2005MSSQLUser$OQMSUPPORT01$ACT7\n        SQLServerMSSQLServerADHelperUser$OQMSUPPORT01\n        BUILTIN\\Users\n        NT AUTHORITY\\SERVICE\n        CONSOLE LOGON\n        NT AUTHORITY\\Authenticated Users\n        This Organization\n        BDESVC\n        BITS\n        CertPropSvc\n        EapHost\n        hkmsvc\n        IKEEXT\n        iphlpsvc\n        LanmanServer\n        MMCSS\n        MSiSCSI\n        RasAuto\n        RasMan\n        RemoteAccess\n        Schedule\n        SCPolicySvc\n        SENS\n        SessionEnv\n        SharedAccess\n        ShellHWDetection\n        wercplsupport\n        Winmgmt\n        wuauserv\n        LOCAL\n        BUILTIN\\Administrators\n\nUSER SETTINGS\n--------------\n\n    Last time Group Policy was applied: 2/10/2010 at 10:00:51 AM\n    Group Policy was applied from:      N/A\n    Group Policy slow link threshold:   500 kbps\n    Domain Name:                        OQMSupport01\n    Domain Type:                        <Local Computer>\n\n    The user is a part of the following security groups\n    ---------------------------------------------------\n        None\n        Everyone\n        Debugger Users\n        HomeUsers\n        BUILTIN\\Administrators\n        BUILTIN\\Users\n        NT AUTHORITY\\INTERACTIVE\n        CONSOLE LOGON\n        NT AUTHORITY\\Authenticated Users\n        This Organization\n        LOCAL\n        NTLM Authentication\n        High Mandatory Level\n\n    The user has the following security privileges\n    ----------------------------------------------\n\n        Bypass traverse checking\n        Manage auditing and security log\n        Back up files and directories\n        Restore files and directories\n        Change the system time\n        Shut down the system\n        Force shutdown from a remote system\n        Take ownership of files or other objects\n        Debug programs\n        Modify firmware environment values\n        Profile system performance\n        Profile single process\n        Increase scheduling priority\n        Load and unload device drivers\n        Create a pagefile\n        Adjust memory quotas for a process\n        Remove computer from docking station\n        Perform volume maintenance tasks\n        Impersonate a client after authentication\n        Create global objects\n        Change the time zone\n        Create symbolic links\n        Increase a process working set\n\nOr if you are logged in to a Windows Server OS with the ActiveDirectory PowerShell Module (or Client OS with the Remote Server Administration Tools) try the Get-ADPrincipalGroupMembership cmdlet:\nC:\\Users\\username\\Documents> Get-ADPrincipalGroupMembership username | Select name\n\nname\n----\nDomain Users\nAll\nAnnouncements\nemployees_US\nremotes\nceo-report\nall-engineering\nnot-sales\nGlobal-NotSales\n",
        "url": "https://serverfault.com/questions/111650/how-can-i-find-out-what-active-directory-groups-im-a-member-of"
    },
    {
        "title": "Is it bad to redirect http to https?",
        "question": "I just installed an SSL Certificate on my server.\nIt then set up a redirect for all traffic on my domain on Port 80 to redirect it to Port 443.\nIn other words, all my http://example.com traffic is now redirected to the appropriate https://example.com version of the page.\nThe redirect is done in my Apache Virtual Hosts file with something like this...\nRewriteEngine on\nReWriteCond %{SERVER_PORT} !^443$\nRewriteRule ^/(.*) https://%{HTTP_HOST}/$1 [NC,R,L] \n\nMy question is, are there any drawbacks to using SSL?\nSince this is not a 301 Redirect, will I lose link juice/ranking in search engines by switching to https?\nI appreciate the help. I have always wanted to set up SSL on a server, just for the practice of doing it, and I finally decided to do it tonight. It seems to be working well so far, but I am not sure if it's a good idea to use this on every page. My site is not eCommerce and doesn't handle sensitive data; it's mainly for looks and the thrill of installing it for learning.\n\nUPDATED ISSUE\nStrangely Bing creates this screenshot from my site now that it is using HTTPS everywhere...\n ",
        "top_answer": "The [R]\u00a0flag on its own is a 302 redirection (Moved Temporarily). If you really want people using the HTTPS version of your site (hint: you do), then you should be using [R=301] for a permanent redirect:\nRewriteEngine on\nRewriteCond %{SERVER_PORT} !^443$\nRewriteRule ^/(.*) https://%{HTTP_HOST}/$1 [NC,R=301,L] \n\nA 301 keeps all your google-fu and hard-earned pageranks intact. Make sure mod_rewrite is enabled:\na2enmod rewrite\n\nTo answer your exact question:\n\nIs it bad to redirect http to https?\n\nHell no. It's very good.",
        "url": "https://serverfault.com/questions/570288/is-it-bad-to-redirect-http-to-https"
    },
    {
        "title": "How to run a command multiple times, using bash shell?",
        "question": "Is there a way to run a command (e.g. ps aux|grep someprocess) for n times?\nSomething like:\nrun -n 10  'ps aux|grep someprocess'\n\nI want to use it interactively.\nUpdate: The reason I am asking this is, that I do work on a lot of machines and I don't want to import all my adaped scripts etc into every box to get the same functionality accross every machine.",
        "top_answer": "I don't think a command or shell builtin for this exists, as it's a trivial subset of what the Bourne shell for loop is designed for and implementing a command like this yourself is therefore quite simple.\nPer JimB's suggestion, use the Bash builtin for generating sequences:\nfor i in {1..10}; do command; done\n\nFor very old versions of bash, you can use the seq command:\nfor i in `seq 10`; do command; done\n\nThis iterates ten times executing command each time - it can be a pipe or a series of commands separated by ; or &&. You can use the $i variable to know which iteration you're in.\nIf you consider this one-liner a script and so for some unspecified (but perhaps valid) reason undesireable you can implement it as a command, perhaps something like this on your .bashrc (untested):\n#function run\nrun() {\n    number=$1\n    shift\n    for i in `seq $number`; do\n      $@\n    done\n}\n\nUsage:\nrun 10 command\n\nExample:\nrun 5 echo 'Hello World!'\n",
        "url": "https://serverfault.com/questions/273238/how-to-run-a-command-multiple-times-using-bash-shell"
    },
    {
        "title": "How can I see which flags Nginx was compiled with?",
        "question": "How can I check what modules have been added to an nginx installation?",
        "top_answer": "nginx -V will list all the configured modules. There is no explicit enable/load command.",
        "url": "https://serverfault.com/questions/223509/how-can-i-see-which-flags-nginx-was-compiled-with"
    },
    {
        "title": "What is the difference between Unix sockets and TCP/IP sockets?",
        "question": "It's from this answer:\nhttps://stackoverflow.com/questions/2482411/is-this-pdo-bug-fixed-now/2482424#2482424\n\nWhen the host is \"localhost\", MySQL Unix clients use a Unix socket, AKA Unix Domain Socket, rather than a TCP/IP socket for the connection, thus the TCP port doesn't matter.\n",
        "top_answer": "A UNIX socket, AKA Unix Domain Socket, is an inter-process communication mechanism that allows bidirectional data exchange between processes running on the same machine.\nIP sockets (especially TCP/IP sockets) are a mechanism allowing communication between processes over the network. In some cases, you can use TCP/IP sockets to talk with processes running on the same computer (by using the loopback interface).\nUNIX domain sockets know that they\u2019re executing on the same system, so they can avoid some checks and operations (like routing); which makes them faster and lighter than IP sockets. So if you plan to communicate with processes on the same host, this is a better option than IP sockets.\nEdit: As per Nils Toedtmann's comment: UNIX domain sockets are subject to file system permissions, while TCP sockets can be controlled only on the packet filter level.",
        "url": "https://serverfault.com/questions/124517/what-is-the-difference-between-unix-sockets-and-tcp-ip-sockets"
    },
    {
        "title": "What is this very short power cable called?",
        "question": "I have a couple of networking components in my rack that take giant AC adapters (\"power bricks\") that don't fit neatly into my rackmount PDU. \nI have one \"thingy\" that is shown below, and I need to buy a few more. But I have no idea what I'm searching for because I don't know what the \"thingy\" is called.\n\nYes, this drawing is terrible. I would ask my 4-year-old to draw it for me because she's a better artist, but she's taking a nap.",
        "top_answer": "I would just call it a \"very short extension cord\", and in fact a Google search for \"short extension cord\" turns up lots of results of exactly what you're looking for.  E.g., these, which have a pass-through plug.",
        "url": "https://serverfault.com/questions/231161/what-is-this-very-short-power-cable-called"
    },
    {
        "title": "How to use Let's Encrypt DNS-01 challenge validation?",
        "question": "Let's Encrypt has announced they have:\n\nTurned on support for the ACME DNS challenge\n\nHow do I make ./letsencrypt-auto generate a new certificate using DNS challenge domain validation?\nEDIT\nI mean: How do I avoid http/https port binding, by using the newly announced  feature (2015-01-20) that lets you prove the domain ownership by adding a specific TXT record in the DNS zone of the target domain?",
        "top_answer": "Currently it is possible to perform DNS validation, also with the certbot LetsEncrypt client in manual mode. Automation is possible as well (see below).\nManual plugin\nYou can either perform a manual verification - with the manual plugin.\ncertbot -d bristol3.pki.enigmabridge.com --manual --preferred-challenges dns certonly\n\nCertbot will then provide you instructions to manually update a TXT record for the domain in order to proceed with the validation.\nPlease deploy a DNS TXT record under the name\n_acme-challenge.bristol3.pki.enigmabridge.com with the following value:\n\n667drNmQL3vX6bu8YZlgy0wKNBlCny8yrjF1lSaUndc\n\nOnce this is deployed,\nPress ENTER to continue\n\nOnce you have updated the DNS record, press Enter, certbot will continue and if the LetsEncrypt CA verifies the challenge, the certificate is issued as normally.\nYou may also use a command with more options to minimize interactivity and answering certbot questions. Note that the manual plugin does not yet support non-interactive mode.\ncertbot --text --agree-tos --email [email\u00a0protected] -d bristol3.pki.enigmabridge.com --manual --preferred-challenges dns --expand --renew-by-default  --manual-public-ip-logging-ok certonly\n\nRenewal does not work with the manual plugin as it runs in non-interactive mode. More info in the official certbot documentation.\nUpdate: manual hooks\nIn the new certbot version you can use hooks, e.g., --manual-auth-hook, --manual-cleanup-hook. The hooks are external scripts executed by certbot to perform the task.\nInformation is passed in environment variables - e.g., domain to validate, challenge token. Vars: CERTBOT_DOMAIN, CERTBOT_VALIDATION, CERTBOT_TOKEN.\ncertbot certonly --manual --preferred-challenges=dns --manual-auth-hook /path/to/dns/authenticator.sh --manual-cleanup-hook /path/to/dns/cleanup.sh -d secure.example.com\n\nYou can write your own handler or use already existing ones. There are many available, e.g., for Cloudflare DNS.\nMore info on official certbot hooks documentation.\nAutomation, Renewal, Scripting\nIf you would like to automate DNS challenge validation it is not currently possible with vanilla certbot. Update: some automation is possible with the certbot hooks.\nWe thus created a simple plugin that supports scripting with DNS automation. It's available as certbot-external-auth.\npip install certbot-external-auth\n\nIt supports the DNS, HTTP, TLS-SNI validation methods. You can either use it in handler mode or in JSON output mode.\nHandler mode\nIn handler mode, the certbot + plugin calls external hooks (a program, shell script, Python, ...) to perform the validation and installation. In practice you write a simple handler/shell script which gets the input arguments - domain, token and makes the change in DNS. When the handler finishes, certbot proceeds with validation as usual.\nThis gives you extra flexibility, renewal is also possible.\nHandler mode is also compatible with Dehydrated DNS hooks (former letsencrypt.sh). There are already many DNS hooks for common providers (e.g., CloudFlare, GoDaddy, AWS). In the repository there is a README with extensive examples and example handlers.\nExample with Dehydrated DNS hook:\ncertbot \\\n    --text --agree-tos --email [email\u00a0protected] \\\n    --expand --renew-by-default \\\n    --configurator certbot-external-auth:out \\\n    --certbot-external-auth:out-public-ip-logging-ok \\\n    -d \"bristol3.pki.enigmabridge.com\" \\\n    --preferred-challenges dns \\\n    --certbot-external-auth:out-handler ./dehydrated-example.sh \\\n    --certbot-external-auth:out-dehydrated-dns \\\n    run \n\nJSON mode\nAnother plugin mode is JSON mode. It produces one JSON object per line. This enables a more complicated integration - e.g., when Ansible or some deployment manager is calling certbot. Communication is performed via STDOUT and STDIN. Cerbot produces JSON objects with data to perform the validation, for example:\ncertbot \\\n    --text --agree-tos --email [email\u00a0protected] \\\n    --expand --renew-by-default \\\n    --configurator certbot-external-auth:out \\\n    --certbot-external-auth:out-public-ip-logging-ok \\\n    -d \"bristol3.pki.enigmabridge.com\" \\\n    --preferred-challenges dns \\\n    certonly 2>/dev/null\n\n{\"cmd\": \"perform_challenge\", \"type\": \"dns-01\", \"domain\": \"bs3.pki.enigmabridge.com\", \"token\": \"3gJ87yANDpmuuKVL2ktfQ0_qURQ3mN0IfqgbTU_AGS4\", \"validation\": \"ejEDZXYEeYHUxqBAiX4csh8GKkeVX7utK6BBOBshZ1Y\", \"txt_domain\": \"_acme-challenge.bs3.pki.enigmabridge.com\", \"key_auth\": \"3gJ87yANDpmuuKVL2ktfQ0_qURQ3mN0IfqgbTU_AGS4.tRQM98JsABZRm5-NiotcgD212RAUPPbyeDP30Ob_7-0\"}\n\nOnce DNS is updated, the caller sends the new-line character to STDIN of certbot to signal it can continue with validation.\nThis enables automation and certificate management from the central management server. For installation you can deploy certificates over SSH.\nFor more info please refer to the readme and examples on certbot-external-auth GitHub.\nEDIT: There is also a new blog post describing the DNS validation problem and the plugin usage.\nEDIT: We currently work on Ansible 2-step validation, will be soon off.",
        "url": "https://serverfault.com/questions/750902/how-to-use-lets-encrypt-dns-01-challenge-validation"
    },
    {
        "title": "Environment variables of a running process on Unix?",
        "question": "I need to troubleshoot some problems related to environment variables on a Unix system.\nOn Windows, I can use a tool such as ProcessExplorer to select particular a process and view values of each environment variable.\nHow can I accomplish the same thing on Unix? echoing and env cmd just show values at present time, but I want to view what values the running process is using currently.",
        "top_answer": "cat /proc/<pid>/environ\n\nIf you want to have pid(s) of a given running executable you can, among a number of other possibilities, use pidof:\nAlberT$ pidof sshd   \n30690 6512 \n\nEDIT:\nI totally quote Dennis Williamson and Teddy comments to achieve a more readable output.\nMy solution is the following:\ntr '\\0' '\\n' < /proc/<pid>/environ\n",
        "url": "https://serverfault.com/questions/66363/environment-variables-of-a-running-process-on-unix"
    },
    {
        "title": "How to tell which local branch is tracking which remote branch in Git?",
        "question": "I would like to know if there's a way to tell which local branch is tracking which remote branch in Git.\nI'm using one remote server, which is named \"origin\".",
        "top_answer": "Using the example of my copy of Puppet checked out from the upstream Git repository on Github.com... \n$ git remote show origin\n* remote origin\n  Fetch URL: git://github.com/reductivelabs/puppet.git\n  Push  URL: git://github.com/reductivelabs/puppet.git\n  HEAD branch: master\n  Remote branches:\n    0.24.x                 tracked\n    0.25.x                 tracked\n    2.6.x                  tracked\n    master                 tracked\n    next                   tracked\n    primordial-ooze        tracked\n    reins-on-a-horse       tracked\n    testing                tracked\n    testing-17-march       tracked\n    testing-18-march       tracked\n    testing-2-april        tracked\n    testing-2-april-midday tracked\n    testing-20-march       tracked\n    testing-21-march       tracked\n    testing-24-march       tracked\n    testing-26-march       tracked\n    testing-29-march       tracked\n    testing-31-march       tracked\n    testing-5-april        tracked\n    testing-9-april        tracked\n    testing4268            tracked\n  Local branch configured for 'git pull':\n    master merges with remote master\n  Local ref configured for 'git push':\n    master pushes to master (up to date)\n\nThen if I were to execute the following:\n$ git checkout -b local_2.6 -t origin/2.6.x \nBranch local_2.6 set up to track remote branch 2.6.x from origin.\nSwitched to a new branch 'local_2.6'\n\nAnd finally re-run the git remote show origin command again I will then see the following down near the bottom:\n  Local branches configured for 'git pull':\n    local_2.6 merges with remote 2.6.x\n    master    merges with remote master\n",
        "url": "https://serverfault.com/questions/175052/how-to-tell-which-local-branch-is-tracking-which-remote-branch-in-git"
    },
    {
        "title": "How do I do 'mount --bind' in /etc/fstab?",
        "question": "I'm trying to add mount --bind /proc/ /chroot/mysql/proc to /etc/fstab. How can I do this?",
        "top_answer": "The mount command accepts --bind or -o bind.\nIn the /etc/fstab file, you can use the following line:\n/source /destination none defaults,bind 0 0\n",
        "url": "https://serverfault.com/questions/613179/how-do-i-do-mount-bind-in-etc-fstab"
    },
    {
        "title": "Using DD for disk cloning",
        "question": "There's been a number of questions regarding disk cloning tools and dd has been suggested at least once. I've already considered using dd myself, mainly because ease of use, and that it's readily available on pretty much all bootable Linux distributions.\nWhat is the best way to use dd for cloning a disk? I did a quick Google search, and the first result was an apparent failed attempt. Is there anything I need to do after using dd, i.e. is there anything that CAN'T be read using dd?",
        "top_answer": "dd is most certainly the best cloning tool, it will create a 100% replica simply by using the following command.  I've never once had any problems with it.\ndd if=/dev/sda of=/dev/sdb bs=32M status=progress\n\nBe aware that while cloning every byte, you should not use this on a drive or partition that is being used. Especially applications like databases can't cope with this very well and you might end up with corrupted data.",
        "url": "https://serverfault.com/questions/4906/using-dd-for-disk-cloning"
    },
    {
        "title": "How to install tzdata on a ubuntu docker image?",
        "question": "I have the following line in the Dockerfile.\nRUN apt-get install -y tzdata\n\nWhen I run it, it asks for my input. After I provided my input, it hung there. Does anybody know how to solve this problem?\nStep 25/25 : RUN apt-get install -y tzdata\n ---> Running in ee47a1beff84\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following NEW packages will be installed:\n  tzdata\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 189 kB of archives.\nAfter this operation, 3104 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 tzdata all 2018i-0ubuntu0.18.04 [189 kB]\ndebconf: unable to initialize frontend: Dialog\ndebconf: (TERM is not set, so the dialog frontend is not usable.)\ndebconf: falling back to frontend: Readline\ndebconf: unable to initialize frontend: Readline\ndebconf: (This frontend requires a controlling tty.)\ndebconf: falling back to frontend: Teletype\ndpkg-preconfigure: unable to re-open stdin: \nFetched 189 kB in 1s (219 kB/s)\nSelecting previously unselected package tzdata.\n(Reading database ... 25194 files and directories currently installed.)\nPreparing to unpack .../tzdata_2018i-0ubuntu0.18.04_all.deb ...\nUnpacking tzdata (2018i-0ubuntu0.18.04) ...\nSetting up tzdata (2018i-0ubuntu0.18.04) ...\ndebconf: unable to initialize frontend: Dialog\ndebconf: (TERM is not set, so the dialog frontend is not usable.)\ndebconf: falling back to frontend: Readline\nConfiguring tzdata\n------------------\n\nPlease select the geographic area in which you live. Subsequent configuration\nquestions will narrow this down by presenting a list of cities, representing\nthe time zones in which they are located.\n\n  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc\n  2. America     5. Arctic     8. Europe    11. SystemV\n  3. Antarctica  6. Asia       9. Indian    12. US\nGeographic area:\n``\n",
        "top_answer": "One line only:\nRUN DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get -y install tzdata\n",
        "url": "https://serverfault.com/questions/949991/how-to-install-tzdata-on-a-ubuntu-docker-image"
    },
    {
        "title": "How do I share a Git repository with multiple users on a machine?",
        "question": "I have a Git repository on a staging server which multiple developers need to be able to pull to. git-init seems to have a flag very close to what I'm looking for: --shared, except I'd like multiple people to pull to that repository, as well. The git-clone's --shared flag does something entirely different.\nWhat's the easiest way to change an existing repository's permissions?",
        "top_answer": "Permissions are a pest.\nBasically, you need to make sure that all of those developers can write to everything in the git repo.\nSkip down to The New-Wave Solution for the superior method of granting a group of developers write capability.\nThe Standard Solution\nIf you put all the developers in a specially-created group, you can, in principle, just do:\nchgrp -R <whatever group> gitrepo\nchmod -R g+swX gitrepo\n\nThen change the umask for the users to 002, so that new files get created with group-writable permissions.  \nThe problems with this are legion; if you\u2019re on a distro that assumes a umask of 022 (such as having a common users group that includes everyone by default), this can open up security problems elsewhere.  And sooner or later, something is going to screw up your carefully crafted permissions scheme, putting the repo out of action until you get root access and fix it up (i.e., re-running the above commands).\nThe New-Wave Solution\nA superior solution\u2014though less well understood, and which requires a bit more OS/tool support\u2014is to use POSIX extended attributes.  I\u2019ve only come to this area fairly recently, so my knowledge here isn\u2019t as hot as it could be. But basically, an extended ACL is the ability to set permissions on more than just the 3 default slots (user/group/other).  \nSo once again, create your group, then run:\nsetfacl -R -m g:<whatever group>:rwX gitrepo\nfind gitrepo -type d | xargs setfacl -R -m d:g:<whatever group>:rwX\n\nThis sets up the extended ACL for the group so that the group members can read/write/access whatever files are already there (the first line); then, also tell all existing directories that new files should have this same ACL applied (the second line).\nHope that gets you on your way.",
        "url": "https://serverfault.com/questions/26954/how-do-i-share-a-git-repository-with-multiple-users-on-a-machine"
    },
    {
        "title": "\"Add correct host key in known_hosts\" / multiple ssh host keys per hostname?",
        "question": "Trying to ssh into a computer I control, I'm getting the familiar message:\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\n[...].\nPlease contact your system administrator.\nAdd correct host key in /home/sward/.ssh/known_hosts to get rid of this message.\nOffending RSA key in /home/sward/.ssh/known_hosts:86\nRSA host key for [...] has changed and you have requested strict checking.\nHost key verification failed.\n\nI did indeed change the key.  And I read a few dozen postings saying that the way to resolve this problem is by deleting the old key from the known_hosts file.\nBut what I would like is to have ssh accept both the old key and the new key.  The language in the error message (\"Add correct host key\") suggests that there should be some way to add the correct host key without removing the old one.\nI have not been able to figure out how to add the new host key without removing the old one.\nIs this possible, or is the error message just extremely misleading?",
        "top_answer": "\nget the rsa key of your server, where server_ip is your server's IP address, such as 192.168.2.1:\n$ ssh-keyscan -t rsa server_ip\n\nSample response:\n\n# server_ip SSH-2.0-OpenSSH_4.3\nserver_ip ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwH5EXZG...\n\n\nand on the client, copy the entire response line server_ip ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwH5EXZG..., and add this key to the bottom of your ~/.ssh/known_hosts file:\nserver_ip ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAqx9m529...(the offending key, and/or the very bottom of the `known_hosts` file)\nserver_ip ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwH5EXZG... (line you're adding, copied and pasted from above)\n\n",
        "url": "https://serverfault.com/questions/321167/add-correct-host-key-in-known-hosts-multiple-ssh-host-keys-per-hostname"
    },
    {
        "title": "How can I override CMD when running a docker image?",
        "question": "I want to inspect a docker image created by someone else with both an entrypoint and cmd specified, for example:\nENTRYPOINT [\"/usr/sbin/apache2ctl\"]\nCMD [\"-D\", \"FOREGROUND\"]\n\nI currently do:\ndocker run --interactive --tty --entrypoint=/bin/bash $IMAGE --login\n\nIs there a way to override CMD to be empty (so I don't have to use \"--login\") ?",
        "top_answer": "You could just enter via docker run -it --entrypoint=/bin/bash $IMAGE -i (you 'll launch a new container from the image and get a bash shell in interactive mode), then run the entrypoint command in that container.\nYou can then inspect the running container in the state it should be running.\nEDIT: Since Docker 1.3 you can use exec to run a process in a running container. Start your container as you 'd normally do, and then enter it by issuing:\ndocker exec -it $CONTAINER_ID /bin/bash\nAssuming bash is installed you will be given shell access to the running container.",
        "url": "https://serverfault.com/questions/594281/how-can-i-override-cmd-when-running-a-docker-image"
    },
    {
        "title": "How to force or redirect to SSL in nginx?",
        "question": "I have a signup page on a subdomain like: https://signup.example.com\nIt should only be accessible via HTTPS but I'm worried people might somehow stumble upon it via HTTP and get a 404. \nMy html/server block in nginx looks like this: \nhtml {\n  server {\n    listen 443;\n    server_name signup.example.com;\n\n    ssl                        on;\n    ssl_certificate        /path/to/my/cert;\n    ssl_certificate_key  /path/to/my/key;\n\n    ssl_session_timeout 30m;\n\n    location / {\n      root /path/to/my/rails/app/public;\n      index index.html;\n        passenger_enabled on;\n    }\n  }\n}\n\nWhat can I add so that people who go to http://signup.example.com get redirected to https://signup.example.com ? (FYI I know there are Rails plugins that can force SSL but was hoping to avoid that)",
        "top_answer": "According to nginx pitfalls, it's slightly better to omit the unnecessary capture, using $request_uri instead. In that case, append a question mark to prevent nginx from doubling any query args.\nserver {\n    listen      80;\n    server_name signup.mysite.com;\n    rewrite     ^   https://$server_name$request_uri? permanent;\n}\n",
        "url": "https://serverfault.com/questions/250476/how-to-force-or-redirect-to-ssl-in-nginx"
    },
    {
        "title": "In my /etc/hosts/ file on Linux/OSX, how do I do a wildcard subdomain?",
        "question": "I need to test sub-domains on my localhost.  How can I effectively have this result of adding *.localhost.com to my /etc/hosts/ file?\nIf it's not possible, how do I work around this problem?  I need to test wildcard sub-domains on my localserver.  It is a Django devserver, can the Django dev server handle the sub-domains?  Can some other piece of software/routing give me the end result I want?",
        "top_answer": "I have written a dns proxy in Python. It will read wildcard entries in /etc/hosts. See here: https://github.com/hubdotcom/marlon-tools/blob/master/tools/dnsproxy/dnsproxy.py",
        "url": "https://serverfault.com/questions/118378/in-my-etc-hosts-file-on-linux-osx-how-do-i-do-a-wildcard-subdomain"
    },
    {
        "title": "How do you validate fstab without rebooting?",
        "question": "I modified /etc/fstab.\nI verified the new devices and I can mount them with the mount command.\nHow may I validate the modifications made to /etc/fstab ?",
        "top_answer": "You can simple run: mount -a\n\n-a\nMount all filesystems (of the given types) mentioned in fstab.\n\nThis command will mount all (not-yet-mounted) filesystems mentioned in fstab and is used in system script startup during booting.",
        "url": "https://serverfault.com/questions/174181/how-do-you-validate-fstab-without-rebooting"
    },
    {
        "title": "Nginx config reload without downtime",
        "question": "I use nginx as a reverse proxy.\nWhenever I update the config for it using\nsudo \"cp -r #{nginx_config_path}* /etc/nginx/sites-enabled/\"\nsudo \"kill -s HUP `cat /var/run/nginx.pid`\"\n\nI face a brief downtime. How can I avoid that?",
        "top_answer": "Run service nginx reload, /etc/init.d/nginx reload, /usr/sbin/nginx -s reload or /usr/sbin/nginx reload\nIt will do a hot reload of the configuration without downtime. If you have pending requests, then there will be lingering nginx processes that will handle those connections before it dies, so it's an extremely graceful way to reload configs.\nSometimes you may want to prepend with sudo",
        "url": "https://serverfault.com/questions/378581/nginx-config-reload-without-downtime"
    },
    {
        "title": "When does /tmp get cleared?",
        "question": "I'm taking to putting various files in /tmp, and I wondered about the rules on deleting them?\nI'm imagining it's different for different distributions, and I'm particularly interested in Ubuntu and Fedora desktop versions.\nBut a nice general way of finding out would be a great thing.\nEven better would be a nice general way of controlling it! (Something like 'every day at 3 in the morning, delete any /tmp files older than 60 days, but don't clear the directory on reboot')",
        "top_answer": "That depends on your distribution. On some systems, it's deleted only when booted; others have cron jobs deleting items older than n hours.\n\nOn Debian and derivatives\n\nOn Debian and in general, on boot rules are defined in\n\n/etc/default/rcS. \n\n\n\n\nOn Ubuntu since version 15.10 and its derivatives: using tmpfiles.d.\n\nThe default tmp.conf only clears /tmp on boot.\nFor more details, see this answer.\n\n\nOn Ubuntu 15.04 and before:\n- using tmpreaper2\n\nwhich gets called by /etc/cron.daily,\nconfigured via /etc/default/rcS and /etc/tmpreaper.conf. (Credits to this answer).\n\n\nOn distributions from Red Hat and its derivatives: by age\n\nRHEL 7, 8 and others with SYSTEMD\n\nconfigured via /usr/lib/tmpfiles.d/tmp.conf,\ncalled by systemd-tmpfiles-clean.service.\n\n\nRHEL 6\n\nused /etc/cron.daily/tmpwatch.\n\n\n\n\nOn Gentoo:\n\nused /etc/conf.d/bootmisc.\n\n\n",
        "url": "https://serverfault.com/questions/377348/when-does-tmp-get-cleared"
    },
    {
        "title": "How do you add a Windows environment variable without rebooting?",
        "question": "I would like to add an Environment variable to a Windows machine (desktop or server) and be able to use it without rebooting that machine. \nSay you have a production server which hosts a variety of apps and a new app needs a particular Environment variable to run. You do not want to reboot it while users are connected to your other apps. What choices do you have? I don't like the wait-until-a-good-time-to-reboot option. There must be a better way. What am I missing?",
        "top_answer": "Changes to environment variables should take effect immediately, if you make the change via the main Properties dialog for the computer in question (go to My Computer | Properties | Advanced | Environment Variables). After the changes are saved, Explorer broadcasts a WM_SETTINGCHANGE message to all windows to inform them of the change. Any programs spawned via Explorer after this should get the updated environment, although already-running programs will not, unless they handle the setting change message.\nI'm not able to tell from your problem description what specific problem you're having with this. Can you tell us more about the specific scenario that isn't working?\nThis KB article may also be of use: How to propagate environment variables to the system",
        "url": "https://serverfault.com/questions/8855/how-do-you-add-a-windows-environment-variable-without-rebooting"
    },
    {
        "title": "What's the best way of handling permissions for Apache 2's user www-data in /var/www?",
        "question": "Has anyone got a nice solution for handling files in /var/www? We're running Name Based Virtual Hosts and the Apache 2 user is www-data.\nWe've got two regular users & root. So when messing with files in /var/www, rather than having to...\nchown -R www-data:www-data\n\n...all the time, what's a good way of handling this?\nSupplementary question: How hardcore do you then go on permissions?\nThis one has always been a problem in collaborative development environments.",
        "top_answer": "Attempting to expand on @Zoredache's answer, as I give this a go myself:\n\nCreate a new group (www-pub) and add the users to that group\ngroupadd www-pub\nusermod -a -G www-pub usera ## must use -a to append to existing groups\nusermod -a -G www-pub userb\ngroups usera ## display groups for user\nChange the ownership of everything under /var/www to root:www-pub\nchown -R root:www-pub /var/www ## -R for recursive\nChange the permissions of all the folders to 2775\nchmod 2775 /var/www ## 2=set group id, 7=rwx for owner (root), 7=rwx for group (www-pub), 5=rx for world (including apache www-data user)\nSet group ID (SETGID) bit (2) causes the group (www-pub) to be copied to all new files/folders created in that folder.  Other options are SETUID (4) to copy the user id, and STICKY (1) which I think lets only the owner delete files.\nThere's a -R recursive option, but that won't discriminate between files and folders, so you have to use find, like so:\nfind /var/www -type d -exec chmod 2775 {} +\nChange all the files to 0664\nfind /var/www -type f -exec chmod 0664 {} +\nChange the umask for your users to 0002\nThe umask controls the default file creation permissions, 0002 means files will have 664 and directories 775.  Setting this (by editing the umask line at the bottom of /etc/profile in my case) means files created by one user will be writable by other users in the www-group without needing to chmod them.\n\nTest all this by creating a file and directory and verifying the owner, group and permissions with ls -l.\nNote: You'll need to logout/in for changes to your groups to take effect!",
        "url": "https://serverfault.com/questions/6895/whats-the-best-way-of-handling-permissions-for-apache-2s-user-www-data-in-var"
    },
    {
        "title": "How do I load a sql.gz file to my database? (importing)",
        "question": "I'm trying to import a gzipped SQL file into mysql directly. Is this the right way?\nmysql -uroot -ppassword mydb > myfile.sql.gz\n",
        "top_answer": "zcat /path/to/file.sql.gz | mysql -u 'root' -p your_database\n\n> will write the output of the mysql command on stdout into the file myfile.sql.gz which is most probably not what you want. Additionally, this command will prompt you for the password of the MySQL user \"root\".",
        "url": "https://serverfault.com/questions/137965/how-do-i-load-a-sql-gz-file-to-my-database-importing"
    },
    {
        "title": "Shell command to monitor changes in a file",
        "question": "I know there was a command on Unix that I could use to monitor a file and see changes that are getting written to it. This was quite useful especially for checking log files.\nDo you know what it is called?",
        "top_answer": "Do you mean \ntail -f logfile.log\n\n?\n(Man page for tail)",
        "url": "https://serverfault.com/questions/1669/shell-command-to-monitor-changes-in-a-file"
    },
    {
        "title": "nginx HTTPS serving with same config as HTTP",
        "question": "Is there a way to share configuration directives across two nginx server {} blocks? I'd like to avoid duplicating the rules, as my site's HTTPS and HTTP content are served with the exact same config.\nCurrently, it's like this:\nserver {\n  listen 80;\n  ...\n}\n\nserver {\n  listen 443;\n\n  ssl on; # etc.\n  ...\n}\n\nCan I do something along the lines of:\nserver {\n  listen 80, 443;\n  ...\n\n  if(port == 443) {\n    ssl on; #etc\n  }\n}\n",
        "top_answer": "You can combine this into one server block like so:\nserver {\n    listen 80;\n    listen 443 default_server ssl;\n\n    # other directives\n}\n\nOfficial How-To",
        "url": "https://serverfault.com/questions/10854/nginx-https-serving-with-same-config-as-http"
    },
    {
        "title": "How to check sshd log?",
        "question": "I have Ubuntu 9.10 installed with sshd and I can successfully connect to it using login and password. I have configured an RSA key login and now have \"Server refused our key\" as expected. Ok, now I want to check sshd log in order to figure out a problem. I have examined /etc/ssh/sshd_config and it have\nSyslogFacility AUTH\nLogLevel INFO\n\nOk. I'm looking at /var/log/auth.log and... it's empty O_O. Changing Loglevel to VERBOSE helps nothing - auth.log is still empty. Any hints how I can check sshd log?",
        "top_answer": "If no one else is using the system at the moment you could do what i've done in such cases:\n\nstop sshd service (at least i've been able to do this while logged in via ssh)\nstart sshd manually and add some -d options to get more verbose debug output. Unless you have something funky going on it should use the same keys and config it does when started properly\n",
        "url": "https://serverfault.com/questions/130482/how-to-check-sshd-log"
    },
    {
        "title": "How to check if a library is installed?",
        "question": "In Linux, how do I check if a library is installed or not? (from the command line of course).\nIn my specific case now, I want to check whether libjpeg is installed.",
        "top_answer": "To do this in a distro-independent* fashion you can use ldconfig with grep, like this:\nldconfig -p | grep libjpeg\n\nIf libjpeg is not installed, there will be no output. If it is installed, you will get a line for each version available. \nReplace libjpeg by any library you want, and you have a generic, distro-independent* way of checking for library availability.\nIf for some reason the path to ldconfig is not set, you can try to invoke it using its full path, usually /sbin/ldconfig.\n**99% of the times*",
        "url": "https://serverfault.com/questions/54736/how-to-check-if-a-library-is-installed"
    },
    {
        "title": "Symlink not updating as expected when using an absolute with relative path",
        "question": "I am trying to use symbolic links. I did some reading and found the following commands:\nCreation -> ln -s {/path/to/file-name} {link-name}\nUpdate -> ln -sfn {/path/to/file-name} {link-name}\nDeletion -> rm {link-name}\n\nCreations and deletions work fine. But updates do not work. After performing this command, the symlink becomes invalid. \nI have read here and there that it is not possible to update/override a symlink. So there is contradictory information on the net. Who is right? If a symlink can be updated/overridden, how can I achieve this?\nUpdate\nHere is my directory structure:\n~/scripts/test/\n~/scripts/test/remote_loc/\n~/scripts/test/remote_loc/site1/\n~/scripts/test/remote_loc/site1/stuff1.txt\n~/scripts/test/remote_loc/site2/\n~/scripts/test/remote_loc/site2/stuff2.txt\n~/scripts/test/remote_loc/site2/\n~/scripts/test/remote_loc/site3/stuff3.txt\n\nFrom ~/scripts/test/, when I perform:\nln -s /remote_loc/site1 test_link\n\na test_link is created, and I can ls -l it, but it seems broken (contrary to what I said above in my question).\nHow can I perform a multiple directory level link?",
        "top_answer": "Using -f with ln will overwrite any link that was already there, so as long as you have the correct permissions, it should work... It's always worked for me. What operating system are you using?",
        "url": "https://serverfault.com/questions/389997/symlink-not-updating-as-expected-when-using-an-absolute-with-relative-path"
    },
    {
        "title": "Permission denied (publickey). SSH from local Ubuntu to Amazon EC2 server",
        "question": "I have an instance of an application running in the cloud on an Amazon EC2 instance, and I need to connect to it from my local Ubuntu. It works fine on one local ubuntu and also laptop. I got this message, Permission denied (publickey)., when trying to SSH to EC2 from a different local Ubuntu.\nI'm thinking there may be problems with security settings on the Amazon EC2, which has limited IP access to one instance; or maybe a certificate needs to regenerate.\nDoes anyone know a solution to the Permission denied error?",
        "top_answer": "The first thing to do in this situation is to use the -v option to ssh, so you can see what types of authentication is tried and what the result is. Does that help enlighten the situation?\nIn your update to your question, you mention \"on another local Ubuntu\". Have you copied over the ssh private key to the other machine?",
        "url": "https://serverfault.com/questions/39733/permission-denied-publickey-ssh-from-local-ubuntu-to-amazon-ec2-server"
    },
    {
        "title": "Difference in sites-available vs sites-enabled vs conf.d directories (Nginx)?",
        "question": "I have some experience using linux but none using nginx.  I have been tasked with researching load-balancing options for an application server.  \nI have used apt-get to install nginx and all seems fine.\nI have a couple of questions.\nWhat is the difference between the sites-available folder and the conf.d folder.  Both of those folders were INCLUDED in the default configuration setup for nginx.  Tutorials use both.  What are they for and what is the best practice?\nWhat is the sites-enabled folder used for?  How do I use it?\nThe default configuration references a www-data user?  Do I have to create that user?  How do I give that user optimal permissions for running nginx?",
        "top_answer": "The sites-* folders are managed by nginx_ensite and nginx_dissite. For Apache httpd users who find this with a search, the equivalents is a2ensite/a2dissite.\nThe sites-available folder is for storing all of your vhost configurations, whether or not they're currently enabled.\nThe sites-enabled folder contains symlinks to files in the sites-available folder. This allows you to selectively disable vhosts by removing the symlink.\nconf.d does the job, but you have to move something out of the folder, delete it, or make changes to it when you need to disable something. The sites-* folder abstraction makes things a little more organized and allows you to manage them with separate support scripts.\n(unless you're like me, and one of many debian admins who just managed the symlinks directly, not knowing about the scripts...)",
        "url": "https://serverfault.com/questions/527630/difference-in-sites-available-vs-sites-enabled-vs-conf-d-directories-nginx"
    },
    {
        "title": "Setting the hostname: FQDN or short name?",
        "question": "I've noticed that the \"preferred\" method of setting the system hostname is fundamentally different between Red Hat/CentOS and Debian/Ubuntu systems.\nCentOS documentation and the RHEL deployment guide say the hostname should be the FQDN:\n\nHOSTNAME=<value>, where <value> should be the Fully Qualified Domain\nName (FQDN), such as hostname.example.com, but can be whatever\nhostname is necessary.\n\nThe RHEL install guide is slightly more ambiguous:\n\nSetup prompts you to supply a host name for this computer, either as a\nfully-qualified domain name (FQDN) in the format hostname.domainname\nor as a short host name in the format hostname.\n\nThe Debian reference says the hostname should not use the FQDN:\n\n3.5.5. The hostname\nThe kernel maintains the system hostname. The init script in runlevel\nS which is symlinked to \"/etc/init.d/hostname.sh\" sets the system\nhostname at boot time (using the hostname command) to the name stored\nin \"/etc/hostname\". This file should contain only the system hostname,\nnot a fully qualified domain name.\n\nI haven't seen any specific recommendations from IBM about which to use, but some software seems to have a preference.\nMy questions:\n\nIn a heterogeneous environment, is it better to use the vendor recommendation, or choose one and be consistent across all hosts?\nWhat software have you encountered which is sensitive to whether the hostname is set to the FQDN or short name?\n",
        "top_answer": "I would choose a consistent approach across the entire environment. Both solutions work fine and will remain compatible with most applications. There is a difference in manageability, though.\nI go with the short name as the HOSTNAME setting, and set the FQDN as the first column in /etc/hosts for the server's IP, followed by the short name.\nI have not encountered many software packages that enforce or display a preference between the two. I find the short name to be cleaner for some applications, specifically logging. Maybe I've been unlucky in seeing internal domains like server.northside.chicago.rizzomanufacturing.com. Who wants to see that in the logs or a shell prompt?\nSometimes, I'm involved in company acquisitions or restructuring where internal domains and/or subdomains change. I like using the short hostname in these cases because logging, kickstarts, printing, systems monitoring, etc. do not need full reconfiguration to account for the new domain names. \nA typical RHEL/CentOS server setup for a server named \"rizzo\" with internal domain \"ifp.com\", would look like:\n/etc/sysconfig/network:\nHOSTNAME=rizzo\n...\n\n-\n/etc/hosts:\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n172.16.100.13   rizzo.ifp.com rizzo\n\n-\n[root@rizzo ~]# hostname \nrizzo\n\n-\n/var/log/messages snippet:\nDec 15 10:10:13 rizzo proftpd[19675]: 172.16.100.13 (::ffff:206.15.236.182[::ffff:206.15.236.182]) - Preparing to               \n chroot to directory '/app/upload/GREEK'\nDec 15 10:10:51 rizzo proftpd[20660]: 172.16.100.13 (::ffff:12.28.170.2[::ffff:12.28.170.2]) - FTP session opened.\nDec 15 10:10:51 rizzo proftpd[20660]: 172.16.100.13 (::ffff:12.28.170.2[::ffff:12.28.170.2]) - Preparing to chroot                \nto directory '/app/upload/ftp/SRRID'\n",
        "url": "https://serverfault.com/questions/331936/setting-the-hostname-fqdn-or-short-name"
    },
    {
        "title": "Any benefit or detriment from removing a pagefile on an 8 GB RAM machine?",
        "question": "I'm running Windows 7 on a dual core, x64 AMD with 8 GB RAM.\nDo I even need a page file?\nWill removing it help or hurt performance?\nWould it make a difference if this is a server or a desktop?\nDoes Windows 7 vs. Windows 2008 make a difference with a page file?",
        "top_answer": "TL;DR version: Let Windows handle your memory/pagefile settings. The people at MS have spent a lot more hours thinking about these issues than most of us sysadmins.\nMany people seem to assume that Windows pushes data into the pagefile on demand. EG: something wants a lot of memory, and there is not enough RAM to fill the need, so Windows begins madly writing data from RAM to disk at this last minute, so that it can free up RAM for the new demands.\nThis is incorrect. There's more going on under the hood. Generally speaking, Windows maintains a backing store, meaning that it wants to see everything that's in memory also on the disk somewhere. Now, when something comes along and demands a lot of memory, Windows can clear RAM very quickly, because that data is already on disk, ready to be paged back into RAM if it is called for. So it can be said that much of what's in pagefile is also in RAM; the data was preemptively placed in pagefile to speed up new memory allocation demands.\nDescribing the specific mechanisms involved would take many pages (see chapter 7 of Windows Internals, and note that a new edition will soon be available), but there are a few nice things to note. First, much of what's in RAM is intrinsically already on the disk - program code fetched from an executable file or a DLL for example. So this doesn't need to be written to the pagefile; Windows can simply keep track of where the bits were originally fetched from. Second, Windows keeps track of which data in RAM is most frequently used, and so clears from RAM that data which has gone longest without being accessed.\nRemoving pagefile entirely can cause more disk thrashing. Imagine a simple scenario where some app launches and demands 80% of existing RAM. This would force current executable code out of RAM - possibly even OS code. Now every time those other apps - or the OS itself (!!) need access to that data, the OS must page them in from backing store on disk, leading to much thrashing. Because without pagefile to serve as backing store for transient data, the only things that can be paged are executables and DLLs which had inherent backing stores to start with.\nThere are of course many resource/utilization scenarios. It is not impossible that you have one of the scenarios under which there would be no adverse effects from removing pagefile, but these are the minority. In most cases, removing or reducing pagefile will lead to reduced performance under peak-resource-utilization scenarios.\nSome references:\n\nWindows Internals book(s) (4th edition and 5th edition)\nPushing the Limits of Windows: Physical Memory\nPushing the Limits of Windows: Virtual Memory\nInside the Windows Vista Kernel: Part 1\nInside the Windows Vista Kernel: Part 2\nInside the Windows Vista Kernel: Part 3\nUnderstanding Virtual Memory\nRAM, Virtual Memory, Pagefile and all that stuff (here's a longer version)\nThe Out-of-Memory Syndrome, or: Why Do I Still Need a Pagefile?\n\ndmo noted a recent Eric Lippert post which helps in the understanding of virtual memory (though is less related to the question). I'm putting it here because I suspect some people won't scroll down to other answers - but if you find it valuable, you owe dmo a vote, so use the link to get there!",
        "url": "https://serverfault.com/questions/23621/any-benefit-or-detriment-from-removing-a-pagefile-on-an-8-gb-ram-machine"
    },
    {
        "title": "How to read backward from the end of file in less or more?",
        "question": "I've found one way so far: less +G filename, but it scrolls up line-by-line only with \u2191.\nWhat's a more powerful less usage which provides scrolling by page, backward pattern search, and so on?",
        "top_answer": "I'm sure someone else has a better answer, but\nWith \"less\" after you've opened the file:\nG goes to the bottom of the file\n\n^b goes up one page\n\n? searches backwards.\n\nAs you said, you can open the file with +G and then use ? and ^b to scroll up.  There are likely clever awk things you can do to achieve the same thing in a script.",
        "url": "https://serverfault.com/questions/151635/how-to-read-backward-from-the-end-of-file-in-less-or-more"
    },
    {
        "title": "VirtualBox: How to set up networking so both host and guest can access internet and talk to each other",
        "question": "I was wondering if someone could give me a simple guide on how to set up virtual networking in VirtualBox (4.0.2) so that the following scenarios work:\n\nBoth Host and Guest can access the Internet\nHost can ping Guest and vice versa\nHost can access, for example, an apache web server running on Guest and vice versa\n\nI've been fiddling around with the various Network Adapters available in the settings for my Guest, but I'm just not able to figure it out. Is there anyone that can help me out here?\nThe host is running Windows 7 32-bit and the guest is running Ubuntu 10.10 32-bit.",
        "top_answer": "Try this:\n\nSetup the virtualbox to use 2 adapters:\n\n\nThe first adapter is set to NAT (that will give you the internet connection).\nThe second adapter is set to host only.\n\nStart the virtual machine and assign a static IP for the second adapter in Ubuntu (for instance 192.168.56.56). The host Windows will have 192.168.56.1 as IP for the internal network (VirtualBox Host-Only Network is the name in network connections in Windows). What this will give you is being able to access the apache server on ubuntu, from windows, by going to 192.168.56.56. Also, Ubuntu will have internet access, since the first adapter (set to NAT) will take care of that.\nNow, to make the connection available both ways (accessing the windows host from the ubuntu guest) there's still one more step to be performed. Windows will automatically add the virtualbox host-only network to the list of public networks and that cannot be changed. This entails that the firewall will prevent proper access.\nTo overcome this and not make any security breaches in your setup:\n\n\ngo to the windows firewall section, in control panel,\nclick on advanced settings. In the page that pops up,\nclick on inbound rules (left column), then on new rule (right column). Chose custom rule, set the rule to allow all programs, and any protocol. For the scope, add in the first box (local IP addresses) 192.168.56.1, and in the second box (remote IP) 192.168.56.56. Click next, select allow the connection, next, check all profiles, next, give it a name and save.\n\n\nThat's it, now you have 2 way communication, with apache/any other service available as well as internet. \nThe final step is to setup a share. Do not use the shared folders feature in virtualbox, it's quite buggy especially with windows 7 (and 64 bit). Instead use samba shares - fast and efficient.\nFollow this link for how to set that up: https://wiki.ubuntu.com/MountWindowsSharesPermanently",
        "url": "https://serverfault.com/questions/225155/virtualbox-how-to-set-up-networking-so-both-host-and-guest-can-access-internet"
    },
    {
        "title": "SSH use only my password, Ignore my ssh key, don't prompt me for a passphrase",
        "question": "This is a question regarding the OpenSSH client on Linux, MacOSX and FreeBSD.\nNormally, I log into systems using my SSH key.\nOccasionally, I want my SSH client to ignore my SSH key and use a password instead. If I 'ssh hostname', my client prompts me for the Passphrase to my SSH key which is an annoyance. Instead, I want the client to simply ignore my SSH key, so that the server will ask me for my password instead.\nI tried the following, but I am still prompted for the passphrase to my SSH key. After this, I am prompted for my password.\nssh -o PreferredAuthentications=password host.example.org\n\nI want to do this on the client side, without any modification of the remote host.",
        "top_answer": "Try ssh -o PasswordAuthentication=yes -o PreferredAuthentications=keyboard-interactive,password -o PubkeyAuthentication=no host.example.org\nIn ssh v2, keyboard-interactive is another way to say \"password\".  The -o PubkeyAuthentication=no option instructs the client not to attempt key pair authentication.\nIn addition, the PasswordAuthentication=yes option is to override any previously configured ssh options that may have disabled it.",
        "url": "https://serverfault.com/questions/130346/ssh-use-only-my-password-ignore-my-ssh-key-dont-prompt-me-for-a-passphrase"
    },
    {
        "title": "How to setup passwordless `sudo` on Linux?",
        "question": "How can passwordless sudo access be setup on either RHEL (Fedora, CentOS, etc) or Ubuntu distributions? (If it's the same across distros, that's even better!)\nSetting: personal and/or lab/training equipment with no concern for unauthorized access (ie, the devices are on non-public networks, and any/all users are fully trusted, and the contents of the devices are \"plain-vanilla\").",
        "top_answer": "I'm switching this around a little bit from previous versions of this answer. I would suggest one of three approaches:\n\nRecommended: make a group of users who can use sudo without a password:\n%wheel         ALL = (ALL) NOPASSWD: ALL\n\nand add all user accounts which people might use to this group.\n\nSecure-ish but annoying: grant passwordless sudo access to an explicit list of users:\nUser_Alias     EVERYONE = user1, user2, user3, ...\nEVERYONE       ALL = (ALL) NOPASSWD: ALL\n\nbut you will have to edit the sudo configuration each time you want to change which users have access.\n\nInsecure: if you really want to give all user accounts passwordless sudo access, thanks to a comment by medina, you can write\nALL            ALL = (ALL) NOPASSWD: ALL\n\n\n\nIn each case, the lines in the code block should be added to /etc/sudoers (using the visudo command, of course, which will ensure that you haven't made any syntax errors), or to a file under /etc/sudoers.d if your system is set up to include those files in the sudo configuration (thanks Xetius). Note that lines in the sudo configuration are processed in order from top to bottom and later ones override earlier ones, so if you want to avoid something else overriding this, you should put it toward the end (thanks a1an).\nThe reason I recommend using a group rather than granting blanket passwordless sudo access to all users - even though I know that's what the question was asking for - is that, especially in the modern world, a significant security risk comes from compromise of other services running on the system. Sure, you might be able to totally trust all the real people who have access, but do you trust the mail server? The system logger? The Docker daemon? Whatever other services are running on the machine? And more to the point, do you trust whatever random person on the internet might have exploited a vulnerability in one of these services to make it do things? Giving passwordless sudo access to all users means that anyone who hacks into one of those services in a way that lets them execute commands can jump right to running commands as root, which probably means total compromise of the system. It's true that denying them access to sudo doesn't necessarily mean they can't compromise the system anyway, but you can at least make it harder for them.\nOf course, different systems have different needs, and you might very well be in a situation where you really can trust that nobody will get the wrong kind of access to this computer, or that there is really nothing too bad anyone could do if they did. But at least stop to think about it. And when in doubt, it's better to limit access by default, because it sometimes turns out that you can't trust your system as much as you think you can, and if something does go wrong, it's going to be too late.",
        "url": "https://serverfault.com/questions/160581/how-to-setup-passwordless-sudo-on-linux"
    },
    {
        "title": "How to refresh hosts file without rebooting",
        "question": "On Windows, how do you refresh the hosts file without rebooting?",
        "top_answer": "You don't need to reboot. Any changes you make to the hosts file are immediate. You used to need to reboot for changes to take effect in Windows 9x. That is no longer the case.\nHowever, you may need to restart any applications that do internal hostname or DNS caching, such as web browsers.",
        "url": "https://serverfault.com/questions/9050/how-to-refresh-hosts-file-without-rebooting"
    },
    {
        "title": "How do diff over ssh?",
        "question": "How do I diff files/folders across machines provided that the only connectivity available is ssh?",
        "top_answer": "You can do it with Bash's process substitution:\n diff foo <(ssh myServer 'cat foo')\n\nor the same with\n ssh myServer cat foo | diff foo -\n\nOr, if both are on remote servers:\ndiff <(ssh myServer1 'cat foo') <(ssh myServer2 'cat foo')\n",
        "url": "https://serverfault.com/questions/59140/how-do-diff-over-ssh"
    },
    {
        "title": "How to use rsync over FTP",
        "question": "Any unix:\nI have the following cmd line which works fine.\nrsync -avr -e ssh /home/dir [email\u00a0protected]:/home/\n\nBut I need to set it up now to rsync to a remote server that only has an FTP server on it.\nHow do I go about that?\nI looked at the rsync help but quickly got lost (I don't do this stuff very often).",
        "top_answer": "You don't. rsync can't do that for you, it is a protocol of its own and doesn't work over FTP.\nYou might, however, want to try csync. IIRC it provides rsync-like behaviour over HTTP. I can't comment on whether it works over FTP, you'll have to try it.",
        "url": "https://serverfault.com/questions/24622/how-to-use-rsync-over-ftp"
    },
    {
        "title": "Check if array is empty in Bash",
        "question": "I have an array which gets filled with different error messages as my script runs.\nI need a way to check if it is empty of not at the end of the script and take a specific action if it is.\nI have already tried treating it like a normal VAR and using -z to check it, but that does not seem to work.  Is there a way to check if an array is empty or not in Bash?",
        "top_answer": "Supposing your array is $errors, just check to see if the count of elements is zero.\nif [ ${#errors[@]} -eq 0 ]; then\n    echo \"No errors, hooray\"\nelse\n    echo \"Oops, something went wrong...\"\nfi\n",
        "url": "https://serverfault.com/questions/477503/check-if-array-is-empty-in-bash"
    },
    {
        "title": "How to reconnect to a disconnected ssh session",
        "question": "Is there a way to connect to an ssh session that was disconnected?  We are having problems with our network connection to a remote site that we are working on separately; however, in the mean time we experience a large number of disconnects due to lost packets while connected to servers at the remote location.  Many times the session stays active for a while, and sometimes it happens to be in the middle of some action (file editing, running some process, etc...) that I need to get back to rather than restart if possible.",
        "top_answer": "UPDATE: For an actual answer see zero_r's answer below\nThis isn't an answer, but a workaround. Use screen. \nWhen you first log in, run screen. You get another shell, run commands in that. If you're disconnected, the screen process keeps the terminal alive so that your shell and the processes it is running don't fall over. When you reconnect, run 'screen -r' to resume.\nThere's a bunch more to configuring and using screen, but the above should workaround your problem.",
        "url": "https://serverfault.com/questions/19634/how-to-reconnect-to-a-disconnected-ssh-session"
    },
    {
        "title": "How to view all ssl certificates in a bundle?",
        "question": "I have a certificate bundle .crt file.  \ndoing openssl x509 -in bundle.crt -text -noout only shows the root certificate. \nhow do i see all the other certificates?",
        "top_answer": "http://comments.gmane.org/gmane.comp.encryption.openssl.user/43587 suggests this one-liner:\nopenssl crl2pkcs7 -nocrl -certfile CHAINED.pem | openssl pkcs7 -print_certs -text -noout\n\nIt indeed worked for me, but I don't understand the details so can't say if there are any caveats.\n\nupdated june 22:\n\nfor openssl 1.1.1 and higher: a single-command answer can be found here serverfault.com/a/1079893 (openssl storeutl -noout -text -certs bundle.crt)\nTo list just the subjects:\nopenssl storeutl -noout -text -certs \\\n   $(openssl version -d | sed -E 's/OPENSSLDIR: \"([^\"]*)\"/\\1/')/cert.pem | \\\n   grep Subject: \n",
        "url": "https://serverfault.com/questions/590870/how-to-view-all-ssl-certificates-in-a-bundle"
    },
    {
        "title": "Difference between KVM and QEMU",
        "question": "I have been reading about KVM and Qemu for sometime. As of now I have a clear understanding of what they do.\nKVM supports hardware virtualization to provide near native performance to the Guest Operating sytems. On the other hand QEmu emulates the target operating system.\nWhat I am confused is to what level these two co-ordinate. Like\n\nWho manages the sharing of RAM and/or memory?\nWho schedules I/O operations?\n",
        "top_answer": "Qemu:\nQEmu is a complete and standalone software of its own.  You use it to emulate machines, it is very flexible and portable.  Mainly it works by a special 'recompiler' that transforms binary code written for a given processor into another one (say, to run MIPS code on a PPC mac, or ARM in an x86 PC).\nTo emulate more than just the processor, Qemu includes a long list of peripheral emulators: disk, network, VGA, PCI, USB, serial/parallel ports, etc.\nKQemu:\nIn the specific case where both source and target are the same architecture (like the common case of x86 on x86), it still has to parse the code to remove any 'privileged instructions' and replace them with context switches.  To make it as efficient as possible on x86 Linux, there's a kernel module called KQemu that handles this.\nBeing a kernel module, KQemu is able to execute most code unchanged, replacing only the lowest-level ring0-only instructions.  In that case, userspace Qemu still allocates all the RAM for the emulated machine, and loads the code.  The difference is that instead of recompiling the code, it calls KQemu to scan/patch/execute it.  All the peripheral hardware emulation is done in Qemu.\nThis is a lot faster than plain Qemu because most code is unchanged, but still has to transform ring0 code (most of the code in the VM's kernel), so performance still suffers.\nKVM:\nKVM is a couple of things: first it is a Linux kernel module\u2014now included in mainline\u2014that switches the processor into a new 'guest' state.  The guest state has its own set of ring states, but privileged ring0 instructions fall back to the hypervisor code.  Since it is a new processor mode of execution, the code doesn't have to be modified in any way.\nApart from the processor state switching, the kernel module also handles a few low-level parts of the emulation like the MMU registers (used to handle VM) and some parts of the PCI emulated hardware.\nSecond, KVM is a fork of the Qemu executable.  Both teams work actively to keep differences at a minimum, and there are advances in reducing it.  Eventually, the goal is that Qemu should work anywhere, and if a KVM kernel module is available, it could be automatically used.  But for the foreseeable future, the Qemu team focuses on hardware emulation and portability, while KVM folks focus on the kernel module (sometimes moving small parts of the emulation there, if it improves performance), and interfacing with the rest of the userspace code.\nThe kvm-qemu executable works like normal Qemu: allocates RAM, loads the code, and instead of recompiling it, or calling KQemu, it spawns a thread (this is important). The thread calls the KVM kernel module to switch to guest mode and proceeds to execute the VM code.  On a privileged instruction, it switches back to the KVM kernel module, which, if necessary, signals the Qemu thread to handle most of the hardware emulation.\nOne of the nice things of this architecture is that the guest code is emulated in a posix thread which you can manage with normal Linux tools.  If you want a VM with 2 or 4 cores, kvm-qemu creates 2 or 4 threads, each of them calls the KVM kernel module to start executing.  The concurrency\u2014if you have enough real cores\u2014or scheduling\u2014if not\u2014is managed by the normal Linux scheduler, keeping code small and surprises limited.",
        "url": "https://serverfault.com/questions/208693/difference-between-kvm-and-qemu"
    },
    {
        "title": "hosts file ignored, how to troubleshoot?",
        "question": "The hosts file on Windows computers is used to bind certain name strings to specific IP addresses to override other name resolution methods.\nOften, one decides to change the hosts file, and discovers that the changes refuse to take effect, or that even old entries of the hosts file are ignored thereafter. A number of \"gotcha\" mistakes can cause this, and it can be frustrating to figure out which one.\nWhen faced with the problem of Windows ignoring a hosts file, what is a comprehensive troubleshoot protocol that may be followed?\n\nThis question has duplicates on SO, such as HOSTS file being ignored \nHowever, these tend to deal with a specific case, and once whatever mistake the OP made is found out, the discussion is over. If you don't happen to have made the same error, such a discussion isn't very useful. So I thought it would be more helpful to have a general protocol for resolving all hosts-related issues that would cover all cases.",
        "top_answer": "Based on my own experience and what I encountered while Googling, here are some things to try:\n1. Did you check that it works correctly?\nChanges to hosts should take effect immediately, but Windows caches name resolution data so for some time the old records may be used. Open a command line (Windows+R, cmd, Enter) and type:\nipconfig /flushdns\n\nTo drop the old data. To check if it works, use (assuming you have an ipv4 entry in your hosts for www.example.com, or an ipv6 entry in your hosts for ipv6.example.com):\nping -4 www.example.com -n 1\nping -6 www.example.com -n 1\n\nAnd see if it uses the correct IP. If yes, your hosts file is fine and the problem is elsewhere.\nAlso, you can reset the NetBios cache with (open the console as an admin or it will fail):\nnbtstat -R\n\nYou can check the current data in the DNS cache with:\nipconfig /displaydns | more\n\nNB: nslookup does not look at the hosts file. See NSLOOKUP and NBLOOKUP give one IP address; PING finds another\n2. Basics\n\nIs your hosts file named correctly? It should be hosts and not host, etc.\nIs the extension correct? It should have no extension (hosts not hosts.txt) - be careful if you have configured windows to hide known extensions, check the properties to be sure: The correct hosts file's type will show up as just \"File\".\nDid you follow the correct syntax? Did you accidentally prefix lines with a hash (#) which indicates comments?\nDid you take care of all variants (www.example.com and example.com - safest to just add both)?\n\n3. Whitespace\nThe format for each line is IP address, then a horizontal tab (escape code \\t, ASCII HT, hex 0x09) or a single space (hex 0x20), then the host name, ie. www.example.com, then finally a carriage return followed by a line feed, (escape codes \\r\\n, ASCII CRLF, hex 0x0d 0x0a).\nSample entries, using Unicode control pictures to indicate control characters. (Don't copy and paste these into your hosts file!)\n192.0.2.1\u2409www.example.com\u240d\u240a\n2001:db8:8:4::2\u2409ipv6.example.com\u240d\u240a\n\nThe individual bytes may be viewed in Notepad++ with the hex editor plugin. Notepad++ will also show special characters (View -> Show Symbol) so you can easily inspect the number and kind of whitespace characters.\nIf you copied and pasted hosts entries from somewhere, you may end up with multiple spaces. In theory hosts supports multiple spaces separating the two columns, but it's another thing to try if nothing else works.\nTo be on the safe side, make sure all lines in your hosts file either use tabs or spaces, not both.\nLastly, terminate the file with a blank line.\n4. Registry Key\nThere is a registry key specifying the location of the hosts file. Supposedly, Windows doesn't actually support putting the hosts file in other locations, but you might want to check. The key is:\n\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\\DataBasePath\n\nThe entry should be:\n%SystemRoot%\\System32\\drivers\\etc\n\nOr, in a Command Prompt window, type:\nreg query HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters -v DataBasePath\n\nwhich should display something similar to:\nHKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\nDataBasePath    REG_EXPAND_SZ    %SystemRoot%\\System32\\drivers\\etc\n\n5. Permissions\nSometimes there are issues with permissions on the file, the file attributes, and similar things. To recreate the file with default permissions:\n\nCreate a new text file on your desktop.\nCopy and paste the contents of your current hosts file into this file in Notepad.\nSave the new text file and rename it to hosts.\nCopy (do not move) the file to your %SystemRoot%\\System32\\drivers\\etc directory, and overwrite the old file.\n\nLast point is important: Copying works, moving doesn't.\nThe local Users account must be able to read the hosts file. To make sure (in Windows 7):\n\nNavigate to %SystemRoot%\\System32\\drivers\\etc in Windows Explorer.\nIf you can't see the hosts file, ensure you can see hidden and system files.\nRight-click on the hosts file and select Properties from the context menu.\nIn the hosts Properties window, click on the Security tab.\nExamine the list of names in the Group or user names: box. If %COMPUTERNAME%\\Users is present, click on it to view permissions.\nIf Users is not present, or is present but does not have Read permission, click Edit....\nIf Users is not present, click Add..., type Users, click Check Names, and click OK or press Enter.\nSelect Users, and ensure Read & execute is checked in the Allow column. Click OK. If a Windows Security alert box pops up, choose Yes to continue.\nClick OK to close the hosts Properties window.\nGo up to section 1 of this answer and follow the directions to check if it's working now.\n\nOr, in a Command Prompt window, type:\nicacls %SystemRoot%\\System32\\drivers\\etc\\hosts\n\nwhich should display something like:\nC:\\WINDOWS\\System32\\drivers\\etc\\hosts NT AUTHORITY\\SYSTEM:(F)\n                                      NT AUTHORITY\\SYSTEM:(I)(F)\n                                      BUILTIN\\Administrators:(I)(F)\n                                      BUILTIN\\Users:(I)(RX)\n                                      APPLICATION PACKAGE AUTHORITY\\ALL APPLICATION PACKAGES:(I)(RX)\n                                      APPLICATION PACKAGE AUTHORITY\\ALL RESTRICTED APPLICATION PACKAGES:(I)(RX)\n\nYou should see an (R) after BUILTIN\\Users.\n6. Encoding\nThe hosts file should encoded in ANSI or UTF-8 without BOM. You can do this with File -> Save As.\n7. Proxies\nIf you have a proxy configured, it may bypass the hosts file. The solution is to not use the proxy, or configure it to not do this.\nTo check, go to your Internet Explorer -> Internet Options -> Connections -> LAN settings. If everything is blank and \"Automatically detect settings\" is checked, you aren't using a proxy.\nIf you rely on a proxy to access the web and therefore don't want to disable it, you can add exceptions by going to Internet Explorer -> Internet Options -> Connections -> LAN settings -> Proxy Server / Advanced. Then add your exceptions to the Exceptions text box. e.g. localhost;127.0.0.1;*.dev\n8. DNS address\n(This may also resolve proxy issues.)\nGo to your network connections properties, then TCP/IP settings, and change the first DNS server to 127.0.0.1 (localhost). The second should probably be your actual DNS's IP.\nThis is not necessary for the hosts file to work, but it may help in your case if something is configured strangely.\n9. .local addresses\nIf you are using a .local domain entry in the form of myhost.local and it gets ignored please try the following:\nx.x.x.x myhost.local www.myhost.local\n\neven if the www.myhost.local does not exist. Windows somehow does not append its workgroup or localdomain.\n10. Line / count limits\n(added to this answer to make it visible as it's been mentioned a few times)\nWindows hosts file seems to have a line or host limit. If you have more than 150 characters on a line or more than 8 hosts entries for an IP create a new line\ne.g. instead of:\n1.2.3.4 host1.com host2.com host3.com host4.com host5.com host6.com host7.com host8.com host9.com \n\nTry this:\n1.2.3.4 host1.com host2.com host3.com host4.com host5.com\n1.2.3.4 host6.com host7.com host8.com host9.com\n \n",
        "url": "https://serverfault.com/questions/452268/hosts-file-ignored-how-to-troubleshoot"
    },
    {
        "title": "What does a + mean at the end of the permissions from ls -l?",
        "question": "What is the plus sign at the end of the permissions telling me?\nls -l\ntotal 4\ndrwxrwxrwx+ 2 benson avahi-autoipd 4096 Jan 27 17:37 docs\n\nHere's the context:\ncat /etc/issue\n\\CentOS release 5.3 (Final)\nKernel \\r on an \\m\n",
        "top_answer": "It means your file has extended permissions called ACLs.\nYou have to run getfacl <file> to see the full permissions.\nSee Access Control Lists for more details.",
        "url": "https://serverfault.com/questions/227852/what-does-a-mean-at-the-end-of-the-permissions-from-ls-l"
    },
    {
        "title": "What is a glue record?",
        "question": "\nThis is a Canonical Question about DNS glue records.\n\nWhat exactly (but briefly) is a DNS glue record?  Why are they needed and how do they work?",
        "top_answer": "A glue record is a term for a record that's served by a DNS server that's not authoritative for the zone, to avoid a condition of impossible dependencies for a DNS zone.\nSay I own a DNS zone for example.com.  I want to have DNS servers that're hosting the authoritative zone for this domain so that I can actually use it - adding records for the root of the domain, www, mail, etc.  So, I put the name servers in the registration to delegate to them - those are always names, so we'll put in ns1.example.com and ns2.example.com.\nThere's the trick.  The TLD's servers will delegate to the DNS servers in the whois record - but they're within example.com.  They try to find ns1.example.com, ask the .com servers, and get referred back to... ns1.example.com.\nWhat glue records do is to allow the TLD's servers to send extra information in their response to the query for the example.com zone - to send the IP address that's configured for the name servers, too.  It's not authoritative, but it's a pointer to the authoritative servers, allowing for the loop to be resolved.",
        "url": "https://serverfault.com/questions/309622/what-is-a-glue-record"
    },
    {
        "title": "How do I convert a .cer certificate to .pem?",
        "question": "I have a .cer certificate and I would like to convert it to the .pem format.\nIf I remember correctly, I used to be able to convert them by exporting the .cer in Base64, then renaming the file to .pem .\nHow do I convert a .cer certificate to .pem?",
        "top_answer": "Convert a DER file (.crt .cer .der) to PEM\nopenssl x509 -inform der -in certificate.cer -out certificate.pem\n\nSource",
        "url": "https://serverfault.com/questions/254627/how-do-i-convert-a-cer-certificate-to-pem"
    },
    {
        "title": "How to sleep in a batch file?",
        "question": "How to pause execution for a while in a Windows batch file between a command and the next one?",
        "top_answer": "The correct way to sleep in a batch file is to use the timeout command, introduced in Windows 2000.\nTo wait somewhere between 29 and 30 seconds:\ntimeout /t 30\n\nThe timeout would get interrupted if the user hits any key; however, the command also accepts the optional switch /nobreak, which effectively ignores anything the user may press, except an explicit CTRL-C:\ntimeout /t 30 /nobreak\n\nAdditionally, if you don't want the command to print its countdown on the screen, you can redirect its output to NUL:\ntimeout /t 30 /nobreak > NUL\n",
        "url": "https://serverfault.com/questions/432322/how-to-sleep-in-a-batch-file"
    },
    {
        "title": "How do I verify the speed of my NIC?",
        "question": "I just installed a new gigabit network interface card (NIC) in Linux.  How do I tell if it is really set to gigabit speeds?  I see ethtool has an option to set the speed, but I can't seem to figure out how to report its current speed.",
        "top_answer": "Just use a command like: ethtool eth0 to get the needed info. Ex:\n$ ethtool eth0 | grep Speed\nSpeed: 1000Mb/s\n",
        "url": "https://serverfault.com/questions/207474/how-do-i-verify-the-speed-of-my-nic"
    },
    {
        "title": "How do I ask apt-get to skip any interactive post-install configuration steps?",
        "question": "I have a server build script which uses apt-get to install packages. It then puts pre-written configuration files directly in place, so the interactive post-install configuration dialog in packages such as postfix is not needed. How do I skip this stage of the installation? It creates a piece of manual intervention that I would rather avoid.\nI am aware of the -qq option, but the manpage warns against using it without specifying a no-action modifier. I do want to perform an action, I just want to suppress a specific part of it.",
        "top_answer": "You can do a couple of things for avoiding this. Setting the DEBIAN_FRONTEND variable to noninteractive and using -y flag. For example:\nexport DEBIAN_FRONTEND=noninteractive\napt-get -yq install [packagename]\n\nIf you need to install it via sudo, use:\nsudo DEBIAN_FRONTEND=noninteractive apt-get -yq install [packagename]\n",
        "url": "https://serverfault.com/questions/227190/how-do-i-ask-apt-get-to-skip-any-interactive-post-install-configuration-steps"
    },
    {
        "title": "Meaning of the buffers/cache line in the output of free",
        "question": "Why does my server show\n            total      used      free   shared buffers    cached\n    Mem: 12286456  11715372    571084        0   81912   6545228\n-/+ buffers/cache:  5088232   7198224\n   Swap: 24571408     54528  24516880\n\nI have no idea on calculating the memory in linux. I think it says that 5088232 is used where as 7198224 is free, meaning it is actually consuming 5GB of RAM?",
        "top_answer": "Meaning of the values\nThe first line means:\n\ntotal: Your total (physical) RAM (excluding a small bit that the kernel permanently reserves for itself at startup); that's why it shows ca. 11.7 GiB , and not 12 GiB, which you probably have.\n\nused: memory in use by the OS.\n\nfree: memory not in use.\n\nshared / buffers / cached: This shows memory usage for specific purposes, these values are included in the value for used.\n\n\nThe second line gives first line values adjusted. It gives the original value for used minus the sum buffers+cached and the original value for free plus the sum buffers+cached, hence its title. These new values are often more meaningful than those of first line.\nThe last line (Swap:) gives information about swap space usage (i.e. memory contents that have been temporarily moved to disk).\nBackground\nTo actually understand what the numbers mean, you need a bit of background about the virtual memory (VM) subsystem in Linux.  Just a short version: Linux (like most modern OS) will always try to use free RAM for caching stuff, so Mem: free will almost always be very low. Therefore the line -/+ buffers/cache: is shown, because it shows how much memory is free when ignoring caches; caches will be freed automatically if memory gets scarce, so they do not really matter.\nA Linux system is really low on memory if the free value in -/+ buffers/cache: line gets low.\nFor more details about the meaning of the numbers, see e.g. the questions:\n\nIn Linux, what is the difference between \"buffers\" and \"cache\" reported by the free command?\nWhy does Red Hat Linux report less free memory on the system than is actually available?\n\nChanges in procps 3.3.10\nNote that the output of free was changed in procps 3.3.10 (released in 2014). The columns reported are now \"total\", \"used\", \"free\", \"shared\", \"buff/cache\", \"available\", and the meanings of some of the values changed, mainly to better account for the Linux kernel's slab cache.\nSee Debian Bug report #565518 for the motivation, and What do the changes in free output from 14.04 to 16.04 mean? for more details information.",
        "url": "https://serverfault.com/questions/85470/meaning-of-the-buffers-cache-line-in-the-output-of-free"
    },
    {
        "title": "Non interactive git clone (ssh fingerprint prompt) [duplicate]",
        "question": "I want to clone a repo in a non-interactive way. When cloning, git asks to confirm host's fingerprint:\nThe authenticity of host 'bitbucket.org (207.223.240.182)' can't be established.\nRSA key fingerprint is 97:8c:1b:f2:6f:14:6b:5c:3b:ec:aa:46:46:74:7c:40.\nAre you sure you want to continue connecting (yes/no)? no\n\nHow do I force \"yes\" every time this questions pops up? I tried using yes yes | git clone ..., but it doesn't work.\nEDIT:\nHere's a solution: Can I automatically add a new host to known_hosts? (adds entires to known_hosts with ssh-keyscan).",
        "top_answer": "I don't think that is the best solution, but it was a solution for me.\nANSWER:\nAdding the domainnames to the known_hosts file using the ssh-keyscan command solved the issue:\nssh-keyscan <enter_domainname_e.g._github.com> >> ~/.ssh/known_hosts",
        "url": "https://serverfault.com/questions/447028/non-interactive-git-clone-ssh-fingerprint-prompt"
    },
    {
        "title": "Possible to change email address in keypair?",
        "question": "I've created an RSA keypair that I used for SSH, and it includes my email address. (At the end of the public key.)\nI've now changed my email address.\nIs it possible to change the email address on the key, or is it part of the key and I would have to make a new one?",
        "top_answer": "\nI've created an RSA keypair that I used for SSH, and it includes my email address. (At the end of the public key.)\n\nThat part of an ssh key is just a comment.  You can change it to anything you want at any time.  It doesn't even need to be the same on different servers.  You can remove it as well.  It is only there to help you or someone else figure out what to delete when you have many keys in an authorized_keys file and you need to revoke or change one of them.\nssh-rsa AAAAB3N....NMqKM= this_is_a_comment\n\nWhen I create my keys with ssh-keygen I usually use a command like this to set a different comment.  I don't think the username@host is very useful.  You can certainly put it whatever comment that you like that will be useful to you and any other admins to help identify who the key belongs to.\nssh-keygen ... -C YYYYMMDD_surname_givenname\n",
        "url": "https://serverfault.com/questions/309171/possible-to-change-email-address-in-keypair"
    },
    {
        "title": "How to start/stop/restart launchd services from the command line?",
        "question": "How do I restart, say for example my httpd or afpd, running any Mac OS X >= 10.5 (Leopard-), without having to use the GUI and go to System Preferences -> Sharing and unchecking/checking \"Web Sharing\"?\nI'm looking for the canonical equivalent to Debian's invoke-rc.d apache2 restart.\nEDIT: The question is about launchd controlled services in general, not specifically Apache (-which was simply an example).",
        "top_answer": "launchctl(8) is your friend. Just keep in mind that some of the services (sshd for example) are disabled in the configuration file so you will need to use the -w switch when loading them. Here is an example - sshd:\n$ sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist \n\nYou can stop the service using the unload subcommand.\n$ sudo launchctl unload /System/Library/LaunchDaemons/ssh.plist \n\nTo list the services, as you might have already guessed use the 'list' subcommand ;)",
        "url": "https://serverfault.com/questions/194832/how-to-start-stop-restart-launchd-services-from-the-command-line"
    },
    {
        "title": "What is a challenge password?",
        "question": "I'm setting up SSL on an Ubuntu server. One of fields it asks for as part of setting up the CSR is a \"challenge password\".  What is that?  The default is blank.  Do I need to enter one?",
        "top_answer": "And now at the top, by popular request, the TL;DR bit:\nThe challenge password requested as part of the CSR generation is not the same thing as the passphrase used to encrypt the secret key. The challenge password is basically a shared-secret nonce between you and the SSL certificate-issuing authority, embedded in the CSR, which the issuer may use to authenticate you should that ever be needed.\n\nThe old, full answer, because reasons:\nThe \"challenge password\" requested as part of the CSR generation, is different from the passphrase used to encrypt the secret key (requested at key generation time, or when a plaintext key is later encrypted - and then requested again each time the SSL-enabled service that uses it starts up).\nHere's a key being generated, and the beginning of the generated key:\n$ openssl genpkey -algorithm rsa -out foo.key\n............++++++\n...++++++\n\n$ head -3 foo.key\n-----BEGIN PRIVATE KEY-----\nMIICdgIBADANBgkqhkiG9w0BAQEFAASCAmAwggJcAgEAAoGBAJ9jNAG4Noy//r/S\neeK/gEgGOV0BZm0CYmgSQGj4P6N3cJsPlGsG80qKTxTFwoEiXnM3BVeBpDdXhGKt\n\nThis key has no passphrase.  I wasn't prompted for one at creation, and haven't entered one.  Now, let's generate an encrypted key:\n$ openssl genpkey -algorithm rsa -des3 -out bar.key\n...........................................++++++\n.....................................++++++\nEnter PEM pass phrase:\nVerifying - Enter PEM pass phrase:\n\n$ head -3 bar.key\n-----BEGIN ENCRYPTED PRIVATE KEY-----\nMIICxjBABgkqhkiG9w0BBQ0wMzAbBgkqhkiG9w0BBQwwDgQInfwj1iv3icMCAggA\nMBQGCCqGSIb3DQMHBAizMHBklBexiwSCAoDtRKf1WtMiVMH7HraGTIG0rlQS6Xuj\n\nSo it should be clear what an encrypted private key (which apache, or any other SSL-enabled server, will need unlocking for it when it starts) and a plaintext private key (which doesn't require unlocking at service start time) look like.  Now I'll generate a CSR with a challenge password from the unencrypted key:\n$ openssl req -new -key foo.key\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:\nState or Province Name (full name) []:\nLocality Name (eg, city) [Default City]:\nOrganization Name (eg, company) [Default Company Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server's hostname) []:\nEmail Address []:\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:asdfasdf\nAn optional company name []:\n-----BEGIN CERTIFICATE REQUEST-----\nMIIBmzCCAQQCAQAwQjELMAkGA1UEBhMCWFgxFTATBgNVBAcMDERlZmF1bHQgQ2l0\neTEcMBoGA1UECgwTRGVmYXVsdCBDb21wYW55IEx0ZDCBnzANBgkqhkiG9w0BAQEF\nAAOBjQAwgYkCgYEAn2M0Abg2jL/+v9J54r+ASAY5XQFmbQJiaBJAaPg/o3dwmw+U\nawbzSopPFMXCgSJeczcFV4GkN1eEYq2Cmam3tH6t8mVDh0/UryJSWBsaFm9mh9RF\ngIpP0hEkYZTf/0X+X06ukt9S/Id9Z/tVgPsZA3TcNjNhJfVaTm81/4ykq8UCAwEA\nAaAZMBcGCSqGSIb3DQEJBzEKDAhhc2RmYXNkZjANBgkqhkiG9w0BAQUFAAOBgQCa\nivuDRBlHOhBjg6wPbH9NvCnvEnxeEAkYi0Sl/Grdo/WCk17e+sv9wgqEW1QSIdbV\nXzMeWidurv4AtcATwhfk9tBcYBCTxANkTONzhJG7Yk9OAz8g8Ljo8EEvPf4oHqpw\ntBg10DCD2op0lCwL2LBdPO3RG20f/HD6fEXPVxZdOQ==\n-----END CERTIFICATE REQUEST-----\n\nAnd just to show that the key hasn't magically become encrypted:\n$ head -3 foo.key\n-----BEGIN PRIVATE KEY-----\nMIICdgIBADANBgkqhkiG9w0BAQEFAASCAmAwggJcAgEAAoGBAJ9jNAG4Noy//r/S\neeK/gEgGOV0BZm0CYmgSQGj4P6N3cJsPlGsG80qKTxTFwoEiXnM3BVeBpDdXhGKt\n\nSo I say again: the \"challenge password\" requested as part of the CSR generation is not the same thing as a passphrase used to encrypt the secret key.  The \"challenge password\" is basically a shared-secret nonce between you and the SSL certificate-issuer (aka Certification Authority, or CA), embedded in the CSR, which the issuer may use to authenticate you should that ever be needed.  Some SSL certificate-issuers make that clearer than others; look down at the bottom of this page to see where they say the challenge password is needed - it's not when you restart apache:\n\nShould you choose to enter and use a challenge password, you will need\nto make sure that you save that password in a secure place. If you\never need to reinstall your certificate for any reason, you will be\nrequired to enter that password.\n",
        "url": "https://serverfault.com/questions/266232/what-is-a-challenge-password"
    },
    {
        "title": "Top level domain/domain suffix for private network?",
        "question": "At our office, we have a local area network with a purely internal DNS setup, on which clients all named as whatever.lan.  I also have a VMware environment, and on the virtual-machine-only network, I name the virtual machines whatever.vm.\nCurrently, this network for the virtual machines isn't reachable from our local area network, but we're setting up a production network to migrate these virtual machines to, which will be reachable from the LAN.  As a result, we're trying to settle on a convention for the domain suffix/TLD we apply to the guests on this new network we're setting up, but we can't come up with a good one, given that .vm, .local and .lan all have existing connotations in our environment.\nSo, what's the best practice in this situation?  Is there a list of TLDs or domain names somewhere that's safe to use for a purely internal network?",
        "top_answer": "Since the previous answers to this question were written, there have been a couple of RFCs that alter the guidance somewhat.  RFC 6761 discusses special-use domain names without providing specific guidance for private networks.  RFC 6762 still recommends not using unregistered TLDs, but also acknowledges that there are cases where it will be done anyway.  Since the commonly used .local conflicts with Multicast DNS (the main topic of the RFC), Appendix G.  Private DNS Namespaces recommends the following TLDs:\n\nintranet\ninternal\nprivate\ncorp\nhome\nlan\n\nIANA appears to recognize both RFCs but does not (currently) incorporate the names listed in Appendix G.\nIn other words: you shouldn't do it.  But when you decide to do it anyway, use one of the above names.\nUpdate 1/2024: We're getting closer to having some definitive guidance on this.  IANA has made a provisional determination that ICANN has opened up for public comment (the recommendation is to use .internal)\nUpdate 8/2024: ICANN has officially reserved .INTERNAL (note: all caps) for private use.",
        "url": "https://serverfault.com/questions/17255/top-level-domain-domain-suffix-for-private-network"
    },
    {
        "title": "Heartbleed: What is it and what are options to mitigate it?",
        "question": "\nThis is a Canonical Question about understanding and remediating the Heartbleed security issue.\n\nWhat exactly is CVE-2014-0160 AKA \"Heartbleed\"? What is the cause, what OSs and versions of OpenSSL are vulnerable, what are the symptoms, are there any methods to detect a successful exploit? \nHow can I check to see if my system is affected? How can this vulnerability be mitigated? Should I be concerned that my keys or other private data have been compromised? What other side effects should I be concerned about?",
        "top_answer": "First, before freaking out, be sure that you understand whether or not this vulnerability actually applies to you. If you have a server, but have never actually had any applications using TLS, then this is not a high-priority thing for you to fix. If, on the other hand, you have ever had TLS-enabled applications, well then you're in for a treat. Read on:\n\nWhat exactly is CVE-2014-0160 aka \"Heartbleed\"?\n\nIt's a big fricking mess, that's what it is. In short, a remotely-exploitable vulnerability was discovered in OpenSSL versions 1.0.1 through 1.0.1f through which an attacker can read certain parts of system memory. Those parts being that which hold sensitive data such as private keys, preshared keys, passwords and high valued corporate data among other things.\nThe bug was independently discovered by Neel Mehta of Google Security (March 21, 2014) and Finnish IT security testing firm Codenomicon (April 2, 2014).\n\nWhat is the cause?\n\nWell, errant code in OpenSSL. Here is the commit that introduced the vulnerability, and here is the commit that fixed the vulnerability. The bug showed up in December of 2011 and was patched today, April 7th, 2014.\nThe bug can also be seen as a symptom of a larger problem. The two related problems are (1) what process are in place to ensure errant code is not introduced to a code base, and (2) why are the protocols and extensions so complex and hard to test. Item (1) is a governance and process issue with OpenSSL and many other projects. Many developers simply resist practices such as code reviews, analysis and scanning. Item (2) is being discussed on the IETF's TLS WG. See Heartbleed / protocol complexity.\n\nWas the errant code maliciously inserted?\n\nI won't speculate on whether this was truly a mistake or possibly a bit of code slipped in on behalf of a bad actor. However, the person who developed the code for OpenSSL states it was inadvertent. See Man who introduced serious 'Heartbleed' security flaw denies he inserted it deliberately.\n\nWhat OSs and versions of OpenSSL are vulnerable?\n\nAs mentioned above, any operating system that is using, or application that is linked against OpenSSL 1.0.1 through 1.0.1f.\n\nWhat are the symptoms, are any methods to detect a successful exploit?\n\nThis is the scary part. As far as we know, there is no known way to detect whether or not this vulnerability has been exploited. It is theoretically possible that IDS signatures will be released soon that can detect this exploit, but as of this writing, those are not available.\nThere is evidence that Heartbleed was being actively exploited in the wild as early as November, 2013. See the EFF's Wild at Heart: Were Intelligence Agencies Using Heartbleed in November 2013? And Bloomberg reports the NSA had weaponized the exploit shortly after the vulnerability was introduced. See NSA Said to Exploit Heartbleed Bug for Intelligence for Years. However, the US Intelligence Community denies Bloomberg's claims. See IC ON THE RECORD.\n\nHow can I check to see if my system is affected?\n\nIf you are maintaining OpenSSL on your system, then you can simply issue openssl version:\n$ openssl version\nOpenSSL 1.0.1g 7 Apr 2014\n\nIf the distribution is maintaining OpenSSL, then you probably can't determine the version of OpenSSL due to back patching using openssl command or the package information (for example, apt-get, dpkg, yum or rpm). The back patching process used by most (all?) distributions only uses the base version number (for example, \"1.0.1e\"); and does not include an effective security version (for example, \"1.0.1g\").\nThere's an open question on Super User to determine the effective security version for OpenSSL and other packages when packages are backpatched. Unfortunately, there are no useful answers (other than check the distro's website). See Determine Effective Security Version when faced with Backpatching?.\nAs a rule of thumb: if you have ever installed one of the affected versions, and have ever run programs or services that linked against OpenSSL for TLS support, then you are vulnerable.\n\nWhere can I find a program to test for the vulnerability?\n\nWithin hours of the Heartbleed announcement, several people on the internet had publicized publicly-accessible web applications that supposedly could be used to check a server for the presence of this vulnerability. As of this writing, I have not reviewed any, so I won't further publicize their applications. They can be found relatively easily with the help of your preferred search engine.\n\nHow is this vulnerability mitigated?\n\nUpgrade to a non-vulnerable version and reset or re-secure vulnerable data. As noted on the Heartbleed site, appropriate response steps are broadly:\n\nPatch vulnerable systems.\nRegenerate new private keys.\nSubmit new CSR to your CA.\nObtain and install new signed certificate.\nInvalidate session keys and cookies\nReset passwords and shared secrets\nRevoke old certificates.\n\nFor a more detailed analysis and answer, see What should a website operator do about the Heartbleed OpenSSL exploit? on the Security Stack Exchange.\n\nShould I be concerned that my keys or other private data have been\n  compromised? What other side effects should I be concerned about?\n\nAbsolutely. Systems Administrators need to assume that their servers which used vulnerable OpenSSL versions are indeed compromised and respond accordingly.\nShortly after the vulnerability was disclosed, Cloudfare offered a challenge to see if a server's private key could be recovered in practice. The challenge was independently won by Fedor Indutny and Ilkka Mattila. See The Heartbleed Challenge.\n\nWhere can I find more information?\n\nLink dump, for those looking for more details:\n\nOpenSSL SECADV 2014047\nCVE-2014-0160\nHeartbleed\nUbuntu announcement\nRHEL announcement\nOfficial OpenSSL announcement\n\n\nA rather detailed timeline of the disclosure events can be found at Heartbleed disclosure timeline: who knew what and when.\n\nIf you are a programmer and are interested in various programming tricks like detecting a Heartbleed attack through OpenSSL's msg_cb callback, then see OpenSSL's Security Advisory 2014047.",
        "url": "https://serverfault.com/questions/587329/heartbleed-what-is-it-and-what-are-options-to-mitigate-it"
    },
    {
        "title": "DNS - NSLOOKUP what is the meaning of the non-authoritative answer?",
        "question": "For some domains nslookup gives me a Non-authoritative answer section. What does this mean?\nGot answer:\n    HEADER:\n        opcode = QUERY, id = 3, rcode = NXDOMAIN\n        header flags:  response, want recursion, recursion avail.\n        questions = 1,  answers = 0,  authority records = 1,  additional =\n\n    QUESTIONS:\n        www.example.com.SME, type = AAAA, class = IN\n    AUTHORITY RECORDS:\n    ->  (root)\n        ttl = 1787 (29 mins 47 secs)\n        primary name server = a.root-servers.net\n        responsible mail addr = nstld.verisign-grs.com\n\n------------\nNon-authoritative answer:\n------------\n\n------------\nName:    example.com\nAddress:  93.184.216.34\nAliases:  www.example.com\n",
        "top_answer": "Basically, it's what the name says it is.  An authoritative answer comes from a nameserver that is considered authoritative for the domain which it's returning a record for (one of the nameservers in the list for the domain you did a lookup on), and a non-authoritative answer comes from anywhere else (a nameserver not in the list for the domain you did a lookup on).\nIt's basically a distinction between a nameserver that's an official nameserver for the domain you're querying, and a nameserver that isn't.  Nameservers that aren't authoritative are getting their answers second (or third or fourth...) hand - just relaying the information along from somewhere else.\nSo, for example, If I did an nslookup of maps.google.com right now, I would get a response from one of my configured nameservers.  (Either from my ISP, or my domain.)  It would come back as non-authoritative because neither my ISP's nameservers, nor my own are in the list of nameservers for google.com.  They aren't Google's nameservers, so they're not the authoritative source that creates the NS records.\nThe list of authoritative nameservers for Google is below (from whois.internic.net).\n\nDomain Name: GOOGLE.COM\nRegistrar: MARKMONITOR INC.\nWhois Server: whois.markmonitor.com\nName Server: NS1.GOOGLE.COM\nName Server: NS2.GOOGLE.COM\nName Server: NS3.GOOGLE.COM\nName Server: NS4.GOOGLE.COM\nUpdated Date: 20-jul-2011\nCreation Date: 15-sep-1997\nExpiration Date: 14-sep-2020\n\nIf I changed my configured DNS server to one of the ones in that list, and then did an nslookup against maps.google.com, I'd get an authoritative answer back.  Those servers are the authority, (or source) for what are valid names in Google's domains, and what aren't.  All other nameservers, non-authoritative nameservers, get their NS records from the authoritative servers somewhere down the line.",
        "url": "https://serverfault.com/questions/413124/dns-nslookup-what-is-the-meaning-of-the-non-authoritative-answer"
    },
    {
        "title": "ssh-agent forwarding and sudo to another user",
        "question": "If I have a server A into which I can login with my ssh key and I have the ability to \"sudo su - otheruser\", I lose key forwarding, because the env variables are removed and the socket is only readable by my original user. Is there a way I can bridge the key forwarding through the \"sudo su - otheruser\", so I can do stuff on a server B with my forwarded key (git clone and rsync in my case)?\nThe only way I can think of is adding my key to authorized_keys of otheruser and \"ssh otheruser@localhost\", but that's cumbersome to do for every user and server combination I may have.\nIn short:\n$ sudo -HE ssh user@host\n(success)\n$ sudo -HE -u otheruser ssh user@host\nPermission denied (publickey). \n",
        "top_answer": "As you mentioned, the environment variables are removed by sudo, for security reasons.\nBut fortunately sudo is quite configurable: you can tell it precisely which environment variables you want to keep thanks to the env_keep configuration option in /etc/sudoers.\nFor agent forwarding, you need to keep the SSH_AUTH_SOCK environment variable.  To do so, simply edit your /etc/sudoers configuration file (always using visudo) and set the env_keep option to the appropriate users.  If you want this option to be set for all users, use the Defaults line like this:\nDefaults    env_keep+=SSH_AUTH_SOCK\n\nman sudoers for more details.\nYou should now be able to do something like this (provided user1's public key is present in ~/.ssh/authorized_keys in user1@serverA and user2@serverB, and serverA's /etc/sudoers file is setup as indicated above):\nuser1@mymachine> eval `ssh-agent`  # starts ssh-agent\nuser1@mymachine> ssh-add           # add user1's key to agent (requires pwd)\nuser1@mymachine> ssh -A serverA    # no pwd required + agent forwarding activated\nuser1@serverA> sudo su - user2     # sudo keeps agent forwarding active :-)\nuser2@serverA> ssh serverB         # goto user2@serverB w/o typing pwd again...\nuser2@serverB>                     # ...because forwarding still works\n",
        "url": "https://serverfault.com/questions/107187/ssh-agent-forwarding-and-sudo-to-another-user"
    },
    {
        "title": "SSL Certificate Location on UNIX/Linux",
        "question": "Is there any standard or convention for where SSL certificates and associated private keys should go on the UNIX/Linux filesystem?",
        "top_answer": "For system-wide use, OpenSSL should provide you /etc/ssl/certs and /etc/ssl/private. The latter of which will be restricted 700 to root:root.\nIf you have an application that doesn\u2019t perform initial privilege separation from root, then it might suit you to locate them somewhere local to the application with the relevantly restricted ownership and permissions.",
        "url": "https://serverfault.com/questions/62496/ssl-certificate-location-on-unix-linux"
    },
    {
        "title": "Nginx enable site command",
        "question": "We all know how to enable a website using apache on Linux.\nI'm pretty sure that we all agree on using the a2ensite command.\nUnfortunately, there is no default equivalent command that comes with Nginx, but it did happen that I installed some package on ubuntu that allowed me to enable/disable sites and list them.\nThe problem is I don't remember the name of this package.\nDoes anybody know what I'm talking about?\nPlease tell me the name of this package and the command name.",
        "top_answer": "If you have installed the nginx package from the Ubuntu repositories, you will have two directories.\n/etc/nginx/sites-enabled and /etc/nginx/sites-available.\nIn the main nginx configuration, /etc/nginx/nginx.conf, you have the following line:\ninclude /etc/nginx/sites-enabled/*.conf;\n\nSo basically to list all available virtualhosts, you can run the following command:\nls /etc/nginx/sites-available\n\nTo activate one of them, run the following command:\nln -s /etc/nginx/sites-available/www.example.org.conf /etc/nginx/sites-enabled/\n\nThe scripts that comes with Apache is basically just simple shell wrappers that does something similar as above.\nAfter linking the files, remember to run sudo service nginx reload/ service nginx reload",
        "url": "https://serverfault.com/questions/424452/nginx-enable-site-command"
    },
    {
        "title": "What is \"-bash: !\": event not found\"",
        "question": "Try executing the following under a bash shell echo \"Reboot your instance!\"\nOn my installation:\nroot@domU-12-31-39-04-11-83:/usr/local/bin# bash --version\nGNU bash, version 4.1.5(1)-release (i686-pc-linux-gnu)\nCopyright (C) 2009 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nroot@domU-12-31-39-04-11-83:/usr/local/bin# uname -a\nLinux domU-12-31-39-04-11-83 2.6.35-22-virtual #35-Ubuntu SMP Sat Oct 16 23:57:40 UTC 2010 i686 GNU/Linux\nroot@domU-12-31-39-04-11-83:/usr/local/bin# echo \"Reboot your instance!\"\n-bash: !\": event not found\n\nCan anyone please explain what is \"bash events?\" I've never heard this concept before. Also, how should I output \"!\" at the end of the sentence?",
        "top_answer": "You can turn off history substitution using set +H.",
        "url": "https://serverfault.com/questions/208265/what-is-bash-event-not-found"
    },
    {
        "title": "LVM dangers and caveats",
        "question": "I have recently started using LVM on some servers for hard drives larger than 1 TB. They're useful, expandable and quite easy to install. However, I could not find any data about the dangers and caveats of LVM.\nWhat are the downsides of using LVM?",
        "top_answer": "Summary\nRisks of using LVM:\n\nVulnerable to write caching issues with SSD or VM hypervisor\nHarder to recover data due to more complex on-disk structures\nHarder to resize filesystems correctly\nClassic snapshots are hard to use, slow and buggy\nRequires some skill to configure correctly given these issues\n\nThe first two LVM issues combine: if write caching isn't working correctly and you have a power loss (e.g. PSU or UPS fails), you may well have to recover from backup, meaning significant downtime.  A key reason for using LVM is higher uptime (when adding disks, resizing filesystems, etc), but it's important to get the write caching setup correct to avoid LVM actually reducing uptime.\n-- Updated Feb 2025: new material on LVM snapshots of thin-provisioned logical volumes, and update on resizing LVs\nMitigating the risks\nLVM can still work well if you:\n\nGet your write caching setup right in hypervisor, kernel, and SSDs\nAvoid LVM snapshots\nUse recent LVM versions to resize filesystems\nHave good backups\n\nDetails\nI've researched this quite a bit in the past having experienced some data loss associated with LVM.  The main LVM risks and issues I'm aware of are:\nVulnerable to hard disk write caching due to VM hypervisors, disk caching or old Linux kernels, and makes it harder to recover data due to more complex on-disk structures - see below for details.  I have seen complete LVM setups on several disks get corrupted without any chance of recovery, and LVM plus hard disk write caching is a dangerous combination.\n\nWrite caching and write re-ordering by the hard drive is important to good performance, but can fail to flush blocks to disk correctly due to VM hypervisors, hard drive write caching, old Linux kernels, etc.\nWrite barriers mean the kernel guarantees that it will complete certain disk writes before the \"barrier\" disk write, to ensure that filesystems and RAID can recover in the event of a sudden power loss or crash.  Such barriers can use a FUA (Force Unit Access) operation to immediately write certain blocks to the disk, which is more efficient than a full cache flush.  Barriers can be combined with efficient tagged/native command queuing (issuing multiple disk I/O requests at once) to enable the hard drive to perform intelligent write re-ordering without increasing risk of data loss.\nVM hypervisors can have similar issues: running LVM in a Linux guest on top of a VM hypervisor such as VMware, Xen, KVM, Hyper-V or VirtualBox can create similar problems to a kernel without write barriers, due to write caching and write re-ordering.  Check your hypervisor documentation carefully for a \"flush to disk\" or write-through cache option (present in KVM, VMware, Xen, VirtualBox and others) - and test it with your setup. Some hypervisors such as VirtualBox have a default setting that ignores any disk flushes from the guest.\nEnterprise servers with LVM should always use a battery backed RAID controller and disable the hard disk write caching (the controller has battery backed write cache which is fast and safe) - see this comment by the author of this XFS FAQ entry.  It may also be safe to turn off write barriers in the kernel, but testing is recommended.\nIf you don't have a battery-backed RAID controller, disabling hard drive write caching will slow writes significantly but make LVM safe.   You should also use the equivalent of ext3's data=ordered option (or data=journal for extra safety), plus barrier=1 to ensure that kernel caching doesn't affect integrity. (Or use ext4 which enables barriers by default.) This is the simplest option and provides good data integrity at cost of performance.  (Linux changed the default ext3 option to the more dangerous data=writeback a while back, so don't rely on the default settings for the FS.)\nTo disable hard drive write caching: add hdparm -q -W0 /dev/sdX for all drives in /etc/rc.local (for SATA) or use sdparm for SCSI/SAS.  However, according to this entry in the XFS FAQ (which is very good on this topic), a SATA drive may forget this setting after a drive error recovery - so you should use SCSI/SAS, or if you must use SATA then put the hdparm command in a cron job running every minute or so.\nTo keep SSD / hard drive write caching enabled for better performance: this is a complex area - see section below.\nIf you are using Advanced Format drives i.e. 4 KB physical sectors, see below - disabling write caching may have other issues.\nUPS is critical for both enterprise and SOHO but not enough to make LVM safe: anything that causes a hard crash or a power loss (e.g. UPS failure, PSU failure, or laptop battery exhaustion) may lose data in hard drive caches.\nSummary: you must take care in the filesystem, RAID, VM hypervisor, and hard drive/SSD setup used with LVM.  You must have very good backups if you are using LVM, and be sure to specifically back up the LVM metadata, physical partition setup, MBR and volume boot sectors.  It's also advisable to use SCSI/SAS drives as these are less likely to lie about how they do write caching - more care is required to use SATA drives.\n\nKeeping write caching enabled for performance (and coping with lying drives)\nA more complex but performant option is to keep SSD / hard drive write caching enabled and rely on kernel write barriers working with LVM (double-check by looking for \"barrier\" messages in the logs).\nYou should also ensure that the RAID setup, VM hypervisor setup and filesystem uses write barriers (i.e. requires the drive to flush pending writes before and after key metadata/journal writes).  XFS does use barriers by default, but ext3 does not, so with ext3 you should use barrier=1 in the mount options, and still use data=ordered or data=journal as above.\n\nUnfortunately, some hard drives and SSDs lie about whether they have really flushed their cache to the disk (particularly older drives, but including some SATA drives and some enterprise SSDs) - more details here.  There is a great summary from an XFS developer.\nThere's a simple test tool for lying drives (Perl script), or see this background with another tool testing for write re-ordering as a result of the drive cache.  This answer covered similar testing of SATA drives that uncovered a write barrier issue in software RAID - these tests actually exercise the whole storage stack.\nMore recent SATA drives supporting Native Command Queuing (NCQ) may be less likely to lie - or perhaps they perform well without write caching due to NCQ, and very few drives cannot disable write caching.\nSCSI/SAS drives are generally OK as they don't need write caching to perform well (through SCSI Tagged Command Queuing, similar to SATA's NCQ).\nIf your hard drives or SSDs do lie about flushing their cache to disk, you really can't rely on write barriers and must disable write caching.  This is a problem for all filesystems, databases, volume managers, and software RAID, not just LVM.\n\nSSDs are problematic because the use of write cache is critical to the lifetime of the SSD.  It's best to use an SSD that has a supercapacitor (to enable cache flushing on power failure, and hence enable cache to be write-back not write-through).\n\nMost enterprise SSDs should be OK on write cache control, and some include supercapacitors.\nSome cheaper SSDs have issues that can't be fixed with write-cache configuration - the PostgreSQL project's mailing list and Reliable Writes wiki page are good sources of information.  Consumer SSDs can have major write caching problems that will cause data loss, and don't include supercapacitors so are vulnerable to power failures causing corruption.\n\nAdvanced Format drive setup - write caching, alignment, RAID, GPT\n\nWith newer Advanced Format drives that use 4 KiB physical sectors, it may be important to keep drive write caching enabled, since most such drives currently emulate 512 byte logical sectors (\"512 emulation\"), and some even claim to have 512-byte physical sectors while really using 4 KiB.\nTurning off the write cache of an Advanced Format drive may cause a very large performance impact if the application/kernel is doing 512 byte writes, as such drives rely on the cache to accumulate 8 x 512-byte writes before doing a single 4 KiB physical write.  Testing is recommended to confirm any impact if you disable the cache.\nAligning the LVs on a 4 KiB boundary is important for performance but should happen automatically as long as the underlying partitions for the PVs are aligned, since LVM Physical Extents (PEs) are 4 MiB by default. RAID must be considered here - this LVM and software RAID setup page suggests putting the RAID superblock at the end of the volume and (if necessary) using an option on pvcreate to align the PVs. This LVM email list thread points to the work done in kernels during 2011 and the issue of partial block writes when mixing disks with 512 byte and 4 KiB sectors in a single LV.\nGPT partitioning with Advanced Format needs care, especially for boot+root disks, to ensure the first LVM partition (PV) starts on a 4 KiB boundary.\n\nHarder to recover data due to more complex on-disk structures:\n\nAny recovery of LVM data required after a hard crash or power loss (due to incorrect write caching) is a manual process at best, because there are apparently no suitable tools. LVM is good at backing up its metadata under /etc/lvm, which can help restore the basic structure of LVs, VGs and PVs, but will not help with lost filesystem metadata.\nHence a full restore from backup is likely to be required.  This involves a lot more downtime than a quick journal-based fsck when not using LVM, and data written since the last backup will be lost.\nTestDisk,  ext3grep, ext3undel and other tools  can recover partitions and files from non-LVM disks but they don't directly support LVM data recovery.  TestDisk can discover that a lost physical partition contains an LVM PV, but none of these tools understand LVM logical volumes. File carving tools such as  PhotoRec and many others would work as they bypass the filesystem to re-assemble files from data blocks, but this is a last-resort, low-level approach for valuable data, and works less well with fragmented files.\nManual LVM recovery is possible in some cases, but is complex and time consuming - see this example and this, this, and this for how to recover.\n\nHarder to resize filesystems correctly - easy filesystem resizing is often given as a benefit of LVM, but you need to run half a dozen shell commands to resize an LVM based FS - this can be done with the whole server still up, and in some cases with the FS mounted.  However, I would never risk doing this on a mounted FS without up to date backups and using commands pre-tested on an equivalent server (e.g. disaster recovery clone of production server).\n\nUpdate: Current (2025) versions of lvextend support the -r (--resizefs) option which is a safer and quicker way to resize the LV and the filesystem, particularly if you are shrinking the FS, so you can mostly skip this section.\n\nOlder material - use lvextend --resizefs instead:\n\nMost guides to resizing LVM-based FSs don't take account of the fact that the FS must be somewhat smaller than the size of the LV: detailed explanation here.   When shrinking a filesystem, you will need to specify the new size to the FS resize tool, e.g. resize2fs for ext3, and to lvextend or lvreduce.  Without great care, the sizes may be slightly different due to the difference between 1 GB (10^9) and 1 GiB (2^30), or the way the various tools round sizes up or down.\n\nIf you don't do the calculations exactly right (or use some extra steps beyond the most obvious ones), you may end up with an FS that is too large for the LV.  Everything will seem fine for months or years, until you completely fill the FS, at which point you will get serious corruption - and unless you are aware of this issue it's hard to find out why, as you may also have real disk errors by then that cloud the situation.  (It's possible this issue only affects reducing the size of filesystems - however, it's clear that resizing filesystems in either direction does increase the risk of data loss, possibly due to user error.)\n\nIt seems that the LV size should be larger than the FS size by 2 x the LVM physical extent (PE) size - but check the link above for details as the source for this is not authoritative.  Often allowing 8 MiB is enough, but it may be better to allow more, e.g. 100 MiB or 1 GiB, just to be safe. To check the PE size, and your logical volume+FS sizes, using 4 KiB = 4096 byte blocks:\nShows PE size in KiB:\nvgdisplay --units k myVGname | grep \"PE Size\"\nSize of all LVs:\nlvs --units 4096b\nSize of (ext3) FS, assumes 4 KiB FS blocksize:\ntune2fs -l /dev/myVGname/myLVname | grep 'Block count'\n\n\nResizing filesystems when not using LVM\nNon-LVM setups make resizing the FS very reliable and easy:\n\nRun Gparted and resize the FSs required, then it will do everything for you. On servers, you can use parted from the shell.\nIt's often best to use the Gparted Live CD or Parted Magic, as these have a recent and often more bug-free Gparted & kernel than the distro version - I once lost a whole FS due to the distro's Gparted not updating partitions properly in the running kernel.  If using the distro's Gparted, be sure to reboot right after changing partitions so the kernel's view is correct.\n\nClassic snapshots are hard to use, slow and buggy - classic snapshots (on volumes not using thin provisioning) have some issues:\n\nIf a snapshot runs out of pre-allocated space it is automatically dropped.  Each snapshot of a given LV is a delta against that LV (not against previous snapshots) which can require a lot of space when snapshotting filesystems with significant write activity (every snapshot is larger than the previous one).  It is safe to create a snapshot LV that's the same size as the original LV, as the snapshot will then never run out of free space.\nSnapshots can also be very slow (meaning 3 to 6 times slower than without LVM for these MySQL tests) - see this answer covering various snapshot problems.  The slowness is partly because snapshots require many synchronous writes.\nSnapshots have had some significant bugs as of mid to late 2010s, but these may now be fixed.  For example, in some cases they used to make boot very slow or cause boot failures (fixed in 2015).\nHowever, many snapshot race condition bugs were apparently fixed by 2015.\n\nLVM without snapshots generally seems quite well debugged, perhaps because snapshots aren't used as much as the core features.\nSnapshots of thin-provisioned volumes - LVM now includes an option for thin-provisioned volumes as @shodanshok mentioned in their answer.  Snapshots taken on these volumes are implemented in a far more efficient way, so the issues above don't apply.  This does add some complexity in managing special data and metadata LVs that are used in thin-provisioned LVs. This in turn can make recovery more complex - see these questions and answers.\nSnapshot alternatives - filesystems and VM hypervisors\nVM/cloud snapshots:\n\nIf you are using a VM hypervisor or an IaaS cloud provider (e.g. VMware, VirtualBox or Amazon EC2/EBS), their snapshots are often a simpler alternative to LVM snapshots.  You can quite easily take one of these snapshots for backup purposes (but consider freezing the FS before you do).\n\nFilesystem snapshots:\n\nFilesystem level snapshots with ZFS or btrfs are easy to use and generally better than LVM, if you are on bare metal. ZFS seems a lot more mature, but somewhat more hassle to install.  Both provide checksumming of all filesystem blocks.\nZFS: there is now a kernel ZFS implementation, which has been in use for some years, and ZFS seems to be gaining adoption. Ubuntu now has ZFS as an 'out of the box' option, including experimental ZFS on root in 19.10.\nbtrfs: some limitations for production use - even on openSUSE which ships it by default and has team dedicated to btrfs), whereas RHEL has stopped supporting it.  btrfs now has an fsck tool, btrfs-check, but as of 2024 that doc recommends that you consult a developer if you need to repair a broken filesystem.  This openSUSE support note provides some tips.\n\nSnapshots for online backups and fsck\nSnapshots can be used to provide a consistent source for backups, as long as you are careful with space allocated (ideally the snapshot is the same size as the LV being backed up). The excellent rsnapshot (since 1.3.1) even manages the LVM snapshot creation/deletion for you - see this HOWTO on rsnapshot using LVM.  However, note the general issues with snapshots and that a snapshot should not be considered a backup in itself.\nYou can also use LVM snapshots to do an online fsck: snapshot the LV and fsck the snapshot, while still using the main non-snapshot FS - described here - however, it's not entirely straightforward so it's best to use e2croncheck as described by Ted Ts'o, maintainer of ext3.\nYou should \"freeze\" the filesystem temporarily while taking the snapshot - some filesystems such as ext3 and XFS will do this automatically when LVM creates the snapshot.\nConclusions\nDespite all this, I do still use LVM on some systems, but for a desktop setup I prefer raw partitions.  The main benefit I can see from LVM is the flexibility of moving and resizing FSs when  you must have high uptime on a server - if you don't need that, gparted is easier and has less risk of data loss.\nLVM requires great care on write caching setup due to VM hypervisors, hard drive / SSD write caching, and so on - but the same applies to using Linux as a DB server.  The lack of support from most tools (gparted including the critical size calculations, and testdisk etc) makes it harder to use than it should be.\nIf using LVM, take great care with snapshots: use VM/cloud snapshots if possible, or investigate ZFS/btrfs to avoid LVM completely - you may find ZFS or btrfs is sufficiently mature compared to LVM with snapshots.\nBottom line: If you don't know about the issues listed above and how to address them, it's best not to use LVM.",
        "url": "https://serverfault.com/questions/279571/lvm-dangers-and-caveats"
    },
    {
        "title": "how to disable SSH login with password for some users?",
        "question": "On Linux (Debian Squeeze) I would like to disable SSH login using password to some users (selected group or all users except root). But I do not want to disable login using certificate for them.\nedit: thanks a lot for detailed answer! For some reason this does not work on my server:\nMatch User !root\nPasswordAuthentication no\n\n...but can be easily replaced by\nPasswordAuthentication no\nMatch User root\nPasswordAuthentication yes\n",
        "top_answer": "Try Match in sshd_config:\nMatch User user1,user2,user3,user4\n    PasswordAuthentication no\n\nOr by group:\nMatch Group users\n    PasswordAuthentication no\n\nOr, as mentioned in the comment, by negation:\nMatch User !root\n    PasswordAuthentication no\n\nNote that match is effective \"until either another Match line or the end of the file.\" (the indentation isn't significant)",
        "url": "https://serverfault.com/questions/285800/how-to-disable-ssh-login-with-password-for-some-users"
    },
    {
        "title": "How to know from which yum repository a package has been installed?",
        "question": "After I have installed a package by yum (with multiple repositories configured), how can I find from which repository it has been installed?\nIf I run yum info package-name (or yum list package-name), I can only see that the package is \"installed\".",
        "top_answer": "With yum-utils installed, repoquery (with the --info option) will provide the information you seek (here 'epel' being the repository).\n$ repoquery --info cherokee\n\nName        : cherokee\nVersion     : 0.99.49\nRelease     : 1.el5\nArchitecture: i386\nSize        : 8495964\nPackager    : Fedora Project\nGroup       : Applications/Internet\nURL         : http://www.cherokee-project.com/\nRepository  : epel\nSummary     : Flexible and Fast Webserver\nDescription :\nCherokee is a very fast, flexible and easy to configure Web Server. It supports\nthe widespread technologies nowadays: FastCGI, SCGI, PHP, CGI, TLS and SSL\nencrypted connections, Virtual hosts, Authentication, on the fly encoding,\nApache compatible log files, and much more.\n",
        "url": "https://serverfault.com/questions/62026/how-to-know-from-which-yum-repository-a-package-has-been-installed"
    },
    {
        "title": "How to cd into a directory with this name \"-2\" (starting with the hyphen)?",
        "question": "I have a directory named:\n-2\n\nI want to cd into it but the cd complains:\nbash: cd: -2: invalid option\n\nWith no success, I've tried:\ncd \"-2\"\ncd '-2'\ncd \\-2\n\nAny solution?\nEdit: no file browsers like mc, etc. available on the server.",
        "top_answer": "At least two ways:\n\nUse the -- argument.\ncd -- -2\n\nThis uses a convention common to GNU tools which is to not treat anything that appears after -- as a command line option.\nAs a commenter noted, this convention is also defined in the POSIX standard:\n\nDefault Behavior: When this section is listed as \"None.\", it means that the implementation need not support any options. Standard utilities that do not accept options, but that do accept operands, shall recognize \"--\" as a first argument to be discarded.\nThe requirement for recognizing \"--\" is because conforming applications need a way to shield their operands from any arbitrary options that the implementation may provide as an extension. For example, if the standard utility foo is listed as taking no options, and the application needed to give it a pathname with a leading hyphen, it could safely do it as:\nfoo -- -myfile\n\nand avoid any problems with -m used as an extension.\n\nas well as:\n\nGuideline 10:\n  The argument -- should be accepted as a delimiter indicating the end of options. Any following arguments should be treated as operands, even if they begin with the '-' character. The -- argument should not be used as an option or as an operand.\n\nSpecify the path explicitly:\ncd ./-2\n\nThis specifies the path explicitly naming the current directory (.) as the starting point.\ncd $(pwd)/-2\ncd /absolute/path/to/-2\n\nThese are variations on the above. Any number of such variations may be possible; I'll leave it as an exercise to the reader to discover all of them.\n",
        "url": "https://serverfault.com/questions/462739/how-to-cd-into-a-directory-with-this-name-2-starting-with-the-hyphen"
    },
    {
        "title": "Is it normal to get hundreds of break-in attempts per day?",
        "question": "I just checked my server's /var/log/auth.log and found that I'm getting over 500 failed password/break-in attempt notifications per day! My site is small, and its URL is obscure. Is this normal? Should I be taking any measures?",
        "top_answer": "In today's internet this is quite normal sadly. There are hordes of botnets trying to login to each server they find in whole IP networks. Typically, they use simple dictionary attacks on well-known accounts (like root or certain applications accounts).\nThe attack targets are not found via Google or DNS entries, but the attackers just try every IP address in a certain subnet (e.g. of known root-server hosting companies). So it doesn't matter that your URL (hence the DNS entry) is rather obscure.\nThat's why it is so important to:\n\ndisallow root-login in SSH (howto)\nuse strong passwords everywhere (also in your web applications)\nfor SSH, use public-key authentication if possible and disable password-auth completely (howto)\n\nAdditionally, you can install fail2ban which will scan the authlog and if it finds a certain amount of failed login attempts from an IP, it will proceed to add that IP to /etc/hosts.deny or iptables/netfilter in order to lock out the attacker for a few minutes.\nIn addition to the SSH attacks, it is also becoming common to scan your webserver for vulnerable web-applications (some blogging apps, CMSs, phpmyadmin, etc.). So make sure to keep those up-to-date and securely configured too!",
        "url": "https://serverfault.com/questions/244614/is-it-normal-to-get-hundreds-of-break-in-attempts-per-day"
    },
    {
        "title": "Resolve host name from IP address",
        "question": "I'm looking for a command line tool which gets an IP address and returns the host name, for Windows.",
        "top_answer": "The command you are looking for is called nslookup, works fine for reverse lookups IFF someone has configured a reverse zone file, which they don't always do.",
        "url": "https://serverfault.com/questions/74042/resolve-host-name-from-ip-address"
    },
    {
        "title": "Filename length limits on linux?",
        "question": "Are there any filename or path length limits on Linux?   ",
        "top_answer": "See the Wikipedia page about file systems comparison, especially in column Maximum filename length.\nHere are some filename length limits in popular file systems:\nBTRFS   255 bytes\nexFAT   255 UTF-16 characters\next2    255 bytes\next3    255 bytes\next3cow 255 bytes\next4    255 bytes\nFAT32   8.3 (255 UCS-2 code units with VFAT LFNs)\nNTFS    255 characters\nXFS     255 bytes\n",
        "url": "https://serverfault.com/questions/9546/filename-length-limits-on-linux"
    },
    {
        "title": "What are the different widely used RAID levels and when should I consider them?",
        "question": "\nThis is a Canonical Question about RAID levels. \n\nWhat are:\n\nthe RAID levels typically used (including the RAID-Z family)? \ndeployments are they commonly found in?\nbenefits and pitfalls of each?\n",
        "top_answer": "RAID: Why and When\nRAID stands for Redundant Array of Independent Disks (some are taught \"Inexpensive\" to indicate that they are \"normal\" disks; historically there were internally redundant disks which were very expensive; since those are no longer available the acronym has adapted).\nAt the most general level, a RAID is a group of disks that act on the same reads and writes. SCSI IO is performed on a volume (\"LUN\"), and these are distributed to the underlying disks in a way that introduces a performance increase and/or a redundancy increase. The performance increase is a function of striping: data is spread across multiple disks to allow reads and writes to use all the disks' IO queues simultaneously. Redundancy is a function of mirroring. Entire disks can be kept as copies, or individual stripes can be written multiple times. Alternatively, in some types of raid, instead of copying data bit for bit, redundancy is gained by creating special stripes that contain parity information, which can be used to recreate any lost data in the event of a hardware failure.\nThere are several configurations that provide different levels of these benefits, which are covered here, and each one has a bias toward performance, or redundancy.\nAn important aspect in evaluating which RAID level will work for you depends on its advantages and hardware requirements (E.g.: number of drives).\nAnother important aspect of most of these types of RAID (0,1,5) is that they do not ensure the integrity of your data, because they are abstracted away from the actual data being stored. So RAID does not protect against corrupted files. If a file is corrupted by any means, the corruption will be mirrored or paritied and committed to the disk regardless. However, RAID-Z does claim to provide file-level integrity of your data.\n\n\nDirect attached RAID: Software and Hardware\nThere are two layers at which RAID can be implemented on direct attached storage: hardware and software. In true hardware RAID solutions, there is a dedicated hardware controller with a processor dedicated to RAID calculations and processing. It also typically has a battery-backed cache module so that data can be written to disk, even after a power failure. This helps to eliminate inconsistencies when systems are not shut down cleanly. Generally speaking, good hardware controllers are better performers than their software counterparts, but they also have a substantial cost and increase complexity.\nSoftware RAID typically does not require a controller, since it doesn't use a dedicated RAID processor or a separate cache. Typically these operations are handled directly by the CPU. In modern systems, these calculations consume minimal resources, though some marginal latency is incurred. RAID is handled by either the OS directly, or by a faux controller in the case of FakeRAID. \nGenerally speaking, if someone is going to choose software RAID, they should avoid FakeRAID and use the OS-native package for their system such as Dynamic Disks in Windows, mdadm/LVM in Linux, or ZFS in Solaris, FreeBSD, and other related distributions. FakeRAID use a combination of hardware and software which results in the initial appearance of hardware RAID, but the actual performance of software RAID. Additionally it is commonly extremely difficult to move the array to another adapter (should the original fail).\n\n\nCentralized Storage\nThe other place RAID is common is on centralized storage devices, usually called a SAN (Storage Area Network) or a NAS (Network Attached Storage). These devices manage their own storage and allow attached servers to access the storage in various fashions. Since multiple workloads are contained on the same few disks, having a high level of redundancy is generally desirable. \nThe main difference between a NAS and a SAN is block vs. file system level exports.  A SAN exports a whole \"block device\" such as a partition or logical volume (including those built on top of a RAID array).  Examples of SANs include Fibre Channel and iSCSI.  A NAS exports a \"file system\" such as a file or folder.  Examples of NASs include CIFS/SMB (Windows file sharing) and NFS.\n\n\nRAID 0\nGood when: Speed at all costs!\nBad when: You care about your data\nRAID0 (aka Striping) is sometimes referred to as \"the amount of data you will have left when a drive fails\". It really runs against the grain of \"RAID\", where the \"R\" stands for \"Redundant\".\nRAID0 takes your block of data, splits it up into as many pieces as you have disks (2 disks \u2192 2 pieces, 3 disks \u2192 3 pieces) and then writes each piece of the data to a separate disk.\nThis means that a single disk failure destroys the entire array (because you have Part 1 and Part 2, but no Part 3), but it provides very fast disk access.\nIt is not often used in production environments, but it could be used in a situation where you have strictly temporary data that can be lost without repercussions. It is used somewhat commonly for caching devices (such as an L2Arc device).\nThe total usable disk space is the sum of all the disks in the array added together (e.g. 3x 1TB disks = 3TB of space).\n\n\nRAID 1\nGood when: You have limited number of disks but need redundancy\nBad when: You need a lot of storage space\nRAID 1 (aka Mirroring) takes your data and duplicates it identically on two or more disks (although typically only 2 disks). If more than two disks are used the same information is stored on each disk (they're all identical). It is the only way to ensure data redundancy when you have less than three disks.\nRAID 1 sometimes improves read performance. Some implementations of RAID 1 will read from both disks to double the read speed. Some will only read from one of the disks, which does not provide any additional speed advantages. Others will read the same data from both disks, ensuring the array's integrity on every read, but this will result in the same read speed as a single disk.\nIt is typically used in small servers that have very little disk expansion, such as 1RU servers that may only have space for two disks or in workstations that require redundancy. Because of its high overhead of \"lost\" space, it can be cost prohibitive with small-capacity, high-speed (and high-cost) drives, as you need to spend twice as much money to get the same level of usable storage.\nThe total usable disk space is the size of the smallest disk in the array (e.g. 2x 1TB disks = 1TB of space).\n\n\nRAID 1E\nThe 1E RAID level is similar to RAID 1 in that data is always written to (at least) two disks. But unlike RAID1, it allows for an odd number of disks by simply interleaving the data blocks among several disks.\nPerformance characteristics are similar to RAID1, fault tolerance is similar to RAID 10. This scheme can be extended to odd numbers of disks more than three (possibly called RAID 10E, though rarely).\n\n\nRAID 10\nGood when: You want speed and redundancy\nBad when: You can't afford to lose half your disk space\nRAID 10 is a combination of RAID 1 and RAID 0. The order of the 1 and 0 is very important. Say you have 8 disks, it will create 4 RAID 1 arrays, and then apply a RAID 0 array on top of the 4 RAID 1 arrays. It requires at least 4 disks, and additional disks have to be added in pairs.\nThis means that one disk from each pair can fail. So if you have sets A, B, C and D with disks A1, A2, B1, B2, C1, C2, D1, D2, you can lose one disk from each set (A,B,C or D) and still have a functioning array.\nHowever, if you lose two disks from the same set, then the array is totally lost. You can lose up to (but not guaranteed) 50% of the disks.\nYou are guaranteed high speed and high availability in RAID 10.\nRAID 10 is a very common RAID level, especially with high capacity drives where a single disk failure makes a second disk failure more likely before the RAID array is rebuilt. During recovery, the performance degradation is much lower than its RAID 5 counterpart as it only has to read from one drive to reconstruct the data.\nThe available disk space is 50% of the sum of the total space. (e.g. 8x 1TB drives = 4TB of usable space). If you use different sizes, only the smallest size will be used from each disk.\nIt is worth noting that the Linux kernel's software raid driver called md allows for RAID 10 configurations with an odd amount of drives, i.e. a 3 or 5 disk RAID 10.\n\n\nRAID 01\nGood when: never\nBad when: always\nIt is the reverse of RAID 10. It creates two RAID 0 arrays, and then puts a RAID 1 over the top. This means that you can lose one disk from each set (A1, A2, A3, A4 or B1, B2, B3, B4). It's very rare to see in commercial applications, but is possible to do with software RAID.\nTo be absolutely clear: \n\nIf you have a RAID10 array with 8 disks and one dies (we'll call it A1) then you'll have 6 redundant disks and 1 without redundancy. If another disk dies there's a 85% chance your array is still working.\nIf you have a RAID01 array with 8 disks and one dies (we'll call it A1) then you'll have 3 redundant disks and 4 without redundancy. If another disk dies there's a 43% chance your array is still working.\n\nIt provides no additional speed over RAID 10, but substantially less redundancy and should be avoided at all costs.\n\nRAID 5\nGood when: You want a balance of redundancy and disk space or have a mostly random read workload\nBad when: You have a high random write workload or large drives\nRAID 5 has been the most commonly-used RAID level for decades. It provides the system performance of all the drives in the array (except for small random writes, which incur a slight overhead). It uses a simple XOR operation to calculate parity. Upon single drive failure, the information can be reconstructed from the remaining drives using the XOR operation on the known data.\nUnfortunately, in the event of a drive failure, the rebuilding process is very IO-intensive. The larger the drives in the RAID, the longer the rebuild will take, and the higher the chance for a second drive failure. Since large slow drives both have a lot more data to rebuild and a lot less performance to do it with, it is not usually recommended to use RAID 5 with anything 7200 RPM or lower.\nPerhaps the most critical issue with RAID 5 arrays, when used in consumer applications, is that they are almost guaranteed to fail when the total capacity exceeds 12TB. This is because the unrecoverable read error (URE) rate of SATA consumer drives is one per every 1014 bits, or ~12.5TB.\nIf we take an example of a RAID 5 array with seven 2 TB drives: when a drive fails there are six drives left. In order to rebuild the array the controller needs to read through six drives at 2 TB each. Looking at the figure above it is almost certain another URE will occur before the rebuild has finished. Once that happens the array and all data on it is lost.\n\nhttp://www.zdnet.com/article/why-raid-5-stops-working-in-2009\n\nHowever the URE/data loss/array failure with RAID 5 issue in consumer drives has been somewhat mitigated by the fact that most hard disk manufacturers have increased their newer drives' URE ratings to one in 1015 bits. As always, check the specification sheet before buying!\n\nhttps://www.zdnet.com/article/why-raid-5-still-works-usually/\n\nIt is also imperative that RAID 5 be put behind a reliable (battery-backed) write cache. This avoids the overhead for small writes, as well as flaky behaviour that can occur upon a failure in the middle of a write.\nRAID 5 is the most cost-effective solution of adding redundant storage to an array, as it requires the loss of only 1 disk (E.g. 12x 146GB disks = 1606GB of usable space). It requires a minimum of 3 disks.\n\n\nRAID 6\nGood when: You want to use RAID 5, but your disks are too large or slow\nBad when: You have a high random write workload\nRAID 6 is similar to RAID 5 but it uses two disks worth of parity instead of just one (the first is XOR, the second is a LSFR), so you can lose two disks from the array with no data loss. The write penalty is higher than RAID 5 and you have one less disk of space.\nIt is worth considering that eventually a RAID 6 array will encounter similar problems as a RAID 5. Larger drives cause larger rebuild times and more latent errors, eventually leading to a failure of the entire array and loss of all data before a rebuild has completed.\n\nhttp://www.zdnet.com/article/why-raid-6-stops-working-in-2019 \nhttp://queue.acm.org/detail.cfm?id=1670144\n\n\n\nRAID 50\nGood when: You have a lot of disks that need to be in a single array and RAID 10 isn't an option because of capacity\nBad when: You have so many disks that many simultaneous failures are possible before rebuilds complete, or when you don't have many disks\nRAID 50 is a nested level, much like RAID 10. It combines two or more RAID 5 arrays and stripes data across them in a RAID 0. This offers both performance and multiple disk redundancy, as long as multiple disks are lost from different RAID 5 arrays.\nIn a RAID 50, disk capacity is n-x, where x is the number of RAID 5s that are striped across. For example, if a simple 6-disk RAID 50, the smallest possible, if you had 6x1TB disks in two RAID 5s that were then striped across to become a RAID 50, you would have 4TB usable storage.\n\nRAID 60\nGood when: You have a similar use case to RAID 50, but need more redundancy\nBad when: You don't have a substantial number of disks in the array\nRAID 6 is to RAID 60 as RAID 5 is to RAID 50. Essentially, you have more than one RAID 6 that data is then striped across in a RAID 0. This setup allows for up to two members of any individual RAID 6 in the set to fail without data loss. Rebuild times for RAID 60 arrays can be substantial, so it's usually a good idea to have one hot-spare for each RAID 6 member in the array.\nIn a RAID 60, disk capacity is n-2x, where x is the number of RAID 6s that are striped across. For example, if a simple 8 disk RAID 60, the smallest possible, if you had 8x1TB disks in two RAID 6s that were then striped across to become a RAID 60, you would have 4TB usable storage. As you can see, this gives the same amount of usable storage that a RAID 10 would give on an 8 member array. While RAID 60 would be slightly more redundant, the rebuild times would be substantially larger. Generally, you want to consider RAID 60 only if you have a large number of disks.\n\nRAID-Z\nGood when: You are using ZFS on a system that supports it\nBad when: Performance demands hardware RAID acceleration\nRAID-Z is a bit complicated to explain since ZFS radically changes how storage and file systems interact. ZFS encompasses the traditional roles of volume management (RAID is a function of a Volume Manager) and file system. Because of this, ZFS can do RAID at the file's storage block level rather than at the volume's strip level. This is exactly what RAID-Z does, write the file's storage blocks across multiple physical drives including a parity block for each set of stripes.\nAn example may make this much more clear. Say you have 3 disks in a ZFS RAID-Z pool, the block size is 4KB. Now you write a file to the system that is exactly 16KB. ZFS will split that into four 4KB blocks (as would a normal operating system); then it will calculate two blocks of parity. Those six blocks will be placed on the drives similar to how RAID-5 would distribute data and parity. This is an improvement over RAID5 in that there was no reading of existing data stripes to calculate the parity.\nAnother example builds on the previous. Say the file was only 4KB. ZFS will still have to build one parity block, but now the write load is reduced to 2 blocks. The third drive will be free to service any other concurrent requests. A similar effect will be seen anytime the file being written is not a multiple of the pool's block size multiplied by the number of drives less one (ie [File Size] <> [Block Size] * [Drives - 1]).\nZFS handling both Volume Management and File System also means you don't have to worry about aligning partitions or stripe-block sizes. ZFS handles all that automatically with the recommended configurations.\nThe nature of ZFS counteracts some of the classic RAID-5/6 caveats.  All writes in ZFS are done in a copy-on-write fashion; all changed blocks in a write operation are written to a new location on disk, instead of overwriting the existing blocks.  If a write fails for any reason, or the system fails mid-write, the write transaction either occurs completely after system recovery (with the help of the ZFS intent log) or does not occur at all, avoiding potential data corruption.  Another issue with RAID-5/6 is potential data loss or silent data corruption during rebuilds; regular zpool scrub operations can help to catch data corruption or drive issues before they cause data loss, and checksumming of all data blocks will ensure that all corruption during a rebuild is caught.\nThe main disadvantage to RAID-Z is that it is still software raid (and suffers from the same minor latency incurred by the CPU calculating the write load instead of letting a hardware HBA offload it). This may be resolved in the future by HBAs that support ZFS hardware acceleration.\nOther RAID and Non-Standard Functionality\nBecause there's no central authority enforcing any sort of standard functionality, the various RAID levels have evolved and been standardized by prevalent use. Many vendors have produced products which deviate from the above descriptions. It's also quite common for them to invent some fancy new marketing terminology to describe one of the above concepts (this happens most frequently in the SOHO market). When possible, try to get the vendor to actually describe the functionality of the redundancy mechanism (most will volunteer this information, as there's really no secret sauce anymore).\nWorth mentioning, there are RAID 5-like implementations which allow you to start an array with only two disks. It would store data on one stripe and parity on the other, similar to RAID 5 above. This would perform like RAID 1 with the extra overhead of the parity calculation. The advantage is that you could add disks to the array by recalculating the parity.",
        "url": "https://serverfault.com/questions/339128/what-are-the-different-widely-used-raid-levels-and-when-should-i-consider-them"
    },
    {
        "title": "Should I quit using Ifconfig?",
        "question": "With the servers that mount Infiniband cards, when I use the ifconfig command, I get this warning:\nIfconfig uses the ioctl access method to get the full address \ninformation, which limits hardware addresses to 8 bytes.\nBecause Infiniband address has 20 bytes, only the first 8 bytes \nare displayed correctly.\nIfconfig is obsolete! For replacement check ip.\n\nShould I quit using ifconfig? Is it deprecated in favor of the ip command? Or will it be updated in the near future?\n\n\nNote: This question and answers are in regards to GNU/Linux's \"major\" distributions. It should not be assumed that the information applies to all distributions, and especially not other OSes.\n\n",
        "top_answer": "Quoting Thomas Pircher's website (cc-by-sa): \nifconfig vs ip\nThe command /bin/ip has been around for some time now. But people continue using the older command /sbin/ifconfig. Let's be clear: ifconfig will not quickly go away, but its newer version, ip, is more powerful and will eventually replace it.\nThe man page of ip may look intimidating at first, but once you get familiar with the command syntax, it is an easy read. This page will not introduce the new features of ip. It rather features a side-by-side comparison if ifconfig and ip to get a quick overview of the command syntax.\nShow network devices and configuration \nifconfig\n\nip addr show\nip link show\n\nEnable a network interface\nifconfig eth0 up\nip link set eth0 up\n\nA network interface is disabled in a similar way:\nifconfig eth0 down\nip link set eth0 down\n",
        "url": "https://serverfault.com/questions/458628/should-i-quit-using-ifconfig"
    },
    {
        "title": "I am under DDoS. What can I do?",
        "question": "\nThis is a Canonical Question about DoS and DDoS mitigation.\n\nI found a massive traffic spike on a website that I host today; I am getting thousands of connections a second and I see I'm using all 100Mbps of my available bandwidth.  Nobody can access my site because all the requests time out, and I can't even log into the server because SSH times out too!  This has happened a couple times before, and each time it's lasted a couple hours and gone away on its own.\nOccasionally, my website has another distinct but related problem: my server's load average (which is usually around .25) rockets up to 20 or more and nobody can access my site just the same as the other case.  It also goes away after a few hours.\nRestarting my server doesn't help; what can I do to make my site accessible again, and what is happening?\nRelatedly, I found once that for a day or two, every time I started my service, it got a connection from a particular IP address and then crashed.  As soon as I started it up again, this happened again and it crashed again.  How is that similar, and what can I do about it?",
        "top_answer": "You are experiencing a denial of service attack.  If you see traffic coming from multiple networks (different IPs on different subnets) you've got a distributed denial of service (DDoS); if it's all coming from the same place you have a plain old DoS.  It can be helpful to check, if you are able; use netstat to check.  This might be hard to do, though.\nDenial of service usually falls into a couple categories: traffic-based, and load-based.  The last item (with the crashing service) is exploit-based DoS and is quite different.\nIf you're trying to pin down what type of attack is happening, you may want to capture some traffic (using wireshark, tcpdump, or libpcap).  You should, if possible, but also be aware that you will probably capture quite a lot of traffic.\nAs often as not, these will come from botnets (networks of compromised hosts under the central control of some attacker, whose bidding they will do).  This is a good way for the attacker to (very cheaply) acquire the upstream bandwidth of lots of different hosts on different networks to attack you with, while covering their tracks.  The Low Orbit Ion Cannon is one example of a botnet (despite being voluntary instead of malware-derived); Zeus is a more typical one.\nTraffic-based\nIf you're under a traffic-based DoS, you're finding that there is just so much traffic coming to your server that its connection to the Internet is completely saturated.  There is a high packet loss rate when pinging your server from elsewhere, and (depending on routing methods in use) sometimes you're also seeing really high latency (the ping is high).  This kind of attack is usually a DDoS.\nWhile this is a really \"loud\" attack, and it's obvious what is going on, it's hard for a server administrator to mitigate (and basically impossible for a user of shared hosting to mitigate).  You're going to need help from your ISP; let them know you're under a DDoS and they might be able to help.\nHowever, most ISPs and transit providers will proactively realize what is going on and publish a blackhole route for your server.  What this means is that they publish a route to your server with as little cost as possible, via 0.0.0.0: they make traffic to your server no longer routeable on the  Internet.  These routes are typically /32s and eventually they are removed.  This doesn't help you at all; the purpose is to protect the ISP's network from the deluge.  For the duration, your server will effectively lose Internet access.\nThe only way your ISP (or you, if you have your own AS) is going to be able to help is if they are using intelligent traffic shapers that can detect and rate-limit probable DDoS traffic.  Not everyone has this technology.  However, if the traffic is coming from one or two networks, or one host, they might also be able to block the traffic ahead of you.\nIn short, there is very little you can do about this problem.  The best long-term solution is to host your services in many different locations on the Internet which would have to be DDoSed individually and simultaneously, making the DDoS much more expensive.  Strategies for this depend on the service you need to protect; DNS can be protected with multiple authoritative nameservers, SMTP with backup MX records and mail exchangers, and HTTP with round-robin DNS or multihoming (but some degradation might be noticeable for the duration anyway).\nLoad balancers are rarely an effective solution to this problem, because the load balancer itself is subject to the same problem and merely creates a bottleneck.  IPTables or other firewall rules will not help because the problem is that your pipe is saturated.  Once the connections are seen by your firewall, it is already too late; the bandwidth into your site has been consumed.  It doesn't matter what you do with the connections; the attack is mitigated or finished when the amount of incoming traffic goes back down to normal.\nIf you are able to do so, consider using a content distribution network (CDN) like Akamai, Limelight and CDN77, or use a DDoS scrubbing service like CloudFlare or Prolexic.  These services take active measures to mitigate these types of attacks, and also have so much available bandwidth in so many different places that flooding them is not generally feasible.\nIf you decide to use CloudFlare (or any other CDN/proxy) remember to hide your server's IP. If an attacker finds out the IP, he can again DDoS your server directly, bypassing CloudFlare. To hide the IP, your server should never communicate directly with other servers/users unless they are safe. For example your server should not send emails directly to users.  This doesn't apply if you host all your content on the CDN and don't have a server of your own.\nAlso, some VPS and hosting providers are better at mitigating these attacks than others.  In general, the larger they are, the better they will be at this; a provider which is very well-peered and has lots of bandwidth will be naturally more resilient, and one with an active and fully staffed network operations team will be able to react more quickly.\nLoad-based\nWhen you are experiencing a load-based DDoS, you notice that the load average is abnormally high (or CPU, RAM, or disk usage, depending on your platform and the specifics).  Although the server doesn't appear to be doing anything useful,  it is very busy.  Often, there will be copious amounts of entries in the logs indicating unusual conditions.  More often than not this is coming from a lot of different places and is a DDoS, but that isn't necessarily the case.  There don't even have to be a lot of different hosts.\nThis attack is based on making your service do a lot of expensive stuff.  This could be something like opening a gargantuan number of TCP connections and forcing you to maintain state for them, or uploading excessively large or numerous files to your service, or perhaps doing really expensive searches, or really doing anything that is expensive to handle.  The traffic is within the limits of what you planned for and can take on, but the types of requests being made are too expensive to handle so many of.\nFirstly, that this type of attack is possible is often indicative of a configuration issue or bug in your service.  For instance, you may have overly verbose logging turned on, and may be storing logs on something that's very slow to write to.  If someone realizes this and does a lot of something which causes you to write copious amounts of logs to disk, your server will slow to a crawl.  Your software might also be doing something extremely inefficient for certain input cases; the causes are as numerous as there are programs, but two examples would be a situation that causes your service to not close a session that is otherwise finished, and a situation that causes it to spawn a child process and leave it.  If you end up with tens of thousands of open connections with state to keep track of, or tens of thousands of child processes, you'll run into trouble.\nThe first thing you might be able to do is use a firewall to drop the traffic.  This isn't always possible, but if there is a characteristic you can find in the incoming traffic (tcpdump can be nice for this if the traffic is light), you can drop it at the firewall and it will no longer cause trouble.  The other thing to do is to fix the bug in your service (get in touch with the vendor and be prepared for a long support experience).\nHowever, if it's a configuration issue, start there.  Turn down logging on production systems to a reasonable level (depending on the program this is usually the default, and will usually involve making sure \"debug\" and \"verbose\" levels of logging are off; if everything a user does is logged in exact and fine detail, your logging is too verbose).  Additionally, check child process and request limits, possibly throttle incoming requests, connections per IP, and the number of allowed child processes, as applicable.\nIt goes without saying that the better configured and better provisioned your server is, the harder this type of attack will be.  Avoid being stingy with RAM and CPU in particular.  Ensure your connections to things like backend databases and disk storage are fast and reliable.\nExploit-based\nIf your service mysteriously crashes extremely quickly after being brought up, particularly if you can establish a pattern of requests that precede the crash and the request is atypical or doesn't match expected use patterns, you might be experiencing an exploit-based DoS.  This can come from as few as just one host (with pretty much any type of internet connection), or many hosts.\nThis is similar to a load-based DoS in many respects, and has basically the same causes and mitigations.  The difference is merely that in this case, the bug doesn't cause your server to be wasteful, but to die.  The attacker is usually exploiting a remote crash vulnerability, such as garbled input that causes a null-dereference or something in your service.\nHandle this similarly to an unauthorized remote access attack.  Firewall against the originating hosts and type of traffic if they can be pinned down.  Use validating reverse proxies if applicable.  Gather forensic evidence (try and capture some of the traffic), file a bug ticket with the vendor, and consider filing an abuse complaint (or legal complaint) against the origin too.\nThese attacks are fairly cheap to mount, if an exploit can be found, and they can be very potent, but also relatively easy to track down and stop.  However, techniques that are useful against traffic-based DDoS are generally useless against exploit-based DoS.",
        "url": "https://serverfault.com/questions/531941/i-am-under-ddos-what-can-i-do"
    },
    {
        "title": "How do I sleep for a millisecond in bash or ksh",
        "question": "sleep is a very popular command and we can start sleep from 1 second:\n# wait one second please \nsleep 1\n\nbut what the alternative if I need to wait only 0.1 second or between 0.1 to 1 second ?\n\nremark: on linux or OS X sleep 0.XXX works fine , but on solaris sleep 0.1 or sleep 0.01 - illegal syntax\n",
        "top_answer": "Bash has a \"loadable\" sleep which supports fractional seconds, and eliminates overheads of an external command:\n$ cd bash-3.2.48/examples/loadables\n$ make sleep && mv sleep sleep.so\n$ enable -f sleep.so sleep\n\nThen:\n$ which sleep\n/usr/bin/sleep\n$ builtin sleep\nsleep: usage: sleep seconds[.fraction]\n$ time (for f in `seq 1 10`; do builtin sleep 0.1; done)\nreal    0m1.000s\nuser    0m0.004s\nsys     0m0.004s\n\nThe downside is that the loadables may not be provided with your bash binary, so you would need to compile them yourself as shown (though on Solaris it would not necessarily be as simple as above). \nAs of bash-4.4 (September 2016) all the loadables are now built and installed by default on platforms that support it, though they are built as separate shared-object files, and without a .so suffix. Unless your distro/OS has done something creative (sadly RHEL/CentOS 8 build bash-4.4 with loadable extensions deliberately removed), you should be able to do instead:\n[ -z \"$BASH_LOADABLES_PATH\" ] &&\n  BASH_LOADABLES_PATH=$(pkg-config bash --variable=loadablesdir 2>/dev/null)  \nenable -f sleep sleep\n\n(The man page implies BASH_LOADABLES_PATH is set automatically, I find this is not the case in the official distribution as of 4.4.12. If and when it is set correctly you need only enable -f filename commandname as required.)\nIf that's not suitable, the next easiest thing to do is build or obtain sleep from GNU coreutils, this supports the required feature. The POSIX sleep command is minimal, older Solaris versions implemented only that. Solaris 11 sleep does support fractional seconds.\nAs a last resort you could use perl (or any other scripting that you have to hand) with the caveat that initialising the interpreter may be comparable to the intended sleep time:\n$ perl -e \"select(undef,undef,undef,0.1);\"\n$ echo \"after 100\" | tclsh\n",
        "url": "https://serverfault.com/questions/469247/how-do-i-sleep-for-a-millisecond-in-bash-or-ksh"
    },
    {
        "title": "Run an interactive bash subshell with initial commands without returning to the (\"super\") shell immediately",
        "question": "I want to run a bash subshell, (1) run a few commands, (2) and then remain in that subshell to do as I please. I can do each of these individually:\n\nRun command using -c flag:\n$> bash -c \"ls; pwd; <other commands...>\"\n\nhowever, it immediately returns to the \"super\" shell after the commands are executed. I can also just run an interactive subshell:\nStart new bash process:\n$> bash\n\nand it won't exit the subshell until I say so explicitly... but I can't run any initial commands. The closest solution I've found is:\n$> bash -c \"ls; pwd; <other commands>; exec bash\"\n\nwhich works, but not the way I wanted to, as it runs the given commands in one subshell, and then opens a separate one for interaction.\n\nI want to do this on a single line. Once I exit the subshell, I should return back to the regular \"super\"shell without incident. There must be a way~~\nNB: What I am not asking...\n\nnot asking where to get a hold of the bash man page\nnot asking how to read initializing commands from a file... I know how to do this, it's not the solution I'm looking for\nnot interested in using tmux or gnu screen\nnot interested in giving context to this. I.e., the question is meant to be general, and not for any specific purpose\nif possible, I want to avoid using workarounds that sort of accomplish what I want, but in a \"dirty\" way. I just want to do this on a single line. In particular, I don't want to do something like xterm -e 'ls'\n",
        "top_answer": "This can be easily done with temporary named pipes:\nbash --init-file <(echo \"ls; pwd\")\n\nCredit for this answer goes to the comment from Lie Ryan. I found this really useful, and it's less noticeable in the comments, so I thought it should be its own answer.",
        "url": "https://serverfault.com/questions/368054/run-an-interactive-bash-subshell-with-initial-commands-without-returning-to-the"
    },
    {
        "title": "Physically locating the server",
        "question": "I've got a CentOS server somewhere in the building; I can login into it remotely and VNC, etc. Now I've got to physically move it, and for that I need to physically locate the machine among the lookalikes around the office.\nWhat can I do remotely to make the machine visibly or audibly identify itself?",
        "top_answer": "Use IPMI to trigger LEDs, increase fan RPMs or sound the beep/alarm.  Take a look at the man page for ipmitool https://linux.die.net/man/1/ipmitool depending on the server you may be able to set the LEDs, LCD display, fan RPM offset(listen when nobody is in the office).  Some other IPMI or BMC interfaces may allow you to sound the beep but this functionality is more platform specific.  \na powerful workstation or server will sound like someone vacuuming with the fans turned up all the way. \nEDIT:\nTo use the Identifier lights as mentioned in comments, this will however require setting making sure that an appropriate IPMI interface is setup, there are several guides and tutorials available, and depending on the OEM there may be proprietary interfaces and management systems like Intel's Data Center Manager(http://www.intel.com/content/www/us/en/software/intel-dcm-product-detail.html).  I have used this tutorial before but there are others https://www.thomas-krenn.com/en/wiki/Configuring_IPMI_under_Linux_using_ipmitool\nipmitool -I <appropriate interface for system> -U<username> chassis identify force should force the ID to an on state, depending on the interface and configuration you may need to specify authentication type and other command line options.",
        "url": "https://serverfault.com/questions/807611/physically-locating-the-server"
    },
    {
        "title": "What is the difference between Load Balancer and Reverse Proxy?",
        "question": "I'm not clear about the difference between Load Balancer and Reverse Proxy. They both seems having same behavior: distributing incoming requests to backend servers.",
        "top_answer": "Your confusion is reasonable - they are often the same thing. But not always. When you refer to a load balancer you are referring to a very specific thing - a server or device that balances inbound requests across two or more web servers to spread the load. A reverse proxy, however, typically has any number of features:\n\nload balancing: as discussed above\ncaching: it can cache content from the web server(s) behind it and thereby reduce the load on the web server(s) and return some static content back to the requester without having to get the data from the web server(s)\nsecurity: it can protect the web server(s) by preventing direct access from the internet; it might do this through simple means by just obfuscating the web server(s) or it may have some more active components that actually review inbound requests looking for malicious code\nSSL acceleration: when SSL is used; it may serve as a termination point for those SSL sessions so that the workload of dealing with the encryption is offloaded from the web server(s)\n\nI think this covers most of it but there are probably a few other features I've missed. Certainly it isn't uncommon to see a device or piece of software marketed as a load balancer/reverse proxy because the features are so commonly bundled together.",
        "url": "https://serverfault.com/questions/127021/what-is-the-difference-between-load-balancer-and-reverse-proxy"
    },
    {
        "title": "Why couldn't MAC addresses be used instead of IPv4|6 for networking? [closed]",
        "question": "I am reading up on TCP/IP and other related protocols and technologies. MAC addresses are described as being (reasonably :) unique, and as having a large possibility space (several hundred trillions), while also being assigned to all network interfaces. What are the historical and technical reasons why IPv4 or IPv6 addresses are used instead of MAC addresses for internetwork communication?\nAm I missing something fundamental or is it just a silly reason (e.g. building on top of legacy tech)?",
        "top_answer": "The MAC address might be unique, but there's nothing special about the number that would indicate where it is. MAC 00-00-00-00-00-00 might be on the other side of the planet from 00-00-00-00-00-01.\nIP is an arbitrary numbering scheme imposed in a hierarchical fashion on a group of computers to logically distinguish them as a group (that's what a subnet is). Sending messages between those groups is done by routing tables, themselves divided into multiple levels so that we don't have to keep track of every single subnet. For instance, 17.x.x.x is within the Apple network. From there, Apple will know where each of its thousands of subnets are located and how to get to them (nobody else needs to know this information, they just need to know that 17.anything goes to Apple).\nIt's also pretty easy to relate this to another pair of systems. You have a State Issued ID Number, why would you need a mailing address if that ID number is already unique to just you? You need the mailing address because it's an arbitrary system that describes where the unique destination for communications to you should go.",
        "url": "https://serverfault.com/questions/410626/why-couldnt-mac-addresses-be-used-instead-of-ipv46-for-networking"
    },
    {
        "title": "How do I prevent accidental rm -rf /*?",
        "question": "I just ran rm -rf /* accidentally, but I meant rm -rf ./* (notice the star after the slash).\nalias rm='rm -i' and --preserve-root by default didn't save me, so are there any automatic safeguards for this?\n\nI wasn't root and cancelled the command immediately, but there were some relaxed permissions somewhere or something because I noticed that my Bash prompt broke already. I don't want to rely on permissions and not being root (I could make the same mistake with sudo), and I don't want to hunt for mysterious bugs because of one missing file somewhere in the system, so, backups and sudo are good, but I would like something better for this specific case.\n\nAbout thinking twice and using the brain. I am using it actually! But I'm using it to solve some complex programming task involving 10 different things. I'm immersed in this task deeply enough, there isn't any brain power left for checking flags and paths, I don't even think in terms of commands and arguments, I think in terms of actions like 'empty current dir', different part of my brain translates them to commands and sometimes it makes mistakes. I want the computer to correct them, at least the dangerous ones.",
        "top_answer": "One of the tricks I follow is to put # in the beginning while using the rm command.\nroot@localhost:~# #rm -rf /\n\nThis prevents accidental execution of rm on the wrong file/directory. Once verified, remove # from the beginning. This trick works, because in Bash a word beginning with # causes that word and all remaining characters on that line to be ignored. So the command is simply ignored.\nOR\nIf you want to prevent any important directory, there is one more trick.\nCreate a file named -i in that directory. How can such a odd file be created? Using touch -- -i or touch ./-i\nNow try rm -rf *:\nsachin@sachin-ThinkPad-T420:~$ touch {1..4}\nsachin@sachin-ThinkPad-T420:~$ touch -- -i\nsachin@sachin-ThinkPad-T420:~$ ls\n1  2  3  4  -i\nsachin@sachin-ThinkPad-T420:~$ rm -rf *\nrm: remove regular empty file `1'? n\nrm: remove regular empty file `2'? \n\nHere the * will expand -i to the command line, so your command ultimately becomes rm -rf -i. Thus command will prompt before removal. You can put this file in your /, /home/, /etc/, etc.\nOR\nUse --preserve-root as an option to rm. In the rm included in newer coreutils packages, this option is the default.\n--preserve-root\n              do not remove `/' (default)\n\nOR\nUse safe-rm\nExcerpt from the web site:\n\nSafe-rm is a safety tool intended to prevent the accidental deletion\n  of important files by replacing /bin/rm with a wrapper, which checks\n  the given arguments against a configurable blacklist of files and\n  directories that should never be removed.\nUsers who attempt to delete one of these protected files or\n  directories will not be able to do so and will be shown a warning\n  message instead:\n$ rm -rf /usr\nSkipping /usr\n\n",
        "url": "https://serverfault.com/questions/337082/how-do-i-prevent-accidental-rm-rf"
    },
    {
        "title": "What port does SFTP use?",
        "question": "Does SFTP use port 21 or port 22?",
        "top_answer": "While TCP port 22 is the general right answer, this is dependent on the fact that SSH is configured to use the standard port and not an alternative port.\nAs SFTP runs as a subsystem of SSH it runs on whatever port the SSH daemon is listening on and that is administrator configurable.",
        "url": "https://serverfault.com/questions/74176/what-port-does-sftp-use"
    },
    {
        "title": "List of files installed from apt package",
        "question": "How do I get a list of files that were or will-be installed when I apt-get a package?  Conversely, can I find what package(s) caused a particular file to be installed?",
        "top_answer": "Note: in the following commands, a command beginning with 'root#' means it needs to be run as root.\nTo find which files were installed by a package, use dpkg -L:\n$ dpkg -L $package\n\napt-file can tell you which files will be installed by a package before installing it:\nroot# apt-get install apt-file\nroot# apt-file update\n$ apt-file list $package\n\nOr if you have the package as a .deb file locally already, you can run dpkg on it:\n$ dpkg --contents $package.deb\n\nTo find which package provides a file that is already on your system, use:\n$ dpkg -S /path/to/file\n\nTo find which package provides a file that is not currently on your system, use apt-file again:\n$ apt-file search /path/to/file\n",
        "url": "https://serverfault.com/questions/96964/list-of-files-installed-from-apt-package"
    },
    {
        "title": "What tool do you use to monitor your servers?",
        "question": "\nFor a more comprehensive list of monitoring tools and their features, check out this Wikipedia page.\n\nAs the question states, what are the most commonly used tools used for this task and what are their strengths and weaknesses?",
        "top_answer": "I've used Nagios in the past with success. It's very extensible (over 200 add-ons), relatively easy to use and lots of reports. A negative would be the initial setup.",
        "url": "https://serverfault.com/questions/44/what-tool-do-you-use-to-monitor-your-servers"
    },
    {
        "title": "How to create a UUID in bash?",
        "question": "In Java it is possible to create a random UUID: \nUUID uuid = UUID.randomUUID();\n\nHow to do this in Bash?",
        "top_answer": "See the uuidgen program which is part of the e2fsprogs package.\nAccording to this, libuuid is now part of util-linux and the inclusion in e2fsprogs is being phased out. However, on new Ubuntu systems, uuidgen is now in the uuid-runtime package.\nTo create a uuid and save it in a variable:\nuuid=$(uuidgen)\n\nOn my Ubuntu system, the alpha characters are output as lower case and on my OS X system, they are output as upper case (thanks to David for pointing this out in a comment).\nTo switch to all upper case (after generating it as above):\nuuid=${uuid^^}\n\nTo switch to all lower case:\nuuid=${uuid,,}\n\nIf, for example, you have two UUIDs and you want to compare them in Bash, ignoring their case, you can do a tolower() style comparison like this:\nif [[ ${uuid1,,} == ${uuid2,,} ]]\n",
        "url": "https://serverfault.com/questions/103359/how-to-create-a-uuid-in-bash"
    },
    {
        "title": "What does \"Warning: untrusted X11 forwarding setup failed: xauth key data not generated\" mean when ssh'ing with -X?",
        "question": "When I use ssh -X on my Mac (running OS X 10.6.7) to connect to my Ubuntu (11.04) box, I get the following warning:\n\nWarning: untrusted X11 forwarding\n  setup failed: xauth key data not\n  generated Warning: No xauth data;\n  using fake authentication data for X11\n  forwarding.\n\nIs there something I can do to make this warning go away?  If not, can I safely ignore it?\nX11 forwarding seems to work fine, though I do see this message:\n\nXlib:  extension \"RANDR\" missing on\n  display \"localhost:10.0\".\n\nIs that related to the warning?  (I'm guessing not.  If it's not, I'll file a new question about that.)",
        "top_answer": "Try ssh -Y\nAny reason you don't want to use the -Y flag instead of the -X flag?\nQuite simply, the difference between -X and -Y is that -Y enables trusted X11 forwarding.",
        "url": "https://serverfault.com/questions/273847/what-does-warning-untrusted-x11-forwarding-setup-failed-xauth-key-data-not-ge"
    },
    {
        "title": "How do you answer yes for yum install automatically",
        "question": "In centos how do you answer yes automatically for yum install so that it is an unassisted install?",
        "top_answer": "You can use:\nyum -y install packagename\nThe \"-y\" implies \"yes\".\nThe long option is --assumeyes",
        "url": "https://serverfault.com/questions/442088/how-do-you-answer-yes-for-yum-install-automatically"
    },
    {
        "title": "Why is DNS failover not recommended?",
        "question": "From reading, it seems like DNS failover is not recommended just because DNS wasn't designed for it. But if you have two webservers on different subnets hosting redundant content, what other methods are there to ensure that all traffic gets routed to the live server if one server goes down?\nTo me it seems like DNS failover is the only failover option here, but the consensus is it's not a good option. Yet services like DNSmadeeasy.com provide it, so there must be merit to it. Any comments?",
        "top_answer": "By 'DNS failover' I take it you mean DNS Round Robin combined with some monitoring, i.e. publishing multiple IP addresses for a DNS hostname, and removing a dead address when monitoring detects that a server is down. This can be workable for small, less trafficked websites.\nBy design, when you answer a DNS request you also provide a Time To Live (TTL) for the response you hand out. In other words, you're telling other DNS servers and caches \"you may store this answer and use it for x minutes before checking back with me\". The drawbacks come from this:\n\nWith DNS failover, a unknown percentage of your users will have your DNS data cached with varying amounts of TTL left. Until the TTL expires these may connect to the dead server. There are faster ways of completing failover than this.\nBecause of the above, you're inclined to set the TTL quite low, say 5-10 minutes. But setting it higher gives a (very small) performance benefit, and may help your DNS propagation work reliably even if there is a short glitch in network traffic. So using DNS based failover goes against high TTLs, but high TTLs are a part of DNS and can be useful.\n\nThe more common methods of getting good uptime involve:\n\nPlacing servers together on the same LAN.\nPlace the LAN in a datacenter with highly available power and network planes.\nUse a HTTP load balancer to spread load and fail over on individual server failures.\nGet the level of redundancy / expected uptime you require for your firewalls, load balancers and switches.\nHave a communication strategy in place for full-datacenter failures, and the occasional failure of a switch / database server / other resource that cannot easily be mirrored.\n\nA very small minority of web sites use multi-datacenter setups, with 'geo-balancing' between datacenters.",
        "url": "https://serverfault.com/questions/60553/why-is-dns-failover-not-recommended"
    },
    {
        "title": "Good tools that fit on a thumb drive [closed]",
        "question": "I have been on the lookout lately for some good tools to fill up my flash drive and I thought I would ask the Server Fault community for recommendations on good tools that will fit onto a thumb drive.\nSome I use are Driver Packs, CCleaner and the portable apps suite.",
        "top_answer": "These are the utilities I have on my drive: \n\nCurrPorts displays the list of\nall currently opened TCP/IP and UDP\nports on your local computer.\nftpserver3lite is an FTP server\nftpwanderer2 is an FTP client\nipnetinfo answers questions\nabout an IP address: owner,\ncountry/state, range, contact info,\netc.\nmiranda general messaging\nsolution (supports most P2P messaging\nnetworks)\nomziff encryption decryption\ntool.\nFoxitReader wonderful\nalternative to adobe's PDF reader.\nlight and fast and portable.\nQm (The Quick Mailer) if you\njust want to send an Email the old\npasion way with no installation.\nRestoration quick and basic\nundelete utility.\nsmsniff basic TCP sniffer.\ntorpark a Firefox-based browser\nfor completely discrete browsing.\ntreepad just a nice utility to\norganize your data in, much like\nfreemind and other mind maps.\ncpicture a picture viewer\nDriveMan for managing hard\ndrives on the local computer.\nFollowMeIPLite very much like\nwww.whatismyip.com only much quicker.\nhfs opens a small HTTP file\nserver from desired folder, for\ninstant file sharing.\nangry ip scanner scans IP's\nkill.exe - needs no introduction :)\nputty a telnet utility every system\nadministrator has got to be familiar with.\nstartup control panel,\nStartupList, regcleaner -\nreally there are many registry\ncleaners/managers out there, lots of\nthem fits nicely in a thumb-drive.\nRevealer reveals passwords from\npassword fields. It is very useful in many\nsituations.\nvncviewer client for the VNC\nremote desktop protocol\nWinAudit audits a Windows\nmachine. Lots of useful information.\nxcopy.exe - it is still useful to have\naround.\nTcpView shows all all TCP and UDP endpoints on your system.\n\nBeyond Compare is fantastic, btw. Also, you might want to check out portable freeware.",
        "url": "https://serverfault.com/questions/261/good-tools-that-fit-on-a-thumb-drive"
    },
    {
        "title": "How can I read pcap files in a friendly format?",
        "question": "a simple cat on the pcap file looks terrible:\n$cat tcp_dump.pcap\n\n?\u00f2????YVJ?\n          JJ\n            ?@@.?E<??@@\n?CA??qe?U????\u0438h?\n.Ceh?YVJ??\n          JJ\n            ?@@.?E<??@@\nCA??qe?U????\u0435z?\n.ChV?YVJ$?JJ\n            ?@@.?E<-/@@A?CA\u037c?9????F???A&?\n.Ck??YVJgeJJ@@.?\u04e2#3E<@3{n\u037c?9CA??P?\u025d?F???<K?\n?\u051b`.Ck??YVJgeBB\n               ?@@.?E4-0@@AFCA\u037c?9????F?P?\u0280???\n.Ck??\u051b`?YVJ?\"\"@@.?\u04e2#3E?L@3?I\u037c?9CA??P?\u029d?F?????\n?\u051b?.Ck?220-rly-da03.mx\n\netc.\nI tried to make it prettier with:\nsudo tcpdump -ttttnnr tcp_dump.pcap\nreading from file tcp_dump.pcap, link-type EN10MB (Ethernet)\n2009-07-09 20:57:40.819734 IP 67.23.28.65.49237 > 216.239.113.101.25: S 2535121895:2535121895(0) win 5840 <mss 1460,sackOK,timestamp 776168808 0,nop,wscale 5>\n2009-07-09 20:57:43.819905 IP 67.23.28.65.49237 > 216.239.113.101.25: S 2535121895:2535121895(0) win 5840 <mss 1460,sackOK,timestamp 776169558 0,nop,wscale 5>\n2009-07-09 20:57:47.248100 IP 67.23.28.65.42385 > 205.188.159.57.25: S 2644526720:2644526720(0) win 5840 <mss 1460,sackOK,timestamp 776170415 0,nop,wscale 5>\n2009-07-09 20:57:47.288103 IP 205.188.159.57.25 > 67.23.28.65.42385: S 1358829769:1358829769(0) ack 2644526721 win 5792 <mss 1460,sackOK,timestamp 4292123488 776170415,nop,wscale 2>\n2009-07-09 20:57:47.288103 IP 67.23.28.65.42385 > 205.188.159.57.25: . ack 1 win 183 <nop,nop,timestamp 776170425 4292123488>\n2009-07-09 20:57:47.368107 IP 205.188.159.57.25 > 67.23.28.65.42385: P 1:481(480) ack 1 win 1448 <nop,nop,timestamp 4292123568 776170425>\n2009-07-09 20:57:47.368107 IP 67.23.28.65.42385 > 205.188.159.57.25: . ack 481 win 216 <nop,nop,timestamp 776170445 4292123568>\n2009-07-09 20:57:47.368107 IP 67.23.28.65.42385 > 205.188.159.57.25: P 1:18(17) ack 481 win 216 <nop,nop,timestamp 776170445 4292123568>\n2009-07-09 20:57:47.404109 IP 205.188.159.57.25 > 67.23.28.65.42385: . ack 18 win 1448 <nop,nop,timestamp 4292123606 776170445>\n2009-07-09 20:57:47.404109 IP 205.188.159.57.25 > 67.23.28.65.42385: P 481:536(55) ack 18 win 1448 <nop,nop,timestamp 4292123606 776170445>\n2009-07-09 20:57:47.404109 IP 67.23.28.65.42385 > 205.188.159.57.25: P 18:44(26) ack 536 win 216 <nop,nop,timestamp 776170454 4292123606>\n2009-07-09 20:57:47.444112 IP 205.188.159.57.25 > 67.23.28.65.42385: P 536:581(45) ack 44 win 1448 <nop,nop,timestamp 4292123644 776170454>\n2009-07-09 20:57:47.484114 IP 67.23.28.65.42385 > 205.188.159.57.25: . ack 581 win 216 <nop,nop,timestamp 776170474 4292123644>\n2009-07-09 20:57:47.616121 IP 67.23.28.65.42385 > 205.188.159.57.25: P 44:50(6) ack 581 win 216 <nop,nop,timestamp 776170507 4292123644>\n2009-07-09 20:57:47.652123 IP 205.188.159.57.25 > 67.23.28.65.42385: P 581:589(8) ack 50 win 1448 <nop,nop,timestamp 4292123855 776170507>\n2009-07-09 20:57:47.652123 IP 67.23.28.65.42385 > 205.188.159.57.25: . ack 589 win 216 <nop,nop,timestamp 776170516 4292123855>\n2009-07-09 20:57:47.652123 IP 67.23.28.65.42385 > 205.188.159.57.25: P 50:56(6) ack 589 win 216 <nop,nop,timestamp 776170516 4292123855>\n2009-07-09 20:57:47.652123 IP 67.23.28.65.42385 > 205.188.159.57.25: F 56:56(0) ack 589 win 216 <nop,nop,timestamp 776170516 4292123855>\n2009-07-09 20:57:47.668124 IP 67.23.28.65.49239 > 216.239.113.101.25: S 2642380481:2642380481(0) win 5840 <mss 1460,sackOK,timestamp 776170520 0,nop,wscale 5>\n2009-07-09 20:57:47.692126 IP 205.188.159.57.25 > 67.23.28.65.42385: P 589:618(29) ack 57 win 1448 <nop,nop,timestamp 4292123893 776170516>\n2009-07-09 20:57:47.692126 IP 67.23.28.65.42385 > 205.188.159.57.25: R 2644526777:2644526777(0) win 0\n2009-07-09 20:57:47.692126 IP 205.188.159.57.25 > 67.23.28.65.42385: F 618:618(0) ack 57 win 1448 <nop,nop,timestamp 4292123893 776170516>\n2009-07-09 20:57:47.692126 IP 67.23.28.65.42385 > 205.188.159.57.25: R 2644526777:2644526777(0) win 0\n\nWell...that is much prettier but it doesn't show the actual messages.  I can actually extract more information just viewing the RAW file.  What is the best ( and preferably easiest) way to just view all the contents of the pcap file?\nUPDATE\nThanks to the responses below, I made some progress.  Here is what it looks like now:\ntcpdump -qns 0 -A -r blah.pcap\n    20:57:47.368107 IP 205.188.159.57.25 > 67.23.28.65.42385: tcp 480\n        0x0000:  4500 0214 834c 4000 3306 f649 cdbc 9f39  [email\u00a0protected]\n        0x0010:  4317 1c41 0019 a591 50fe 18ca 9da0 4681  C..A....P.....F.\n        0x0020:  8018 05a8 848f 0000 0101 080a ffd4 9bb0  ................\n        0x0030:  2e43 6bb9 3232 302d 726c 792d 6461 3033  .Ck.220-rly-da03\n        0x0040:  2e6d 782e 616f 6c2e 636f 6d20 4553 4d54  .mx.aol.com.ESMT\n        0x0050:  5020 6d61 696c 5f72 656c 6179 5f69 6e2d  P.mail_relay_in-\n        0x0060:  6461 3033 2e34 3b20 5468 752c 2030 3920  da03.4;.Thu,.09.\n        0x0070:  4a75 6c20 3230 3039 2031 363a 3537 3a34  Jul.2009.16:57:4\n        0x0080:  3720 2d30 3430 300d 0a32 3230 2d41 6d65  7.-0400..220-Ame\n        0x0090:  7269 6361 204f 6e6c 696e 6520 2841 4f4c  rica.Online.(AOL\n        0x00a0:  2920 616e 6420 6974 7320 6166 6669 6c69  ).and.its.affili\n        0x00b0:  6174 6564 2063 6f6d 7061 6e69 6573 2064  ated.companies.d\n\netc.\nThis looks good, but it still makes the actual message on the right difficult to read.  Is there a way to view those messages in a more friendly way?\nUPDATE\nThis made it pretty:\ntcpick -C -yP -r tcp_dump.pcap\n\nThanks!",
        "top_answer": "Wireshark is probably the best, but if you want/need to look at the payload without loading up a GUI you can use the -X or -A options\ntcpdump -qns 0 -X -r serverfault_request.pcap\n14:28:33.800865 IP 10.2.4.243.41997 > 69.59.196.212.80: tcp 1097\n        0x0000:  4500 047d b9c4 4000 4006 63b2 0a02 04f3  E..}..@[email\u00a0protected].....\n        0x0010:  453b c4d4 a40d 0050 f0d4 4747 f847 3ad5  E;.....P..GG.G:.\n        0x0020:  8018 f8e0 1d74 0000 0101 080a 0425 4e6d  .....t.......%Nm\n        0x0030:  0382 68a1 4745 5420 2f71 7565 7374 696f  ..h.GET./questio\n        0x0040:  6e73 2048 5454 502f 312e 310d 0a48 6f73  ns.HTTP/1.1..Hos\n        0x0050:  743a 2073 6572 7665 7266 6175 6c74 2e63  t:.serverfault.c\n        0x0060:  6f6d 0d0a 5573 6572 2d41 6765 6e74 3a20  om..User-Agent:.\n        0x0070:  4d6f 7a69 6c6c 612f 352e 3020 2858 3131  Mozilla/5.0.(X11\n        0x0080:  3b20 553b 204c 696e 7578 2069 3638 363b  ;.U;.Linux.i686;\n\ntcpdump -qns 0 -A -r serverfault_request.pcap\n14:29:33.256929 IP 10.2.4.243.41997 > 69.59.196.212.80: tcp 1097\nE..}..@[email\u00a0protected].\n...E;...^M.P..^w.G.......t.....\n.%.}..l.GET /questions HTTP/1.1\nHost: serverfault.com\n\nThere are many other tools for reading and getting stats, extracting payloads and so on.  A quick look on the number of things that depend on libpcap in the debian package repository gives a list of 50+ tools that can be used to slice, dice, view, and manipulate captures in various ways.\nFor example.\n\ntcpick\ntcpxtract\n",
        "url": "https://serverfault.com/questions/38626/how-can-i-read-pcap-files-in-a-friendly-format"
    },
    {
        "title": "Disk full, du tells different. How to further investigate?",
        "question": "I have a SCSI disk in a server (hardware Raid 1), 32G, ext3 filesytem. df tells me that the disk is 100% full. If I delete 1G this is correctly shown.\nHowever, if I run a du -h -x / then du tells me that only 12G are used (I use -x because of some Samba mounts).\nSo my question is not about subtle differences between the du and df commands but about how I can find out what causes this huge difference?\nI rebooted the machine for a fsck that went w/out errors. Should I run badblocks? lsof shows me no open deleted files, lost+found is empty and there is no obvious warn/err/fail statement in the messages file.\nFeel free to ask for further details of the setup. ",
        "top_answer": "Check for files on located under mount points.  Frequently if you mount a directory (say a sambafs) onto a filesystem that already had a file or directories under it, you lose the ability to see those files, but they're still consuming space on the underlying disk.  I've had file copies while in single user mode dump files into directories that I couldn't see except in single usermode (due to other directory systems being mounted on top of them).",
        "url": "https://serverfault.com/questions/275206/disk-full-du-tells-different-how-to-further-investigate"
    },
    {
        "title": "How do you restart php-fpm?",
        "question": "I need to reload my php.ini and there's nothing in the help dialog about restarting it.",
        "top_answer": "Note: prepend sudo if not root\n\nUsing SysV Init scripts directly:\n/etc/init.d/php-fpm restart    # typical\n/etc/init.d/php5-fpm restart   # debian-style\n/etc/init.d/php7.0-fpm restart # debian-style PHP 7\n\nUsing service wrapper script\nservice php-fpm restart    # typical\nservice php5-fpm restart   # debian-style\nservice php7.0-fpm restart # debian-style PHP 7\n\nUsing Upstart (e.g. ubuntu):    \nrestart php7.0-fpm         # typical (ubuntu is debian-based) PHP 7\nrestart php5-fpm           # typical (ubuntu is debian-based)\nrestart php-fpm            # uncommon\n\nUsing systemd (newer servers):\nsystemctl restart php-fpm.service    # typical\nsystemctl restart php5-fpm.service   # uncommon\nsystemctl restart php7.0-fpm.service # uncommon PHP 7\n\n\nOr whatever the equivalent is on your system.",
        "url": "https://serverfault.com/questions/189940/how-do-you-restart-php-fpm"
    },
    {
        "title": "Find the location of a systemd unit file service",
        "question": "There are many different places where systemd unit files may be placed. Is there a quick and easy way to ask systemd where it read a service\u2019s declaration from, given just the service name?",
        "top_answer": "For units that are defined in actual, static files, this can be seen in systemctl status:\n$ systemctl status halt-local.service\n\u25cf halt-local.service - /usr/sbin/halt.local Compatibility\n   Loaded: loaded (/lib/systemd/system/halt-local.service; static)\n   Active: inactive (dead)\n\nBut there are units that are not defined by files, e.g. with systemd-cron installed. These have no useful location listed with status:\n$ systemctl status cron-jojo-0.timer\n\u25cf cron-jojo-0.timer - [Cron] \"*/10 * * * * ...\"\n   Loaded: loaded (/var/spool/cron/crontabs/jojo)\n   Active: active (waiting) since Mon 2015-05-18 14:53:01 UTC; 9min ago\n\nIn either case, though, the FragmentPath field is educating:\n$ systemctl show -P FragmentPath cron-daily.service\n/lib/systemd/system/cron-daily.service\n$ systemctl show -P FragmentPath cron-jojo-0.service\n/run/systemd/generator/cron-jojo-0.service\n$ systemctl show -P FragmentPath halt-local.service\n/lib/systemd/system/halt-local.service\n",
        "url": "https://serverfault.com/questions/692771/find-the-location-of-a-systemd-unit-file-service"
    },
    {
        "title": "Force dig to resolve without using cache",
        "question": "I'm wondering if there is a way to query a DNS server and bypass caching (with dig). Often I change a zone on the DNS server and I want to check if it resolves correctly from my workstation. But since the server caches resolved requests, I often get the old ones. Restarting or -loading the server is not really something nice.",
        "top_answer": "You can use the @ syntax to look up the domain from a particular server.  If the DNS server is authoritative for that domain, the response will not be a cached result.\ndig @ns1.example.com example.com\n\nYou can find the authoritative servers by asking for the NS records for a domain:\ndig example.com NS\n",
        "url": "https://serverfault.com/questions/372066/force-dig-to-resolve-without-using-cache"
    },
    {
        "title": "How do I diff two text files in Windows Powershell?",
        "question": "I have two text files and want to find the differences between them using Windows Powershell. Is there something similar to the Unix diff tool available? Or is there another other way I haven't considered?\nI've tried compare-object, but got this cryptic output:\nPS C:\\> compare-object one.txt two.txt\n\nInputObject                                                 SideIndicator\n-----------                                                 -------------\ntwo.txt                                                     =>\none.txt                                                     <=\n",
        "top_answer": "Figured it out myself. Because Powershell works with .net objects rather than text, you need to use get-content to expose the contents of the text files. So to perform what I was trying to do in the question, use:\ncompare-object (get-content one.txt) (get-content two.txt)\n",
        "url": "https://serverfault.com/questions/5598/how-do-i-diff-two-text-files-in-windows-powershell"
    },
    {
        "title": "Sudo as different user and running screen",
        "question": "Found out today that running screen as a different user that I sudo into won't work!\ni.e.\nssh bob@server         # ssh into server as bob\nsudo su \"monitor\" -\nscreen                 # fails: Cannot open your terminal '/dev/pts/0'\n\nI have a script that runs as the \"monitor\" user. We run it in a screen session in order to see output on the screen. The problem is, we have a number of user who logs in with their own account (i.e. bob, james, susie, etc...) and then they sudo into the \"monitor\" user. Giving them access to the \"monitor\" user is out of the question.",
        "top_answer": "Try running script /dev/null as the user you su to before launching screen - its a ghetto little hack, but it should make screen happy.",
        "url": "https://serverfault.com/questions/116775/sudo-as-different-user-and-running-screen"
    },
    {
        "title": "What does Virtual memory size in top mean?",
        "question": "I am running top to monitor my server performance and 2 of my java processes show virtual memory of up to 800MB-1GB. Is that a bad thing?\nWhat does virtual memory mean? \nAnd oh btw, I have swap of 1GB and it shows 0% used. So I am confused.\nJava process = 1 Tomcat server + my own java daemon Server = Ubuntu 9.10 (karmic)",
        "top_answer": "Virtual memory isn't even necessarily memory.  For example, if a process memory-maps a large file, the file is actually stored on disk, but it still takes up \"address space\" in the process.\nAddress space (ie. virtual memory in the process list) doesn't cost anything; it's not real.  What's real is the RSS (RES) column, which is resident memory.  That's how much of your actual memory a process is occupying.\nBut even that isn't the whole answer.  If a process calls fork(), it splits into two parts, and both of them initially share all their RSS.  So even if RSS was initially 1 GB, the result after forking would be two processes, each with an RSS of 1 GB, but you'd still only be using 1 GB of memory.\nConfused yet?  Here's what you really need to know: use the free command and check the results before and after starting your program (on the +/- buffers/cache line).  That difference is how much new memory your newly-started program used.",
        "url": "https://serverfault.com/questions/138427/what-does-virtual-memory-size-in-top-mean"
    },
    {
        "title": "What's the difference between authorized_keys and authorized_keys2?",
        "question": "Just wanted a quick summary of the differences between them and why there are two?",
        "top_answer": "In OpenSSH prior to version 3, the sshd man page used to say:\n\nThe $HOME/.ssh/authorized_keys file\n  lists the RSA keys that are permitted \n  for RSA authentication in SSH\n  protocols 1.3 and 1.5 Similarly, the \n  $HOME/.ssh/authorized_keys2 file lists\n  the DSA and RSA keys that are permitted\n  for public key authentication\n  (PubkeyAuthentication) in SSH protocol 2.0.\n\nThe release announcement for version 3 states that authorized_keys2 is deprecated and all keys should be put in the authorized_keys file.",
        "url": "https://serverfault.com/questions/116177/whats-the-difference-between-authorized-keys-and-authorized-keys2"
    },
    {
        "title": "Is it possible to generate RSA key without pass phrase?",
        "question": "I'm working with Apache2 and Passenger for a Rails project. \nI would like to create a self-signed SSL Certificate for testing purposes. \nsudo openssl rsa -des3 -in server.key -out server.key.new\n\nWhen i enter the above command, it says\nwriting RSA key\nEnter PEM pass phrase:\n\nIf i do not enter the pass phrse, im getting the below error\nunable to write key\n3079317228:error:28069065:lib(40):UI_set_result:result too small:ui_lib.c:869:Yo\nu must type in 4 to 1024 characters\n3079317228:error:0906406D:PEM routines:PEM_def_callback:problems getting passwor\nd:pem_lib.c:111:\n3079317228:error:0906906F:PEM routines:PEM_ASN1_write_bio:read key:pem_lib.c:382\n\nIs it possible to generate a RSA key without giving pass phrase, since I am not sure how the /etc/init.d/httpd script will start the HTTP server without human intervention (i.e. If I give a 4 character pass phrase, it expects me to provide this while starting the Apache HTTP server).",
        "top_answer": "If you are generating a self signed cert, you can do both the key and cert in one command like so:\nopenssl req  -nodes -new -x509  -keyout server.key -out server.cert\n\nOh, and what @MadHatter said in his answer about omitting the -des3 flag.",
        "url": "https://serverfault.com/questions/366372/is-it-possible-to-generate-rsa-key-without-pass-phrase"
    },
    {
        "title": "How to remove the path with an nginx proxy_pass",
        "question": "I have a running web-application at http://example.com/, and want to \"mount\" another application, on a separate server on http://example.com/en. Upstream servers and proxy_pass seem to work, but for one issue:\nupstream luscious {\n server lixxxx.members.linode.com:9001;\n}\n\nserver {\n  root /var/www/example.com/current/public/;\n  server_name example.com;\n\n  location /en {\n    proxy_pass http://luscious;\n  }\n}\n\nWhen opening example.com/en, my upstream application returns 404 not found /en. This makes sense, as the upstream does not have the path /en. \nIs proxy_path the right solution? Should I rewrite \"upstream\" so it listens to /en instead, as it root path? Or is there a directive that allows me to rewrite the path passed along to upstream?",
        "top_answer": "This is likely the most efficient way to do what you want, without the use of any regular expressions:\nlocation = /en {\n    return 302 /en/;\n}\nlocation /en/ {\n    proxy_pass http://luscious/;  # note the trailing slash here, it matters!\n}\n",
        "url": "https://serverfault.com/questions/562756/how-to-remove-the-path-with-an-nginx-proxy-pass"
    },
    {
        "title": "Run a .bat file in a scheduled task without a window",
        "question": "I have a scheduled task that starts a batch script that runs robocopy every hour. Every time it runs a window pops up  on the desktop with robocopy's output, which I don't really want to see. \nI managed to make the window appear minimized by making the scheduled job run \ncmd /c start /min mybat.bat\n\nbut that gives me a new command window every hour. I was surprised by this, given cmd /c \"Carries out the command specified by string and then terminates\" - I must have misunderstood the docs.\nIs there a way to run a batch script without it popping up a cmd window?",
        "top_answer": "You could run it silently using a Windows Script file instead. The Run Method allows you running a script in invisible mode. Create a .vbs file like this one\nDim WinScriptHost\nSet WinScriptHost = CreateObject(\"WScript.Shell\")\nWinScriptHost.Run Chr(34) & \"C:\\Scheduled Jobs\\mybat.bat\" & Chr(34), 0\nSet WinScriptHost = Nothing\n\nand schedule it. The second argument in this example sets the window style. 0 means \"hide the window.\"\nIn Task Scheduler, run the .vbs file using wscript.exe by setting the 'Action' to run wscript and using the fully-qualified path to the .vbs file as an argument.\nComplete syntax of the Run method:\n object.Run(strCommand, [intWindowStyle], [bWaitOnReturn])\n\nArguments:\n\nobject: WshShell object.\nstrCommand: String value indicating the command line you want to run. You must include any parameters you want to pass to the executable file.\nintWindowStyle: Optional. Integer value indicating the appearance of the program's window. Note that not all programs make use of this information.\nbWaitOnReturn: Optional. Boolean value indicating whether the script should wait for the program to finish executing before continuing to the next statement in your script. If set to true, script execution halts until the program finishes, and Run returns any error code returned by the program. If set to false (the default), the Run method returns immediately after     starting the program, automatically returning 0 (not to be interpreted as an error code).\n",
        "url": "https://serverfault.com/questions/9038/run-a-bat-file-in-a-scheduled-task-without-a-window"
    },
    {
        "title": "Is it possible to detach a process from its terminal? (Or, \"I should have used screen!\") [duplicate]",
        "question": "\nPossible Duplicate:\nCan I nohup/screen an already-started process? \n\nOn Unix (specifically, Linux), I've started a job in a regular ssh->bash session. I'd like to leave work soon, but I now realize that the job is going to take several hours.\nIf I had just started this job in screen, I could detach and go home. But I didn't.\nIs there any way to disconnect the job from its ssh session, so I can shut down my computer, (dropping the TCP connection and killing the ssh session), and yet have the program keep running?\nI don't care about its output -- in fact, I redirected stdout to a file. I just want it to run to completion.",
        "top_answer": "You can press ctrl-z to interrupt the process and then run bg to make it run in the background.\nYou can show a numbered list all processes backgrounded in this manner with jobs.\nThen you can run disown %1 (replace 1 with the process number output by jobs) to detach the process from the terminal.\nIn spite of the name, the process will still be owned by you after running disown, it will just be detached from the terminal you started it in.\nThis answer has more information",
        "url": "https://serverfault.com/questions/34750/is-it-possible-to-detach-a-process-from-its-terminal-or-i-should-have-used-s"
    },
    {
        "title": "How to add dependency on a Windows Service AFTER the service is installed",
        "question": "I have a Windows Service that makes use of a SQL Server database.  I don't have control over the installation of the service, but would like to add a dependency on the service to ensure that it starts after SQL server has started.  (SQL server is running on the same machine as the service in question)\nIs there a tool to add a dependency or possibly editing the registry directly?",
        "top_answer": "This can also be done via an elevated command prompt using the sc command. The syntax is:\nsc config [service name] depend= <Dependencies(separated by / (forward slash))>\n\nNote: There is a space after the equals sign, and there is not one before it.\nWarning: depend= parameter will overwrite existing dependencies list, not append. So for example, if ServiceA already depends on ServiceB and ServiceC, if you run depend= ServiceD, ServiceA will now depend only on ServiceD. (Thanks Matt!)\nExamples\nDependency on one other service:\nsc config ServiceA depend= ServiceB\n\nAbove means that ServiceA will not start until ServiceB has started. If you stop ServiceB, ServiceA will stop automatically.\nDependency on multiple other services:\nsc config ServiceA depend= ServiceB/ServiceC/ServiceD/\"Service Name With Spaces\"\n\nAbove means that ServiceA will not start until ServiceB, ServiceC, and ServiceD have all started. If you stop any of ServiceB, ServiceC, or ServiceD, ServiceA will stop automatically.\nTo remove all dependencies:\nsc config ServiceA depend= /\n\nTo list current dependencies:\nsc qc ServiceA\n",
        "url": "https://serverfault.com/questions/24821/how-to-add-dependency-on-a-windows-service-after-the-service-is-installed"
    },
    {
        "title": "GRANT SELECT to all tables in postgresql",
        "question": "Is there a one-liner that grants the SELECT permissions to a new user postgresql?\nSomething that would implement the following pseudo-code:\nGRANT SELECT ON TABLE * TO my_new_user;\n",
        "top_answer": "I thought it might be helpful to mention that, as of 9.0, postgres does have the syntax to grant privileges on all tables (as well as other objects) in a schema:\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO user;\nGRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO user;\n\nTo grant permissions also for tables created in the future use:\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO user;\n\nHere's the link.",
        "url": "https://serverfault.com/questions/60508/grant-select-to-all-tables-in-postgresql"
    },
    {
        "title": "Find name of Active Directory domain controller",
        "question": "How can I find out the name/IP address of the AD domain controller on my network?",
        "top_answer": "On any computer, that has DNS configured to use AD's DNS server do:\n\nStart -> Run -> nslookup \nset type=all\n_ldap._tcp.dc._msdcs.DOMAIN_NAME\n\n\nReplace DOMAIN_NAME with the actual domain name e.g. example.com. Read more here.",
        "url": "https://serverfault.com/questions/78089/find-name-of-active-directory-domain-controller"
    },
    {
        "title": "Running a cron job manually and immediately",
        "question": "(I have already read How can I test a new cron script ?.)\nI have a specific problem (cron job doesn't appear to run, or run properly), but the issue is general: I'd like to debug scripts that are cronned.  I am aware that I can set up a * * * * * crontab line, but that is not a fully satisfactory solution.  I would like to be able to run a cron job from the command line as if cron were running it (same user, same environment variables, etc.).  Is there a way to do this?  Having to wait 60 seconds to test script changes is not practical.",
        "top_answer": "Here's what I did, and it seems to work in this situation.  At least, it shows me an error, whereas running from the command line as the user doesn't show the error.\n\nStep 1: I put this line temporarily in the user's crontab:\n* * * * *   /usr/bin/env > /home/username/tmp/cron-env\n\nthen took it out once the file was written.\nStep 2: Made myself a little run-as-cron bash script containing:\n#!/bin/bash\n/usr/bin/env -i $(cat /home/username/tmp/cron-env) \"$@\"\n\n\nSo then, as the user in question, I was able to\nrun-as-cron /the/problematic/script --with arguments --and parameters\n\nThis solution could obviously be expanded to make use of sudo or such for more flexibility.\nHope this helps others.",
        "url": "https://serverfault.com/questions/85893/running-a-cron-job-manually-and-immediately"
    },
    {
        "title": "Why can't a CNAME record be used at the apex (aka root) of a domain?",
        "question": "\nThis is a Canonical Question about CNAMEs at the apices (or roots) of zones\n\nIt's relatively common knowledge that CNAME records at the apex of a domain are a taboo practice.\nExample:\nexample.com. IN CNAME ithurts.example.net.\nIn a best case scenario nameserver software might refuse to load the configuration, and in the worst case it might accept this configuration and invalidate the configuration for example.com.\nRecently I had a webhosting company pass instructions to a business unit that we needed to CNAME the apex of our domain to a new record. Knowing that this would be a suicide config when fed to BIND, I advised them that we would not be able to comply and that this was bunk advice in general. The webhosting company took the stance that it is not outright forbidden by standard defining RFCs and that their software supports it. If we could not CNAME the apex, their advice was to have no apex record at all and they would not provide a redirecting webserver. ...What?\nMost of us know that RFC1912 insists that A CNAME record is not allowed to coexist with any other data., but let's be honest with ourselves here, that RFC is only Informational. The closest I know to verbiage that forbids the practice is from RFC1034:\n\nIf a CNAME RR is present at a node, no other data should be\npresent; this ensures that the data for a canonical name and its aliases\ncannot be different.\n\nUnfortunately I've been in the industry long enough to know that \"should not\" is not the same as \"must not\", and that's enough rope for most software designers to hang themselves with. Knowing that anything short of a concise link to a slam dunk would be a waste of my time, I ended up letting the company get away with a scolding for recommending configurations that could break commonly used software without proper disclosure.\nThis brings us to the Q&A. For once I'd like us to get really technical about the insanity of apex CNAMEs, and not skirt around the issue like we usually do when someone posts on the subject. RFC1912 is off limits, as are any other Informational RFC applicable here that I didn't think of. Let's shut this baby down.",
        "top_answer": "CNAME records were originally created to allow multiple names that provide the same resource to be aliased to a single \"canonical name\" for the resource. With the advent of name based virtual hosting, it has instead become commonplace to use them as a generic form of IP address aliasing. Unfortunately, most people who come from a web hosting background expect CNAME records to indicate equivalence in the DNS, which has never been the intent. The apex contains record types which are clearly not used in the identification of a canonical host resource (NS, SOA), which cannot be aliased without breaking the standard at a fundamental level. (particularly in regards to zone cuts)\nUnfortunately, the original DNS standard was written before the standards governing bodies realized that explicit verbiage was necessary to define consistent behavior (RFC 2119). It was necessary to create RFC 2181 to clarify several corner cases due to vague wording, and the updated verbiage makes it clearer that a CNAME cannot be used to achieve apex aliasing without breaking the standard.\n\n6.1. Zone authority\nThe authoritative servers for a zone are enumerated in the NS records\nfor the origin of the zone, which, along with a Start of Authority\n(SOA) record are the mandatory records in every zone.  Such a server\nis authoritative for all resource records in a zone that are not in\nanother zone.  The NS records that indicate a zone cut are the\nproperty of the child zone created, as are any other records for the\norigin of that child zone, or any sub-domains of it.  A server for a\nzone should not return authoritative answers for queries related to\nnames in another zone, which includes the NS, and perhaps A, records\nat a zone cut, unless it also happens to be a server for the other\nzone.\n\nThis establishes that SOA and NS records are mandatory, but it says nothing about A or other types appearing here. It may seem superfluous that I quote this then, but it will become more relevant in a moment.\nRFC 1034 was somewhat vague about the problems that can arise when a CNAME exists alongside other record types. RFC 2181 removes the ambiguity and explicitly states the record types that are allowed to exist alongside them:\n\n10.1. CNAME resource records\nThe DNS CNAME (\"canonical name\") record exists to provide the\ncanonical name associated with an alias name.  There may be only one\nsuch canonical name for any one alias.  That name should generally be\na name that exists elsewhere in the DNS, though there are some rare\napplications for aliases with the accompanying canonical name\nundefined in the DNS.  An alias name (label of a CNAME record) may,\nif DNSSEC is in use, have SIG, NXT, and KEY RRs, but may have no\nother data.  That is, for any label in the DNS (any domain name)\nexactly one of the following is true:\n\none CNAME record exists, optionally accompanied by SIG, NXT, and\nKEY RRs,\none or more records exist, none being CNAME records,\nthe name exists, but has no associated RRs of any type,\nthe name does not exist at all.\n\n\n\"alias name\" in this context is referring to the left hand side of the CNAME record. The bulleted list makes it explicitly clear that a SOA, NS, and A records cannot be seen at a node where a CNAME also appears. When we combine this with section 6.1, it is impossible for a CNAME to exist at the apex as it would have to live alongside mandatory SOA and NS records.\n(This seems to do the job, but if someone has a shorter path to proof please give a crack at it.)\n\nUpdate:\nIt seems that the more recent confusion is coming from Cloudflare's recent decision to allow an illegal CNAME record to be defined at the apex of domains, for which they will synthesize A records. \"RFC compliant\" as described by the linked article refers to the fact that the records synthesized by Cloudflare will play nicely with DNS. This does not change the fact that it is a completely custom behavior.\nIn my opinion this is a disservice to the larger DNS community: it is not in fact a CNAME record, and it misleads people into believing that other software is deficient for not allowing it. (as my question demonstrates)",
        "url": "https://serverfault.com/questions/613829/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain"
    },
    {
        "title": "tar - Remove leading directory components on extraction",
        "question": "How can you extract only the target dir and not the complete dir tree?\ncompress\ntar cf /var/www_bak/site.tar /var/www/site\n\nextract\ntar xf /var/www/site.tar -C /tmp\n\nThis will produce:\n/tmp/var/www/site\n\nHow is it possible to avoid the whole dir tree to be created when the file is extracted?\nWhat I want it to extract to:\n/tmp/site\n",
        "top_answer": "You want to use the --strip-components=NUMBER option of tar:\n --strip-components=NUMBER\n       strip NUMBER leading components from file names on extraction\n\nYour command would be:\ntar xfz /var/www/site.gz --strip-components=2 -C /tmp\n",
        "url": "https://serverfault.com/questions/330127/tar-remove-leading-directory-components-on-extraction"
    },
    {
        "title": "How to remove empty/blank lines from a file in Unix (including spaces)?",
        "question": "How do I remove empty/blank (including spaces only) lines in a file in Unix/Linux using the command line?\ncontents of file.txt\nLine:Text\n1:<blank>\n2:AAA\n3:<blank>\n4:BBB\n5:<blank>\n6:<space><space><space>CCC\n7:<space><space>\n8:DDD\n\noutput desired \n1:AAA\n2:BBB\n3:<space><space><space>CCC\n4:DDD\n",
        "top_answer": "This sed line should do the trick:\nsed -i '/^$/d' file.txt\n\nThe -i means it will edit the file in-place.",
        "url": "https://serverfault.com/questions/252921/how-to-remove-empty-blank-lines-from-a-file-in-unix-including-spaces"
    },
    {
        "title": "How bad is IPv4 address exhaustion really?",
        "question": "For years the press has been writing about the problem that there are now very few IPv4 addresses available. But on the other hand, I'm using a server hosting company which gladly gives out public IPv4 addresses for a small amount of money. And my private internet connection comes with a public IPv4 address.\nHow is that possible? Is the problem as bad as the press wants us to believe?",
        "top_answer": "It's very bad. Here is a list of examples of what I have first hand experience with consumer ISPs doing to fight the shortage of IPv4 addresses:\n\nRepeatedly shuffling around IPv4 blocks between cities causing brief outages and connection resets for customers.\nShortening DHCP lease times from days to minutes.\nAllow users to choose if they want network address translation (NAT) on the Customer Premise Equipment (CPE) or not, then retroactively turn it on for everybody anyway.\nEnabling NAT on CPE for customers who already used the opportunity to opt out of NAT.\nReducing the cap on number of concurrently active media access control (MAC) addresses enforced by CPE.\nDeploying carrier-grade NAT (CGN) for customers who had a real IP address when they signed up for the service.\n\nAll of these are reducing the quality of the product the ISP is selling to their customers. The only sensible explanation for why they would be doing this to their customers is shortage of IPv4 addresses.\nThe shortage of IPv4 addresses has lead to fragmentation of the address space which has multiple shortcomings:\n\nAdministrative overhead which not only costs time and money, but also is error prone and has lead to outages.\nLarge usage of content addressable memory (CAM) capacity on backbone routers which a few years back lead to a significant outage across multiple ISPs when it crossed the limit of a particular popular model of routers.\n\nWithout NAT there is no way we could get by today with the 3700 million routable IPv4 addresses. But NAT is a brittle solution which gives you a less reliable connectivity and problems that are difficult to debug. The more layers of NAT the worse it will be. Two decades of hard work has made a single layer of NAT mostly work, but we have already crossed the point where a single layer of NAT was sufficient to work around the shortage of IPv4 addresses.",
        "url": "https://serverfault.com/questions/894488/how-bad-is-ipv4-address-exhaustion-really"
    },
    {
        "title": "bash: print stderr in red color",
        "question": "Is there a way to make bash display stderr messages in red color?",
        "top_answer": "Method 1: Use process substitution directly:\ncommand 2> >(sed $'s,.*,\\e[31m&\\e[m,'>&2)\n\nMethod 2: Create a function in bash or zsh :\ncolor()(set -o pipefail;\"$@\" 2> >(sed $'s,.*,\\e[31m&\\e[m,'>&2))\nexport -f color\n\nUse it like this:\n$ color command\n\nBoth methods will show the command's stderr in red.\nKeep reading for an explanation of how it works.  There are some interesting features demonstrated by these commands.  The first 3 bullet points only apply to Method 2.  The rest apply to both methods.\n\ncolor()... \u2014 Creates a bash function called color.\nset -o pipefail \u2014 This is a shell option that preserves the error return code of a command whose output is piped into another command.  This is done in a subshell, which is created by the parentheses, so as not to change the pipefail option in the outer shell.\n\"$@\" \u2014 Executes the arguments to the function as a new command.  \"$@\" is equivalent to \"$1\" \"$2\" ...\n2> >(...) \u2014 The >(...) syntax is called process substitution.  Preceded by 2> , it connects the stderr of the main command to the stdin of the sed process inside the parentheses.\nsed ... \u2014 Because of the redirects above, sed's stdin is the stderr of the executed command.  Its function is to surround each line with color codes.\n$'...' A bash construct that causes it to understand backslash-escaped characters\n.* \u2014 Matches the entire line.\n\\e[31m \u2014 The ANSI escape sequence that causes the following characters to be red\n& \u2014 The sed replace character that expands to the entire matched string (the entire line in this case).\n\\e[m \u2014 The ANSI escape sequence that resets the color.\n>&2 \u2014 Shorthand for 1>&2, this redirects sed's stdout to stderr.\n",
        "url": "https://serverfault.com/questions/59262/bash-print-stderr-in-red-color"
    },
    {
        "title": "Getting \"Cannot ioctl TUNSETIFF tun: Operation not permitted\" when trying to connect to OpenVPN",
        "question": "I'm trying to setup an OpenVPN Access Server in AWS using the market place AMI, but I;m struggling to connect to it.\nThe access server is up and running. I've also added a user with Auto-Login and generated the relevant client config and certificates.\nI then copied said files down to my machine and tried to connect using openvpn client.ovpn but got the following output and error,\nWed Nov 26 12:41:10 2014 OpenVPN 2.3.2 x86_64-pc-linux-gnu [SSL (OpenSSL)] [LZO] [EPOLL] [PKCS11] [eurephia] [MH] [IPv6] built on Feb  4 2014\nWed Nov 26 12:41:10 2014 Control Channel Authentication: using 'ta.key' as a OpenVPN static key file\nWed Nov 26 12:41:10 2014 Outgoing Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication\nWed Nov 26 12:41:10 2014 Incoming Control Channel Authentication: Using 160 bit message hash 'SHA1' for HMAC authentication\nWed Nov 26 12:41:10 2014 Socket Buffers: R=[212992->200000] S=[212992->200000]\nWed Nov 26 12:41:10 2014 UDPv4 link local: [undef]\nWed Nov 26 12:41:10 2014 UDPv4 link remote: [AF_INET]<REMOVED_IP>:1194\nWed Nov 26 12:41:10 2014 TLS: Initial packet from [AF_INET]<REMOVED_IP>:1194, sid=2a06a918 c4ecc6df\nWed Nov 26 12:41:11 2014 VERIFY OK: depth=1, CN=OpenVPN CA\nWed Nov 26 12:41:11 2014 VERIFY OK: nsCertType=SERVER\nWed Nov 26 12:41:11 2014 VERIFY OK: depth=0, CN=OpenVPN Server\nWed Nov 26 12:41:11 2014 Data Channel Encrypt: Cipher 'BF-CBC' initialized with 128 bit key\nWed Nov 26 12:41:11 2014 Data Channel Encrypt: Using 160 bit message hash 'SHA1' for HMAC authentication\nWed Nov 26 12:41:11 2014 Data Channel Decrypt: Cipher 'BF-CBC' initialized with 128 bit key\nWed Nov 26 12:41:11 2014 Data Channel Decrypt: Using 160 bit message hash 'SHA1' for HMAC authentication\nWed Nov 26 12:41:11 2014 Control Channel: TLSv1, cipher TLSv1/SSLv3 DHE-RSA-AES256-SHA, 2048 bit RSA\nWed Nov 26 12:41:11 2014 [OpenVPN Server] Peer Connection Initiated with [AF_INET]54.173.232.46:1194\nWed Nov 26 12:41:14 2014 SENT CONTROL [OpenVPN Server]: 'PUSH_REQUEST' (status=1)\nWed Nov 26 12:41:14 2014 PUSH: Received control message: 'PUSH_REPLY,explicit-exit-notify,topology subnet,route-delay 5 30,dhcp-pre-release,dhcp-renew,dhcp-release,route-metric 101,ping 12,ping-restart 50,comp-lzo yes,redirect-private def1,redirect-private bypass-dhcp,redirect-private autolocal,redirect-private bypass-dns,route-gateway 172.16.224.129,route 172.16.1.0 255.255.255.0,route 172.16.224.0 255.255.255.0,block-ipv6,ifconfig 172.16.224.131 255.255.255.128'\nWed Nov 26 12:41:14 2014 Unrecognized option or missing parameter(s) in [PUSH-OPTIONS]:4: dhcp-pre-release (2.3.2)\nWed Nov 26 12:41:14 2014 Unrecognized option or missing parameter(s) in [PUSH-OPTIONS]:5: dhcp-renew (2.3.2)\nWed Nov 26 12:41:14 2014 Unrecognized option or missing parameter(s) in [PUSH-OPTIONS]:6: dhcp-release (2.3.2)\nWed Nov 26 12:41:14 2014 Unrecognized option or missing parameter(s) in [PUSH-OPTIONS]:18: block-ipv6 (2.3.2)\nWed Nov 26 12:41:14 2014 OPTIONS IMPORT: timers and/or timeouts modified\nWed Nov 26 12:41:14 2014 OPTIONS IMPORT: explicit notify parm(s) modified\nWed Nov 26 12:41:14 2014 OPTIONS IMPORT: LZO parms modified\nWed Nov 26 12:41:14 2014 OPTIONS IMPORT: --ifconfig/up options modified\nWed Nov 26 12:41:14 2014 OPTIONS IMPORT: route options modified\nWed Nov 26 12:41:14 2014 OPTIONS IMPORT: route-related options modified\nWed Nov 26 12:41:14 2014 ROUTE_GATEWAY 192.168.0.1/255.255.255.0 IFACE=wlan0 HWADDR=c4:85:08:c9:14:f4\nWed Nov 26 12:41:14 2014 ERROR: Cannot ioctl TUNSETIFF tun: Operation not permitted (errno=1)\nWed Nov 26 12:41:14 2014 Exiting due to fatal error\n\nAny idea what the problem is? I assume it's failing to create the tunnel due to the ERROR line?\nI'm running server version 2.0.10 and client version,\nOpenVPN 2.3.2 x86_64-pc-linux-gnu [SSL (OpenSSL)] [LZO] [EPOLL] [PKCS11] [eurephia] [MH] [IPv6] built on Feb  4 2014\nOriginally developed by James Yonan\nCopyright (C) 2002-2010 OpenVPN Technologies, Inc. <[email\u00a0protected]>\nCompile time defines: enable_crypto=yes enable_debug=yes enable_def_auth=yes enable_dependency_tracking=no enable_dlopen=unknown enable_dlopen_self=unknown enable_dlopen_self_static=unknown enable_eurephia=yes enable_fast_install=yes enable_fragment=yes enable_http_proxy=yes enable_iproute2=yes enable_libtool_lock=yes enable_lzo=yes enable_lzo_stub=no enable_maintainer_mode=no enable_management=yes enable_multi=yes enable_multihome=yes enable_pam_dlopen=no enable_password_save=yes enable_pedantic=no enable_pf=yes enable_pkcs11=yes enable_plugin_auth_pam=yes enable_plugin_down_root=yes enable_plugins=yes enable_port_share=yes enable_selinux=no enable_server=yes enable_shared=yes enable_shared_with_static_runtimes=no enable_small=no enable_socks=yes enable_ssl=yes enable_static=yes enable_strict=no enable_strict_options=no enable_systemd=no enable_win32_dll=yes enable_x509_alt_username=yes with_crypto_library=openssl with_gnu_ld=yes with_ifconfig_path=/sbin/ifconfig with_iproute_path=/sbin/ip with_mem_check=no with_plugindir='${prefix}/lib/openvpn' with_route_path=/sbin/route with_sysroot=no\n",
        "top_answer": "Looks like this is a simple matter of sudo.\nsudo openvpn client.ovpn worked a treat.",
        "url": "https://serverfault.com/questions/647231/getting-cannot-ioctl-tunsetiff-tun-operation-not-permitted-when-trying-to-con"
    },
    {
        "title": "Best practices for setting a cron job for Let's Encrypt (Certbot) renewal?",
        "question": "Is this correct way to set cron for renewal of Let's Encrypt cert in Apache2 ? \nI use Ubuntu 16.04.\n@monthly letsencrypt renew && service apache2 reload\n",
        "top_answer": "Monthly is not frequent enough.\nThis script should run at least weekly, and preferably daily. Remember that certs don't get renewed unless they are near to expiration, and monthly could cause your existing certs to occasionally be expired already before they get renewed.\nThe name of the program is certbot, which was renamed from letsencrypt. If you are still using letsencrypt, you need to update to the current version.\nAside from those issues, it's about the same as my cron jobs.\n43 6 * * * certbot renew --post-hook \"systemctl reload nginx\"\n\n\nNote: in 18.04 LTS the letsencrypt package has been (finally) renamed to certbot. It now includes a systemd timer which you can enable to schedule certbot renewals, with systemctl enable certbot.timer and systemctl start certbot.timer. However, Ubuntu did not provide a way to specify hooks. You'll need to set up an override for certbot.service to override ExecStart= with your desired command line, until Canonical fixes this.",
        "url": "https://serverfault.com/questions/790772/best-practices-for-setting-a-cron-job-for-lets-encrypt-certbot-renewal"
    },
    {
        "title": "What causes the 'Connection Refused' message?",
        "question": "\nThis is a Canonical Question about Connection Refused \n\nWe see a lot of questions to the effect \n\nWhen I try to connect to a system I get a message\nConnection refused\nWhy is this ?\n",
        "top_answer": "\nNote: This message is a symptom of the problem you are trying to solve. Understanding the cause of the message will ultimately lead you to solving your problem.\n\nThe message 'Connection Refused' has two main causes:\n\nNothing is listening on the IP:Port you are trying to connect to.\nThe port is blocked by a firewall.\n\nNo process is listening.\nThis is by far the most common reason for the message. First ensure that you are trying to connect to the correct system. If you are then to determine if this is the problem, on the remote system run netstat or ss1 e.g. if you are expecting a process to be listening on port 22222\nsudo netstat -tnlp | grep :22222\n\nor\nss -tnlp | grep :22222\n\nFor OSX a suitable command is\nsudo netstat -tnlp tcp | grep '\\.22222 '\n\nIf nothing is listening then the above will produce no output. If you see some output then confirm that it's what you expect then see the firewall section below.\nIf you don't have access to the remote system and want to confirm the problem before reporting it to the relevant administrators you can use tcpdump (wireshark or similar).\nWhen a connection is attempted to an IP:port where nothing is listening, the response from the remote system to the initial SYN packet is a packet with the flags RST,ACK set. This closes the connection and causes the Connection Refused message e.g.\n\n$ sudo tcpdump -n host 192.0.2.1 and port 22222 \ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on enp14s0, link-type EN10MB (Ethernet), capture size 262144 bytes \n12:31:27.013976 IP 192.0.2.2.34390 > 192.0.2.1.22222: Flags [S], seq 1207858804, win 29200, options [mss 1460,sackOK,TS val 15306344 ecr 0,nop,wscale 7], length 0 \n12:31:27.020162 IP 192.0.2.1.22222 > 192.0.2.2.34390: Flags [R.], seq 0, ack 1207858805, win 0, length 0 \n\nNote that tcpdump uses a . to represent the ACK flag.\nPort is blocked by a firewall\nIf the port is blocked by a firewall and the firewall has been configured to respond with icmp-port-unreachable this will also cause a connection refused message. Again you can see this with tcpdump (or similar)\n\n$ sudo tcpdump -n icmp \ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on enp14s0, link-type EN10MB (Ethernet), capture size 262144 bytes\n13:03:24.149897 IP 192.0.2.1 > 192.0.2.2: ICMP 192.0.2.1 tcp port 22222 unreachable, length 68\n\nNote that this also tells us where the blocking firewall is.\n\nSo now you know what's causing the Connection refused message you should take appropriate action e.g. contact the firewall administrator or investigate the reason for the process not listening.\n1 Other tools are likely available.",
        "url": "https://serverfault.com/questions/725262/what-causes-the-connection-refused-message"
    },
    {
        "title": "How to check if an RSA public / private key pair match",
        "question": "I have two files, id_rsa and id_rsa.pub. What command can be used to validate if they are a valid pair?",
        "top_answer": "I would prefer the ssh-keygen -y -e -f <private key> way instead of the accepted answer of How do you test a public/private DSA keypair? on  Stack Overflow.\nssh-keygen -y -e -f <private key> takes a private key and prints the corresponding public key which can be directly compared to your available public keys. (Hint: beware of comments or key-options.)\n(How the hell is it doing that? I can only hope the public key is encoded directly or indirectly in the private key...)\nI needed this myself and used the following Bash one-liner. It should output nothing if the keys belong together. Apply a little -q to the diff in scripts and diff only sets the return code appropriately.\nPRIVKEY=id_rsa\nTESTKEY=id_rsa.pub\ndiff <( ssh-keygen -y -e -f \"$PRIVKEY\" ) <( ssh-keygen -y -e -f \"$TESTKEY\" )\n",
        "url": "https://serverfault.com/questions/426394/how-to-check-if-an-rsa-public-private-key-pair-match"
    },
    {
        "title": "What does 'set -e' do, and why might it be considered dangerous?",
        "question": "This question has appeared on a pre-interview quiz and it's making me crazy. Can anyone answer this and put me at ease? The quiz has no reference to a particular shell but the job description is for a unix sa.\nagain the question is simply...\n\nWhat does 'set -e' do, and why might it be considered dangerous?\n",
        "top_answer": "set -e causes the shell to exit if any subcommand or pipeline returns a non-zero status.\nThe answer the interviewer was probably looking for is: \n\nIt would be dangerous to use \"set -e\" when creating init.d scripts:\n\nFrom http://www.debian.org/doc/debian-policy/ch-opersys.html 9.3.2 --\n\nBe careful of using set -e in init.d scripts. Writing correct init.d scripts requires accepting various error exit statuses when daemons are already running or already stopped without aborting the init.d script, and common init.d function libraries are not safe to call with set -e in effect. For init.d scripts, it's often easier to not use set -e and instead check the result of each command separately.\n\nThis is a valid question from an interviewer standpoint because it gauges a candidates working knowledge of server-level scripting and automation",
        "url": "https://serverfault.com/questions/143445/what-does-set-e-do-and-why-might-it-be-considered-dangerous"
    },
    {
        "title": "What's the best way to check if a volume is mounted in a Bash script?",
        "question": "What's the best way to check if a volume is mounted in a Bash script?\nWhat I'd really like is a method that I can use like this:\nif <something is mounted at /mnt/foo> \nthen\n   <Do some stuff>\nelse\n   <Do some different stuff>\nfi\n",
        "top_answer": "Avoid using /etc/mtab because it may be inconsistent.\nAvoid piping mount because it needn't be that complicated.\nSimply:\nif grep -qs '/mnt/foo ' /proc/mounts; then\n    echo \"It's mounted.\"\nelse\n    echo \"It's not mounted.\"\nfi\n\n(The space after the /mnt/foo is to avoid matching e.g. /mnt/foo-bar.)",
        "url": "https://serverfault.com/questions/50585/whats-the-best-way-to-check-if-a-volume-is-mounted-in-a-bash-script"
    },
    {
        "title": "How can I debug a docker container initialization?",
        "question": "I had an issue with a container, even though it builds perfectly it does not properly start. The cause is a workaround I've added to the Dockerfile (for having a self-configured /etc/hosts routing)\nRUN mkdir -p -- /lib-override /etc-override && cp /lib/libnss_files.so.2 /lib-override\nADD hosts.template /etc-override/hosts\nRUN perl -pi -e 's:/etc/hosts:/etc-override/hosts:g' /lib-override/libnss_files.so.2\nENV LD_LIBRARY_PATH /lib-override\n\nObviously there's some error in there, but I wonder how can I get more info on what docker is doing while running. for example, this works:\n$ docker run image ls\nusr bin ...\n\nBut this doesn't:\n$ docker run image ls -l\n$\n\nThere is nothing in the logs and I can't call an interactive shell either. I can use strace to see what's happening but I was hoping theres a better way.\nIs there any way I can set docker to be more verbose?\nEDIT: Thanks to Andrew D. I now know what's wrong with the code above (I left it so his answer can be understood). Now the issue is still how might I debug something like this or get some insides at why ls -l failed why ls did not.\nEDIT: The -D=true might give more output, though not in my case...",
        "top_answer": "Docker events command may help and Docker logs command can fetch logs even after the image failed to start.\nFirst start docker events in the background to see whats going on.\ndocker events&\n\nThen run your failing docker run ... command. \nThen you should see something like the following on screen:\n2015-12-22T15:13:05.503402713+02:00 xxxxxxxacd8ca86df9eac5fd5466884c0b42a06293ccff0b5101b5987f5da07d: (from xxx/xxx:latest) die\n\nThen you can get the startup hex id from previous message or the output of the run command. Then you can use it with the logs command:\ndocker logs <copy the instance id from docker events messages on screen>\n\nYou should now see some output from the failed image startup.\nAs @alexkb suggested in a comment: docker events& can be troublesome if your container is being constantly restarted from something like AWS ECS service. In this scenario it may be easier to get the container hex id out of the logs in /var/log/ecs/ecs-agent.log.<DATE>. Then use docker logs <hex id>.",
        "url": "https://serverfault.com/questions/596994/how-can-i-debug-a-docker-container-initialization"
    },
    {
        "title": "How to output variable in nginx log for debugging",
        "question": "I am testing nginx and want to output variables to the log files. How can I do that and which log file will it go (access or error).",
        "top_answer": "You can send nginx variable values via headers. Handy for development. \nadd_header X-uri \"$uri\";\n\nand you'll see in your browser's response headers:\nX-uri:/index.php\n\nI sometimes do this during local development. \nIt's also handy for telling you if a subsection is getting executed or not. Just sprinkle it inside your clauses to see if they're getting used.\nlocation ~* ^.+.(jpg|jpeg|gif|css|png|js|ico|html|xml|txt)$ {\n    add_header X-debug-message \"A static file was served\" always;\n    ...\n}\n\nlocation ~ \\.php$ {\n    add_header X-debug-message \"A php file was used\" always;\n    ...\n}\n\nSo visiting a url like http://www.example.com/index.php will trigger the latter header while visiting http://www.example.com/img/my-ducky.png will trigger the former header.",
        "url": "https://serverfault.com/questions/404626/how-to-output-variable-in-nginx-log-for-debugging"
    },
    {
        "title": "Is there a global, persistent CMD history?",
        "question": "Sometimes I forget how the exact syntax of a CMD command looks and then I would like to search my own CMD history. Clearly, within the same session, you can browse it with the up and down arrow keys but what about the history of former CMD sessions? Is there a file, a log the history gets written to or does it all go to digital Nirvana?\nThanks!",
        "top_answer": "No, Windows command prompt history can't be saved when a session ends.",
        "url": "https://serverfault.com/questions/95404/is-there-a-global-persistent-cmd-history"
    },
    {
        "title": "Are SSD drives as reliable as mechanical drives (2013)?",
        "question": "SSD drives have been around for several years now.  But the issue of reliability still comes up.\nI guess this is a follow up from this question posted 4 years ago, and last updated in 2011.  It's now 2013, has much changed?  I guess I'm looking for some real evidence, more than just a gut feel.  Maybe you're using them in your DC.  What's been your experience?\nReliability of ssd drives\n\nUPDATE:\nIt's now 2016.  I think the answer is probably yes (a pity they still cost more per GB though).\nThis report gives some evidence:\nFlash Reliability in Production: The Expected and the Unexpected\nAnd some interesting data on (consumer) mechanical drives:\nBackblaze: Hard Drive Data and Stats",
        "top_answer": "This is going to be a function of your workload and the class of drive you purchase...\nIn my server deployments, I have not had a properly-spec'd SSD fail. That's across many different types of drives, applications and workloads. \nRemember, not all SSDs are the same!!\nSo what does \"properly-spec'd\" mean?\nIf your question is about SSD use in enterprise and server applications, quite a bit has changed over the past few years since the original question. Here are a few things to consider:\n\nIdentify your use-case: There are consumer drives, enterprise drives and even ruggedized industrial application SSDs. Don't buy a cheap disk meant for desktop use and run a write-intensive database on it. \nMany form-factors are available: Today's SSDs can be found in PCIe cards, SATA and SAS 1.8\", 2.5\", 3.5\" and other variants.\nUse RAID for your servers: You wouldn't depend on a single mechanical drive in a server situation. Why would you do the same for an SSD? \nDrive composition: There are DRAM-based SSDs, as well as the MLC, eMLC and SLC flash types. The latter have finite lifetimes, but they're well-defined by the manufacturer. e.g. you'll see daily write limits like 5TB/day for 3 years.\nDrive application matters: Some drives are for general use, while there are others that are read-optimized or write-optimized. DRAM-based drives like the sTec ZeusRAM and DDRDrive won't wear-out. These are ideal for high-write environments and to front slower disks. MLC drives tend to be larger and optimized for reads. SLC drives have a better lifetime than the MLC drives, but enterprise MLC really appears to be good enough for most scenarios.\nTRIM doesn't seem to matter: Hardware RAID controllers still don't seem to fully support it. And most of the time I use SSDs, it's going to be on a hardware RAID setup. It isn't something I've worried about in my installations. Maybe I should?\nEndurance: Over-provisioning is common in server-class SSDs. Sometimes this can be done at the firmware level, or just by partitioning the drive the right way. Wear-leveling algorithms are better across the board as well. Some drives even report lifetime and endurance statistics. For example, some of my HP-branded Sandisk enterprise SSDs show 98% life remaining after two years of use.\nPrices have fallen considerably: SSDs hit the right price:performance ratio for many applications. When performance is really needed, it's rare to default to mechanical drives now.\nReputations have been solidified: e.g. Intel is safe but not high-performance. OCZ is unreliable. Sandforce-based drives are good. sTec/STEC is extremely-solid and is the OEM for a lot of high-end array drives. Sandisk/Pliant is similar. OWC has great SSD solutions with a superb warranty for low-impact servers and for workstation/laptop deployment.\nPower-loss protection is important: Look at drives with supercapacitors/supercaps to handle outstanding writes during power events. Some drives boost performance with onboard caches or leverage them to reduce wear. Supercaps ensure that those writes are flushed to stable storage.\nHybrid solutions: Hardware RAID controller vendors offer the ability to augment standard disk arrays with SSDs to accelerate reads/writes or serve as intelligent cache. LSI has CacheCade and its Nytro hardware/software offerings. Software and OS-level solutions have also exist to do things like provide local cache on application, database or hypervisor systems. Advanced filesystems like ZFS make very intelligent use of read and write-optimized SSDs; ZFS can be configured to use separate devices for secondary caching and for the intent log, and SSDs are often used in that capacity even for HDD pools.\nTop-tier flash has arrived: PCIe flash solutions like FusionIO have matured to the point where organizations are comfortable deploying critical applications that rely on the increased performance. Appliance and SAN solutions like RanSan and Violin Memory are still out there as well, with more entrants coming into that space.  \n\n",
        "url": "https://serverfault.com/questions/507521/are-ssd-drives-as-reliable-as-mechanical-drives-2013"
    },
    {
        "title": "Command line to list users in a Windows Active Directory group?",
        "question": "Is there a command line way to list all the users in a particular Active Directory group?\nI can see who is in the group by going to Manage Computer --> Local User / Groups --> Groups and double clicking the group.\nI just need a command line way to retrieve the data, so I can do some other automated tasks.",
        "top_answer": "try\ndsget group \"CN=GroupName,DC=domain,DC=name,DC=com\" -members\n",
        "url": "https://serverfault.com/questions/49405/command-line-to-list-users-in-a-windows-active-directory-group"
    },
    {
        "title": "How to add a timestamp to bash script log?",
        "question": "I have a constantly running script that I output to a log file:\nscript.sh >> /var/log/logfile\n\nI'd like to add a timestamp before each line that is appended to the log.  Like:\nSat Sep 10 21:33:06 UTC 2011 The server has booted up.  Hmmph.\n\nIs there any jujitsu I can use?",
        "top_answer": "You can pipe the script's output through a loop that prefixes the current date and time:\n./script.sh | while IFS= read -r line; do printf '%s %s\\n' \"$(date)\" \"$line\"; done >>/var/log/logfile\n\nIf you'll be using this a lot, it's easy to make a bash function to handle the loop:\nadddate() {\n    while IFS= read -r line; do\n        printf '%s %s\\n' \"$(date)\" \"$line\";\n    done\n}\n\n./thisscript.sh | adddate >>/var/log/logfile\n./thatscript.sh | adddate >>/var/log/logfile\n./theotherscript.sh | adddate >>/var/log/logfile\n",
        "url": "https://serverfault.com/questions/310098/how-to-add-a-timestamp-to-bash-script-log"
    },
    {
        "title": "Delete Amazon EC2 terminated instance",
        "question": "I created an Amazon Free tier Usage Account. I launched two amazon ec2 instances using the online tool. After that one instance was created and running while other was pending which quickly shifted to terminated state. \nIn description it shows State Transition Reason: Server.InternalError: Internal error on launch\nIs there any where I could restart the terminated instance or remove it from table. It looks very annoying ",
        "top_answer": "Terminated instances will go away after a few hours. There is nothing you can do to manually remove them. \nNot to worry, you won't get billed for it. ",
        "url": "https://serverfault.com/questions/393417/delete-amazon-ec2-terminated-instance"
    },
    {
        "title": "What's the difference between include_tasks and import_tasks?",
        "question": "In Ansible 2.4, the include module is deprecated. In its place, it ships with two replacement modules, import_tasks and include_tasks. But they have very similar descriptions:\n\ninclude_tasks: Includes a file with a list of tasks to be executed in the current playbook.\nimport_tasks: Imports a list of tasks to be added to the current playbook for subsequent execution.\n\nWhen should I use the former, and when should I use the latter?",
        "top_answer": "There's quite a bit about this topic in the documentation:\n\nIncludes vs. Imports\nDynamic vs. Static\n\nThe main difference is:\n\nAll import* statements are pre-processed at the time playbooks are parsed.\n  All include* statements are processed as they encountered during the execution of the playbook.\n\nSo import is static, include is dynamic.\nFrom my experience, you should use import when you deal with logical \"units\". For example, separate long list of tasks into subtask files:\nmain.yml:\n- import_tasks: prepare_filesystem.yml\n- import_tasks: install_prerequisites.yml\n- import_tasks: install_application.yml\n\nBut you would use include to deal with different workflows and take decisions based on some dynamically gathered facts:\ninstall_prerequisites:\n- include_tasks: prerequisites_{{ ansible_os_family | lower }}.yml\n",
        "url": "https://serverfault.com/questions/875247/whats-the-difference-between-include-tasks-and-import-tasks"
    },
    {
        "title": "Sometimes PowerShell stops sending output until I press enter. Why?",
        "question": "Periodically I notice PowerShell seems to take forever to finish doing whatever it is I told it to do until it occurs to me to \"wake it up\" by pressing enter. This is not the fault of any one process as best I can tell, as I have even run custom apps that just log their output to the screen every few seconds and even in these cases, PowerShell will stop doing anything after a while until I \"give it a kick\" by pressing enter.\nAny ideas what might be causing this?",
        "top_answer": "If the QuickEdit Mode and\\or Insert options are checked within the console\\window properties, and you click within the console, it will pause the output. If those options are not checked, the output can't be paused by clicking within the console.\n\nTo get to these settings, right-click on the PowerShell-Logo in the top-left of your terminal window, then select 'Properties' (at least that's one way to do it)",
        "url": "https://serverfault.com/questions/204150/sometimes-powershell-stops-sending-output-until-i-press-enter-why"
    },
    {
        "title": "Amazon Cloudfront with S3. Access Denied",
        "question": "We're trying to distribute out S3 buckets via Cloudfront but for some reason the only response is an AccessDenied XML document like the following:\n<Error>\n    <Code>AccessDenied</Code>\n    <Message>Access Denied</Message>\n    <RequestId>89F25EB47DDA64D5</RequestId>\n    <HostId>Z2xAduhEswbdBqTB/cgCggm/jVG24dPZjy1GScs9ak0w95rF4I0SnDnJrUKHHQC</HostId>\n</Error>\n\nHere's the setting's we're using:\n\n\nAnd here's the policy for the bucket\n{\n    \"Version\": \"2008-10-17\",\n    \"Id\": \"PolicyForCloudFrontPrivateContent\",\n    \"Statement\": [\n        {\n            \"Sid\": \"1\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity *********\"\n            },\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::x***-logos/*\"\n        }\n    ]\n}\n",
        "top_answer": "If you're accessing the root of your CloudFront distribution, you need to set a default root object:\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DefaultRootObject.html\nTo specify a default root object using the CloudFront console:\n\nSign in to the AWS Management Console and open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/.\nIn the list of distributions in the top pane, select the distribution to update.\nIn the Distribution Details pane, on the General tab, click Edit.\nIn the Edit Distribution dialog box, in the Default Root Object field, enter the file name of the default root object.\nEnter only the object name, for example, index.html. Do not add a / before the object name.\nTo save your changes, click Yes, Edit.\n",
        "url": "https://serverfault.com/questions/581268/amazon-cloudfront-with-s3-access-denied"
    },
    {
        "title": "How do I make ssh fail rather than prompt for a password if the public-key authentication fails?",
        "question": "I've got a script that SSHes several servers using public key authentication. One of the servers has stopped letting the script log in due to a configuration issue, which means that the script gets stuck with a \"Password:\" prompt, which it obviously cannot answer, so it doesn't even try the rest of the servers in the list.\nIs there a way to tell the ssh client not to prompt for a password if key authentication fails, but instead to just report an error connecting and let my script carry on?",
        "top_answer": "For OpenSSH there is BatchMode, which in addition to disabling password prompting, should disable querying for passphrase(s) for keys.\n\nBatchMode\n\nIf set to \u201cyes\u201d, passphrase/password querying will be disabled.\n    This option is useful in scripts and other batch jobs where no\n    user is present to supply the password.  The argument must be\n    \u201cyes\u201d or \u201cno\u201d.  The default is \u201cno\u201d.\n\n\nSample usage:\nssh -oBatchMode=yes -l <user> <host> <dostuff>\n",
        "url": "https://serverfault.com/questions/61915/how-do-i-make-ssh-fail-rather-than-prompt-for-a-password-if-the-public-key-authe"
    },
    {
        "title": "How do I find the UUID of a file system?",
        "question": "I'm running Ubuntu, and want to find out the UUID of a particular filesystem (not partition). I know I can use e2label /dev/sda1 to find out the file system label, but there doesn't seem to be a similar way to find the UUID.",
        "top_answer": "Another command that might be available and also works quite well for this is 'blkid'.  It's part of the e2fsprogs package.  Examples of its usage:\nLook up data on /dev/sda1:\ntopher@crucible:~$ sudo blkid /dev/sda1\n/dev/sda1: UUID=\"727cac18-044b-4504-87f1-a5aefa774bda\" TYPE=\"ext3\"\n\nShow UUID data for all partitions:\ntopher@crucible:~$ sudo blkid\n\n/dev/sda1: UUID=\"727cac18-044b-4504-87f1-a5aefa774bda\" TYPE=\"ext3\"\n/dev/sdb: UUID=\"467c4aa9-963d-4467-8cd0-d58caaacaff4\" TYPE=\"ext3\"\n\nShow UUID data for all partitions in an easier-to-read format:\n(Note: in newer releases, blkid -L has a different meaning, and blkid -o list should be used instead)\ntopher@crucible:~$ sudo blkid -L\n\ndevice     fs_type label    mount point    UUID\n-------------------------------------------------------------------------------\n/dev/sda1 ext3             /              727cac18-044b-4504-87f1-a5aefa774bda\n/dev/sdc  ext3             /home          467c4aa9-963d-4467-8cd0-d58caaacaff4\n\nShow just the UUID for /dev/sda1 and nothing else:\ntopher@crucible:~$ sudo blkid -s UUID -o value /dev/sda1\n\n727cac18-044b-4504-87f1-a5aefa774bda\n",
        "url": "https://serverfault.com/questions/3132/how-do-i-find-the-uuid-of-a-file-system"
    },
    {
        "title": "How to remove the \"installed manually\" flag and revert to \"automatically installed\" with apt-get?",
        "question": "To compile something, I needed the zlib1g-dev package to be installed so I launched an apt-get install zlib1g-dev. \napt-get informed me nicely that the package was already auto-installed because of an other package, and that it understands that I want it installed explicitly now :\n# apt-get install zlib1g-dev\nzlib1g-dev is already the newest version.\nzlib1g-dev set to manually installed.\n\nMy compilation done, I don't need it any more explicitly, so I want to revert its status to the previous one : auto-installed. This way it will be pruned automatically when it will not be needed any more with a simple apt-get autoremove.\nI cannot do an apt-get remove zlib1g-dev since some packages still depends on it. \nSo how may I revert the package zlib1g-devinstallation state to auto-installed ?\nI know that I might edit /var/lib/apt/extended_states by hand from \nPackage: zlib1g-dev\nAuto-Installed: 0\n\nto\nPackage: zlib1g-dev\nAuto-Installed: 1\n\n... but it just doesn't feel right.",
        "top_answer": "Aptitude can help you when you initially install the package:\naptitude install \"zlib1g-dev&M\"\n\nOr, after your have installed the package:\naptitude markauto \"zlib1g\"\n\nEdit: If you do not have aptitude, you can use\napt-mark auto zlib1g-dev\n",
        "url": "https://serverfault.com/questions/87933/how-to-remove-the-installed-manually-flag-and-revert-to-automatically-install"
    },
    {
        "title": "Why do consoles sometimes hang forever when SSH connection breaks?",
        "question": "I've seen this with so many consoles (on Linux, Mac, ...), and with lots of different machines  in many different networks. I can never pinpoint the exact reason, why this happens: All you have to do is log in to a machine via SSH. If the connection breaks for some reason (for simplicity, let's say the network cable was pulled), then sometimes the console just hangs forever - at other times, it just exits fine to the parent shell.\nIt's so annoying when this happens (e.g. you lose the command history.) Is there maybe a secret keyboard shortcut which can force an exit (Ctrl-C or Ctrl-D don't work)? And what's the reason for this random \"bug\" across all the implementations anyway?",
        "top_answer": "There is a \"secret\" keyboard shortcut to force an exit :~) From the frozen session, hit these keys in order: Enter~. The tilde (only after a newline) is recognized as an escape sequence by the ssh client, and the period tells the client to terminate it's business without further ado.\nThe long-hang behavior on communication issues is not a bug, the SSH session is hanging out hoping the other side will come back. If the network breaks, sometimes even days later you can get an SSH session back. Of course you can specifically tell it to give up and die with the sequence above. There are also various things you can do such as setting keep-alive timeouts in your client so that if it doesn't have an active link for a certain amount of time it shuts off on it's own, but the default behavior is to stay as connected as possible!\nEdit: Another useful application of this interrupt key is to get the attention of the local ssh client and background it to get back to your local shell for a minute \u2014say to get something from your history\u2014 then forground it to keep working remotely. Enter~ Ctrl+Z to send the ssh client to the background job queue of your local shell, then fg as normal to get it back.\nEdit: When dealing with nested SSH sessions, you can add multiple tilde characters to only break out of one of the SSH sessions in the chain, but retain the others. For example, if you're nested in 3 levels, (i.e. you ssh from local->Machine1->Machine2->Machine3), Enter~. will get you back to your local session, Enter~~. will leave you in Machine1, and Enter~~~. will leave you in Machine2. This works for other escape sequences as well, such as moving the ssh session to background temporarily. The above works for any level of nesting, by just adding more tilde's. \nFinally, you can use Enter~? to print a help menu of available escape commands. \nTL;DR - the supported escape commands are\nSupported escape sequences:\n ~.   - terminate connection (and any multiplexed sessions)\n ~B   - send a BREAK to the remote system\n ~C   - open a command line\n ~R   - request rekey\n ~V/v - decrease/increase verbosity (LogLevel)\n ~^Z  - suspend ssh\n ~#   - list forwarded connections\n ~&   - background ssh (when waiting for connections to terminate)\n ~?   - this message\n ~~   - send the escape character by typing it twice\n(Note that escapes are only recognized immediately after newline.)\n",
        "url": "https://serverfault.com/questions/283129/why-do-consoles-sometimes-hang-forever-when-ssh-connection-breaks"
    },
    {
        "title": "Make nginx to pass hostname of the upstream when reverseproxying",
        "question": "I run several docker containers with hostnames:\nweb1.local\nweb2.local\nweb3.local\nRouting to these done based on hostname by nginx. I have a proxy in front of this setup (on different machine connected to internet) where I define upstream as:\n    upstream main {\n      server web1.local:80;\n      server web2.local:80;\n      server web3.local:80;\n    }\n\nAnd actual virtual host description:\n    server {\n      listen 80;\n      server_name example.com;\n      location / {\n        proxy_pass http://main;\n      }\n    }\n\nNow, because containers receive hostname \"main\" instead of \"web1.local\", they do not respond properly to the request. \nQuestion: how I can tell nginx to pass name of the upstream server instead of name of upstream group of servers in Host: header when proxying request?",
        "top_answer": "Actually you can do that via proxy_set_header. \nFor more details look here: http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header or see an example use-case here: https://stackoverflow.com/questions/12847771/configure-nginx-with-proxy-pass\nI have included the dynamic approach into your above posted configuration:\nserver {\n  listen 80;\n  server_name example.com;\n  location / {\n    proxy_pass       http://main;\n    proxy_set_header Host            $host;\n    proxy_set_header X-Forwarded-For $remote_addr;\n  }\n}\n\nHere is an example with a static host name:\nserver {\n  listen 80;\n  server_name example.com;\n  location / {\n    proxy_pass       http://main;\n    proxy_set_header Host            www.example.com;\n    proxy_set_header X-Forwarded-For $remote_addr;\n  }\n}\n",
        "url": "https://serverfault.com/questions/598202/make-nginx-to-pass-hostname-of-the-upstream-when-reverseproxying"
    },
    {
        "title": "Are networks now faster than disks?",
        "question": "This is a software design question\nI used to work on the following rule for speed\ncache memory > memory > disk > network\n\nWith each step being 5-10 times the previous step (e.g. cache memory is 10 times faster than main memory).\nNow, it seems that gigabit ethernet has latency less than local disk. So, maybe operations to read out of a large remote in-memory DB are faster than local disk reads. This feels like heresy to an old timer like me. (I just spent some time building a local cache on disk to avoid having to do network round trips - hence my question)\nDoes anybody have any experience / numbers / advice in this area?\nAnd yes I know that the only real way to find out is to build and measure, but I was wondering about the general rule.\nedit:\nThis is the interesting data from the top answer:\n\nRound trip within same datacenter 500,000 ns \nDisk seek 10,000,000 ns\n\nThis is a shock for me; my mental model is that a network round trip is inherently slow. And its not - its 10x faster than a disk 'round trip'.\nJeff attwood posted this v good blog on the topic http://blog.codinghorror.com/the-infinite-space-between-words/",
        "top_answer": "Here are some numbers that you are probably looking for, as quoted by Jeff Dean, a Google Fellow:\n\n\n\nNumbers Everyone Should Know\nL1 cache reference                             0.5 ns\nBranch mispredict                              5 ns\nL2 cache reference                             7 ns\nMutex lock/unlock                            100 ns (25)\nMain memory reference                        100 ns\nCompress 1K bytes with Zippy              10,000 ns (3,000)\nSend 2K bytes over 1 Gbps network         20,000 ns\nRead 1 MB sequentially from memory       250,000 ns\nRound trip within same datacenter        500,000 ns\nDisk seek                             10,000,000 ns\nRead 1 MB sequentially from network   10,000,000 ns\nRead 1 MB sequentially from disk      30,000,000 ns (20,000,000)\nSend packet CA->Netherlands->CA      150,000,000 ns\n\n\n\n\nIt's from his presentation titled Designs, Lessons and Advice from Building Large Distributed Systems and you can get it here:\n\nDr Jeff Dean Keynote PDF or on slideshare.net\n\nThe talk was given at Large-Scale Distributed Systems and Middleware (LADIS) 2009.\nOther Info\n\nGoogle Pro Tip: Use Back-Of-The-Envelope-Calculations To Choose The Best Design\nStanford 295 Talk Software Engineering Advice from Building Large-Scale Distributed Systems\n\n\nIt's said that gcc -O4 emails your code to Jeff Dean for a rewrite.\n",
        "url": "https://serverfault.com/questions/238417/are-networks-now-faster-than-disks"
    },
    {
        "title": "SSH Suddenly returning Invalid format",
        "question": "So a while ago I set up a server on AWS, and used their generated SSH key. I saved the key to Lastpass, and have successfully retrieved it from there before, and got it working. However, after trying that again today, I can't get it to work.\n-rw-------  1 itsgreg users 1674 Jun  6 12:51 key_name\nI've tried ssh -i key_name, ssh-keygen -f key_name, but nothing works, I always get this error message:\nLoad key \"key_name\": invalid format\nIs there any way to fix this?",
        "top_answer": "Check the contents of key_name, if the agent says invalid format, then there's something wrong with the key - like .. are you sure that's the correct key? Even if it's not the private key you need, the ssh agent won't return invalid format if the key is working, you simply won't be able to connect. You might have placed your public key in there, for some reason. Check it!\nOne common issue is if the file does not end with a newline. Add a newline, save, and try again.",
        "url": "https://serverfault.com/questions/854208/ssh-suddenly-returning-invalid-format"
    },
    {
        "title": "How do I rename a task in Task Scheduled on Windows Server 2008 R2",
        "question": "I have some tasks in the Task Scheduler on Windows 2008 R2. I created them as the admin and I'm logged in as the admin. I have no easy way to rename the tasks. The only way I can is to export the task config to a XML file and re-import to a new task, change the name there, then delete the old task. Is there an easier way?",
        "top_answer": "EDIT: After well over a decade, this answer of mine has a better answer down below that I will gladly defer to: https://serverfault.com/a/1159533/9770",
        "url": "https://serverfault.com/questions/219930/how-do-i-rename-a-task-in-task-scheduled-on-windows-server-2008-r2"
    },
    {
        "title": "In a PowerShell script, how can I check if I'm running with administrator privileges?",
        "question": "In a PowerShell script, how can I check if I'm running with administrator privileges?",
        "top_answer": "$currentPrincipal = New-Object Security.Principal.WindowsPrincipal([Security.Principal.WindowsIdentity]::GetCurrent())\n$currentPrincipal.IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)\n\n(from Command line safety tricks)",
        "url": "https://serverfault.com/questions/95431/in-a-powershell-script-how-can-i-check-if-im-running-with-administrator-privil"
    },
    {
        "title": "best way to clear all iptables rules",
        "question": "I currently have this snippet:\n# flush all chains\niptables -F\niptables -t nat -F\niptables -t mangle -F\n# delete all chains\niptables -X\n\nIs there a possibility that some impervious rule will stay alive after running this?\nThe idea is to have a completely clean iptables config, that can be easily replaced by new ruleset (nevermind routes/ifconfig's parameters).",
        "top_answer": "To answer your question succinctly, no: there would not be any \"leftover\" rules after flushing every table. In the interest of being thorough however, you may want to set the policy for the built-in INPUT and FORWARD chains to ACCEPT, as well:\niptables -P INPUT ACCEPT\niptables -P FORWARD ACCEPT\niptables -P OUTPUT ACCEPT\niptables -t nat -F\niptables -t mangle -F\niptables -F\niptables -X\n\nClear ip6tables rules:\nip6tables -P INPUT ACCEPT\nip6tables -P FORWARD ACCEPT\nip6tables -P OUTPUT ACCEPT\nip6tables -t nat -F\nip6tables -t mangle -F\nip6tables -F\nip6tables -X\n\n...and that should do it. iptables -nvL should produce this (or very similar) output:\nChain INPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n",
        "url": "https://serverfault.com/questions/200635/best-way-to-clear-all-iptables-rules"
    },
    {
        "title": "Wildcard SSL certificate for second-level subdomain",
        "question": "I'd like to know if any certificates support a double wildcard like *.*.example.com? I've just been on the phone with my current SSL provider (register.com) and the girl there said they don't offer anything like that and that she didn't think it was possible anyway. \nCan anyone tell me if this is possible, and if browsers support this?",
        "top_answer": "All answers here are outdated or not fully correct, not considering the RFC 6125 from 2011.\nAccording to the RFC 6125, only a single wildcard is allowed in the most left fragment.\nValid:\n*.sub.domain.tld\n*.domain.tld\n\nInvalid:\nsub.*.domain.tld\n*.*.domain.tld\ndomain.*\n*.tld\nsub.*.*\n\nA fragment, or also called \"label\", is a closed component, e.g.: *.com (2 labels) does not match label.label.com (3 labels) - this has already been defined in RFC 2818.\nBefore 2011 in RFC 2818 the setting was not fully clear:\n\nSpecifications for existing application technologies are not clear\nor consistent about the allowable location of the wildcard\ncharacter.\n\nThis has changed with RFC 6125 from 2011 (6.4.3):\n\nThe client SHOULD NOT attempt to match a presented identifier in\nwhich the wildcard character comprises a label other than the\nleft-most label (e.g., do not match bar.*.example.net).\n",
        "url": "https://serverfault.com/questions/104160/wildcard-ssl-certificate-for-second-level-subdomain"
    },
    {
        "title": "How to set up Nginx as a caching reverse proxy?",
        "question": "I heard recently that Nginx has added caching to its reverse proxy feature. I looked around but couldn't find much info about it.\nI want to set up Nginx as a caching reverse proxy in front of Apache/Django: to have Nginx proxy requests for some (but not all) dynamic pages to Apache, then cache the generated pages and serve subsequent requests for those pages from cache.\nIdeally I'd want to invalidate cache in 2 ways:\n\nSet an expiration date on the cached item\nTo explicitly invalidate the cached item. E.g. if my Django backend has updated certain data, I'd want to tell Nginx to invalidate the cache of the affected pages\n\nIs it possible to set Nginx to do that? How?",
        "top_answer": "I don't think that there is a way to explicitly invalidate cached items, but here is an example of how to do the rest. Update: As mentioned by Piotr in another answer, there is a cache purge module that you can use. You can also force a refresh of a cached item using nginx's proxy_cache_bypass - see Cherian's answer for more information.\nIn this configuration, items that aren't cached will be retrieved from example.net and stored. The cached versions will be served up to future clients until they are no longer valid (60 minutes).\nYour Cache-Control and Expires HTTP headers will be honored, so if you want to explicitly set an expiration date, you can do that by setting the correct headers in whatever you are proxying to.\nThere are lots of parameters that you can tune - see the nginx Proxy module documentation for more information about all of this including details on the meaning of the different settings/parameters:\nhttp://nginx.org/r/proxy_cache_path\nhttp {\n  proxy_cache_path  /var/www/cache levels=1:2 keys_zone=my-cache:8m max_size=1000m inactive=600m;\n  proxy_temp_path /var/www/cache/tmp; \n\n\n  server {\n    location / {\n      proxy_pass http://example.net;\n      proxy_cache my-cache;\n      proxy_cache_valid  200 302  60m;\n      proxy_cache_valid  404      1m;\n    }\n  }\n}\n",
        "url": "https://serverfault.com/questions/30705/how-to-set-up-nginx-as-a-caching-reverse-proxy"
    },
    {
        "title": "Is it possible to make Nginx listen to different ports?",
        "question": "I created one Nginx with one Linux Azure VM, is it possible to make nginx listen to different ports so that when I change the port number, the content would be different. I found there would be a collision if I created two or more ports related to HTTP on VM. Can anyone help me with that?",
        "top_answer": "Yes, it is.\nWhat you probably want is multiple \"server\" stanzas, each with a different port, but possibly (probably?) the same server_name, serving the \"different\" content appropriately within each one, maybe with a different document root in each server.\nFull documentation is here:\nhttp://nginx.org/en/docs/http/server_names.html\nExample:\nserver {\n    listen       80;\n    server_name  example.org  www.example.org;\n    root         /var/www/port80/\n}\n\nserver {\n    listen       81;\n    server_name  example.org  www.example.org;\n    root         /var/www/port81/\n}\n",
        "url": "https://serverfault.com/questions/655067/is-it-possible-to-make-nginx-listen-to-different-ports"
    },
    {
        "title": "What version of RHEL am I using?",
        "question": "From the shell and without root privileges, how can I determine what Red Hat Enterprise Linux version I'm running?\nIdeally, I'd like to get both the major and minor release version, for example RHEL 4.0 or RHEL 5.1, etc.",
        "top_answer": "You can use the lsb_release command on various Linux distributions:\nlsb_release -i -r \n\nThis will tell you the Distribution and Version and is a little bit more accurate than accessing files that may or may not have been modified by the admin or a software package. As well as working across multiple distros.\nFor RHEL, you should use:\ncat /etc/redhat-release\n",
        "url": "https://serverfault.com/questions/89654/what-version-of-rhel-am-i-using"
    },
    {
        "title": "How do VLANs work?",
        "question": "What are VLANs? What problems do they solve?\nI'm helping a friend learn basic networking, as he's just become the sole sysadmin at a small company. I've been pointing him at various questions/answers on Serverfault relating to various networking topics, and noticed a gap - there doesn't appear to be an answer which explains from first principles what VLANs are. In the spirit of How does Subnetting Work, I thought it would be useful to have a question with a canonical answer here.\nSome potential topics to cover in an answer:\n\nWhat are VLANs?\nWhat problems were they intended to solve?\nHow did things work before VLANs?\nHow do VLANs relate to subnets?\nWhat are SVIs?\nWhat are trunk ports and access ports?\nWhat is VTP?\n\nEDIT: to be clear, I already know how VLANs work - I just think that Serverfault should have an answer that covers these questions. Time permitting, I'll be submitting my own answer as well.",
        "top_answer": "Virtual LANs (VLANs) are an abstraction to permit a single physical network to emulate the functionality of multiple parallel physical networks. This is handy because there may be situations where you need the functionality of multiple parallel physical networks but you'd rather not spend the money on buying parallel hardware. I'll be speaking about Ethernet VLANs in this answer (even though other networking technologies can support VLANs) and I won't be diving deeply into every nuance.\nA Contrived Example and a Problem\nAs a purely contrived example scenario, imagine you own an office building that you lease to tenants. As a benefit of the lease, each tenant will get live Ethernet jacks in each room of the office. You buy a Ethernet switch for each floor, wire them up to jacks in each office on that floor, and wire all the switches together.\nInitially, you lease space to two different tenants-- one on the floor 1 and one on 2. Each of these tenants configures their computers w/ static IPv4 addresses. Both tenants use different TCP/IP subnets and everything seems to work just fine.\nLater, a new tenant rents half of floor 3 and brings up one of these new-fangled DHCP servers. Time passes and the 1st floor tenant decides to jump on the DHCP bandwagon, too. This is the point when things start to go awry. The floor 3 tenants report that some of their computers are getting \"funny\" IP addresses from a machine that isn't their DHCP server. Soon, the floor 1 tenants report the same thing.\nDHCP is a protocol that takes advantage of the broadcast capability of Ethernet to allow client computers to obtain IP addresses dynamically. Because the tenants are all sharing the same physical Ethernet network they share the same broadcast domain. A broadcast packet sent from any computer in the network will flood out all the switch ports to every other computer. The DHCP servers on floors 1 and 3 will receive all requests for IP address leases and will, effectively, duel to see who can answer first. This is clearly not the behavior you intend your tenants to experience. This is the behavior, though, of a \"flat\" Ethernet network w/o any VLANs.\nWorse still, a tenant on floor 2 acquires this \"Wireshark\" software and reports that, from time to time, they see traffic coming out of their switch that references computers and IP addresses that they've never heard of. One of their employees has even figured out that he can communicate with these other computers by changing the IP address assigned to his PC from 192.168.1.38 to 192.168.0.38! Presumably, he's just a few short steps away from performing \"unauthorized pro-bono system administration services\" for one of the other tenants. Not good.\nPotential Solutions\nYou need a solution! You could just pull the plugs between the floors and that would cut off all unwanted communication! Yeah! That's the ticket...\nThat might work, except that you have a new tenant who will be renting half of the basement and the unoccupied half of floor 3. If there isn't a connection between the floor 3 switch and the basement switch the new tenant won't be able to get communication between their computers that will be spread around both of their floors. Pulling the plugs isn't the answer. Worse still, the new tenant is bringing yet another one of these DHCP servers!\nYou flirt with the idea of buying physically separate sets of Ethernet switches for each tenant, but seeing as how your building has 30 floors, any of which can be subdivided up to 4 ways, the potential rats nest of floor-to-floor cables between massive numbers of parallel Ethernet switches could be a nightmare, not to mention expensive. If only there was a way to make a single physical Ethernet network act like it was multiple physical Ethernet networks, each with its own broadcast domain.\nVLANs to the Rescue\nVLANs are an answer to this messy problem. VLANs permit you to subdivide an Ethernet switch into logically disparate virtual Ethernet switches. This allows a single Ethernet switch to act as though it's multiple physical Ethernet switches. In the case of your subdivided floor 3, for example, you could configure your 48 port switch such that the lower 24 ports are in a given VLAN (which we'll call VLAN 12) and the higher 24 ports are in a given VLAN (which we'll call VLAN 13). When you create the VLANs on your switch you'll have to assign them some type of VLAN name or number. The numbers I'm using here are mostly arbitrary, so don't worry about what specific numbers I choose.\nOnce you've divided the floor 3 switch into VLANs 12 and 13 you find that the new floor 3 tenant can plug in their DHCP server to one of the ports assigned to VLAN 13 and a PC plugged into a port assigned to VLAN 12 doesn't get an IP address from the new DHCP server. Excellent! Problem solved!\nOh, wait... how do we get that VLAN 13 data down to the basement?\nVLAN Communication Between Switches\nYour half-floor 3 and half-basement tenant would like to connect computers in the basement to their servers on floor 3. You could run a cable directly from one of the ports assigned to their VLAN in the floor 3 switch to the basement and life would be good, right?\nIn the early days of VLANs (pre-802.1Q standard) you might do just that. The entire basement switch would be, effectively, part of VLAN 13 (the VLAN you've opted to assign to the new tenant on floor 3 and the basement) because that basement switch would be \"fed\" by a port on floor 3 that's assigned to VLAN 13.\nThis solution would work until you rent the other half of the basement to your floor 1 tenant who also wants to have communication between their 1st floor and basement computers. You could split the basement switch using VLANs (into, say, VLANS 2 and 13) and run a cable from floor 1 to a port assigned to VLAN 2 in the basement, but you better judgement tells you that this could quickly become a rat's nest of cables (and is only going to get worse). Splitting switches using VLANs is good, but having to run multiple cables from other switches to ports which are members of different VLANs seems messy. Undoubtedly, if you had to divide the basement switch 4 ways between tenants who also had space on higher floors you'd use 4 ports on the basement switch just to terminate \"feeder\" cables from upstairs VLANs.\nIt should now be clear that some type of generalized method of moving traffic from multiple VLANs between switches on a single cable is needed. Just adding more cables between switches to support connections between different VLANs isn't a scalable strategy. Eventually, with enough VLANs, you'll be eating up all the ports on your switches with these inter-VLAN / inter-switch connections. What's needed is a way to carry the packets from multiple VLANs along a single connection-- a \"trunk\" connection between switches.\nUp to this point, all the switch ports we've talked about are called \"access\" ports. That is, these ports are dedicated to accessing a single VLAN. The devices plugged into these ports have no special configuration themselves. These devices don't \"know\" that any VLANs are present. Frames the client devices send are delivered to the switch which then takes care of making sure that the frame is only sent to ports assigned as members of the VLAN assigned to the port where the frame entered the switch. If a frame enters the switch on a port assigned as a member of VLAN 12 then the switch will only send that frame out ports that are members of VLAN 12. The switch \"knows\" the VLAN number assigned to a port from which it receives a frame and somehow knows to only deliver this frame out ports of the same VLAN.\nIf there were some way for a switch to share the VLAN number associated with a given frame to other switches then the other switch could properly handle delivering that frame only to the appropriate destination ports. This is what the 802.1Q VLAN tagging protocol does. (It's worth noting that, prior to 802.1Q, some vendors made up their own standards for VLAN tagging and inter-switch trunking. For the most part these pre-standard methods have all been supplanted by 802.1Q.)\nWhen you have two VLAN-aware switches connected to each other and you want those switches to deliver frames between each other to the proper VLAN you connect those switches using \"trunk\" ports. This involves changing the configuration of a port on each switch from \"access\" mode to \"trunk\" mode (in a very basic configuration).\nWhen a port is configured in trunk mode each frame that the switch sends out that port will have a \"VLAN tag\" included in the frame. This \"VLAN tag\" wasn't part of the original frame that the client sent. Rather, this tag is added by the sending switch prior to sending the frame out the trunk port. This tag denotes the VLAN number associated with the port from which the frame originated. \nThe receiving switch can look at the tag to determine which VLAN the frame originated from and, based on that information, forward the frame out only ports that are assigned to the originating VLAN. Because the devices connected to \"access\" ports aren't aware that VLANs are being used the \"tag\" information must be stripped from the frame before it's sent out a port configured in access mode. This stripping of the tag information causes the entire VLAN trunking process to be hidden from client devices since the frame they receive will not bear any VLAN tag information.\nBefore you configure VLANs in real life I'd recommend configuring a port for trunk mode on a test switch and monitoring the traffic being sent out that port using a sniffer (like Wireshark). You can create some sample traffic from another computer, plugged into an access port, and see that the frames leaving the trunk port will, in fact, be larger than the frames being send by your test computer. You'll see the VLAN tag information in the frames in Wireshark. I find that it's worth actually seeing what happens in a sniffer. Reading up on the 802.1Q tagging standard is also a decent thing to do at this point (especially since I'm not talking about things like \"native VLANs\" or double-tagging).\nVLAN Configuration Nightmares and the Solution\nAs you rent more and more space in your building the number of VLANs grows. Each time you add a new VLAN you find that you have to logon to increasingly more Ethernet switches and add that VLAN to the list. Wouldn't it be great if there were some method by which you could add that VLAN to a single configuration manifest and have it automatically populate the VLAN configuration of each switch?\nProtocols like Cisco's proprietary \"VLAN Trunking Protocol\" (VTP) or the standards-based \"Multiple VLAN Registration Protocol\" (MVRP-- previously spelled GVRP) fulfill this function. In a network using these protocols a single VLAN creation or deletion entry results in protocol messages being sent to all switches in the network. That protocol message communicates the change in VLAN configuration to the rest of the switches which, in turn, modify their VLAN configurations.  VTP and MVRP aren't concerned with which specific ports are configured as access ports for specific VLANs, but rather are useful in communicating the creation or deletion of VLANs to all the switches.\nWhen you've gotten comfortable with VLANs you'll probably want to go back and read about \"VLAN pruning\", which is associated with protocols like VTP and MVRP. For now it's nothing to be tremendously concerned with. (The VTP article on Wikipedia has a nice diagram that explains VLAN pruning and the benefits therewith.)\nWhen Do You Use VLANs In Real Life?\nBefore we go much further it's important to think about real life rather than contrived examples. In lieu of duplicating the text of another answer here I'll refer you to my answer re: when to create VLANs. It's not necessarily \"beginner-level\", but it's worth taking a look at now since I'm going to make reference to it briefly before moving back to a contrived example.\nFor the \"tl;dr\" crowd (who surely have all stopped reading at this point, anyway), the gist of that link above is: Create VLANs to make broadcast domains smaller or when you want to segregate traffic for some particular reason (security, policy, etc). There aren't really any other good reasons to use VLANs.\nIn our example we're using VLANs to limit broadcast domains (to keep protocols like DHCP working right) and, secondarily, because we want isolation between the various tenants' networks.\nAn Aside re: IP Subnets and VLANs\nGenerally speaking there is a typically a one-to-one relationship between VLANs and IP subnets as a matter of convenience, to facilitate isolation, and because of how the ARP protocol works.\nAs we saw at the beginning of this answer two different IP subnets can be used on the same physical Ethernet without issue. If you're using VLANs to shrink broadcast domains you won't want to share the same VLAN with two different IP subnets since you'll be combining their ARP and other broadcast traffic.\nIf you're using VLANs to segregate traffic for security or policy reasons then you also probably won't want to combine multiple subnets in the same VLAN since you'll be defeating the purpose of isolation.\nIP uses a broadcast-based protocol, Address Resolution Protocol (ARP), to map IP addresses onto physical (Ethernet MAC) addresses. Since ARP is broadcast based, assigning different parts of the same IP subnet to different VLANs would be problematic because hosts in one VLAN wouldn't be able to receive ARP replies from hosts in the other VLAN, since broadcasts aren't forwarded between VLANs. You could solve this \"problem\" by using proxy-ARP but, ultimately, unless you have a really good reason to need to split an IP subnet across multiple VLANs it's better not to do so.\nOne Last Aside: VLANs and Security\nFinally, it's worth noting that VLANs aren't a great security device. Many Ethernet switches have bugs that permit frames originating from one VLAN to be sent out ports assigned to another VLAN. Ethernet switch manufacturers have worked hard to fix these bugs, but it's doubtful that there will ever be a completely bug free implementation.\nIn the case of our contrived example the floor 2 employee who is moments away from providing free systems administration \"services\" to another tenant might be stopped from doing so by isolating his traffic into a VLAN. He might also figure out how to exploit bugs in the switch firmware, though, to allow his traffic to \"leak\" out onto another tenant's VLAN as well.\nMetro Ethernet providers are relying, increasingly, on VLAN tagging functionality and the isolation that switches provide. It's not fair to say that there's no security offered by using VLANs. It is fair to say, though, that in situations with untrusted Internet connections or DMZ networks it's probably better to use physically separate switches to carry this \"touchy\" traffic rather than VLANs on switches that also carry your trusted \"behind the firewall\" traffic.\nBringing Layer 3 into the Picture\nSo far everything this answer has talked about relates to layer 2-- Ethernet frames. What happens if we start bringing layer 3 into this?\nLet's go back to the contrived building example. You've embraced VLANs opted to configure each tenant's ports as members of separate VLANs. You've configured trunk ports such that each floor's switch can exchange frames tagged with the originating VLAN number to the switches on the floor above and below. One tenant can have computers spread across multiple floors but, because of your adept VLAN configuring skills, these physically distributed computers can all appear to be part of the same physical LAN.\nYou're so full of your IT accomplishments that you decide to start offering Internet connectivity to your tenants. You buy a fat Internet pipe and a router. You float the idea to all your tenants and two of them immediately buy-in. Luckily for you your router has three Ethernet ports. You connect one port to your fat Internet pipe, another port to a switch port assigned for access to the first tenant's VLAN, and the other to a port assigned for access to the second tenant's VLAN. You configure your router's ports with IP addresses in each tenant's network and the tenants start accessing the Internet through your service! Revenue increases and you're happy.\nSoon, though, another tenant decides to get onto your Internet offering. You're out of ports on your router, though. What to do?\nFortunately you bought a router that supports configuring \"virtual sub-interfaces\" on its Ethernet ports. In short this functionality allows the router to receive and interpret frames tagged with originating VLAN numbers, and to have virtual (that is, non-physical) interfaces configured with IP addresses appropriate for each VLAN it will communicate with. In effect this permits you to \"multiplex\" a single Ethernet port on the router such that it appears to function as multiple physical Ethernet ports. \nYou attach your router to a trunk port on one of your switches and configure virtual sub-interfaces corresponding to each tenant's IP addressing scheme. Each virtual sub-interface is configured with the VLAN number assigned to each Customer. When a frame leaves the trunk port on the switch, bound for the router, it will carry a tag with the originating VLAN number (since it's a trunk port). The router will interpret this tag and treat the packet as though it arrived on a dedicated physical interface corresponding to that VLAN. Likewise, when the router sends a frame to the switch in response to a request it will add a VLAN tag to the frame such that the switch knows to which VLAN the response frame should be delivered. In effect, you've configured the router to \"appear\" as a physical device in multiple VLANs while only using a single physical connection between the switch and the router.\nRouters on Sticks and Layer 3 Switches\nUsing virtual sub-interfaces you've been able to sell Internet connectivity to all your tenants without having to buy a router that has 25+ Ethernet interfaces. You're fairly happy with your IT accomplishments so you respond positively when two of your tenants come to you with a new request.\nThese tenants have opted to \"partner\" on a project and they want to allow access from client computers in one tenant's office (one given VLAN) to a server computer in the other tenant's office (another VLAN). Since they're both Customers of your Internet service it's a fairly simple change of an ACL in your core Internet router (on which there is a virtual sub-interface configured for each of these tenant's VLANs) to allow traffic to flow between their VLANs as well as to the Internet from their VLANs. You make the change and send the tenants on their way.\nThe next day you receive complaints from both tenants that access between the client computers in one office to the server in the second office is very slow. The server and client computers both have gigabit Ethernet connections to your switches but the files only transfer at around 45Mbps which, coincidentally, is roughly half of the speed with which your core router connects to its switch. Clearly the traffic flowing from the source VLAN to the router and back out from the router to the destination VLAN is being bottlenecked by the router's connection to the switch.\nWhat you've done with your core router, allowing it to route traffic between VLANs, is commonly known as \"router on a stick\" (an arguably stupidly whimsical euphemism). This strategy can work well, but traffic can only flow between the VLANs up to the capacity of the router's connection to the switch. If, somehow, the router could be conjoined with the \"guts\" of the Ethernet switch itself it could route traffic even faster (since the Ethernet switch itself, per the manufacturer's spec sheet, is capable of switching over 2Gbps of traffic).\nA \"layer 3 switch\" is an Ethernet switch that, logically speaking, contains a router buried inside itself. I find it tremendously helpful to think of a layer 3 switch as having a tiny and fast router hiding inside the switch. Further, I would advise you to think about the routing functionality as a distinctly separate function from the Ethernet switching function that the layer 3 switch provides. A layer 3 switch is, for all intents and purposes, two distinct devices wrapped up in a single chassis. \nThe embedded router in a layer 3 switch is connected to the switch's internal switching fabric at a speed that, typically, allows for routing of packets between VLANs at or near wire-speed. Analogously to the virtual sub-interfaces you configured on your \"router on a stick\" this embedded router inside the layer 3 switch can be configured with virtual interfaces that \"appear\" to be \"access\" connections into each VLAN. Rather than being called virtual sub-interfaces these logical connections from the VLANs into the embedded router inside a layer 3 switch are called Switch Virtual Interfaces (SVIs). In effect, the embedded router inside a layer 3 switch has some quantity of \"virtual ports\" that can be \"plugged in\" to any of the VLANs on the switch.\nThe embedded router performs the same way as a physical router except that it typically doesn't have all of the same dynamic routing protocol or access-control list (ACL) features as a physical router (unless you've bought a really nice layer 3 switch). The embedded router has the advantage, however, of being very fast and not having a bottleneck associated with a physical switch port that it's plugged into. \nIn the case of our example here with the \"partnering\" tenants you might opt to obtain a layer 3 switch, plug it into trunk ports such that traffic from both Customers VLANs reaches it, then configure SVIs with IP addresses and VLAN memberships such that it \"appears\" in both Customers VLANs. Once you've done that it's just a matter of tweaking the routing table on your core router and the embedded router in the layer 3 switch such that traffic flowing between the tenants' VLANs is routed by the embedded router inside the layer 3 switch versus the \"router on a stick\". \nUsing a layer 3 switch doesn't mean that there still won't be bottlenecks associated with the bandwidth of the trunk ports that interconnect your switches. This is an orthogonal concern to those that VLANs address, though. VLANs have nothing to do with bandwidth problems. Typically bandwidth problems are solved by either obtaining higher-speed inter-switch connections or using link-aggregation protocols to \"bond\" several lower-speed connections together into a virtual higher-speed connection. Unless all the devices creating frames to be routed by the embedded router inside the later 3 switch are, themselves, plugged into ports directly on the layer 3 switch you still need to worry about the bandwidth of the trunks between the switches. A layer 3 switch isn't a panacea, but it's typically faster than a \"router on a stick\".\nDynamic VLANs\nLastly, there is a function in some switches to provide dynamic VLAN membership. Rather than assigning a given port to be an access port for a given VLAN the port's configuration (access or trunk, and for which VLANs) can be altered dynamically when a device is connected. Dynamic VLANs are a more advanced topic but knowing that the functionality exists can be helpful.\nThe functionality varies between vendors but typically you can configure dynamic VLAN membership based on the MAC address of the connected device, 802.1X authentication status of the device, proprietary and standards-based protocols (CDP and LLDP, for example, to allow IP phones to \"discover\" the VLAN number for voice traffic), IP subnet assigned to the client device, or Ethernet protocol type. ",
        "url": "https://serverfault.com/questions/188350/how-do-vlans-work"
    },
    {
        "title": "Show full process name in top",
        "question": "I'm running a Rails stack on Ubuntu.\nWhen I call ps -AF, I get a descriptive process name set by the Apache module like\n00:00:43 Rails: /var/www...\n\nwhich is really helpful in diagnosing load issues.\nBut when I call top, the same process shows up simply as\nruby\nIs there any way to get the ps -AF process name in top?",
        "top_answer": "While top is running, you can press c to toggle between showing the process name and the command line. To remember the toggle state for next time, press W to save the current configuration to ~/.toprc.",
        "url": "https://serverfault.com/questions/139632/show-full-process-name-in-top"
    },
    {
        "title": "In Windows, using the command line, how do you check if a remote port is open?",
        "question": "What is a simple way in Windows to test if traffic gets through to a specific port on a remote machine?",
        "top_answer": "I found a hiddem gem the other day from Microsoft that is designed for testing ports:\nPortqry.exe\n\"Portqry.exe is a command-line utility that you can use to help troubleshoot TCP/IP connectivity issues. Portqry.exe runs on Windows 2000-based computers, on Windows XP-based computers, and on Windows Server 2003-based computers. The utility reports the port status of TCP and UDP ports on a computer that you select. \"",
        "url": "https://serverfault.com/questions/35218/in-windows-using-the-command-line-how-do-you-check-if-a-remote-port-is-open"
    },
    {
        "title": "Is there an equivalent of MySQL's SHOW CREATE TABLE in Postgres?",
        "question": "Is there an equivalent of MySQL's SHOW CREATE TABLE in Postgres? Is this possible? If not what is the next best solution?\nI need the statement because I use it to create the table on an remote server (over WCF).",
        "top_answer": "You can try to trace in the PostgreSQL log file what pg_dump --table table --schema-only really does. Then you can use the same method to write  your own sql function.",
        "url": "https://serverfault.com/questions/231952/is-there-an-equivalent-of-mysqls-show-create-table-in-postgres"
    },
    {
        "title": "connect() failed (111: Connection refused) while connecting to upstream",
        "question": "I'm experiencing 502 Gateway errors when accessing a PHP file in a directory (http://example.com/dev/index.php). The logs simply says this:\n2011/09/30 23:47:54 [error] 31160#0: *35 connect() failed (111: Connection refused) while connecting to upstream, client: xx.xx.xx.xx, server: domain.com, request: \"GET /dev/ HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"domain.com\"\n\nI've never experienced this before. What is the solution for this type of 502 Gateway error?\nThis is the nginx.conf:\nuser www-data;\nworker_processes 4;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 768;\n    # multi_accept on;\n}\n\nhttp {\n\n    ##\n    # Basic Settings\n    ##\n\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    # server_tokens off;\n\n    # server_names_hash_bucket_size 64;\n    # server_name_in_redirect off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    ##\n    # Logging Settings\n    ##\n\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    ##\n    # Gzip Settings\n    ##\n\n    gzip on;\n    gzip_disable \"msie6\";\n\n    # gzip_vary on;\n    # gzip_proxied any;\n    # gzip_comp_level 6;\n    # gzip_buffers 16 8k;\n    # gzip_http_version 1.1;\n    # gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;\n\n    ##\n    # Virtual Host Configs\n    ##\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;\n}\n",
        "top_answer": "It sounds like you haven't started and configured the backend for Nginx. Start php-fpm and add the following to nginx.conf, in the http context:\nserver {\n    listen 127.0.0.1;\n    server_name localhost;\n\n    error_log /var/log/nginx/localhost.error_log info;\n\n    root /var/www/localhost/htdocs;\n\n    location ~ \\.php$ {\n        fastcgi_pass 127.0.0.1:9000;\n        include /etc/nginx/conf.d/*.conf;\n        include /etc/nginx/sites-enabled/*;\n\n        fastcgi_intercept_errors        on;\n        error_page 404 /error/404.php;\n    }\n}\n",
        "url": "https://serverfault.com/questions/317393/connect-failed-111-connection-refused-while-connecting-to-upstream"
    },
    {
        "title": "What is the difference between authentication and authorization?",
        "question": "Basic question from a novice:\nWhat is the difference between authentication and authorization?",
        "top_answer": "\nAuthentication is the process of verifying who you are. When you log on to a PC with a user name and password you are authenticating.\nAuthorization is the process of verifying that you have access to something. Gaining access to a resource (e.g. directory on a hard disk) because the permissions configured on it allow you access is authorization.\n",
        "url": "https://serverfault.com/questions/57077/what-is-the-difference-between-authentication-and-authorization"
    },
    {
        "title": "ssh-keygen does not create RSA private key",
        "question": "I'm trying to create a private key and having an issue.\nWhen I use ssh-keygen -t rsa -b 4096 -C \"[email\u00a0protected]\", I get a private key in the following format.\n-----BEGIN OPENSSH PRIVATE KEY-----\nuTo43HGophPo5awKC8hoOz4KseENpgHDLxe5UX+amx8YrWvZCvsYRh4/wnwxijYx\n...\n-----END OPENSSH PRIVATE KEY-----\n\nAnd this is not being accepted for an application that I'm trying to use.\nI'm expecting a key in the following RSA format.\n-----BEGIN RSA PRIVATE KEY-----\nProc-Type: 4,ENCRYPTED\nDEK-Info: AES-128-CBC,25737CC2C70BFABADB1B4598BD8AB9E9\n\nuTo43HGophPo5awKC8hoOz4KseENpgHDLxe5UX+amx8YrWvZCvsYRh4/wnwxijYx\n...\n-----END RSA PRIVATE KEY-----\n\nHow do I create the correct format? This is weird because every other mac I have creates the correct format, except the one I'm having problem with.\nI'm on a fresh installed Mac OS Mojave",
        "top_answer": "I faced the same problem recently (after upgrade to mojave 10.14.1), here are 2 possible solutions for this issue.\n\nDowngrade your ssh-keygen binary (you can easily get old version from any linux/docker image)\n\nOR\n\nAdd option -m PEM into your ssh-keygen command. For example, you can run ssh-keygen -m PEM -t rsa -b 4096 -C \"[email\u00a0protected]\" to force ssh-keygen to export as PEM format. \n\nIt seems like in the current ssh-keygen version in mojave, the default export format is RFC4716 as mentioned here",
        "url": "https://serverfault.com/questions/939909/ssh-keygen-does-not-create-rsa-private-key"
    },
    {
        "title": "How can I port forward with iptables?",
        "question": "I want connections coming in on ppp0 on port 8001 to be routed to 192.168.1.200 on eth0 on port 8080.\nI've got these two rules\n-A PREROUTING  -p tcp -m tcp --dport 8001 -j DNAT --to-destination 192.168.1.200:8080\n\n-A FORWARD -m state -p tcp -d 192.168.1.200 --dport 8080 --state NEW,ESTABLISHED,RELATED -j ACCEPT\n\nand it doesn't work. What am I missing?",
        "top_answer": "First of all - you should check if forwarding is allowed at all:\ncat /proc/sys/net/ipv4/conf/ppp0/forwarding \ncat /proc/sys/net/ipv4/conf/eth0/forwarding \n\nIf both returns 1 it's ok. If not do the following:\necho '1' | sudo tee /proc/sys/net/ipv4/conf/ppp0/forwarding\necho '1' | sudo tee /proc/sys/net/ipv4/conf/eth0/forwarding\n\nSecond thing - DNAT could be applied on nat table only. So, your rule should be extended by adding table specification as well (-t nat):\niptables -t nat -A PREROUTING -p tcp -i ppp0 --dport 8001 -j DNAT --to-destination 192.168.1.200:8080\niptables -A FORWARD -p tcp -d 192.168.1.200 --dport 8080 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT\n\nBoth rules are applied only to TCP traffic (if you want to alter UDP as well, you need to provide similar rules but with -p udp option set).\nLast, but not least is routing configuration. Type:\nip route\n\nand check if 192.168.1.0/24 is among returned routing entries.",
        "url": "https://serverfault.com/questions/140622/how-can-i-port-forward-with-iptables"
    },
    {
        "title": "What is Active Directory Domain Services and how does it work?",
        "question": "\nThis is a Canonical Question about Active Directory Domain Services (AD DS).\n\nWhat is Active Directory? What does it do and how does it work?\nHow is Active Directory organized: Forest, Child Domain, Tree, Site, or OU\n\nI find myself explaining some of what I assume is common knowledge about it almost daily. This question will, hopefully, serve as a canonical question and answer for most basic Active Directory questions. If you feel that you can improve the answer to this question, please edit away.",
        "top_answer": "What is Active Directory?\nActive Directory Domain Services is Microsoft's Directory Server. It provides authentication and authorization mechanisms as well as a framework within which other related services can be deployed (AD Certificate Services, AD Federated Services, etc). It is an LDAP compliant database that contains objects. The most commonly used objects are users, computers, and groups. These objects can be organized into organizational units (OUs) by any number of logical or business needs. Group Policy Objects (GPOs) can then be linked to OUs to centralize the settings for various users or computers across an organization.\nWhen people say \"Active Directory\" they typically are referring to \"Active Directory Domain Services.\" It is important to note that there are other Active Directory roles/products such as Certificate Services, Federation Services, Lightweight Directory Services, Rights Management Services, etc. This answer refers specifically to Active Directory Domain Services.\nWhat is a domain and what is a forest?\nA forest is a security boundary. Objects in separate forests are not able to interact with each other, unless the administrators of each separate forest create a trust between them. For example, an Enterprise Administrator account for domain1.com, which is normally the most privileged account of a forest, will have, no permissions at all in a second forest named domain2.com, even if those forests exist within the same LAN, unless there is a trust in place.\nIf you have multiple disjoint business units or have the need for separate security boundaries, you need multiple forests.\nA domain is a management boundary. Domains are part of a forest. The first domain in a forest is known as the forest root domain. In many small and medium organizations (and even some large ones), you will only find a single domain in a single forest. The forest root domain defines the default namespace for the forest. For example, if the first domain in a new forest is named domain1.com, then that is the forest root domain. If you have a business need for a child domain, for example - a branch office in Chicago, you might name the child domain chi. The FQDN of the child domain would be chi.domain1.com. You can see that the child domain's name was prepended forest root domain's name. This is typically how it works. You can have disjoint namespaces in the same forest, but that's a whole separate can of worms for a different time.\nIn most cases, you'll want to try and do everything possible to have a single AD domain. It simplifies management, and modern versions of AD make it very easy to delegate control based on OU, which lessens the need for child domains.\nI can name my domain whatever I want, right?\nNot really. dcpromo.exe, the tool that handles the promotion of a server to a DC isn't idiot-proof. It does let you make bad decisions with your naming, so pay attention to this section if you are unsure. (Edit: dcpromo is deprecated in Server 2012. Use the Install-ADDSForest PowerShell cmdlet or install AD DS from Server Manager.)\nFirst of all, don't use made up TLDs like .local, .lan, .corp, or any of that other crap. Those TLDs are not reserved. ICANN is selling TLDs now, so your mycompany.corp that you're using today could actually belong to someone tomorrow. If you own mycompany.com, then the smart thing to do is use something like internal.mycompany.com or ad.mycompany.com for your internal AD name. If you use mycompany.com as an externally resolvable website, you should avoid using that as your internal AD name as well, since you'll end up with a split-brain DNS.\nDomain Controllers and Global Catalogs\nA server that responds to authentication or authorization requests is a Domain Controller (DC). In most cases, a Domain Controller will hold a copy of the Global Catalog. A Global Catalog (GC) is a partial set of objects in all domains in a forest. It is directly searchable, which means that cross-domain queries can usually be performed on a GC without needing a referral to a DC in the target domain. If a DC is queried on port 3268 (3269 if using SSL), then the GC is being queried. If port 389 (636 if using SSL) is queried, then a standard LDAP query is being used and objects existing in other domains may require a referral.\nWhen a user tries to log in to a computer that is joined to AD using their AD credentials, the salted and hashed username and password combination are sent to the DC for both the user account and the computer account that are logging in. Yes, the computer logs in too. This is important, because if something happens to the computer account in AD, like someone resets the account or deletes it, you may get an error that say that a trust relationship doesn't exist between the computer and the domain. Even though your network credentials are fine, the computer is no longer trusted to log into the domain.\nDomain Controller Availability Concerns\nI hear \"I have a Primary Domain Controller (PDC) and want to install a Backup Domain Controller (BDC)\" much more frequently that I would like to believe. The concept of PDCs and BDCs died with Windows NT4. The last bastion for PDCs was in a Windows 2000 transitional mixed mode AD when you still had NT4 DCs around. Basically, unless you're supporting a 15+ year old\ninstall that has never been upgraded, you really don't have a PDC or a BDC, you just have two domain controllers.\nMultiple DCs are capable of answering authentication requests from different users and computers simultaneously. If one fails, then the others will continue to offer authentication services without having to make one \"primary\" like you would have had to do in the NT4 days. It is best practice to have at least two DCs per domain. These DCs should both hold a copy of the GC and should both be DNS servers that hold a copy of the Active Directory Integrated DNS zones for your domain as well.\nFSMO Roles\n\n\"So, if there are no PDCs, why is there a PDC role that only a single DC can have?\"\n\nI hear this a lot. There is a PDC Emulator role. It's different than being a PDC. In fact, there are 5 Flexible Single Master Operations roles (FSMO). These are also called Operations Master roles as well. The two terms are interchangeable. What are they and what do they do? Good question! The 5 roles and their function are:\nDomain Naming Master - There is only one Domain Naming Master per forest. The Domain Naming Master makes sure that when a new domain is added to a forest that it is unique. If the server holding this role is offline, you won't be able to make changes to the AD namespace, which includes things like adding new child domains.\nSchema Master - There is only one Schema Operations Master in a forest. It is responsible for updating the Active Directory Schema. Tasks that require this, such as preparing AD for a new version of Windows Server functioning as a DC or the installation of Exchange, require Schema modifications. These modifications must be done from the Schema Master.\nInfrastructure Master - There is one Infrastructure Master per domain. If you only have a single domain in your forest, you don't really need to worry about it. If you have multiple forests, then you should make sure that this role is not held by a server that is also a GC holder unless every DC in the forest is a GC. The infrastructure master is responsible for making sure that cross-domain references are handled properly. If a user in one domain is added to a group in another domain, the infrastructure master for the domains in question make sure that it is handled properly. This role will not function correctly if it is on a global catalog.\nRID Master - The Relative ID Master (RID Master) is responsible for issuing RID pools to DCs. There is one RID master per domain. Any object in an AD domain has a unique Security Identifier (SID). This is made up of a combination of the domain identifier and a relative identifier. Every object in a given domain has the same domain identifier, so the relative identifier is what makes objects unique. Each DC has a pool of relative IDs to use, so when that DC creates a new object, it appends a RID that it hasn't used yet. Since DCs are issued non-overlapping pools, each RID should remain unique for the duration of the life of the domain. When a DC gets to ~100 RIDs left in its pool, it requests a new pool from the RID master. If the RID master is offline for an extended period of time, object creation may fail.\nPDC Emulator - Finally, we get to the most widely misunderstood role of them all, the PDC Emulator role. There is one PDC Emulator per domain. If there is a failed authentication attempt, it is forwarded to the PDC Emulator. The PDC Emulator functions as the \"tie-breaker\" if a password was updated on one DC and hasn't yet replicated to the others. The PDC Emulator is also the server that controls time sync across the domain. All other DCs sync their time from the PDC Emulator. All clients sync their time from the DC that they logged in to. It's important that everything remain within 5 minutes of each other, otherwise Kerberos breaks and when that happens, everyone cries.\nThe important thing to remember is that the servers that these roles run on is not set in stone. It's usually trivial to move these roles around, so while some DCs do slightly more than others, if they go down for short periods of time, everything will usually function normally. If they're down for a long time, it's easy to transparently transfer the roles. It's much nicer than the NT4 PDC/BDC days, so please stop calling your DCs by those old names. :)\nSo, um...how do the DCs share information if they can function independently of each other?\nReplication, of course. By default, DCs belonging to the same domain in the same site will replicate their data to each other at 15 second intervals. This makes sure that everything is relatively up to date.\nThere are some \"urgent\" events that trigger immediate replication. These events are: An account is locked out for too many failed logins, a change is made to the domain password or lockout policies, the LSA secret is changed, the password is changed on a DC's computer account, or the RID Master role is transferred to a new DC. Any of these events will trigger an immediate replication event.\nPassword changes fall somewhere between urgent and non-urgent and are handled uniquely. If a user's password is changed on DC01 and a user tries to log into a computer that is authenticating against DC02 before replication occurs, you'd expect this to fail, right? Fortunately that doesn't happen. Assume that there is also a third DC here called DC03 that holds the PDC Emulator role. When DC01 is updated with the user's new password, that change is immediately replicated to DC03 also. When thee authentication attempt on DC02 fails, DC02 then forwards that authentication attempt to DC03, which verifies that it is, indeed, good, and the logon is allowed.\nLet's talk about DNS\nDNS is critical to a properly functioning AD. The official Microsoft party line is that any DNS server can be used if it is set up properly. If you try and use BIND to host your AD zones, you're high. Seriously. Stick with using AD Integrated DNS zones and use conditional or global forwarders for other zones if you must. Your clients should all be configured to use your AD DNS servers, so it's important to have redundancy here. If you have two DCs, have them both run DNS and configure your clients to use both of them for name resolution.\nAlso, you're going to want to make sure that if you have more than one DC, that they don't list themselves first for DNS resolution. This can lead to a situation where they are on a \"replication island\" where they are disconnected from the rest of the AD replication topology and cannot recover. If you have two servers DC01 - 10.1.1.1 and DC02 - 10.1.1.2, then their DNS server list should be configured like this:\n\nServer: DC01 (10.1.1.1)\nPrimary DNS - 10.1.1.2\nSecondary DNS - 127.0.0.1\n\n\nServer: DC02 (10.1.1.2)\nPrimary DNS - 10.1.1.1\nSecondary DNS - 127.0.0.1\n\nOK, this seems complicated. Why do I want to use AD at all?\nBecause once you know what you're doing, you life becomes infinitely better. AD allows for the centralization of user and computer management, as well as the centralization of resource access and usage. Imagine a situation where you have 50 users in an office. If you wanted each user to have their own login to each computer, you'd have to configure 50 local user accounts on each PC. With AD, you only have to made the user account once and it can log into any PC on the domain by default. If you wanted to harden security, you'd have to do it 50 times. Sort of a nightmare, right? Also imagine that you have a file share that you only want half of those people to get to. If you're not using AD, you'd either need to replicate their username and passwords by hand on the server to give seemless access, or you'd have to make a shared account and give each user the username and password. One way means that you know (and have to constantly update) users' passwords. The other way means that you have no audit trail. Not good, right?\nYou also get the ability to use Group Policy when you have AD set up. Group Policy is a set of objects that are linked to OUs that define settings for users and/or computers in those OUs. For example, if you want to make it so that \"Shutdown\" isn't on the start menu for 500 lab PCs, you can do that in one setting in Group Policy. Instead of spending hours or days configuring the proper registry entries by hand, you create a Group Policy Object once, link it to the correct OU or OUs, and never have to think about it again. There are hundreds of GPOs that can be configured, and the flexibility of Group Policy is one of the major reasons that Microsoft is so dominant in the enterprise market.",
        "url": "https://serverfault.com/questions/402580/what-is-active-directory-domain-services-and-how-does-it-work"
    },
    {
        "title": "Job scheduling using crontab, what will happen when computer is shutdown during that time?",
        "question": "I schedule some tasks using crontab.\nWhat will happen if my computer is shutdown or turned off during the time when cron was scheduled to do something?\nDoes each missed cron job run after computer is turned on, or are missed jobs for that day ignored? If missed jobs don't resume, is there any way to configure cron such that it runs the missed tasks after the computer is turned back on?",
        "top_answer": "When your computer is shut down (or the cron daemon is otherwise not running), cron jobs will not be started.\nIf you have jobs that you would like to run after the fact during those times when the computer is shut down, use anacron. Installed by default, see \"man anacron\", \"man anacrontab\", or the file /etc/anacrontab for more info.\nUbuntu uses anacron by default for crontab entries in:\n/etc/cron.daily\n/etc/cron.weekly\n/etc/cron.monthly\n\nleaving the remaining crontabs to be handled by the main cron daemon, specifically:\n/etc/crontab\n/etc/cron.d\n/var/spool/cron\n\nNOTES\nAnacron itself does not run as a daemon, but relies on system startup scripts and cron itself to run.\nOn the Ubuntu 8.04 box I'm looking at, /etc/init.d/anacron is run at boot, and again by cron each morning at 07:30.\nThe README at /usr/share/doc/anacron/README.gz has a slight bit more info than is contained in the manpages.\nEXAMPLES\nFor simple \"daily\", \"weekly\", \"monthly\" jobs, put a copy of or a symlink to the script in one of the /etc/cron.{daily|weekly|monthly} directories above. Anacron will take care of running it daily/weekly/monthly, and if your computer is off on the day the \"weekly\" scripts would normally run, it'll run them the next time the computer is on.\nAs another example, assuming you have a script here: /usr/local/sbin/maint.sh\nAnd you wish to run it every three days, the standard entry in /etc/crontab would look like this:\n# m h dom mon dow user  command\n0 0 */3 * * root /usr/local/sbin/maint.sh\n\nIf your computer was not on at 00:00 on the 3rd of the month, the job would not run until the 6th.\nTo have the job instead run on the 4th when the computer is off and \"misses\" the run on the 3rd, you'd use this in /etc/anacrontab (don't forget to remove the line from /etc/crontab):\n# period delay job-identifier command\n3 5 maint-job /usr/local/sbin/maint.sh\n\nThe \"delay\" of \"5\" above means that anacron will wait for 5 minutes before it runs this job. The idea is to prevent anacron from firing things off immediately at boot time.",
        "url": "https://serverfault.com/questions/52335/job-scheduling-using-crontab-what-will-happen-when-computer-is-shutdown-during"
    },
    {
        "title": "Monday morning mistake: sudo rm -rf --no-preserve-root /",
        "question": "\nPlease note: The answers and comments to this  question contains content from another, similar question that has received a lot of attention from outside media but turned out to be hoax question in some kind of viral marketing scheme. As we don't allow ServerFault to be abused in such a way, the original question has been deleted and the answers merged with this question. \n\n\nHere's a an entertaining tragedy. This morning I was doing a bit of maintenance on my production server, when I mistakenly executed the following command:\nsudo rm -rf --no-preserve-root /mnt/hetznerbackup /\nI didn't spot the last space before / and a few seconds later, when warnings was flooding my command line, I realised that I had just hit the self-destruct button. Here's a bit of what burned into my eyes:\nrm: cannot remove `/mnt/hetznerbackup': Is a directory\nrm: cannot remove `/sys/fs/ecryptfs/version': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/inode_readahead_blks': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/mb_max_to_scan': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/delayed_allocation_blocks': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/max_writeback_mb_bump': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/mb_stream_req': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/mb_min_to_scan': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/mb_stats': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/trigger_fs_error': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/session_write_kbytes': Operation not permitted\nrm: cannot remove `/sys/fs/ext4/md2/lifetime_write_kbytes': Operation not permitted\n# and so on..\n\nI stopped the task and was relieved when I discovered that the production service was still running. Sadly, the server no longer accept my public key or password for any user via SSH.\nHow would you move forward from here? I'll swim an ocean of barbed wire to get that SSH-access back.\nThe server is running Ubuntu-12.04 and hosted at Hetzner.",
        "top_answer": "Boot into the rescue system provided by Hetzner and check what damage you have done.\nTransfer out any files to a safe location and redeploy the server afterwards.  \nI'm afraid that is the best solution in your case.",
        "url": "https://serverfault.com/questions/587102/monday-morning-mistake-sudo-rm-rf-no-preserve-root"
    },
    {
        "title": "How to test a HTTPS URL with a given IP address",
        "question": "Let's say a website is load-balanced between several servers. I want to run a command to test whether it's working, such as curl DOMAIN.TLD. So, to isolate each IP address, I specify the IP manually. But many websites may be hosted on the server, so I still provide a host header, like this: curl IP_ADDRESS -H 'Host: DOMAIN.TLD'. In my understanding, these two commands create the exact same HTTP request. The only difference is that in the latter one I take out the DNS lookup part from cURL and do this manually (please correct me if I'm wrong).\nAll well so far. But now I want to do the same for an HTTPS url. Again, I could test it like this curl https://DOMAIN.TLD. But I want to specify the IP manually, so I run curl https://IP_ADDRESS -H 'Host: DOMAIN.TLD'.  Now I get a cURL error:\ncurl: (51) SSL: certificate subject name 'DOMAIN.TLD' does not match target host name 'IP_ADDRESS'.\n\nI can of course get around this by telling cURL not to care about the certificate (the \"-k\" option) but it's not ideal.\nIs there a way to isolate the IP address being connected to from the host being certified by SSL?",
        "top_answer": "Think I found a solution going through the cURL manual:\ncurl https://DOMAIN.EXAMPLE --resolve 'DOMAIN.EXAMPLE:443:192.0.2.17'\n\n\nAdded in [curl] 7.21.3. Removal support added in 7.42.0.\n\nfrom CURLOPT_RESOLVE explained",
        "url": "https://serverfault.com/questions/443949/how-to-test-a-https-url-with-a-given-ip-address"
    },
    {
        "title": "How to list Apache enabled modules?",
        "question": "Is there a command that list all enabled Apache modules?",
        "top_answer": "To list apache loaded modules use:\napachectl -M\n\nor:\napachectl -t -D DUMP_MODULES \n\nor on RHEL,CentoS, Fedora:\nhttpd -M\n\nFor more options man apachectl. All these answers can be found just by little google search.",
        "url": "https://serverfault.com/questions/475173/how-to-list-apache-enabled-modules"
    },
    {
        "title": "What port should I open to allow remote desktop?",
        "question": "What port(s) should I open/NAT to allow me to use Remote Desktop?",
        "top_answer": "Remote Desktop requires TCP port 3389 to be open. Also, opening UDP port 3389 enables acceleration since RDP 8.0.\nIt is possible to change the port used by the terminal server (or PC which is accessed), see this Microsoft support article: How to change the listening port for Remote Desktop. The UDP port for accelerated connection uses the same port number as the TCP setting and cannot be changed separately.\nUDP acceleration is available since RDP 8.0 (shipped with Windows 8 and Windows Server 2012, available via an update on Windows 7 / Windows Server 2008 R2).",
        "url": "https://serverfault.com/questions/12005/what-port-should-i-open-to-allow-remote-desktop"
    },
    {
        "title": "Difference between OpenSSH internal-sftp and sftp-server",
        "question": "Why are there two ways to setup SFTP with OpenSSH and when to use which? Is there any difference between them?\nI mean the first one is using a lib from OpenSSH and the second one says \"use the internal\", so it is also OpenSSH?\nSubsystem sftp /usr/lib/openssh/sftp-server\n\nSubsystem sftp internal-sftp\n",
        "top_answer": "Both sftp-server and internal-sftp are part of OpenSSH. The sftp-server is a standalone binary. The internal-sftp is just a configuration keyword that tells sshd to use the SFTP server code built-into the sshd, instead of running another process (what would typically be the sftp-server).\nThe internal-sftp was added much later (OpenSSH 4.9p1 in 2008?) than the standalone sftp-server binary. But it is used in the default configuration file now. The sftp-server is now redundant and is kept probably for backward compatibility only.\nI believe there's no reason to use the sftp-server for new installations.\n\nFrom a functional point of view, the sftp-server and internal-sftp are almost identical. They are built from the same source code.\nThe main advantage of the internal-sftp is, that it requires no support files when used with ChrootDirectory directive.\nQuotes from the sshd_config(5) man page:\n\nFor Subsystem directive:\n\nThe command sftp-server implements the SFTP file transfer subsystem.\nAlternately the name internal-sftp implements an in-process SFTP server. This may simplify configurations using ChrootDirectory to force a different filesystem root on clients.\n\n\nFor ForceCommand directive:\n\nSpecifying a command of internal-sftp will force the use of an in-process SFTP server that requires no support files when used with ChrootDirectory.\n\n\nFor ChrootDirectory directive:\n\nThe ChrootDirectory must contain the necessary files and directories to support the user's session. For an interactive session this requires at least a shell, typically sh, and basic /dev nodes such as null, zero, stdin, stdout, stderr, and tty devices. For file transfer sessions using SFTP no additional configuration of the environment is necessary if the in-process sftp-server is used, though sessions which use logging may require /dev/log inside the chroot directory on some operating systems (see sftp-server for details).\n\n\n\nAnother advantage of the internal-sftp is a performance, as it's not necessary to run a new sub-process for it.\n\nIt may seem that the sshd could automatically use the internal-sftp, when it encounters the sftp-server, as the functionality is identical and the internal-sftp has even the above advantages. But there are edge cases, where there are differences.\nFew examples:\n\nAdministrator may rely on a login shell configuration to prevent certain users from logging in.  Switching to the internal-sftp would bypass the restriction, as the login shell is no longer involved.\n\nUsing the sftp-server binary (being a standalone process) you can use some hacks, like running the SFTP under sudo.\n\nFor SSH-1 (if anyone is still using it), Subsystem directive is not involved at all. An SFTP client using SSH-1 tells the server explicitly, what binary the server should run. So legacy SSH-1 SFTP clients have the sftp-server name hard-coded.\n\n",
        "url": "https://serverfault.com/questions/660160/difference-between-openssh-internal-sftp-and-sftp-server"
    },
    {
        "title": "What's the meaning of '@' in a DNS zone file?",
        "question": "I have the following data in my DNS zone file for my domain:\n$ORIGIN mydomain.com.\n@       IN      A       208.X.Y.Z\nmail    IN      A       208.X.Y.Z\n... etc..\n\nWhat does the @ line mean? I know what an A record is.. but a host with an ampersand at sign?",
        "top_answer": "RFC 1035 defines the format of a DNS zone file.\n... on page 35 you'll find:\n\n@               A free standing @ is\nused to denote the current origin.\n\nThis means that @ is a shortcut for the name defined with $ORIGIN.\nYou can find more information on $ORIGIN here, which is an excerpt from Pro DNS and BIND, published by Apress.",
        "url": "https://serverfault.com/questions/83874/whats-the-meaning-of-in-a-dns-zone-file"
    },
    {
        "title": "Can I hide all server / os info?",
        "question": "I don't want anyone to be able to detect that I'm using NGINX or even Ubuntu from the internet. There are tools out there (such as BuiltWith) which scan servers to detect what tools they're using. Also, some cracking tools might help with deteting. What's the best / closest to that I can get to hiding all this info from the outside?",
        "top_answer": "You can stop it outputting the version of Nginx and OS by adding\nserver_tokens off;\n\nto a http, server, or location context.\nOr if you want to remove the Server header completely, you need to compile Nginx with the Headers More module in, as the header is hard coded in the Nginx source, and this module allows changing any http headers.\n more_clear_headers Server;\n\nHowever, there are many hidden ways servers perform by accident via their implementation which may help identify the system. e.g. How it responds to a bad SSL request. I don't see a practical way of preventing this.\nSome of the things I might suggest:\n\nchange error templates\nblock all ports except the services needed\n",
        "url": "https://serverfault.com/questions/214242/can-i-hide-all-server-os-info"
    },
    {
        "title": "Meaning of directories on Unix and Unix like systems",
        "question": "I've been using Linux for a couple of years now but I still haven't figured out what the origin or meaning of some the directory names are on Unix and Unix like systems. E.g. what does etc stand for or var? Where does the opt name come from?\nAnd while we're on the topic anyway. Can someone give a clear explanation of what directory is best used for what. I sometimes get confused where certain software is installed or what the most appropriate directory is to install software into.",
        "top_answer": "For more data on the layout of Linux file-systems, look at the Filesystem Hierarchy Standard (now at version 3.0). It does explain some of where the names came from:\n\n/bin - Binaries.\n/boot - Files required for booting.\n/dev - Device files.\n/etc - Et cetera. The name is inherited from the earliest Unixes, which is when it became the spot to put config-files.\n/home - Where home directories are kept.\n/lib - Where code libraries are kept.\n/media - A more modern directory, but where removable media gets mounted.\n/mnt - Where temporary file-systems are mounted.\n/opt - Where optional add-on software is installed. This is discrete from /usr/local/ for reasons I'll get to later.\n/run - Where runtime variable data is kept.\n/sbin - Where super-binaries are stored. These usually only work with root.\n/srv - Stands for \"serve\". This directory is intended for static files that are served out. /srv/http would be for static websites, /srv/ftp for an FTP server.\n/tmp - Where temporary files may be stored.\n/usr - Another directory inherited from the Unixes of old, it stands for \"UNIX System Resources\".  It does not stand for \"user\" (see the Debian Wiki).  This directory should be sharable between hosts, and can be NFS mounted to multiple hosts safely. It can be mounted read-only safely.\n/var - Another directory inherited from the Unixes of old, it stands for \"variable\". This is where system data that varies may be stored. Such things as spool and cache directories may be located here. If a program needs to write to the local file-system and isn't serving that data to someone directly, it'll go here.\n\n/opt vs /usr/local\nThe rule of thumb I've seen is best described as:\n\nUse /usr/local for things that would normally go into /usr, or are overriding things that are already in /usr. Use /opt for things that install all in one directory, or are otherwise special.\n\n/usr/lib vs /lib\nSeveral Linux distributions have taken the step to make /lib a symlink to /usr/lib (Fedora started in 2012, Ubuntu as of 19.04, OpenSUSE began in 2020). This step was controversial as it violates one of the original intentions of having these two directories split: you should be able to boot a system using only what is in /lib. This guidance dates from eras when it was quite common to have many disks  and partitions on a system, allowing administrators to separate what goes into /lib versus /usr/lib and similar. Practice in the last couple of decades has drifted away from managing these directories separately, so the distros decided to go where the users are and effectively unify them. This unification makes it easier for package maintainers, since there are fewer places to manage files.",
        "url": "https://serverfault.com/questions/24523/meaning-of-directories-on-unix-and-unix-like-systems"
    },
    {
        "title": "Is there a reason to use an SSL certificate other than Let's Encrypt's free SSL?",
        "question": "Let's Encrypt are providing free SSL certificates. Are there any downsides compared to other, paid certificates e.g. AWS Certificate Manager?",
        "top_answer": "Certificate lifespan\nSecurity\nShorter lifespan is better. Simply because revocation is mostly theoretical, in practice it cannot be relied on (big weakness in the public PKI ecosystem).\nManagement\nWithout automation: Longer lifespan is more convenient. LE may not be feasible if you, for whatever reason, cannot automate the certificate management\nWith automation: Lifespan doesn't matter.\nEnd-user impression\nEnd-users are unlikely to have any idea one way or another.\nLevel of verification\nSecurity\nLetsencrypt provides DV level of verification only.\nBuying a cert you get whatever you pay for (starting at DV, with the same level of assertion as with LE).\nDV = only domain name control is verified.\nOV = owner entity (organization) information is verified in addition.\nEV = more thorough version of OV, which has traditionally been awarded with the \"green bar\" (but the \"green bar\" appears to be going away soon).\nManagement\nWhen using LE, the work you put in is setting up the necessary automation (in this context, to prove domain control). How much work that is will depend on your environment.\nWhen buying a cert the DV/OV/EV level will define how much manual work will be required to get the cert. For DV it typically boils down going through a wizard paying and copy/pasting something or clicking something, for OV and EV you can pretty much count on needing to be contacted separately to do additional steps to confirm your identity.\nEnd-user impression\nEnd-users probably recognize the current EV \"green bar\" (which is going away), other than that they don't tend to actually look at the certificate contents.\nTheoretically, though, it is clearly more helpful with a certificate that states information about the controlling entity. But browsers (or other client applications) need to start actually showing this in a useful way before that has any effect for the typical user.\nInstallation\nSecurity\nIt is possible to do things incorrectly in ways that expose private keys or similar.\nWith LE, the provided tooling is set up around reasonable practices.\nWith a person who knows what they are doing, manual steps can obviously also be done securely.\nManagement\nLE is very much intended to have all processes automated, their service is entirely API-based and the short lifespan also reflects how everything is centered around automation.\nWhen buying a cert, even with a CA that provides APIs to regular customers (not really the norm at this point) it will be difficult to properly automate anything other than DV and with DV you are paying for essentially the same thing that LE provides.\nIf you are going for OV or EV levels, you can probably only partially automate the process.\nEnd-user impression\nIf the installation is done correctly, the end-user will obviously not know how it was done. The chances of messing things up (eg, forgetting to renew or doing the installation incorrectly when renewing) are less with an automated process.\nOverall\nTraditional means of buying certs are particularly useful if you desire OV/EV certs, are not automating certificate management or want certs used in some other context than HTTPS.",
        "url": "https://serverfault.com/questions/926974/is-there-a-reason-to-use-an-ssl-certificate-other-than-lets-encrypts-free-ssl"
    },
    {
        "title": "Should servers be turned off at night?",
        "question": "There is a server that is used from 4:30 am in the morning until ~ 22:00.\nShould it be turned off? I think that it is a server and that it won't have a problem to stay on, but serious professors are telling me that it is dangerous and that HD can fail within 2 years. The server owner believes that his old server running from 1995 without backup and a single hard disk (if the hard disk fails he is screwed) had no problem because he used to turn it off at nights.\nWhat do you believe for this?\nNow it has a RAID 1 array, external hard disk backup, and serveral full hard disk backups on DVD and over the internet.",
        "top_answer": "To liken it to a car analogy: A taxi can do over 500,000 kilometers before it needs an engine rebuild. The reason for this is because they are always running, 24/7, and after a car's engine is up to temperature, the amount of wear it receives while it is running is greatly reduced.\nA computer is kinda the same. The majority of the \"wear\" on parts can happen when the server is booting up. Just attach an amp meter to your computer, and turn it on. When it starts up, the power it draws climbs very high, and then it settles down once all the disks have spun up and the processor is initalised. Also, think about how much disk activity the server undergoes during boot up vs when it's working. Chances are the disk access from booting the OS is fairly solid activity, whereas when the OS is running, unless it's a very heavy database server (I'm guessing not), the disks will most likely stay fairly idle. If there's any time it's going to fail, chances are it will be on boot up.\nTurning your server on and off is a stupid idea. Not only to mention most servers can take upwards of 2-5 minutes to just get past the BIOS checks, it's a huge amount of wasted time too.\n\n2018 Update: Given that most computers are now essentailly entirely solid-state, this answer may no longer be as accurate as it once was. The taxi analogy doesn't really suit todays modern servers. That said, typically you still generall don't turn servers off.",
        "url": "https://serverfault.com/questions/258064/should-servers-be-turned-off-at-night"
    },
    {
        "title": "How to dump a Microsoft SQL Server database to a SQL script?",
        "question": "Is there any way to export a Microsoft SQL Server database to a SQL script?\nI'm looking for something which behaves similarly to mysqldump, taking a database name, and producing a single script which will recreate all the tables, stored procedures and reinsert all the data etc.\nI've seen this tutorial, but I ideally want something to recreate everything (not just the data) which works in a single step to produce the final script.",
        "top_answer": "In SQL Server Management Studio right-click your database and select Tasks > Generate Scripts. Follow the wizard and you'll get a script that recreates the data structure in the correct order according to foreign keys. On the wizard step entitled Set Scripting Options, click on the button (on the right of the window) labelled Advanced and modify the option Types of data to script and choose Schema and data.\nTIP: In the final step select Script to a New Query Window, it'll work much faster that way.",
        "url": "https://serverfault.com/questions/147638/how-to-dump-a-microsoft-sql-server-database-to-a-sql-script"
    },
    {
        "title": "Keeping a linux process running after I logout",
        "question": "I'm connecting to a Linux machine through SSH, and I'm trying to run a heavy bash script that makes filesystem operations. It's expected to keep running for hours, but I cannot leave the SSH session open because of internet connections issues I have.\nI doubt that running the script with the background operator, the ampersand (&), will do the trick, because I tried it and later found that process was not completed. How can I logout and keep the process running?",
        "top_answer": "The best method is to start the process in a terminal multiplexer. Alternatively you can make the process not receive the HUP signal.\n\nA terminal multiplexer provides \"virtual\" terminals which run independent from the \"real\" terminal (actually all terminals today are \"virtual\" but that is another topic for another day). The virtual terminal will keep running even if your real terminal is closed with your ssh session.\nAll processes started from the virtual terminal will keep running with that virtual terminal. When you reconnect to the server you can reconnect to the virtual terminal and everything will be as if nothing happened, other than the time which passed.\nTwo popular terminal multiplexers are screen and tmux.\nScreen has a steep learning curve. Here is a good tutorial with diagrams explaining the concept: http://www.ibm.com/developerworks/aix/library/au-gnu_screen/\n\nThe HUP signal (or SIGHUP) is sent by the terminal to all its child processes when the terminal is closed. The common action upon receiving SIGHUP is to terminate. Thus when your ssh session gets disconnected all your processes will terminate. To avoid this you can make your processes not receive SIGHUP.\nTwo easy methods to do so are nohup and disown.\nFor more information about how nohup and disown works read this question and answer: https://unix.stackexchange.com/questions/3886/difference-between-nohup-disown-and\nNote: although the processes will keep running you can no longer interact with them because they are no longer attached to any terminal. This method is mainly useful for long running batch processes which, once started, no longer need any user input.",
        "url": "https://serverfault.com/questions/311593/keeping-a-linux-process-running-after-i-logout"
    },
    {
        "title": "How to see active connections and \"current activity\" in PostgreSQL 8.4",
        "question": "I'm investigating an issue with DB connections being left open indefinitely, causing problems on the DB server. How do I see currently open connections to a PostgreSQL server, particularly those using a specific database? Ideally I'd like to see what command is executing there as well. Basically, I'm looking for something equivalent to the \"Current Activity\" view in MSSQL.",
        "top_answer": "OK, got it from someone else. This query should do the trick:\nselect *\nfrom pg_stat_activity\nwhere datname = 'mydatabasename';\n",
        "url": "https://serverfault.com/questions/128284/how-to-see-active-connections-and-current-activity-in-postgresql-8-4"
    },
    {
        "title": "How can I upgrade to Java 1.8 on an Amazon Linux Server?",
        "question": "I tried sudo yum update but it just keeps java \"1.7.0_75\". I need 1.8 for it to work with another application but can't figure out how to upgrade it.\nDo I need to manually install it somehow? There's not much information on this on the internet as far as I can see.\nSpecs: \njava version \"1.7.0_75\"\nOpenJDK Runtime Environment (amzn-2.5.4.0.53.amzn1-x86_64 u75-b13)\nOpenJDK 64-Bit Server VM (build 24.75-b04, mixed mode)\n\nWhen I try update now:   \n[ec2-________]$ sudo yum update\nLoaded plugins: priorities, update-motd, upgrade-helper\namzn-main/latest                                         | 2.1 kB     00:00\namzn-updates/latest                                      | 2.3 kB     00:00\nNo packages marked for update\n\nIs there anything else I need to do?\nThanks.",
        "top_answer": "To remove java 1.7 and install java 1.8:\nsudo yum install java-1.8.0\nsudo yum remove java-1.7.0-openjdk\n",
        "url": "https://serverfault.com/questions/664643/how-can-i-upgrade-to-java-1-8-on-an-amazon-linux-server"
    },
    {
        "title": "robocopy transfer file and not folder",
        "question": "I'm trying to use robocopy to transfer a single file from one location to another but robocopy seems to think I'm always specifying a folder. Here is an example:\nrobocopy \"c:\\transfer_this.txt\" \"z:\\transferred.txt\"\n\nBut I get this error instead:\n2009/08/11 15:21:57 ERROR 123 (0x0000007B) Accessing Source Directory c:\\transfer_this.txt\\\n\n(note the \\ at the end of transfer_this.txt)\nBut if I treat it like an entire folder:\nrobocopy \"c:\\folder\" \"z:\\folder\"\n\nIt works but then I have to transfer everything in the folder.\nHow can I only transfer a single file with robocopy?",
        "top_answer": "See: Robocopy /?\n\nUsage : ROBOCOPY source destination [file [file]...] [options]\n\nrobocopy c:\\folder d:\\folder transfer_this.txt\n",
        "url": "https://serverfault.com/questions/52983/robocopy-transfer-file-and-not-folder"
    },
    {
        "title": "Rsync difference between --checksum and --ignore-times options",
        "question": "Can anyone clarify the differences between the --checksum and --ignore-times options of rsync?\nMy understanding is as follows:\n--checksum\nIf the file size and time match, it will do a checksum at both ends to see if the files are really identical.\n--ignore-times\n'Transfer' every file, regardless of whether file time is same at both ends.  Since it will still use the delta-transfer algorithm, if a file actually is identical, nothing gets transferred.\nThat's the technical difference, but as far as I can tell, they are semantically the same thing.\nSo, what I'm wondering is:\n\nWhat is the practical difference between the two options?\nIn what cases would you use one rather than the other?\nIs there any performance difference between them?\n",
        "top_answer": "Normally, rsync skips files when the files have identical sizes and times on the source and destination sides. This is a heuristic which is usually a good idea, as it prevents rsync from having to examine the contents of files that are very likely identical on the source and destination sides.\n--ignore-times tells rsync to turn off the file-times-and-sizes heuristic, and thus unconditionally transfer ALL files from source to destination. rsync will then proceed to read every file on the source side, since it will need to either use its delta-transfer algorithm, or simply send every file in its entirety, depending on whether the --whole-file option was specified.\n--checksum also modifies the file-times-and-sizes heuristic, but here it ignores times and examines only sizes. Files on the source and destination sides that differ in size are transferred, since they are obviously different. Files with the same size are checksummed (with MD5 in rsync version 3.0.0+, or with MD4 in earlier versions), and those found to have differing sums are also transferred.\nIn cases where the source and destination sides are mostly the same, --checksum will result in most files being checksummed on both sides. This could take long time, but the upshot is that the barest minimum of data will actually be transferred over the wire, especially if the delta-transfer algorithm is used. Of course, this is only a win if you have very slow networks, and/or very fast CPU.\n--ignore-times, on the other hand, will send more data over the network, and it will cause all source files to be read, but at least it will not impose the additional burden of computing many cryptographically-strong hashsums on the source and destination CPUs. I would expect this option to perform better than --checksum when your networks are fast, and/or your CPU relatively slow.\nI think I would only ever use --checksum or --ignore-times if I were transferring files to a destination where it was suspected that the contents of some files were corrupted, but whose modification times were not changed. I can't really think of any other good reason to use either option, although there are probably other use-cases.",
        "url": "https://serverfault.com/questions/211005/rsync-difference-between-checksum-and-ignore-times-options"
    },
    {
        "title": "What's the reverse DNS command line utility?",
        "question": "What's the command to find the name of a computer given its IP address?\nI always forget what this command is, but I know it exists in Windows and I assume it exists on the *nix command-line. ",
        "top_answer": "The commands dig and host should be what you're looking for ;)\nOn *nix systems, you can use this command:\ndig -x [address]\n\nAlternatively, you can add +short at the end of the dig command to output only the DNS result.\nThere's also nslookup on both *nix and Windows systems for reverse DNS requests.",
        "url": "https://serverfault.com/questions/7056/whats-the-reverse-dns-command-line-utility"
    },
    {
        "title": "How can I rename a Unix user?",
        "question": "I have a user named old_username and I want him to be named new_username, but I don't want to change his numeric user ID.\nHow can I accomplish this?",
        "top_answer": "Under Linux, the usermod command changes user names. It modifies the system account files to reflect the changes that are specified on the command line.\nTo change just the username:\nusermod --login new_username old_username\n\nTo change the username and home directory name:\nusermod --login new_username --move-home --home path_to_the_new_home_dir old_username\n\nYou may also want to change the name of the group associated with the user:\ngroupmod --new-name new_username old_username\n",
        "url": "https://serverfault.com/questions/437342/how-can-i-rename-a-unix-user"
    },
    {
        "title": "Where to check log of sendmail?",
        "question": "I can't send out emails,\nneed to look into the logs,\nbut where is the log?",
        "top_answer": "Where are the logs?\nThe default location depends on your linux/unix system, but the most common places are\n\n/var/log/maillog\n/var/log/mail.log\n/var/adm/maillog\n/var/adm/syslog/mail.log\n\nIf it's not there, look up /etc/syslog.conf. You should see something like this\nmail.*         -/var/log/maillog\n\nsendmail writes logs to the mail facility of syslog. Therefore, which file it gets written to depends on how syslog was configured.\nIf you system uses syslog-ng (instead of the more \"traditional\" syslog), then you'll have to look up your syslog-ng.conf file. You'll should something like this:\n# This files are the log come from the mail subsystem.\n#\ndestination mail     { file(\"/var/log/mail.log\"); };\ndestination maillog  { file(\"/var/log/maillog\"); };\ndestination mailinfo { file(\"/var/log/mail.info\"); };\ndestination mailwarn { file(\"/var/log/mail.warn\"); };\ndestination mailerr  { file(\"/var/log/mail.err\"); };\n\nUnable to send out emails?\nOne of the most common reason I've seen for a freshly installed sendmail not being able to send out emails is the DAEMON_OPTIONS being set to listen only on 127.0.0.1\nSee /etc/mail/sendmail.mc\ndnl #\ndnl # The following causes sendmail to only listen on the IPv4 loopback address\ndnl # 127.0.0.1 and not on any other network devices. Remove the loopback\ndnl # address restriction to accept email from the internet or intranet.\ndnl #\nDAEMON_OPTIONS(`Port=smtp,Addr=127.0.0.1, Name=MTA')dnl\n\nIf that's your case, remove the \"Addr=127.0.0.1\" part, rebuild your conf file and you're good to go!\nDAEMON_OPTIONS(`Port=smtp, Name=MTA')dnl\n\n[root@server]$ m4 sendmail.mc > /etc/sendmail.cf\n[root@server]$/etc/init.d/sendmail restart\n\nIf you've been making changes to /etc/sendmail.cf manually thus far (instead of the *.m4 file) you can make similar changes in /etc/sendmail.cf. The offending line will look like this:\nO DaemonPortOptions=Port=smtp,Addr=127.0.0.1, Name=MTA\n\nChange it to:\nO DaemonPortOptions=Port=smtp, Name=MTA\n",
        "url": "https://serverfault.com/questions/59602/where-to-check-log-of-sendmail"
    },
    {
        "title": "How can I verify if TLS 1.2 is supported on a remote web server from the RHEL/CentOS shell?",
        "question": "I'm on CentOS 5.9.\nI'd like to determine from the linux shell if a remote web server specifically supports TLS 1.2 (as opposed to TLS 1.0).  Is there an easy way to check for that?\nI'm not seeing a related option on openssl but perhaps I'm overlooking something. ",
        "top_answer": "You should use openssl s_client, and the option you are looking for is -tls1_2. \nAn example command would be: \nopenssl s_client -connect google.com:443 -tls1_2\nIf you get the certificate chain and the handshake you know the system in question supports TLS 1.2. If you see don't see the certificate chain, and something similar to \"handshake error\" you know it does not support TLS 1.2.  You can also test for TLS 1 or TLS 1.1 with -tls1 or tls1_1 respectively.",
        "url": "https://serverfault.com/questions/638691/how-can-i-verify-if-tls-1-2-is-supported-on-a-remote-web-server-from-the-rhel-ce"
    },
    {
        "title": "Run Oracle SQL script and exit from sqlplus.exe via command prompt",
        "question": "I would like to run an Oracle script through SQL Plus via a Windows command prompt. The script does not contain an \"exit\" command, but I would still like SQL Plus to exit, returning control to the command prompt on completion of the script. My goal is to do this without modifying the script. Is this possible?",
        "top_answer": "Another way is to use this command in the batch file:\necho exit | sqlplus user/pass@connect @scriptname\n",
        "url": "https://serverfault.com/questions/87035/run-oracle-sql-script-and-exit-from-sqlplus-exe-via-command-prompt"
    },
    {
        "title": "Is my password compromised because I forgot to hit Enter after ssh username?",
        "question": "I've just tried logging into a Fedora (release 13 Goddard) server using SSH (PuTTY, Windows). For some reason the Enter after typing my username didn't go through and I typed in my password and hit Enter again. I only realized my mistake when the server greeted me with a happy\n\nmyusername MYPASSWORD@server.example.com's password:\n\nI broke off the connection at this point and changed my password on that machine (through a separate SSH connection).\n... now my question is: Is such a failed login stored in plain text in any logfile? In other words, have I just forced my (now-outdated) password in front of the eyes of the remote admin the next time he scans his logs?\nUpdate \nThanks for all the comments about the implied question \"what to do to prevent this in the future\". For quick, one-off connections I'll use this PuTTY feature now:\n\nto replace the where-was-it-again \"auto-login username\" option\n\nI'll also start using ssh keys more often, as explained in the PuTTY docs. ",
        "top_answer": "In short: yes.\n# ssh 192.168.1.1 -l \"myuser mypassword\"\n^C\n# egrep \"mypassword\" /var/log/auth.log\nOct 19 14:33:58 host sshd[19787]: Invalid user myuser mypassword from 192.168.111.78\nOct 19 14:33:58 host sshd[19787]: Failed none for invalid user myuser mypassword from 192.168.111.78 port 53030 ssh2\n",
        "url": "https://serverfault.com/questions/322870/is-my-password-compromised-because-i-forgot-to-hit-enter-after-ssh-username"
    },
    {
        "title": "Can you help me with my capacity planning?",
        "question": "\nThis is a canonical question about capacity planning\nRelated:\n\nHow do you do load testing and capacity planning for web sites?\nHow do you do load testing and capacity planning for databases?\n\n\nI have a question regarding capacity planning. Can the Server Fault community please help with the following:\n\n\nWhat kind of server do I need to handle some number of users?\nHow many users can a server with some specifications handle?\nWill some server configuration be fast enough for my use case?\nI'm building a social networking site: what kind of hardware do I need?\nHow much bandwidth do I need for some project?\nHow much bandwidth will some number of users use in some application?\n",
        "top_answer": "The Server Fault community generally can't help you with capacity planning - the best answer we can offer is \"Benchmark your code on hardware similar to what you'll be using in production, identify any bottlenecks, then determine how much of a workload your current hardware can handle, and/or how much hardware horsepower you need to handle your target workload\".\n\nThere are a number of factors at play in capacity planning which we can't adequately assess on a Question and Answer site:\n\nThe requirements of your particular code/software\nExternal resources (databases, other software/sites/servers)\nYour workload (peak, average, queueing)\nThe business value of performance (cost/benefit analysis)\nThe performance expectations of your users\nAny service level agreements/contractual obligations you may have\n\nDoing a proper analysis on these factors, and others, is beyond the scope of a simple question-and-answer site: They require detailed knowledge about your environment and requirements which only your team (or an adequately-compensated consultant) can gather efficiently.\n\nSome Capacity Planning Axioms\n\nRAM is cheap\nIf you expect your application to use a lot of RAM you should put in as much RAM as you can afford / fit.\nDisk is cheap\nIf you expect to use a lot of disk you should buy big drives - lots of them.\nSAN/NAS storage is less cheap, and should also usually be spec'd large rather than small to avoid costly upgrades later.\nWorkloads grow over time\nAssume your resource needs will increase.\nBear in mind that the increase may not be symmetrical (CPU and RAM may rise faster than disk), and it may not be linear.\nElectricity is expensive\nEven though RAM and disks have decreased in price considerably, the cost of electricity has gone up steadily. All those extra disks and RAM, not to mention CPU power, will increase your electricity bill (or the bill you pay to your provider). Plan accordingly. \n",
        "url": "https://serverfault.com/questions/384686/can-you-help-me-with-my-capacity-planning"
    },
    {
        "title": "Find out symbolic link target via command line",
        "question": "Say that I setup a symbolic link:\nln -s /root/Public/mytextfile.txt /root/Public/myothertextfile.txt\n\nis there a way to see what the target of myothertextfile.txt is using the command line?",
        "top_answer": "Use the -f flag to print the canonicalized version. For example:\nreadlink -f /root/Public/myothertextfile.txt\n\nFrom man readlink:\n-f, --canonicalize\n      canonicalize by following every symlink in every component of the given name recursively; all but the last component must exist\n",
        "url": "https://serverfault.com/questions/76042/find-out-symbolic-link-target-via-command-line"
    },
    {
        "title": "In systemd, what's the difference between After= and Requires=?",
        "question": "I'm creating a systemd .service file and I need help understanding the difference between Requires= and After=.  The man page says that Requires= \"Configures requirement dependencies on other units.\" and After= \"Configures ordering dependencies between units.\"  What's the difference?",
        "top_answer": "After= configures service order (do X only after Y), while Requires= state dependencies.\nIf you don't specify an order, a service depending on another would be started at the same time as the one it is depending on.\nAlso, the way I understand it (although I can't test that now and don't find a reference), After= is \"loose coupling\", which effectively means that a service with an After statement would still run even if the one named in the After= line isn't started at all, while Requires= would prevent its start if the requirement isn't met.\nCiting https://www.freedesktop.org/software/systemd/man/systemd.unit.html:\n\nRequires=\nConfigures requirement dependencies on other units. If this unit gets activated, the units listed here will be activated as well. If one of the other units gets deactivated or its activation fails, this unit will be deactivated. This option may be specified more than once or multiple space-separated units may be specified in one option in which case requirement dependencies for all listed names will be created. Note that requirement dependencies do not influence the order in which services are started or stopped. This has to be configured independently with the After= or Before= options. If a unit foo.service requires a unit bar.service as configured with Requires= and no ordering is configured with After= or Before=, then both units will be started simultaneously and without any delay between them if foo.service is activated. Often, it is a better choice to use Wants= instead of Requires= in order to achieve a system that is more robust when dealing with failing services.\n\nand\n\nBefore=, After=\nA space-separated list of unit names. Configures ordering dependencies between units. If a unit foo.service contains a setting Before=bar.service and both units are being started, bar.service's start-up is delayed until foo.service is started up. Note that this setting is independent of and orthogonal to the requirement dependencies as configured by Requires=. It is a common pattern to include a unit name in both the After= and Requires= option, in which case the unit listed will be started before the unit that is configured with these options. This option may be specified more than once, in which case ordering dependencies for all listed names are created. After= is the inverse of Before=, i.e. while After= ensures that the configured unit is started after the listed unit finished starting up, Before= ensures the opposite, i.e. that the configured unit is fully started up before the listed unit is started. Note that when two units with an ordering dependency between them are shut down, the inverse of the start-up order is applied. i.e. if a unit is configured with After= on another unit, the former is stopped before the latter if both are shut down. Given two units with any ordering dependency between them, if one unit is shut down and the other is started up, the shutdown is ordered before the start-up. It doesn't matter if the ordering dependency is After= or Before=. It also doesn't matter which of the two is shut down, as long as one is shut down and the other is started up. The shutdown is ordered before the start-up in all cases. If two units have no ordering dependencies between them, they are shut down or started up simultaneously, and no ordering takes place.\n",
        "url": "https://serverfault.com/questions/812584/in-systemd-whats-the-difference-between-after-and-requires"
    },
    {
        "title": "The Joel Test for system administrator jobs",
        "question": "Based on \u201cOrganizational issues\u201d \u2014 sore spots of IT?  I think it would be fair to say that system administrators need to determine if a place is worth working at.  There is a similar well known test by Joel for programmers.\nWhat are the 12 questions system administrators should ask at an interview in order to help them decide if it's a good place to work at?\nFollowing Joel's rules: \n\nQuestions should be platform and technology agnostic\nQuestions should elicit a simple response such as yes or no\n\nEDIT: Please post one question at a time so we can see what users are voting for.",
        "top_answer": "Do you use an incident/ticket tracking system?",
        "url": "https://serverfault.com/questions/14832/the-joel-test-for-system-administrator-jobs"
    },
    {
        "title": "How do I get apt-get to ignore some dependencies?",
        "question": "How do I make apt-get ignore some dependencies? For example, I wanted to install mailx so I can use it to send email from cron scripts/report-generating tools. However, installing mailx also installs exim4 and a whole bunch of dependencies (I already have Postfix installed) I don't really need and which I guess mailx can also live without.\nHow do I ignore some dependencies but still use apt-get since it's a nice tool?",
        "top_answer": "Simple and easy solution: just specify the unwanted packages with an extra - after each of them.\nExample without the - switch:\nroot@debian:~# apt-get install bsd-mailx \nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following extra packages will be installed:\n  exim4-base exim4-config exim4-daemon-light liblockfile-bin liblockfile1\n[...]\n\nExample using the switch to avoid installing exim4-base. Notice the - at the end:\nroot@debian:~# apt-get install bsd-mailx exim4-base-\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nPackage 'exim4-base' is not installed, so not removed\nThe following extra packages will be installed:\n  liblockfile-bin liblockfile1 ssmtp\n[...]\n\nAs you can see, apt-get does not try anymore to install the exim4-base package, and it does not try to install its various dependencies (exim4-config etc).\nAnd if you were wrong and needed that exim4-base dependency after all, you can just apt-get install it later!",
        "url": "https://serverfault.com/questions/250224/how-do-i-get-apt-get-to-ignore-some-dependencies"
    },
    {
        "title": "What significance does the user/host at the end of an SSH public key file hold?",
        "question": "I can\u2019t figure out why does an SSH public key file generated by ssh-keygen have a user and host at the end of it.\nExample: id_rsa.pub\nssh-rsa ... rest of file ... /CA9gyE8HRhNMG6ZDwyhPBbDfX root@mydomain\n\nNotice the root@mydomain at the end of the file.\nIf I can use the public key anywhere with any user to authenticate using my private key, what significance does the root@mydomain have on the authentication process?\nOr is it just a place holder to figure our who was it issued by?",
        "top_answer": "This field is a comment, and can be changed or ignored at will. It is set to user@host by default by ssh-keygen.\nThe OpenSSH sshd(8) man page describes the format of a public key thus:\n\nPublic keys consist of the following space-separated fields: options, keytype, base64-encoded key, comment. . . . The comment field is not used for anything (but may be convenient for the user to identify the key).\n\nThe ssh-keygen(1) man page says:\n\nThe key comment may be useful to help identify the key.  The comment is initialized to \u201cuser@host\u201d when the key is created, but can be changed using the -c option.\n",
        "url": "https://serverfault.com/questions/743548/what-significance-does-the-user-host-at-the-end-of-an-ssh-public-key-file-hold"
    },
    {
        "title": "How to recover from \"Too many Authentication Failures for user root\"",
        "question": "I've done several attempts to establish SSH-connecton for user root@host using putty terminal. While doing so I specified wrong credentials several times and after that I've specified them correctly, and then after the credentials were accepted the ssh session breaks with \n\n\"Server unexpectedly closed network\n  connection\".\n\nThis error is reported by putty terminal.  When trying to ssh root@localhost from the local console - it works fine. It also works fine when I ssh otheruser@host from other host. So network connectivity issues are not guilty. The only error I am thinking of is: \"Too many Authentication Failures for user root\" although putty reported a different error.  \nThe question is: how to recover from this error condition and let putty login again? Restarting sshd seems to not help",
        "top_answer": "Are you sure that root login to ssh is allowed?\nCheck sshd_config and verify that root login is permitted.  sshd will need to be restarted if the setting changes.",
        "url": "https://serverfault.com/questions/36291/how-to-recover-from-too-many-authentication-failures-for-user-root"
    },
    {
        "title": "What useful things can one add to one's .bashrc? [closed]",
        "question": "Is there anything that you can't live without and will make my life SO much easier? Here are some that I use ('diskspace' & 'folders' are particularly handy).\n# some more ls aliases\nalias ll='ls -alh'\nalias la='ls -A'\nalias l='ls -CFlh'\nalias woo='fortune'\nalias lsd=\"ls -alF | grep /$\"\n\n# This is GOLD for finding out what is taking so much space on your drives!\nalias diskspace=\"du -S | sort -n -r |more\"\n\n# Command line mplayer movie watching for the win.\nalias mp=\"mplayer -fs\"\n\n# Show me the size (sorted) of only the folders in this directory\nalias folders=\"find . -maxdepth 1 -type d -print | xargs du -sk | sort -rn\"\n\n# This will keep you sane when you're about to smash the keyboard again.\nalias frak=\"fortune\"\n\n# This is where you put your hand rolled scripts (remember to chmod them)\nPATH=\"$HOME/bin:$PATH\"\n",
        "top_answer": "I have a little script that extracts archives, I found it somewhere on the net:\nextract () {\n   if [ -f $1 ] ; then\n       case $1 in\n           *.tar.bz2)   tar xvjf $1    ;;\n           *.tar.gz)    tar xvzf $1    ;;\n           *.bz2)       bunzip2 $1     ;;\n           *.rar)       unrar x $1       ;;\n           *.gz)        gunzip $1      ;;\n           *.tar)       tar xvf $1     ;;\n           *.tbz2)      tar xvjf $1    ;;\n           *.tgz)       tar xvzf $1    ;;\n           *.zip)       unzip $1       ;;\n           *.Z)         uncompress $1  ;;\n           *.7z)        7z x $1        ;;\n           *)           echo \"don't know how to extract '$1'...\" ;;\n       esac\n   else\n       echo \"'$1' is not a valid file!\"\n   fi\n }\n",
        "url": "https://serverfault.com/questions/3743/what-useful-things-can-one-add-to-ones-bashrc"
    },
    {
        "title": "Postgresql: what does GRANT ALL PRIVILEGES ON DATABASE do?",
        "question": "I'm trying to grant all privileges on all tables of a given database to a new postgres user (not the owner). It seems that GRANT ALL PRIVILEGES ON DATABASE my_db TO new_user; does not do that.  After running said command successfully (as the postgres user), I get the following as new_user:\n$ psql -d my_db\nmy_db => SELECT * FROM a_table_in_my_db;\nERROR:  permission denied for relation a_table_in_my_db\n\nTwo questions: \n1)  What does the command above do, then, if not granting all permissions on all tables on my_db?\n2) What's the proper way to grant all permissions on all tables to a user?  (including on all tables created in the future)",
        "top_answer": "The answers to your questions come from the online PostgreSQL 8.4 docs.\n\nGRANT ALL PRIVILEGES ON DATABASE grants the CREATE, CONNECT, and TEMPORARY privileges on a database to a role (users are properly referred to as roles). None of those privileges actually permits a role to read data from a table; SELECT privilege on the table is required for that. \nI'm not sure there is a \"proper\" way to grant all privileges on all tables to a role. The best way to ensure a given role has all privileges on a table is to ensure that the role owns the table. By default, every newly created object is owned by the role that created it, so if you want a role to have all privileges on a table, use that role to create it.\nPostgreSQL 9.0 introduces the following syntax that is almost what you want:\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO new_user;\nThe rub is that if you create tables in schemas outside the default \"public\" schema, this GRANT won't apply to them. If you do use non-public schemas, you'll have to GRANT the privileges to those schemas separately.\n",
        "url": "https://serverfault.com/questions/198002/postgresql-what-does-grant-all-privileges-on-database-do"
    },
    {
        "title": "How do I get the history of \"apt-get install\" on Ubuntu?",
        "question": "I am about do move a server from one Ubuntu box to another. I'm not cloning the old box to the new; I'm creating a new system and will move data as needed. I want to install all the software that I have on the old box on the new one.\nIs there a simple way to find the history of all the \"sudo apt-get install\" commands I have given over time? That is, dpkg -l shows me all the packages that have been installed, but not which top-level package installed them. If there is a way for dpkg to give me the installing package, I can find the unique ones there; otherwise, I want something else to say \"you installed these 24 packages\".",
        "top_answer": "The apt history is in /var/log/apt/history.log as said in a comment above. That said, this will not list packages that were installed manually, using dpkg or GUIs such as gdebi. To see all the packages that went through dpkg, you can look at /var/log/dpkg.log.",
        "url": "https://serverfault.com/questions/175504/how-do-i-get-the-history-of-apt-get-install-on-ubuntu"
    },
    {
        "title": "How to extend an ext4 partition and filesystem?",
        "question": "I have a 400GB disk with a 320GB ext4 partition.\nI would like to grow the ext4 partition to use the left space (80GB of free space).\n+--------------------------------+--------+\n|             ext4               |  Free  |\n+--------------------------------+--------+\n\nHow could I do this?\nI've seen people using resize2fs but I don't understand if it resizes the partition.\nAnother solution would be to use fdisk but I don't want to delete my partition and loose data. How could I simply grow the partition without loosing any file?\nNote: I'm talking about an un-mounted data partition without LVM and I have backups, but I'd like to avoid spending some time on recovery.",
        "top_answer": "You must begin with the partition unmounted.  If you can't unmount it (e.g. it's your root partition or something else the system needs to run), use something like System Rescue CD instead.\n\nRun parted, or gparted if you prefer a GUI, and resize the partition to use the extra space.  I prefer gparted as it gives you a nice graphical representation, very similar to the one you've drawn in your question.\nresize2fs /dev/whatever\ne2fsck /dev/whatever (just to find out whether you are on the safe side)\nRemount your partition.\n\nWhile I've never seen this fail, do back up your data first!",
        "url": "https://serverfault.com/questions/509468/how-to-extend-an-ext4-partition-and-filesystem"
    },
    {
        "title": "How do I reattach to Ubuntu Server's 'do-release-upgrade' process?",
        "question": "I accidentally pressed Ctrl+C during Ubuntu Server's do-release-upgrade process. I'd dropped to a shell to compare a .conf file in /etc/. When I pressed Ctrl-C, it asked whether I wanted to try to reattach to the upgrade process, but it failed to do so.\nSo I quit, and now there's a hanging dpkg process which is holding onto the apt lock. This is a virtualised server with no GUI frontend...\nIs it possible to recover the upgrade process, or do I have to kill the dpkg process and start again?",
        "top_answer": "I usually do release upgrades over VPN, so I've tried this a few times. Whenever it updates my openvpn package I lose connection, so I reconnect afterwards.\ndo-release-upgrade starts a backup SSH session on port 1022 and a backup screen session. If you do not have screen installed this will NOT be available.\nYou can get the screen session by running:\nsudo screen -list\nThere is a screen on:\n    2953.ubuntu-release-upgrade-screen-window   (09/13/2012 04:48:02 AM)    (Detached)\n1 Socket in /var/run/screen/S-root.\n\nThen to reattach do:\nsudo screen -d -r root/2953.ubuntu-release-upgrade-screen-window\n\nUsing the previously listed screen after root/\nYou should be back to where you lost connection.",
        "url": "https://serverfault.com/questions/387547/how-do-i-reattach-to-ubuntu-servers-do-release-upgrade-process"
    },
    {
        "title": "Clean way to write complex multi-line string to a variable",
        "question": "I need to write some complex xml to a variable inside a bash script. The xml needs to be readable inside the bash script as this is where the xml fragment will live, it's not being read from another file or source.\nSo my question is this if I have a long string which I want to be human readable inside my bash script what is the best way to go about it?\nIdeally I want:\n\nto not have to escape any of the characters\nhave it break across multiple lines making it human readable\nkeep it's indentation\n\nCan this be done with EOF or something, could anyone give me an example?\ne.g.\nString = <<EOF\n <?xml version=\"1.0\" encoding='UTF-8'?>\n <painting>\n   <img src=\"madonna.jpg\" alt='Foligno Madonna, by Raphael'/>\n   <caption>This is Raphael's \"Foligno\" Madonna, painted in\n   <date>1511</date>-<date>1512</date>.</caption>\n </painting>\nEOF\n",
        "top_answer": "This will put your text into your variable without needing to escape the quotes. It will also handle unbalanced quotes (apostrophes, i.e. '). Putting quotes around the sentinel (EOF) prevents the text from undergoing parameter expansion. The -d'' causes it to read multiple lines (ignore newlines). read is a Bash built-in so it doesn't require calling an external command such as cat.\nIFS='' read -r -d '' String <<\"EOF\"\n<?xml version=\"1.0\" encoding='UTF-8'?>\n <painting>\n   <img src=\"madonna.jpg\" alt='Foligno Madonna, by Raphael'/>\n   <caption>This is Raphael's \"Foligno\" Madonna, painted in\n   <date>1511</date>-<date>1512</date>.</caption>\n </painting>\nEOF\n",
        "url": "https://serverfault.com/questions/72476/clean-way-to-write-complex-multi-line-string-to-a-variable"
    },
    {
        "title": "How to handle security updates within Docker containers?",
        "question": "When deploying applications onto servers, there is typically a separation between what the application bundles with itself and what it expects from the platform (operating system and installed packages) to provide.  One point of this is that the platform can be updated independently of the application. This is useful for example when security updates need to be applied urgently to packages provided by the platform without rebuilding the entire application.\nTraditionally security updates have been applied simply by executing a package manager command to install updated versions of packages on the operating system (for example \"yum update\" on RHEL).  But with the advent of container technology such as Docker where container images essentially bundle both the application and the platform, what is the canonical way of keeping a system with containers up to date? Both the host and containers have their own, independent, sets of packages that need updating and updating on the host will not update any packages inside the containers. With the release of RHEL 7 where Docker containers are especially featured, it would be interesting to hear what Redhat's recommended way to handle security updates of containers is.\nThoughts on a few of the options:\n\nLetting the package manager update packages on the host will not update packages inside the containers.\nHaving to regenerate all container images to apply updates seems to break the separation between the application and the platform (updating the platform requires access to the application build process which generates the Docker images).\nRunning manual commands inside each of the running containers seems cumbersome and changes are at risk of being overwritten the next time containers are updated from the application release artifacts.\n\nSo none of these approaches seems satisfactory.",
        "top_answer": "A Docker image bundles application and \"platform\", that's correct. But usually the image is composed of a base image and the actual application.\nSo the canonical way to handle security updates is to update the base image, then rebuild your application image.",
        "url": "https://serverfault.com/questions/611082/how-to-handle-security-updates-within-docker-containers"
    },
    {
        "title": "Nginx: How do I forward an HTTP request to another port?",
        "question": "What I want to do is: When someone visits http://localhost/route/abc the server responds exactly the same as http://localhost:9000/abc\nNow I configure my Nginx server like this:\nlocation /route {\n    proxy_pass  http://127.0.0.1:9000;\n}\n\nThe HTTP request is dispatched to port 9000 correctly, but the path it receives is http://localhost:9000/route/abc not http://localhost:9000/abc.\nAny suggestions?",
        "top_answer": "I hate the subtlety here, but try adding a / at the end of 9000 like below. It will no longer append \"route\" to the forwarded request now.\nlocation /route {\n    proxy_pass  http://127.0.0.1:9000/;\n}\n",
        "url": "https://serverfault.com/questions/536576/nginx-how-do-i-forward-an-http-request-to-another-port"
    },
    {
        "title": "What does Apache's \"Require all granted\" really do?",
        "question": "I've just update my Apache server to Apache/2.4.6 which is running under Ubuntu 13.04.  I used to have a vhost file that had the following:\n<Directory \"/home/john/development/foobar/web\">\n    AllowOverride All \n</Directory>\n\nBut when I ran that I got a \"Forbidden. You don't have permission to access /\"\nAfter doing a little bit of googling I found out that to get my site working again I needed to add the following line \"Require all granted\" so that my vhost looked like this:\n<Directory \"/home/john/development/foobar/web\">\n    AllowOverride All \n    Require all granted\n</Directory>\n\nI want to know if this is \"safe\" and does not bring in any security issues.  I read on Apache's page that this \"mimics the functionality the was previously provided by the 'Allow from all' and 'Deny from all' directives. This provider can take one of two arguments which are 'granted' or 'denied'. The following examples will grant or deny access to all requests.\"\nBut it didn't say if this was a security issue of some sort or why we now have to do it when in the past you did not have to.",
        "top_answer": "The access control configuration changed in 2.4, and old configurations aren't compatible without some changes.  See here.\nIf your old config was Allow from all (no IP addresses blocked from accessing the service), then Require all granted is the new functional equivilent.",
        "url": "https://serverfault.com/questions/549517/what-does-apaches-require-all-granted-really-do"
    },
    {
        "title": "What type of DNS record is needed to make a subdomain?",
        "question": "I'm making a website, and I need a sub-domain.\nI need to add the new part to my website, but I don't know which type of DNS record to add in the DNS console to point to this new site.\nIs it A or CNAME?",
        "top_answer": "It depends on whether you want to delegate hosting the subdomain off to a different DNS server (or to the same server, but in a different zone file). You delegate a zone when you want some other entity to control it, such as a different IT department or organization.\nIf you do, then you need NS records. If not, A or CNAME records will suffice.\nLet's say you have the domain example.com. You have an A record for www.example.com and you want to create the subdomain info.example.com with www.info.example.com as a host in it. \nDelegation\nIn the this situation, let's further say you have two DNS servers that will be hosting that subdomain. (They could be the same servers that are currently hosting example.com.) In this case, you will create two NS entries in the example.com zone file:\ninfo        IN NS      192.168.2.2\ninfo        IN NS      192.168.2.3\n\nOn those two servers, you will create the info.example.com zone and populate it as you would any other domain.\nwww         IN A      192.168.2.6\n\nNo delegation\nHere, just add an A record in the example.com zone file, using a dot to indicate that you want to create the www.info host in the example.com domain:\nwww.info    IN A       192.168.2.6\n\nUsing CNAME\nThe decision of whether to use a CNAME is independent of the delegation choice. I generally like to use a CNAME for the \"generic\" names which point to specific machine names. For example, I might name my machines using an organizational naming convention such as cartoon characters (daffy, elmer, mickey, etc.) or something bureaucratic (sc01p6-serv) and point the generic names to them. If the IP address of the machine ever changes, I need look in only one place to modify it.\nwww         IN CNAME   sc01p6-serv\nmail        IN CNAME   sc01p6-serv\nsc01p6-serv IN A       192.168.2.6\n",
        "url": "https://serverfault.com/questions/275982/what-type-of-dns-record-is-needed-to-make-a-subdomain"
    },
    {
        "title": "How do you make it obvious you are on a production system?",
        "question": "A few of us at my company have root access on production servers. We are looking for a good way to make it exceedingly clear when we have ssh'd in. \nA few ideas we have had are:\n\nBright red prompt\nAnswer a riddle before getting a shell\nType a random word before getting a shell\n\nWhat are some techniques you guys use to differentiate production systems?",
        "top_answer": "The red prompt is a good idea, which I also use.\nAnother trick is to put a large ASCII-art warning in the /etc/motd file.\nHaving something like this greet you when you log in should get your attention:\n\n _______ _    _ _____  _____   _____  _____            \n|__   __| |  | |_   _|/ ____| |_   _|/ ____|     /\\    \n   | |  | |__| | | | | (___     | | | (___      /  \\   \n   | |  |  __  | | |  \\___ \\    | |  \\___ \\    / /\\ \\  \n   | |  | |  | |_| |_ ____) |  _| |_ ____) |  / ____ \\ \n   |_|  |_|  |_|_____|_____/  |_____|_____/  /_/    \\_\\\n\n\n _____  _____   ____  _____  _    _  _____ _______ _____ ____  _   _ \n|  __ \\|  __ \\ / __ \\|  __ \\| |  | |/ ____|__   __|_   _/ __ \\| \\ | |\n| |__) | |__) | |  | | |  | | |  | | |       | |    | || |  | |  \\| |\n|  ___/|  _  /| |  | | |  | | |  | | |       | |    | || |  | | . ` |\n| |    | | \\ \\| |__| | |__| | |__| | |____   | |   _| || |__| | |\\  |\n|_|    |_|  \\_\\\\____/|_____/ \\____/ \\_____|  |_|  |_____\\____/|_| \\_|\n\n\n __  __          _____ _    _ _____ _   _ ______ \n|  \\/  |   /\\   / ____| |  | |_   _| \\ | |  ____|\n| \\  / |  /  \\ | |    | |__| | | | |  \\| | |__   \n| |\\/| | / /\\ \\| |    |  __  | | | | . ` |  __|  \n| |  | |/ ____ \\ |____| |  | |_| |_| |\\  | |____ \n|_|  |_/_/    \\_\\_____|_|  |_|_____|_| \\_|______|\n\nYou could generate such a warning on this website or you could use the figlet \ncommand.\n\nLike Nicholas Smith suggested in the comments, you could spice things up with some dragons or other animals using the cowsay command.\n\nInstead of using the /etc/motd file, you could also call cowsay or figlet in the .profile file.",
        "url": "https://serverfault.com/questions/322997/how-do-you-make-it-obvious-you-are-on-a-production-system"
    },
    {
        "title": "how do you create an ssh key for another user?",
        "question": "I'm trying to create an ssh key for another user. I'm logged in as root. Can I just edit the files generated by ssh-keygen and change root to the user I want?",
        "top_answer": "You could do that with ssh-keygen, however, remember that the private key is meant to be private to the user so you should be very careful to keep it safe- as safe as the user's password. Or even safer, as the user is not likely to be required to change it upon first login.\nssh-keygen -f anything creates two files in the current directory. anything.pub is the public key, which you could append to the user's ~/.ssh/authorized_keys on any destination server.\nThe other file, just called anything is the private key and therefore should be stored safely for the user. The default location would be ~username/.ssh/id_rsa (here named id_rsa, which is default for rsa keys). Remember that the .ssh directory cannot be readable or writeable by anyone but the user, and the user's home directory cannot be writeable by anyone but the user. Likewise, permissions must be tight on the private key, as well: Read/write for only the user, and the .ssh directory and private keyfile must be owned by the user.\nTechnically you could store the key anywhere. With ssh -i path/to/privatekey you could specify that location, while connecting. Again, proper ownership and permissions are critical and ssh will not work if you don't have them right.",
        "url": "https://serverfault.com/questions/323958/how-do-you-create-an-ssh-key-for-another-user"
    },
    {
        "title": "How does IPv6 subnetting work and how does it differ from IPv4 subnetting?",
        "question": "\nThis is a Canonical Question about IPv6 Subnetting.\nRelated:\n\nHow does IPv4 Subnetting Work?\n\n\nI know a lot about IPv4 Subnetting, and as I prepare to (deploy|work on) an IPv6 network I need to know how much of this knowledge is transferable and what I still need to learn. IPv6 seems at first glance to be much more complex than IPv4. So I would like to know:\n\nIPv6 is 128 bits, so why is /64 the smallest recommended subnet for hosts?\nRelated to this:\n\n\nWhy is it recommended to use /127 for point to point links between routers, and why was it recommended against in the past? Should I change existing router links to use /127?\nWhy would virtual machines be provisioned with less than a /64 worth of addresses?\nAre there other situations in which I would use a subnet smaller than /64?\n\nCan I map directly from IPv4 subnets to IPv6 subnets? For instance, does an IPv4 /24 correspond directly to an IPv6 /56 or /120?\nMy interfaces have several IPv6 addresses. Must the subnet be the same for all of them?\nWhy do I sometimes see a % rather than a / in an IPv6 address and what does it mean?\nAm I wasting too many subnets? Aren't we just going to run out again?\nIn what other major ways is IPv6 subnetting different from IPv4 subnetting?\n",
        "top_answer": "The first thing that should be mentioned about IPv6 subnetting is that a different mode of thought is called for. In IPv4 you usually think about how many addresses you have available and how you can allocate enough of them to each end user. In IPv6 you usually think about how many /64-subnets you have available and how you can allocate them to end users. You almost never worry about how many IP addresses will be used in a given subnet. Except for some special cases like point to point links, each subnet just simply has far more addresses available than it will ever require, so instead you worry only about allocating subnets, not hosts inside them.\nIPv6 subnets are usually /64 because that is required in order for SLAAC (stateless address auto-configuration) to work. Even where SLAAC is not in use, there may be other reasons to use /64. For example, there might be some end user devices out there that just assume /64, or else routing subnets narrower than /64 might be inefficient on some routers because the router implementer has optimized the case of /64 or wider routes in order to save routing table memory.\nWhy is it recommended to use /127 for point to point links?\nFor the specific case of point-to-point links, /127 is recommended instead of /64 in order to avoid a vulnerability where packets addressed to any one of the quadrillions of unused addresses on the subnet cause unwanted neighbour solicitation requests and table entries that could drown a router. Such misaddressed packets may be malicious or accidental. But even if you actually configure a point-to-point link as /127, some people advocate assigning a whole /64 anyway just to be consistent.\nWhy would virtual machines be provisioned with subnets narrower than /64?\nI don't know specifically why virtual machines would be provisioned with subnets narrower than /64. Perhaps because a hosting provider assumed that a server was like an end-user and required only a single /64 subnet, not anticipating that the server would actually be a collection of VMs requiring an internal routing topology? It could be done also simply as a matter of making the addressing plan easier to memorize: the host gets PREFIX::/64, then each VM gets PREFIX:0:NNNN::/96 where NNNN is unique to the VM and the VM can allocate PREFIX:0:NNNN:XXXX:YYYY as it pleases.\nCan I map directly from IPv4 subnets to IPv6 subnets? For instance, does an IPv4 /24 correspond directly to an IPv6 /56 or /120?\nFrom a low-level perspective of how addressing and routing works, the prefix length has the same meaning in IPv6 and IPv4. On that level, you can make an analogy such as \"an IPv4 /16 uses half the bits for the network address and half the bits for the host address, that's like a /64 in IPv6\". But this comparison is not really apt. Strong conventions have emerged in IPv6 which make the divisions of network sizes look somewhat more like the old world of classful networks in IPv4. To be sure, IPv6 didn't reintroduce classful addressing in which the most significant few bits of the address force a particular netmask, but what IPv6 does have is certain [de facto/conventional] standard network sizes:\n\n/64: the basic size of a single subnet: LAN, WAN, block of addresses for web virtual hosts, etc... \"Normal\" subnets are never expected to be any narrower (longer prefix) than /64. No subnets are ever expected to be wider (shorter prefix) than /64 since a /64's worth of host addresses is much more than we can imagine needing.\n/56: a block of 256 basic subnets. Even though current policies permit ISPs to hand out blocks as large as /48 to every end user and still consider their address utilisation well justified, some ISPs may (and already do) choose to allocate a /56 to consumer-grade customers as a compromise between allocation lots of subnets for them and address economy.\n/48: a block of 65536 basic subnets and the recommended size of block that every ISP customer end site should receive.\n/32: the default size of block that most ISPs will receive each time they request more addresses from a regional address registry.\n\nInside service provider and enterprise networks, many more prefix lengths than these 4 can be seen. When looking at the routing tables of routers inside these networks, IPv4 and IPv6 have much in common including most of the way routing works: routes for longer prefixes override covering routes for shorter prefixes, so it is possible to aggregate (make shorter) and drill down (make longer) routes. Like in IPv4, routes can be aggregated or summarized to larger blocks with shorter prefixes in order to minimize the size of routing tables.\nA different question of mapping between IPv4 and IPv6 would be how to harmonize IPv4 and IPv6 assignments on dual-stack machines so that addressing plans can be readily understood. Far that, there are certainly conventions in common use to do this: embed the IPv4 \"subnet number\" into a portion of the IPv6 prefix, either with BCD (e.g. 10.0.234.0/24 becomes 2001:db8:abcd:234::/64) or binary (10.0.234.0/24 becomes 2001:db8:abcd:ea::/64).\nMy interfaces have several IPv6 addresses. Must the subnet be the same for all of them?\nAbsolutely not! IPv6 hosts are expected to be able to be multihomed by having several IP addresses simultaneously that come from different subnets, just like IPv4. If they are autoconfigured with SLAAC then the different subnets might have come from router advertisements from different routers.\nWhy do I sometimes see a % rather than a / in an IPv6 address and what does it mean?\nYou would not see one instead of the other. They have different meanings. A slash denotes a prefix (subnet), meaning a block of addresses that all start with the same n bits. An address without a slash is a host address. You may think of such an address as having an implied /128 at the end, meaning all 128 bits are specified.\nThe percent sign accompanies a link-local address. In IPv6, every interface has a link-local address in addition to any other IP addresses it might have. But the thing is, link-local addresses are always, without exception, in the fe80::/10 block. But if we attempt to talk to a peer using a link local address and the local host has multiple interfaces, how are we to know which interface to use to talk to this peer? Normally the routing table tells us which interface to use for a particular prefix, but here it will tell us than fe80::/10 is reachable via every interface.\nThe answer is that we must tell it which interface to use using the syntax address%interface. For example, fe80::1234:5678:8765:4321%eth0.\nAm I wasting too many subnets? Aren't we just going to run out again?\nNobody knows. Who can tell the future?\nBut consider this. In IPv6 the number of available subnets is the square of the number of available individual addresses in IPv4. That's really quite a lot. No, I mean really quite a lot!\nBut still: we are automatically handing out a /32 to any ISP who requests one, we are handing out a /48 to every single ISP customer. Perhaps we're exaggerating and we will squander IPv6 after all. But there is a provision for this: Only one eighth of the IPv6 space has been made available for use so far: 2000::/3. The idea is that if we make a horrible mess of the first eighth and we have to drastically revise the liberal allocation policies, we get to try 7 more times before we're in trouble.\nAnd finally: IPv6 doesn't have to last forever. Perhaps it will have a longer lifetime than IPv4 (an impressive lifetime already and it's not over) but like every technology it will someday stop mattering. We only need to make it until then.",
        "url": "https://serverfault.com/questions/426183/how-does-ipv6-subnetting-work-and-how-does-it-differ-from-ipv4-subnetting"
    },
    {
        "title": "Certification authority root certificate expiry and renewal",
        "question": "In 2004, I set up a small certification authority using OpenSSL on Linux and the simple management scripts provided with OpenVPN. In accordance with the guides I found at the time, I set the validity period for the root CA certificate to 10 years. Since then, I have signed many certificates for OpenVPN tunnels, web sites and e-mail servers, all of which also have a validity period of 10 years (this may have been wrong, but I didn't know better at the time).\nI have found many guides about setting up a CA, but only very little information about its management, and in particular, about what has to be done when the root CA certificate expires, which will happen some time in 2014. So I have the following questions:\n\nWill the certificates that have a validity period extending after the expiry of the root CA certificate become invalid as soon as the latter expires, or will they continue to be valid (because they were signed during the validity period of the CA certificate)?\nWhat operations are needed to renew the root CA certificate and ensure a smooth transition over its expiry?\n\n\nCan I somehow re-sign the current root CA certificate with a different validity period, and upload the newly-signed cert to clients so that client certificates remain valid?\nOr do I need to replace all client certificates with new ones signed by a new root CA certificate?\n\nWhen should the root CA certificate be renewed? Close to expiry, or a reasonable time before expiry?\nIf the renewal of the root CA certificate becomes a major piece of work, what can I do better now to ensure a smoother transition at the next renewal (short of setting the validity period to 100 years, of course)?\n\nThe situation is made slightly more complicated by the fact that my only access to some of the clients is through an OpenVPN tunnel that uses a certificate signed by the current CA certificate, so if I have to replace all client certs, I will need to copy the new files to the client, restart the tunnel, cross my fingers and hope that it comes up afterwards.",
        "top_answer": "Keeping the same private key on your root CA allows for all certificates to continue to validate successfully against the new root; all that's required of you is to trust the new root.\nThe certificate signing relationship is based on a signature from the private key; keeping the same private key (and, implicitly, the same public key) while generating a new public certificate, with a new validity period and any other new attributes changed as needed, keeps the trust relationship in place.  CRLs, too, can continue over from the old cert to the new, as they are, like certificates, signed by the private key.\n\nSo, let's verify!\nMake a root CA:\nopenssl req -new -x509 -keyout root.key -out origroot.pem -days 3650 -nodes\n\nGenerate a child certificate from it:\nopenssl genrsa -out cert.key 1024\nopenssl req -new -key cert.key -out cert.csr\n\nSign the child cert:\nopenssl x509 -req -in cert.csr -CA origroot.pem -CAkey root.key -create_serial -out cert.pem\nrm cert.csr\n\nAll set there, normal certificate relationship.  Let's verify the trust:\n# openssl verify -CAfile origroot.pem -verbose cert.pem\ncert.pem: OK\n\nOk, so, now let's say 10 years passed.  Let's generate a new public certificate from the same root private key.\nopenssl req -new -key root.key -out newcsr.csr\nopenssl x509 -req -days 3650 -in newcsr.csr -signkey root.key -out newroot.pem\nrm newcsr.csr\n\nAnd.. did it work?\n# openssl verify -CAfile newroot.pem -verbose cert.pem\ncert.pem: OK\n\nBut.. why?  They're different files, right?\n# sha1sum newroot.pem\n62577e00309e5eacf210d0538cd79c3cdc834020  newroot.pem\n# sha1sum origroot.pem\nc1d65a6cdfa6fc0e0a800be5edd3ab3b603e1899  origroot.pem\n\nYes, but, that doesn't mean that the new public key doesn't cryptographically match the signature on the certificate.  Different serial numbers, same modulus:\n# openssl x509 -noout -text -in origroot.pem\n        Serial Number:\n            c0:67:16:c0:8a:6b:59:1d\n...\n            RSA Public Key: (1024 bit)\n                Modulus (1024 bit):\n                    00:bd:56:b5:26:06:c1:f6:4c:f4:7c:14:2c:0d:dd:\n                    3c:eb:8f:0a:c0:9d:d8:b4:8c:b5:d9:c7:87:4e:25:\n                    8f:7c:92:4d:8f:b3:cc:e9:56:8d:db:f7:fd:d3:57:\n                    1f:17:13:25:e7:3f:79:68:9f:b5:20:c9:ef:2f:3d:\n                    4b:8d:23:fe:52:98:15:53:3a:91:e1:14:05:a7:7a:\n                    9b:20:a9:b2:98:6e:67:36:04:dd:a6:cb:6c:3e:23:\n                    6b:73:5b:f1:dd:9e:70:2b:f7:6e:bd:dc:d1:39:98:\n                    1f:84:2a:ca:6c:ad:99:8a:fa:05:41:68:f8:e4:10:\n                    d7:a3:66:0a:45:bd:0e:cd:9d\n# openssl x509 -noout -text -in newroot.pem\n        Serial Number:\n            9a:a4:7b:e9:2b:0e:2c:32\n...\n            RSA Public Key: (1024 bit)\n                Modulus (1024 bit):\n                    00:bd:56:b5:26:06:c1:f6:4c:f4:7c:14:2c:0d:dd:\n                    3c:eb:8f:0a:c0:9d:d8:b4:8c:b5:d9:c7:87:4e:25:\n                    8f:7c:92:4d:8f:b3:cc:e9:56:8d:db:f7:fd:d3:57:\n                    1f:17:13:25:e7:3f:79:68:9f:b5:20:c9:ef:2f:3d:\n                    4b:8d:23:fe:52:98:15:53:3a:91:e1:14:05:a7:7a:\n                    9b:20:a9:b2:98:6e:67:36:04:dd:a6:cb:6c:3e:23:\n                    6b:73:5b:f1:dd:9e:70:2b:f7:6e:bd:dc:d1:39:98:\n                    1f:84:2a:ca:6c:ad:99:8a:fa:05:41:68:f8:e4:10:\n                    d7:a3:66:0a:45:bd:0e:cd:9d\n\n\nLet's go a little further to verify that it's working in real world certificate validation.\nFire up an Apache instance, and let's give it a go (debian file structure, adjust as needed):\n# cp cert.pem /etc/ssl/certs/\n# cp origroot.pem /etc/ssl/certs/\n# cp newroot.pem /etc/ssl/certs/\n# cp cert.key /etc/ssl/private/\n\nWe'll set these directives on a VirtualHost listening on 443 - remember, the newroot.pem root certificate didn't even exist when cert.pem was generated and signed.\nSSLEngine on\nSSLCertificateFile /etc/ssl/certs/cert.pem\nSSLCertificateKeyFile /etc/ssl/private/cert.key\nSSLCertificateChainFile /etc/ssl/certs/newroot.pem\n\nLet's check out how openssl sees it:\n# openssl s_client -showcerts -CAfile newroot.pem -connect localhost:443\n\nCertificate chain\n 0 s:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=server.lan\n   i:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=root\n-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n 1 s:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=root\n   i:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=root\n-----BEGIN CERTIFICATE-----\nMIICHzCCAYgCCQCapHvpKw4sMjANBgkqhkiG9w0BAQUFADBUMQswCQYDVQQGEwJB\n...\n-----END CERTIFICATE-----\n(this should match the actual contents of newroot.pem)\n...\nVerify return code: 0 (ok)\n\nOk, and how about a browser using MS's crypto API?  Gotta trust the root, first, then it's all good, with the new root's serial number:\n\nAnd, we should still be working with the old root, too.  Switch Apache's config around:\nSSLEngine on\nSSLCertificateFile /etc/ssl/certs/cert.pem\nSSLCertificateKeyFile /etc/ssl/private/cert.key\nSSLCertificateChainFile /etc/ssl/certs/origroot.pem\n\nDo a full restart on Apache, a reload won't switch the certs properly.\n# openssl s_client -showcerts -CAfile origroot.pem -connect localhost:443\n\nCertificate chain\n 0 s:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=server.lan\n   i:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=root\n-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n 1 s:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=root\n   i:/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd/CN=root\n-----BEGIN CERTIFICATE-----\nMIIC3jCCAkegAwIBAgIJAMBnFsCKa1kdMA0GCSqGSIb3DQEBBQUAMFQxCzAJBgNV\n...\n-----END CERTIFICATE-----\n(this should match the actual contents of origroot.pem)\n...\nVerify return code: 0 (ok)\n\nAnd, with the MS crypto API browser, Apache's presenting the old root, but the new root's still in the computer's trusted root store.  It'll automatically find it and validate the cert against the trusted (new) root, despite Apache presenting a different chain (the old root).  After stripping the new root from trusted roots and adding the original root cert, all is well:\n\n\nSo, that's it!  Keep the same private key when you renew, swap in the new trusted root, and it pretty much all just works.  Good luck!",
        "url": "https://serverfault.com/questions/306345/certification-authority-root-certificate-expiry-and-renewal"
    },
    {
        "title": "Curl: disable certificate verification",
        "question": "I am developing and I need to access https://localhost. I know the certificate will not match. I just want curl to ignore that. Currently it gives me the following error message:\ncurl: (51) SSL peer certificate or SSH remote key was not OK\n\nIs it possible to tell curl to perform the access anyway?",
        "top_answer": "Yeah, you can do that. From curl --help or man curl:\n\n-k, --insecure\n(SSL) This option explicitly allows curl to perform \"insecure\" SSL\n  connections and transfers. All SSL connections are attempted to be\n  made secure by using the CA certificate bundle installed by default.\n  This makes all connections considered \"insecure\" fail unless -k,\n  --insecure is used.\nSee this online resource for further details:\n  http://curl.haxx.se/docs/sslcerts.html\n",
        "url": "https://serverfault.com/questions/469824/curl-disable-certificate-verification"
    },
    {
        "title": "Why do we use a OS Base Image with Docker if containers have no Guest OS?",
        "question": "I've just started to study Docker and there's something that's being quite confusing for me. As I've read on Docker's website a container is different from a virtual machine. As I understood a container is just a sandbox inside of which an entire isolated file system is run.\nI've also read that a container doesn't have a Guest OS installed. Instead it relies on the underlying OS Kernel.\nAll of that is fine. What I'm confused is that there are Docker images named after operating systems. We see images like Ubuntu, Debian, Fedora, CentOS and so on.\nMy point is: what are those images, really? How is it different creating a container based on the Debian image than creating a Virtual Machine and installing Debian?\nI thought containers had no Guest OS installed, but when we create images we base them on some image named after one OS.\nAlso, in examples I saw when we do docker run ubuntu echo \"hello world\",\nit seems we are spinning up a VM with Ubuntu and making it run the command echo \"hello world\". \nIn the same way when we do docker run -it ubuntu /bin/bash, it seems we are spinning up a VM with Ubuntu and accessing it using command line.\nAnyway, what are those images named after operating systems all about? How different is it to run a container with one of those images and spinning up a VM with the corresponding Guest OS?\nIs the idea that we just share the kernel with the host OS (and consequently we have access to the underlying machine hardware resources, without the need to virtualize hardware), but still use the files and binaries of each different system on the containers in order to support whatever application we want to run?",
        "top_answer": "Since all Linux distributions run the same (yup, it's a bit simplified) Linux kernel and differ only in userland software, it's pretty easy to simulate a different distribution environment - by just installing that userland software and pretending it's another distribution. Being specific, installing CentOS container inside Ubuntu OS will mean that you will get the userland from CentOS, while still running the same kernel, not even another kernel instance.\nSo lightweight virtualization is like having isolated compartments within same OS. Au contraire real virtualization is having another full-fledged OS inside host OS. That's why docker cannot run FreeBSD or Windows inside Linux.\nIf that would be easier, you can think docker is kind of very sophisticated and advanced chroot environment.",
        "url": "https://serverfault.com/questions/755607/why-do-we-use-a-os-base-image-with-docker-if-containers-have-no-guest-os"
    },
    {
        "title": "How to tell if a Linux system is big endian or little endian?",
        "question": "I know that certain processors are Big Endian and others are Little Endian. But is there a command, bash script, python script or series of commands that can be used at the command line to determine if a system is Big Endian or Little Endian? Something like:\nif <some code> then\n    echo Big Endian\nelse\n    echo Little Endian\nfi\n\nOr is it more simple to just determine what processor the system is using and go with that to determine its Endianess?",
        "top_answer": "On a Big Endian-System (Solaris on SPARC)\n$ echo -n I | od -to2 | head -n1 | cut -f2 -d\" \" | cut -c6 \n\n0\nOn a little endian system (Linux on x86)\n$ echo -n I | od -to2 | head -n1 | cut -f2 -d\" \" | cut -c6 \n\n1\n\nThe solution above is clever and works great for Linux *86 and Solaris Sparc.  \nI needed a shell-only (no Perl) solution that also worked on AIX/Power and HPUX/Itanium.  Unfortunately the last two don't play nice: AIX reports \"6\" and HPUX gives an empty line.\nUsing your solution, I was able to craft something that worked on all these Unix systems:\n$ echo I | tr -d [:space:] | od -to2 | head -n1 | awk '{print $2}' | cut -c6\nRegarding the Python solution someone posted, it does not work in Jython because the JVM treats everything as Big.  If anyone can get it to work in Jython, please post!\nAlso, I found this, which explains the endianness of various platforms.  Some hardware can operate in either mode depending on what the O/S selects:  http://labs.hoffmanlabs.com/node/544\n\nIf you're going to use awk this line can be simplified to:\necho -n I | od -to2 | awk '{ print substr($2,6,1); exit}'\n\nFor small Linux boxes that don't have 'od' (say OpenWrt) then try 'hexdump':\necho -n I | hexdump -o | awk '{ print substr($2,6,1); exit}'\n",
        "url": "https://serverfault.com/questions/163487/how-to-tell-if-a-linux-system-is-big-endian-or-little-endian"
    },
    {
        "title": "How do you increase a KVM guest's disk space?",
        "question": "I setup an Ubuntu guest on a CentOS KVM host with initially 6GB of disk space. How do I go about increasing the Ubuntu guest's disk space from the command line?\nEDIT #1: I'm using a disk image file (qemu).",
        "top_answer": "\nstop the VM\nrun qemu-img resize vmdisk.img +10G to increase image size by 10Gb\nstart the VM, resize the partitions and LVM structure within it normally\n",
        "url": "https://serverfault.com/questions/324281/how-do-you-increase-a-kvm-guests-disk-space"
    },
    {
        "title": "Is there a way to view the members of an Active Directory group if you aren't a domain admin and can't log into to a domain controller?",
        "question": "Is there a way to view the members of an Active Directory group if you aren't a domain admin and can't log into to a domain controller?",
        "top_answer": "Absolutely. From a computer that's a member of the domain, open a command-prompt and run a:\nNET GROUP \"group name\" /DOMAIN\n\nUnless your administrators have changed the stock permissions on the group object you will be able to view the membership that way.\nYou can use AD Users and Computers even if you're not an administrator, but this, at least, can be done w/o installing anything.",
        "url": "https://serverfault.com/questions/22182/is-there-a-way-to-view-the-members-of-an-active-directory-group-if-you-arent-a"
    },
    {
        "title": "Can I make `find` return non-0 when no matching files are found?",
        "question": "Even when /tmp has no file called something, searching for it with find will return 0:\n  $ find /tmp -name something \n  $ echo $?\n  0\n\nHow can I get a non-zero exit status when find does not find anything?",
        "top_answer": "find /tmp -name something | grep .\n\nThe return status will be 0 when something is found, and non-zero otherwise.\nEDIT: Changed from egrep '.*' to the much simpler grep ., since the result is the same.",
        "url": "https://serverfault.com/questions/225798/can-i-make-find-return-non-0-when-no-matching-files-are-found"
    },
    {
        "title": "How to copy file preserving directory path in Linux?",
        "question": "I have Eclipse projects and \".project\" file in them, the directory structure looks like 'myProject/.project'. I want to copy these '.project' files to another directory, but I want the enclosing directory name to be preserved.\nLet's say I have 'a/myProject/.project', I want to copy 'myProject/.project' to 'b', so it be 'b/myProject/.project', but 'b/myProject' doesn't exist. When I try in a: \n cp -r ./myProject/.project ../b\n\nit copies only '.project' file itself, without 'myProject' directory. Please advise.",
        "top_answer": "The switch you need is --parents, e.g.:\njim@prometheus:~$ cp --parents test/1/.moo test2/\njim@prometheus:~$ ls -la test2/\ntotal 42\ndrwxr-xr-x   3 jim jim    72 2010-09-14 09:32 .\ndrwxr-xr-x 356 jim jim 43136 2010-09-14 09:32 ..\ndrwxr-xr-x   3 jim jim    72 2010-09-14 09:32 test\njim@prometheus:~$ ls -la test2/test/1/.moo\n-rw-r--r-- 1 jim jim 0 2010-09-14 09:32 test2/test/1/.moo\n",
        "url": "https://serverfault.com/questions/180853/how-to-copy-file-preserving-directory-path-in-linux"
    },
    {
        "title": "How do I force sync the time on Windows Workstation or Server?",
        "question": "What is the command to sync a Windows workstation or server to its configured time source?",
        "top_answer": "As Kyle said w32tm /resync is the modern way to do this. See this Link to Microsoft Knowledgebase (KB 307897) for more information on the w32tm command.\nThere is also net time which is an older version but perhaps easier.",
        "url": "https://serverfault.com/questions/294787/how-do-i-force-sync-the-time-on-windows-workstation-or-server"
    },
    {
        "title": "How can I monitor hard disk load on Linux?",
        "question": "Is there a good command line utility to monitor hard disk load on linux? Something like top but then monitoring disk activity i.s.o. cpu usage.\nMore specifically, I suspect that for some (heavy load) servers after several optimizations on various parts of the program(s) that run on it, right now the bottleneck is simply the logging to files on the disk. But I find it very difficult to assess how much traffic the servers can handle.\nMy ideal tool would be something that prints \"You're using 35% of your disk bandwidth right now\". Any ideas?",
        "top_answer": "You can get a pretty good measure of this using the iostat tool.\n% iostat -dx /dev/sda 5\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsda               0.78    11.03    1.19    2.82    72.98   111.07    45.80     0.13   32.78   1.60   0.64\n\nThe disk utilisation is listed in the last column. This is defined as\n\nPercentage of CPU time during which I/O requests were issued to the device\n  (band-width utilization for the device). Device saturation\n  occurs when  this  value  is  close to 100%.\n",
        "url": "https://serverfault.com/questions/9428/how-can-i-monitor-hard-disk-load-on-linux"
    },
    {
        "title": "How to forcibly close a socket in TIME_WAIT?",
        "question": "I run a particular program on linux which sometimes crashes.  If you open it quickly after that, it listens on socket 49201 instead of 49200 as it did the first time.  netstat reveals that 49200 is in a TIME_WAIT state.\nIs there a program you can run to immediately force that socket move out of the TIME_WAIT state?",
        "top_answer": "Actually there is a way to kill a connection - killcx. They claim it works in any state of the connection (which I have not verified). You need to know the interface where communication happens though, it seems to assume eth0 by default.\nUPDATE: another solution is cutter which comes in some linux distros' repositories.",
        "url": "https://serverfault.com/questions/329845/how-to-forcibly-close-a-socket-in-time-wait"
    },
    {
        "title": "How can I show user's privileges in MySQL?",
        "question": "I know that I can set user's privileges in the following simple way:\ngrant all on [database name].[table name] to [user name]@[host name];\n\nBut how can I see existing privileges?\nI need to see data similar to those which are used in grant. In other words I want to know that a given user has a given access to a given table of a given database from a given host.\nHow can I get it? ",
        "top_answer": "The command SHOW GRANTS [FOR user] is what you're looking for. See the SHOW GRANTS Statement section for more detail.",
        "url": "https://serverfault.com/questions/117525/how-can-i-show-users-privileges-in-mysql"
    },
    {
        "title": "Does Windows have a built-in ZIP command for the command line?",
        "question": "Since Windows Explorer (since at least Windows XP) has some basic support for ZIP files, it seems like there should be a command-line equivalent, but I can't seem to find any sign of one.\nDoes Windows (XP, Vista, 7, 8, 2003, 2008, 2013) ship with a built-in command-line zip tool, or do I need to stick with third-party tools?",
        "top_answer": "It's not built into Windows, but it's in the Resource Kit Tools as COMPRESS,\nC:\\>compress /?\n\nSyntax:\n\nCOMPRESS [-R] [-D] [-S] [ -Z | -ZX ] Source Destination\nCOMPRESS -R [-D] [-S] [ -Z | -ZX ] Source [Destination]\n\nDescription:\nCompresses one or more files.\n\nParameter List:\n-R Rename compressed files.\n\n-D Update compressed files only if out of date.\n\n-S Suppress copyright information.\n\n-ZX LZX compression. This is default compression.\n\n-Z MS-ZIP compression.\n\nSource Source file specification. Wildcards may be\nused.\n\nDestination Destination file | path specification.\nDestination may be a directory. If Source is\nmultiple files and -r is not specified,\nDestination must be a directory.\n\nExamples:\nCOMPRESS temp.txt compressed.txt\nCOMPRESS -R *.*\nCOMPRESS -R *.exe *.dll compressed_dir\n",
        "url": "https://serverfault.com/questions/39071/does-windows-have-a-built-in-zip-command-for-the-command-line"
    },
    {
        "title": "FreeBSD performance tuning: Sysctl parameter, loader.conf, kernel",
        "question": "I wanted to share knowledge of tuning FreeBSD via sysctl.conf / loader.conf\n/ KENCONF / etc. It was initially based on Igor Sysoev's (author of nginx)\npresentation about FreeBSD tuning up to 100,000-200,000 active connections.\nNewer versions of FreeBSD can handle much much more.\nTunings are for FreeBSD7 - FreeBSD-CURRENT. Since 7.2 amd64 some of them are\ntuned well by default.  Prior 7.0 some of them are boot only (set via\n/boot/loader.conf) or do not exist at all.\nsysctl.conf:\n# No zero mapping feature\n# May break wine\n# (There are also reports about broken samba3)\n#security.bsd.map_at_zero=0\n\n# Servers with threading software apache2 / Pound may want to rise following sysctl\n#kern.threads.max_threads_per_proc=4096\n\n# Max backlog size\n# Note Application can still limit it by passing second argument to listen(2) syscall\n# Note: Listen queue be monitored via `netstat -Lan`\nkern.ipc.somaxconn=4096\n\n# Shared memory\n# Note: Only FreeBSD 7.2+ can use shared memory > 2Gb\n#kern.ipc.shmmax=2147483648\n\n# Sockets\nkern.ipc.maxsockets=204800\n\n# Mbuf 2k clusters (on amd64 7.2+ 25600 is default)\n# Note: defaults for other variables depend on this variable, for example `tcpreass`\n# Note: FreeBSD-7 and older: For such high value vm.kmem_size must be increased to 3G\nkern.ipc.nmbclusters=262144\n\n# Jumbo pagesize(_SC_PAGESIZE)/9k/16k clusters\n# Used as general packet storage for jumbo frames on some network cards\n# Can be monitored via `netstat -m`\n#kern.ipc.nmbjumbop=262144\n#kern.ipc.nmbjumbo9=65536\n#kern.ipc.nmbjumbo16=32768\n\n# For lower latency you can decrease schedulers maximum time slice\n# default: stathz/10 (~ 13)\nkern.sched.slice=1\n\n# Increase max command-line length showed in `ps` (e.g for Tomcat/Java)\n# Default is PAGE_SIZE / 16 or 256 on x86\n# This avoids commands to be presented as [executable] in `ps`\n# For more info see: http://www.freebsd.org/cgi/query-pr.cgi?pr=120749\nkern.ps_arg_cache_limit=4096\n\n# Every socket is a file, so increase them\nkern.maxfiles=204800\nkern.maxfilesperproc=200000\nkern.maxvnodes=200000\n\n# On some systems HPET is almost 2 times faster than default ACPI-fast\n# Useful on systems with lots of clock_gettime / gettimeofday calls\n# See http://old.nabble.com/ACPI-fast-default-timecounter,-but-HPET-83--faster-td23248172.html\n# After revision 222222 HPET became default: http://svnweb.freebsd.org/base?view=revision&revision=222222\n#kern.timecounter.hardware=HPET\n\n\n# Small receive space, only usable on http-server\n# Note: fileservers should increase it to 65535 or even more\n#net.inet.tcp.recvspace=8192\n\n# This is useful on Fat-Long-Pipes\n#kern.ipc.maxsockbuf=10485760\n#net.inet.tcp.recvbuf_max=10485760\n#net.inet.tcp.recvbuf_inc=65535\n\n# Small send space is useful for http servers that serve small files \n# Note: Autotuned since 7.x\n#net.inet.tcp.sendspace=16384\n\n# This is useful on Fat-Long-Pipes\n#net.inet.tcp.sendbuf_max=10485760\n#net.inet.tcp.sendbuf_inc=65535\n\n# Turn off send/receive autotuning if think you know better.\n#net.inet.tcp.recvbuf_auto=0\n#net.inet.tcp.sendbuf_auto=0\n\n# This should be enabled if you going to use big spaces (>64k)\n# Also timestamp field is useful when using syncookies\nnet.inet.tcp.rfc1323=1\n# Turn this off on high-speed, lossless connections (LAN 1Gbit+)\n#net.inet.tcp.delayed_ack=0\n\n# This feature is useful if you are serving data over modems, Gigabit Ethernet, \n# or even high speed WAN links (or any other link with a high bandwidth delay product), \n# especially if you are also using window scaling or have configured a large send window.\n# Automatically disables on small RTT ( http://www.freebsd.org/cgi/cvsweb.cgi/src/sys/netinet/tcp_subr.c?#rev1.237 )\n# This sysctl was removed in 10-CURRENT:\n# See: http://www.mail-archive.com/[email\u00a0protected]/msg06178.html\n#net.inet.tcp.inflight.enable=0\n\n# TCP slowstart algorithm tunings\n# Here we are assuming VERY uncongested network\n# Note: Only takes effect if net.inet.tcp.rfc3390 is set to 0,\n#       otherwise formula taken from http://tools.ietf.org/html/rfc3390\n#net.inet.tcp.slowstart_flightsize=10\n#net.inet.tcp.local_slowstart_flightsize=100\n\n# Disable randomizing of ports to avoid false RST\n# Before use check SA here www.bsdcan.org/2006/papers/ImprovingTCPIP.pdf\n# Note: Port randomization autodisables at high connection rates\n#net.inet.ip.portrange.randomized=0\n\n# Increase portrange\n# For outgoing connections only. Good for seed-boxes and ftp servers.\nnet.inet.ip.portrange.first=1024\nnet.inet.ip.portrange.last=65535\n\n# Dtops route cache degradation during a DDoS.\n# http://www.freebsd.org/doc/en/books/handbook/securing-freebsd.html\n#net.inet.ip.rtexpire=2\nnet.inet.ip.rtminexpire=2\nnet.inet.ip.rtmaxcache=1024\n\n# Security\nnet.inet.ip.redirect=0\nnet.inet.ip.sourceroute=0\nnet.inet.ip.accept_sourceroute=0\nnet.inet.icmp.maskrepl=0\nnet.inet.icmp.log_redirect=0\nnet.inet.icmp.drop_redirect=1\nnet.inet.tcp.drop_synfin=1\n# \n# There is also good example of sysctl.conf with comments:\n# http://www.thern.org/projects/sysctl.conf\n#\n# icmp may NOT rst, helpful for those pesky spoofed \n# icmp/udp floods that end up taking up your outgoing\n# bandwidth/ifqueue due to all that outgoing RST traffic.\n#\n#net.inet.tcp.icmp_may_rst=0\n\n# Security\n# Do not send responses on attempts to connect to the closed ports\n#net.inet.udp.blackhole=1\n#net.inet.tcp.blackhole=2\n\n# IPv6 Security\n# For more info see http://www.fosslc.org/drupal/content/security-implications-ipv6\n# Disable Node info replies\n# To see this vulnerability in action run `ping6 -a sglAac ::1` or `ping6 -w ::1` on unprotected node\nnet.inet6.icmp6.nodeinfo=0\n# Turn on IPv6 privacy extensions\n# For more info see proposal http://unix.derkeiler.com/Mailing-Lists/FreeBSD/net/2008-06/msg00103.html\nnet.inet6.ip6.use_tempaddr=1\nnet.inet6.ip6.prefer_tempaddr=1\n# Disable ICMP redirect\nnet.inet6.icmp6.rediraccept=0\n# Disable acceptation of RA and auto link-local generation if you don't use them\n#net.inet6.ip6.accept_rtadv=0\n#net.inet6.ip6.auto_linklocal=0\n\n# Increases default TTL\n# Default is 64\n#net.inet.ip.ttl=128\n\n# Lessen max segment life to conserve resources\n# ACK waiting time in milliseconds\n# (default: 30000. RFC from 1979 recommends 120000)\nnet.inet.tcp.msl=5000\n\n# Max number of time-wait sockets\nnet.inet.tcp.maxtcptw=200000\n# Don't use tw on local connections\n# As of 15 Apr 2009. Igor Sysoev says that nolocaltimewait has some buggy implementaion.\n# So disable it or now till get fixed\n#net.inet.tcp.nolocaltimewait=1\n\n# FIN_WAIT_2 state fast recycle\nnet.inet.tcp.fast_finwait2_recycle=1\n\n# Time before tcp keepalive probe is sent\n# default is 2 hours (7200000)\n#net.inet.tcp.keepidle=60000\n\n# Use HTCP congestion control (don't forget to load cc_htcp kernel module)\nnet.inet.tcp.cc.algorithm=htcp\n\n# Should be increased until net.inet.ip.intr_queue_drops is zero\nnet.inet.ip.intr_queue_maxlen=4096\n\n# Protocol decoding in interrupt thread.\n# If you have NIC that automatically sets flow_id then it's better to not\n# use direct_force, and use advantages of multithreaded netisr(9)\n# If you have Yandex drives you better off with `net.isr.direct_force=1` and\n# `net.inet.tcp.read_locking=0` otherwise you may run into some TCP related\n# problems.\n# Note: If you have old NIC that don't set flow_ids you may need to\n# patch `ip_input` to manually set FLOW_ID via `nh_m2flow`.\n#\n# FreeBSD 8+\n#net.isr.direct=1\n#net.isr.direct_force=1\n# In FreeBSD 9+ it was renamed to\n#net.isr.dispatch=direct\n\n# This is for routers only\n#net.inet.ip.forwarding=1\n#net.inet.ip.fastforwarding=1\n\n# This speed ups dummynet when channel isn't saturated\nnet.inet.ip.dummynet.io_fast=1\n# Increase dummynet(4) hash\n#net.inet.ip.dummynet.hash_size=65535\n#net.inet.ip.dummynet.max_chain_len=8\n\n# Should be increased when you have A LOT of files on server \n# (Increase until vfs.ufs.dirhash_mem becomes lower)\nvfs.ufs.dirhash_maxmem=67108864\n\n# Note from commit http://svn.freebsd.org/base/head@211031 :\n# For systems with RAID volumes and/or virtualization environments, where\n# read performance is very important, increasing this sysctl tunable to 32\n# or even more will demonstratively yield additional performance benefits.\nvfs.read_max=32\n\n# Explicit Congestion Notification\n# (See http://en.wikipedia.org/wiki/Explicit_Congestion_Notification)\nnet.inet.tcp.ecn.enable=1\n\n# Flowtable - flow caching mechanism\n# Useful for routers\n#net.inet.flowtable.enable=1\n#net.inet.flowtable.nmbflows=65535\n\n# IPFW dynamic rules and timeouts tuning\n# Increase dyn_buckets till net.inet.ip.fw.curr_dyn_buckets is lower\nnet.inet.ip.fw.dyn_buckets=65536\nnet.inet.ip.fw.dyn_max=65536\nnet.inet.ip.fw.dyn_ack_lifetime=120\nnet.inet.ip.fw.dyn_syn_lifetime=10\nnet.inet.ip.fw.dyn_fin_lifetime=2\nnet.inet.ip.fw.dyn_short_lifetime=10\n# Make packets pass firewall only once when using dummynet\n# i.e. packets going thru pipe are passing out from firewall with accept\n#net.inet.ip.fw.one_pass=1\n\n# shm_use_phys Wires all shared pages, making them unswappable\n# Use this to lessen Virtual Memory Manager's work when using Shared Mem.\n# Useful for databases\n#kern.ipc.shm_use_phys=1\n\n# ZFS\n# Enable prefetch. Useful for sequential load type i.e fileserver.\n# FreeBSD sets vfs.zfs.prefetch_disable to 1 on any i386 systems and \n# on any amd64 systems with less than 4GB of available memory\n# See: http://old.nabble.com/Samba-read-speed-performance-tuning-td27964534.html\n#vfs.zfs.prefetch_disable=0\n\n# On highload servers you may notice following message in dmesg:\n# \"Approaching the limit on PV entries, consider increasing either the\n# vm.pmap.shpgperproc or the vm.pmap.pv_entry_max tunable\"   \nvm.pmap.shpgperproc=2048\n\nloader.conf:\n# Accept filters for data, http and DNS requests\n# Useful when your software creates process/thread on each request (i.e. apache)\n# Note: DNS accf available on 8.0+\n# Note: In case of badly written software this can increase performance, \n# but I still would recommend against using accept filters in production because of\n# their opacity - they really break abstractions. Also it's not trivial to debug/monitor\n# their state.\n#accf_data_load=\"YES\" \n#accf_http_load=\"YES\"\n#accf_dns_load=\"YES\"\n\n# Async IO system calls\naio_load=\"YES\"\n\n# Linux specific devices in /dev\n# As for 8.1 it only /dev/full \n#lindev_load=\"YES\"\n\n# Adds NCQ support in FreeBSD\n# WARNING! all ad[0-9]+ devices will be renamed to ada[0-9]+\n# 8.0+ only\n#ahci_load=\"YES\"\n#siis_load=\"YES\"\n\n# FreeBSD 9+\n# New Congestion Control for FreeBSD\ncc_htcp_load=\"YES\"\n#cc_cubic_load=\"YES\"\n\n# Increase kernel memory size to 3G. \n#\n# Use ONLY if you have KVA_PAGES in kernel configuration, and you have more than 3G RAM \n# Otherwise panic will happen on next reboot!\n#\n# It's required for high buffer sizes: kern.ipc.nmbjumbop, kern.ipc.nmbclusters, etc\n# Useful on highload stateful firewalls, proxies or ZFS fileservers\n# (FreeBSD 7.2+ amd64 users: Check that current value is lower!)\n#vm.kmem_size=\"3G\"\n\n# If you have really busy forking webserver (i.e. apache13) you may run out of processes\n#kern.maxproc=10000\n\n# If your server has lots of swap (>4Gb) you should increase following value\n# according to http://lists.freebsd.org/pipermail/freebsd-hackers/2009-October/029616.html\n# Otherwise you'll be getting errors\n# \"kernel: swap zone exhausted, increase kern.maxswzone\"\n#kern.maxswzone=\"256M\" \n\n# Older versions of FreeBSD can't tune maxfiles on the fly\n#kern.maxfiles=\"200000\"\n\n# Useful for databases \n# Sets maximum data size to 1G\n# (FreeBSD 7.2+ amd64 users: Check that current value is lower!)\n#kern.maxdsiz=\"1G\"\n\n# Maximum buffer size(vfs.maxbufspace)\n# You can check current one via vfs.bufspace\n# Should be lowered/upped depending on server's load-type\n# Usually decreased to preserve kmem\n# (default is 10% of mem)\n#kern.maxbcache=\"512M\"\n\n# Sendfile buffers\n# Note: i386 only\n#kern.ipc.nsfbufs=10240\n\n# syncache tuning\nnet.inet.tcp.syncache.hashsize=32768\nnet.inet.tcp.syncache.bucketlimit=32\nnet.inet.tcp.syncache.cachelimit=1048576\n\n# Send RST on listen queue overflow / memory shortage. \n# Hosts behind Load-Balancer should set it to 1 to fail fast.\n# Hosts facing clients should set it to 0 for client to retry connection.\n#net.inet.tcp.syncache.rst_on_sock_fail=0\n\n# Increased hostcache\n# Later host cache can be viewed via net.inet.tcp.hostcache.list hidden sysctl\n# Very useful for it's RTT RTTVAR\n# Must be power of two\nnet.inet.tcp.hostcache.hashsize=65536\n# hashsize * bucketlimit (which is 30 by default)\n# It allocates 255Mb (1966080*136) of RAM\nnet.inet.tcp.hostcache.cachelimit=1966080\n\n# TCP control-block Hash table tuning\n# See: http://serverfault.com/questions/372512/why-change-net-inet-tcp-tcbhashsize-in-freebsd\nnet.inet.tcp.tcbhashsize=524288\n\n# Disable ipfw deny all\n# Should be uncommented when there is a chance that\n# kernel and ipfw binary may be out-of sync on next reboot\n#net.inet.ip.fw.default_to_accept=1\n\n#\n# SIFTR (Statistical Information For TCP Research) is a kernel module that\n# logs a range of statistics on active TCP connections to a log file.\n# See prerelease notes:\n# http://groups.google.com/group/mailing.freebsd.current/browse_thread/thread/b4c18be6cdce76e4\n# and man 4 sitfr\n#siftr_load=\"YES\"\n\n# Enable superpages, for 7.2+ only\n# See: http://lists.freebsd.org/pipermail/freebsd-hackers/2009-November/030094.html\nvm.pmap.pg_ps_enabled=1\n\n# Useful if you are using Intel-Gigabit NIC\n#hw.em.rxd=4096\n#hw.em.txd=4096\n#hw.em.rx_process_limit=-1\n# Also if you have A LOT interrupts on NIC - play with following parameters\n# NOTE: You should set them for every NIC\n#dev.em.0.rx_int_delay: 250\n#dev.em.0.tx_int_delay: 250\n#dev.em.0.rx_abs_int_delay: 250\n#dev.em.0.tx_abs_int_delay: 250\n# There is also multithreaded version of em/igb drivers that can be found here:\n# http://people.yandex-team.ru/~wawa/\n#\n# for additional em monitoring and statistics use \n# sysctl dev.em.0.stats=1 ; dmesg\n# sysctl dev.em.0.debug=1 ; dmesg\n# Also after r209242 (-CURRENT) there is a separate sysctl for each stat variable;   \n# Same tunings for igb\n#hw.igb.rxd=4096\n#hw.igb.txd=4096\n#hw.igb.rx_process_limit=-1\n\n# Some useful netisr tunables. See sysctl net.isr\n#net.isr.maxthreads=4\n#net.isr.defaultqlimit=10240\n#net.isr.maxqlimit=10240\n# Bind netisr threads to CPUs\n#net.isr.bindthreads=1\n\n#\n# FreeBSD 9.x+\n# Increase interface send queue length\n# See commit message http://svn.freebsd.org/viewvc/base?view=revision&revision=207554\n#net.link.ifqmaxlen=1024\n\n# Nicer boot logo =)\nloader_logo=\"beastie\"\n\nAnd finally here is KERNCONF:\n# Just some of them, see also\n# cat /sys/{i386,amd64,}/conf/NOTES\n\n# This one useful only on i386\n#options         KVA_PAGES=512\n# From UPDATING 20121223:\n#    After switching to Clang as the default compiler some users of ZFS\n#    on i386 systems started to experience stack overflow kernel panics.\n#    Please consider using 'options KSTACK_PAGES=4' in such configurations.\n#options         KSTACK_PAGES=4\n\n# You can play with HZ in environments with high interrupt rate (default is 1000) \n# 100 is for my notebook to prolong it's battery life\n#options         HZ=100\n\n# Eliminate datacopy on socket read-write\n# To take advantage with zero copy sockets you should have an MTU >= 4k\n# This req. is only for receiving data.\n# Read more in man zero_copy_sockets\n# Also this epic thread on kernel trap:\n#    http://kerneltrap.org/node/6506\n# In conclusion Linus says:\n#    \"anybody that does it that way (FreeBSD) is totally incompetent\"\n#\n# Also see /usr/src/UPDATING 20121023 for notes about\n# SOCKET_SEND_COW and SOCKET_RECV_PFLIP\n#options         ZERO_COPY_SOCKETS\n\n# Support TCP sign. Used for IPSec\noptions         TCP_SIGNATURE\n# There was stackoverflow found in KAME IPSec stack:\n# See http://secunia.com/advisories/43995/\n# For quick workaround you can use `ipfw add deny proto ipcomp`\noptions         IPSEC\n\n# This ones can be loaded as modules. They described in loader.conf section     \n#options         ACCEPT_FILTER_DATA\n#options         ACCEPT_FILTER_HTTP\n\n# Adding ipfw, also can be loaded as modules\noptions         IPFIREWALL\n# On 8.1+ you can disable verbose to see blocked packets on ipfw0 interface.\n# Also there is no point in compiling verbose into the kernel, because\n# now there is net.inet.ip.fw.verbose tunable.\n#options         IPFIREWALL_VERBOSE\n#options         IPFIREWALL_VERBOSE_LIMIT=10\n# The IPFIREWALL_FORWARD kernel option has been removed. Its\n# functionality now turned on by default.\n#options         IPFIREWALL_FORWARD\n# Adding kernel NAT\noptions         IPFIREWALL_NAT\noptions         LIBALIAS\n# Traffic shaping\noptions         DUMMYNET          \n# Divert, i.e. for userspace NAT\noptions         IPDIVERT\n\n# This is for OpenBSD's pf firewall\ndevice          pf\ndevice          pflog\n# pf's QoS - ALTQ\noptions         ALTQ\noptions         ALTQ_CBQ        # Class Bases Queuing (CBQ)\noptions         ALTQ_RED        # Random Early Detection (RED)\noptions         ALTQ_RIO        # RED In/Out\noptions         ALTQ_HFSC       # Hierarchical Packet Scheduler (HFSC)\noptions         ALTQ_PRIQ       # Priority Queuing (PRIQ)\noptions         ALTQ_NOPCC      # Required for SMP build\n\n# Pretty console \n# Manual can be found here http://forums.freebsd.org/showthread.php?t=6134\n#options         VESA\n#options         SC_PIXEL_MODE\n\n# Disable reboot on Ctrl Alt Del\n#options         SC_DISABLE_REBOOT\n# Change normal|kernel messages color\noptions         SC_NORM_ATTR=(FG_GREEN|BG_BLACK)\noptions         SC_KERNEL_CONS_ATTR=(FG_YELLOW|BG_BLACK)\n# More scroll space\noptions         SC_HISTORY_SIZE=8192\n\n# Adding hardware crypto device\ndevice          crypto\ndevice          cryptodev\n\n# Useful network interfaces\ndevice          vlan\ndevice          tap                     #Virtual Ethernet driver\ndevice          gre                     #IP over IP tunneling\ndevice          if_bridge               #Bridge interface\ndevice          pfsync                  #synchronization interface for PF\ndevice          carp                    #Common Address Redundancy Protocol\ndevice          enc                     #IPsec interface\ndevice          lagg                    #Link aggregation interface\ndevice          stf                     #IPv4-IPv6 port\n\n# Also for my notebook, but may be used with Opteron\ndevice         amdtemp\n# Same for Intel processors\ndevice         coretemp\n\n# man 4 cpuctl\ndevice         cpuctl                   # CPU control pseudo-device\n\n# Support for ECMP. More than one route for destination\n# Works even with default route so one can use it as LB for two ISP\n# For now code is unstable and panics (panic: rtfree 2) on route deletions.\n#options         RADIX_MPATH\n\n# Multicast routing\n#options         MROUTING\n#options         PIM\n\n# Debug & DTrace\noptions        KDB                     # Kernel debugger related code\noptions        KDB_TRACE               # Print a stack trace for a panic\noptions        KDTRACE_FRAME           # amd64-only(?)\noptions        KDTRACE_HOOKS           # all architectures - enable general DTrace hooks\n#options        DDB\n#options        DDB_CTF                 # all architectures - kernel ELF linker loads CTF data\n\n# Adaptive spining in lockmgr (8.x+)\n# See http://www.mail-archive.com/[email\u00a0protected]/msg10782.html\noptions         ADAPTIVE_LOCKMGRS\n\n# UTF-8 in console (8.x+) \n#options         TEKEN_UTF8\n\n# FreeBSD 8.1+\n# Deadlock resolver thread \n# For additional information see http://www.mail-archive.com/[email\u00a0protected]/msg18124.html \n# (FYI: \"resolution\" is panic so use with caution)\n#options         DEADLKRES\n\n# Increase maximum size of Raw I/O and sendfile(2) readahead\n#options MAXPHYS=(1024*1024)\n#options MAXBSIZE=(1024*1024)\n\n# For scheduler debug enable following option.\n# Debug will be available via `kern.sched.stats` sysctl\n# For more information see http://svnweb.freebsd.org/base/head/sys/conf/NOTES?view=markup\n#options SCHED_STATS\n\n# A framework for very efficient packet I/O from userspace, capable of \n# line rate at 10G (FreeBSD10+)\n# See http://svnweb.freebsd.org/base?view=revision&revision=227614\n#device netmap\n\nIf you are tuning network for maximum performance you may wish to play with\nifconfig options like:\n# You can list all capabilities via `ifconfig -m`\nifconfig [-]rxcsum [-]txcsum [-]tso [-]lro mtu\n\nIn case you've enabled DDB in kernel config, you should edit your\n/etc/ddb.conf  and add something like this to enable automatic reboot (and\ntextdump as bonus):\nscript kdb.enter.panic=textdump set; capture on; show pcpu; bt; ps; alltrace; capture off; call doadump; reset\nscript kdb.enter.default=textdump set; capture on; bt; ps; capture off; call doadump; reset\n\nAnd do not forget to add ddb_enable=\"YES\" to /etc/rc.conf\nSince FreeBSD 9 you can select to enable/disable flowcontrol on your NIC:\n# See http://en.wikipedia.org/wiki/Ethernet_flow_control and\n# http://www.mail-archive.com/[email\u00a0protected]/msg07927.html for additional info\nifconfig bge0 media auto mediaopt flowcontrol\n\nMost of FreeBSD's limits can be monitored by:\n# vmstat -z\n\nand\n# limits\n\nVariety of network counters can be  monitored via\n# netstat -s\n\nIn FreeBSD-8+ netstat's -Q option appeared, try following command to display\nnetisr stats\n# netstat -Q\n\nFor solving non-trivial TCP problems one can use net.inet.tcp.log_debug, it\nproduces dmesg output similar to:\nhost kernel: TCP: [0.0.0.0]:0 to [1.1.1.1]:1; syncache_socket: Socket create failed due to limits or memory shortage\nhost kernel: TCP: [0.0.0.0]:0 to [1.1.1.1]:1 tcpflags 0x10<ACK>; tcp_input: Listen socket: Socket allocation failed due to limits or memory shortage, sending RST\n\nNB!\nLast but not the least: if you are into network tuning - it's good\npractice to buy the best network card you can afford. I personally prefer\nIntel's igb(4), list of models can be found in if_igb.c\nPS. also see\n# man 7 tuning\n\nAnd FreeBSD Wiki on network performance tuning made by developers themselves. \nPPS.\nCalomel.org - Open Source Research and Reference blog has nice write up about network performance and recent article about FreeBSD Tuning and Optimization.\nThanks\nI wanted to thank FreeBSD community, especially author of nginx - Igor\nSysoev, nginx-ru@ and FreeBSD-performance@ mailing lists for providing useful\ninformation about FreeBSD tuning. Yandex BSD lovers from noc@ and\nsearch-admin@, especially melifaro@ and zont@.\nDisclaimer\nThis is definitely not something that you should copy/paste into your production\nconfigs! Some of provided \"tunings\" can be even harmful. Use provided data as\nreference for further investigation or A/B testing. I say it again just to be\nexplicit: DO NOT BLINDLY APPLY \"TUNINGS\" YOU'VE FOUND ON INTERNET!.\nBefore applying any sysctl on production system you should investigate it's\nimpact (it's essential to look in kernel's source code) and measure it's\nperformance benefits(if any) in testing environment.\nUse this post at your own risk.\nFreeBSD WIP\n* Whats cooking for FreeBSD 7?\n* Whats cooking for FreeBSD 8?\n* Whats cooking for FreeBSD 9?\n* What's new for FreeBSD 10?\n* What's new for FreeBSD 11?\nQuestion to viewers\nWhat tunings are you using on yours FreeBSD servers?\nYou can also post your /etc/sysctl.conf, /boot/loader.conf, kernel options,\netc with description of its' meaning (do not copy-paste from sysctl -d).\nDon't forget to specify server type (frontend, backend, cache, db, storage,\ngateway, etc)\nLet's share experience!",
        "top_answer": "I'd recommend against options         IPFIREWALL_DEFAULT_TO_ACCEPT. The default is to Default to Deny. The firewall comes up with just one rule deny ip from any to any and stays that way until a script configures exactly what traffic should get through.\nFollow-Up Note: RSA (one of the world's leading security technology companies) was hacked recently when part of their firewall was disabled during a maintenance window. This really underscores how quickly a system can be compromised given the right conditions.\nIf you insist on disabling the firewall until you explicitly block unwanted traffic, please consider using the sysctl available by adding net.inet.ip.fw.default_to_accept=1 to loader.conf. This has the added benefit of being easily modified (no recompiling the kernel) if you change your mind at some point in the future.",
        "url": "https://serverfault.com/questions/64356/freebsd-performance-tuning-sysctl-parameter-loader-conf-kernel"
    },
    {
        "title": "Switching to IPv6 implies dropping NAT. Is that a good thing?",
        "question": "\nThis is a Canonical Question about IPv6 and NAT\nRelated:\n\nHow does IPv6 subnetting work and how does it differ from IPv4 subnetting?\nHow can I 'dip my toes' into dynamic IPv6 network addressing?\nIPv6 without nat but what about an isp change?\n\n\nOur ISP has set up IPv6 recently, and I've been studying what the transition should entail before jumping into the fray.\nI've noticed three very important issues:\n\nOur office NAT router (an old Linksys BEFSR41) does not support IPv6. Nor does any newer router, AFAICT. The book I'm reading about IPv6 tells me that it makes NAT \"unnecessary\" anyway.\n\nIf we're supposed to just get rid of this router and plug everything directly to the Internet, I start to panic. There's no way in hell I'll put our billing database (with lots of credit card information!) on the Internet for everyone to see. Even if I were to propose setting up Windows' firewall on it to allow only 6 addresses to have any access to it at all, I still break out in a cold sweat. I don't trust Windows, Windows' firewall, or the network at large enough to even be remotely comfortable with that.\n\nThere's a few old hardware devices (ie, printers) that have absolutely no IPv6 capability at all. And likely a laundry list of security issues that date back to around 1998. And likely no way to actually patch them in any way. And no funding for new printers.\n\n\nI hear that IPv6 and IPSEC are supposed to make all this secure somehow, but without physically separated networks that make these devices invisible to the Internet, I really can't see how. I can likewise really see how any defences I create will be overrun in short order. I've been running servers on the Internet for years now and I'm quite familiar with the sort of things necessary to secure those, but putting something private on the network like our billing database has always been completely out of the question.\nWhat should I be replacing NAT with if we don't have physically separate networks?",
        "top_answer": "First and foremost, there is nothing to fear from being on a public IP allocation, so long as your security devices are configured right.\n\nWhat should I be replacing NAT with, if we don't have physically separate networks?\n\nThe same thing we've been physically separating them with since the 1980's, routers and firewalls. The one big security gain you get with NAT is that it forces you into a default-deny configuration. In order to get any service through it, you have to explicitly punch holes. The fancier devices even allow you to apply IP-based ACLs to those holes, just like a firewall. Probably because they have 'Firewall' on the box, actually. \nA correctly configured firewall provides exactly the same service as a NAT gateway. NAT gateways are frequently used because they're easier to get into a secure config than most firewalls.\n\nI hear that IPv6 and IPSEC are supposed to make all this secure somehow, but without physically separated networks that make these devices invisible to the Internet, I really can't see how.\n\nThis is a misconception. I work for a University that has a /16 IPv4 allocation, and the vast, vast majority of our IP address consumption is on that public allocation. Certainly all of our end-user workstations and printers. Our RFC1918 consumption is limited to network devices and certain specific servers where such addresses are required. I would not be surprised if you just shivered just now, because I certainly did when I showed up on my first day and saw the post-it on my monitor with my IP address.\nAnd yet, we survive. Why? Because we have an exterior firewall configured for default-deny with limited ICMP throughput. Just because 140.160.123.45 is theoretically routeable, does not mean you can get there from wherever you are on the public internet. This is what firewalls were designed to do.\nGiven the right router configs, and different subnets in our allocation can be completely unreachable from each other. You do can do this in router tables or firewalls. This is a separate network and has satisfied our security auditors in the past.\n\nThere's no way in hell I'll put our billing database (With lots of credit card information!) on the internet for everyone to see.\n\nOur billing database is on a public IPv4 address, and has been for its entire existence, but we have proof you can't get there from here. Just because an address is on the public v4 routeable list does not mean it is guaranteed to be delivered. The two firewalls between the evils of the Internet and the actual database ports filter out the evil. Even from my desk, behind the first firewall, I can't get to that database.\nCredit-card information is one special case. That's subject to the PCI-DSS standards, and the standards state directly that servers that contain such data have to be behind a NAT gateway1. Ours are, and these three servers represent our total server usage of RFC1918 addresses. It doesn't add any security, just a layer of complexity, but we need to get that checkbox checked for audits. \n\nThe original \"IPv6 makes NAT a thing of the past\" idea was put forward before the Internet boom really hit full mainstream. In 1995 NAT was a workaround for getting around a small IP allocation. In 2005 it was enshrined in many Security Best Practices document, and at least one major standard (PCI-DSS to be specific). The only concrete benefit NAT gives is that an external entity performing recon on the network doesn't know what the IP landscape looks like behind the NAT device (though thanks to RFC1918 they have a good guess), and on NAT-free IPv4 (such as my work) that isn't the case. It's a small step in defense-in-depth, not a big one.\nThe replacement for RFC1918 addresses are what are called Unique Local Addresses. Like RFC1918, they don't route unless peers specifically agree to let them route. Unlike RFC1918, they are (probably) globally unique. IPv6 address translators that translate a ULA to a Global IP do exist in the higher range perimeter gear, definitely not in the SOHO gear yet. \nYou can survive just fine with a public IP address. Just keep in mind that 'public' does not guarantee 'reachable', and you'll be fine.\n\n2017 update\nIn the past few months, Amazon aws has been adding IPv6 support. It has just been added to their amazon-vpc offering, and their implementation gives some clues as to how large scale deployments are expected to be done.\n\nYou are given a /56 allocation (256 subnets).\nThe allocation is a fully routeable subnet.\nYou are expected to set your firewall-rules (security-groups) appropriately restrictive.\nThere is no NAT, it's not even offered, so all outbound traffic will come from the actual IP address of the instance.\n\nTo add one of the security benefits of NAT back in, they are now offering an Egress-only Internet Gateway. This offers one NAT-like benefit:\n\nSubnets behind it can't be directly accessed from the internet.\n\nWhich provides a layer of defense-in-depth, in case a misconfigred firewall rule accidentally allows inbound traffic.\nThis offering does not translate the internal address into a single address the way NAT does. Outbound traffic will still have the source IP of the instance that opened the connection. Firewall operators looking to whitelist resources in the VPC will be better off whitelisting netblocks, rather than specific IP addresses.\nRouteable does not always mean reachable.\n\n1: The PCI-DSS standards changed in October 2010, the statement mandating RFC1918 addresses was removed, and 'network isolation' replaced it.",
        "url": "https://serverfault.com/questions/184524/switching-to-ipv6-implies-dropping-nat-is-that-a-good-thing"
    },
    {
        "title": "How do you install Node.JS on CentOS?",
        "question": "I've found numerous installation instructions for Node.js but they all seem so complicated -- I'm not a super sys admin but I can get around. I have yum on the system, but I didn't find any node.js packages, and I'm not sure how to compile code on the server or where to put it. ",
        "top_answer": "su - \nyum install gcc-c++ openssl-devel\ncd /usr/local/src\nwget http://nodejs.org/dist/node-latest.tar.gz\ntar zxvf node-latest.tar.gz\n(cd into extracted folder: ex \"cd node-v0.10.3\")\n./configure\nmake\nmake install\n\n\nNote that this requires Python 2.6+ to use ./configure above. You can modify the \"configure\" file to point to python2.7 in line 1 if necessary.\nTo create an RPM package, you can use FPM:\n# wget http://nodejs.org/dist/node-latest.tar.gz\n# tar zxvf node-latest.tar.gz\n(cd into extracted folder: ex \"cd node-v0.10.3\")\n# ./configure --prefix=/usr/\n# make\n# mkdir /tmp/nodejs\n# make install DESTDIR=/tmp/nodejs/\n# tree -L 3 /tmp/nodejs/\n/tmp/nodejs/\n\u2514\u2500\u2500 usr\n    \u251c\u2500\u2500 bin\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 node\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 node-waf\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 npm -> ../lib/node_modules/npm/bin/npm-cli.js\n    \u251c\u2500\u2500 include\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 node\n    \u251c\u2500\u2500 lib\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 dtrace\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 node\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 node_modules\n    \u2514\u2500\u2500 share\n        \u2514\u2500\u2500 man\n\nNow make the nodejs package:\n# fpm -s dir -t rpm -n nodejs -v 0.8.18 -C /tmp/nodejs/ usr/bin usr/lib\n\nThen install and check the version:\n# rpm -ivh nodejs-0.8.18-1.x86_64.rpm \nPreparing...                ########################################### [100%]\n   1:nodejs                 ########################################### [100%]\n\n# /usr/bin/node --version\nv0.8.18\n\nSource: https://github.com/jordansissel/fpm/wiki/PackageMakeInstall",
        "url": "https://serverfault.com/questions/299288/how-do-you-install-node-js-on-centos"
    },
    {
        "title": "Why does sudo command take long to execute?",
        "question": "I've been picking up Linux (Fedora 10, then 11) over the past few months (and enjoying it immensely-- it's like discovering computers all over again, so many things to learn).\nI've added my user to the last line of the /etc/sudoers file as shown below, so that I don't get asked for my password when I execute the sudo command:\nMyUserName ALL=(ALL) NOPASSWD:ALL\n\nNow every time I execute a command using sudo, it pauses a noticeable amount of time before actually performing the task (~10 seconds).  Why might this be and how might I fix this?  I'm running Sudo version 1.7.1 on Fedora 11 x86 64.",
        "top_answer": "I asked this question over on SO and it got moved here.  That said I no longer have the ability to edit the question as if I owned it, or even accept the correct answer, but this turned out to be the true reason why and how to solve it:\nFound here  User \"rohandhruva\" on there gives the right answer:\n\nThis happens if you change the\n  hostname during the install process.\nTo solve the problem, edit the file\n  /etc/hosts\n127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 <ADD_YOURS_HERE> \n::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 <ADD_YOURS_HERE>\n\n",
        "url": "https://serverfault.com/questions/38114/why-does-sudo-command-take-long-to-execute"
    },
    {
        "title": "What Linux distribution is the Amazon Linux AMI based on?",
        "question": "Amazon Web Services (AWS) offers an officially supported Amazon Machine Image (AMI), but it doesn't indicate which Linux distribution it's based upon.\nIs the official Amazon Linux AMI based on another Linux distribution, and if so, which one?",
        "top_answer": "Instead of guessing which version of RHEL a particular distro is based off, just run:\nrpm -E %{rhel}\n\nFor Amazon Linux 2, this will give you 7.",
        "url": "https://serverfault.com/questions/798427/what-linux-distribution-is-the-amazon-linux-ami-based-on"
    },
    {
        "title": "Why is RAID not a backup?",
        "question": "When someone mentions RAID in a conversation about backups, invariably someone declares that \"RAID is not a backup.\"\nSure, for striping, that's true. But what's the difference between redundancy and a backup?",
        "top_answer": "RAID guards against one kind of hardware failure. There's lots of failure modes that it doesn't guard against.\n\nFile corruption\nHuman error (deleting files by mistake)\nCatastrophic damage (someone dumps water onto the server)\nViruses and other malware\nSoftware bugs that wipe out data\nHardware problems that wipe out data or cause hardware damage (controller malfunctions, firmware bugs, voltage spikes, ...)\n\nand more.",
        "url": "https://serverfault.com/questions/2888/why-is-raid-not-a-backup"
    },
    {
        "title": "Cheat Sheets for System Administrators?",
        "question": "I'd like to start a collection of good, free cheat sheet resources for system administrators. Please add your favorite ones. From the Wikipedia \"cheat sheet\" article:\n\nIn more general usage, a \"cheat sheet\"\n  is any short (one or two page)\n  reference to terms, commands, or\n  symbols where the user is expected to\n  understand the use of such terms etc\n  but not necessarily to have memorized\n  all of them.\n",
        "top_answer": "I add my own favorite: Cheat Sheets on PacketLife.com has some very nice ones on network technology topics.\n\nCheat sheets are in PDF format. You\n  are welcome to use and redistribute\n  them as you please, so long as they\n  remain intact and unmodified.\n\nCurrently there are six categories:\n\nProtocols: BGP, EIGRP, First Hop Redundancy, IEEE 802.11 Wireless, IEEE 802.1X, IPsec, IPv4 Multicast, IPv6, IS-IS, OSPF, Spanning Tree \nApplications: tcpdump, Wireshark Display Filters\nReference: Common Ports, IP Access Lists, Subnetting\nSyntax: Markdown, MediaWiki\nTechnologies: MPLS, Quality of Service, VLANs\nMiscellaneous: Cisco IOS Versions, Physical Terminations\n\nExamples: Common Ports and IPv6 (links to PDF files)\n",
        "url": "https://serverfault.com/questions/31495/cheat-sheets-for-system-administrators"
    },
    {
        "title": "Prevent duplicate cron jobs running",
        "question": "I have scheduled a cron job to run every minute but sometimes the script takes more than a minute to finish and I don't want the jobs to start \"stacking up\" over each other. I guess this is a concurrency problem - i.e. the script execution needs to be mutually exclusive.\nTo solve the problem I made the script look for the existence of a particular file (\"lockfile.txt\") and exit if it exists or touch it if it doesn't. But this is a pretty lousy semaphore! Is there a best practice that I should know about? Should I have written a daemon instead?",
        "top_answer": "There are a couple of programs that automate this feature, take away the annoyance and potential bugs from doing this yourself, and avoid the stale lock problem by using flock behind the scenes, too (which is a risk if you're just using touch).  I've used lockrun and lckdo in the past, but now there's flock(1) (in newish versions of util-linux) which is great.  It's really easy to use:\n* * * * * /usr/bin/flock -n /tmp/fcj.lockfile /usr/local/bin/frequent_cron_job\n",
        "url": "https://serverfault.com/questions/82857/prevent-duplicate-cron-jobs-running"
    },
    {
        "title": "What is this IP address: 169.254.169.254?",
        "question": "I have always noticed an IP something \"169.254.x.x\" in my routing table even when I am not connected to any network in my Windows operating system.\nIn Linux, when I list my routing table.\n$ ip route show \n\nI get an entry like\n169.254.0.0/16 dev eth0  scope link  metric 1000 \n\nCan somebody explain me what is this IP address actually. Whether its something like the 127.0.0.0/8 family.\nEdit: In ec2, each instance can get meta-data regarding their own by making HTTP requests to this IP.\n$ curl -s http://169.254.169.254/user-data/\n\nSo can someone tell me to whom this IP address is actually assigned ?",
        "top_answer": "These are dynamically configured link-local addresses. They are only valid on a single network segment and are not to be routed.\nOf particular note, 169.254.169.254 is used in AWS, Azure, GCP and other cloud computing platforms to host instance metadata service.",
        "url": "https://serverfault.com/questions/427018/what-is-this-ip-address-169-254-169-254"
    },
    {
        "title": "Colors in bash after piping through less?",
        "question": "When I have dircolors defined life is full of... color.\nWhen I pipe ls through less to scroll around I lose the colors.\nAny suggestions?",
        "top_answer": "Most likely your ls is aliased to ls --color=auto, which tells ls to only use colors when its output is a tty.  If you do ls --color (which is morally equivalent to ls --color=always), that will force it to turn on colors.\nYou could also change your alias to do that, but I wouldn't really call that a good idea.  Better to make a different alias with --color.\nless needs -R too, which causes it to output the raw control characters.",
        "url": "https://serverfault.com/questions/26509/colors-in-bash-after-piping-through-less"
    },
    {
        "title": "How can I see Time-To-Live (TTL) for a DNS record?",
        "question": "I would like to see the Time-To-Live (TTL) value for a CNAME record.\nI have access to dig (on Apple Mac OS X), which gives me an answer like this:\n% dig host.example.gov\n<*SNIP*>\n;; ANSWER SECTION:\nhost.example.gov.       43200   IN  CNAME   host1.example.gov.\nhost1.example.gov.      43200   IN  A       192.168.16.10\n\nIs the value '43200' the TTL for this DNS record?",
        "top_answer": "Yes, the number there is the number of seconds left until that record expires (providing we're not querying the authoritative nameserver). Obviously with a CNAME there's a level of redirection, so the TTL for the A record it points to in this case may be important as well.\nIf you wait a couple of seconds and run dig again on your local nameserver, you should see that TTL number decrease by the number of seconds you waited (approximately). When it hits 0, it'll refresh or if your nameserver refreshes the zone for some reason.\nAs mentioned above, there is a difference between dig being run against a nameserver with a cached entry and the nameserver that is authoritative for that entry.\n(in the examples I use below I use the +noauthority +noquestion & +nostats flags just to keep the output terse).\nNote the difference between the following queries:\n$ dig +noauthority +noquestion +nostats stackoverflow.com @ns2.p19.dynect.net.\n\n; <<>> DiG 9.7.0-P1 <<>> +noauthority +noquestion +nostats stackoverflow.com @ns2.p19.dynect.net.\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 50066\n;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 0\n;; WARNING: recursion requested but not available\n\n;; ANSWER SECTION:\nstackoverflow.com.  432000  IN  A   69.59.196.211\n\nSo in the above query, we're querying a nameserver that is authoritative for stackoverflow.com. If you notice the flags section, pay special attention to the aa flag which denotes this is an authoritative answer (i.e. not cached). \n$ dig +noauthority +noquestion +noadditional +nostats stackoverflow.com \n\n; <<>> DiG 9.7.0-P1 <<>> +noauthority +noquestion +noadditional +nostats stackoverflow.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 43514\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 4\n\n;; ANSWER SECTION:\nstackoverflow.com.  246696  IN  A   69.59.196.211\n\nIn the above query, we don't have an aa flag, and the TTL will keep decreasing as we query and query. This is essentially the counter I was talking about previously.",
        "url": "https://serverfault.com/questions/179630/how-can-i-see-time-to-live-ttl-for-a-dns-record"
    },
    {
        "title": "Postfix - how to retry delivery of mail in queue?",
        "question": "I have a backup mail server in case of a failure on the main one.\nIn that case of failure, mails come on the backup server and stay there until the main one is back.\nIf I wait some times, the delivery will be done automatically as soon as the main server is back but it can be long. So how to force a send retry of all the mails?\nFor exemple :\npostqueue -p : give me a list of mails\nI then tried postqueue -f (from man page : Flush the queue: attempt to deliver all queued mail.). It surely flushed the queue but mails were not been delivered...",
        "top_answer": "According to postqueue(1) you can simply run postqueue -f to flush your mail queue. If the mails aren't delivered after flushing the queue but are being requeued instead, you might want to check your mail logs for errors.\nTaking a peek at postsuper(1) might also be helpful. Maybe the messages are on hold and need to be released first.",
        "url": "https://serverfault.com/questions/279803/postfix-how-to-retry-delivery-of-mail-in-queue"
    },
    {
        "title": "Is it possible to rerun kubernetes job?",
        "question": "I have the following Kubernetes Job configuration:\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: dbload\n  creationTimestamp: \nspec:\n  template:\n    metadata:\n      name: dbload\n    spec:\n      containers:\n      - name: dbload\n        image: sdvl3prox001:7001/pbench/tdload\n        command: [\"/opt/pbench/loadTpcdsData.sh\",  \"qas0063\", \"dbc\", \"dbc\", \"1\"]\n      restartPolicy: Never\n      imagePullSecrets: \n        - name: pbenchregkey\nstatus: {}\n\nWhen I do kubectl create -f dbload-deployment.yml --record the job and a pod are created, Docker container runs to completion and I get this status:\n$ kubectl get job dbload\nNAME      DESIRED   SUCCESSFUL   AGE\ndbload    1         1            1h\n$ kubectl get pods -a\nNAME           READY     STATUS      RESTARTS   AGE\ndbload-0mk0d   0/1       Completed   0          1h\n\nThis job is one time deal and I need to be able to rerun it. If I attempt to rerun it with kubectl create command I get this error\n$ kubectl create -f dbload-deployment.yml --record\nError from server: error when creating \"dbload-deployment.yml\": jobs.batch \"dbload\" already exists\n\nOf course I can do kubectl delete job dbload and then run kubectl create but I'm wondering if I can somehow re-awaken the job that already exists?",
        "top_answer": "No. There is definitely no way to rerun a kubernetes job.\nYou need to delete it first.\nYou can however create a new job based on existing job configuration that will be executed immediately:\nkubectl create job --from=job/dbload new-dbload -n <namespace>",
        "url": "https://serverfault.com/questions/809632/is-it-possible-to-rerun-kubernetes-job"
    },
    {
        "title": "FreeBSD: Directory called ^C (really!) - how to remove?",
        "question": "I made a typo:\n$ history\n169 9:34    la /usr/local/etc/\n170 9:35    sudo mkdir ^C\n171 9:36    sudo mkdir /usr/local/etc/dnsmasq.d\n\nNow I have a file that is called ^C (ctrl+C)!!\nWhen I use ls I just see a questionmark (probably due to the locale?)\n% ls -al\ntotal 60\ndrwxr-xr-x  2 root   wheel    512 Jan 21 09:35 ?        <- this one\ndrwxr-xr-x  5 admin  wheel    512 Jan 21 16:24 .\ndrwxr-xr-x  3 root   wheel    512 Jan 20 14:29 ..\n-rw-r--r--  1 admin  nobody  1114 Jan 20 19:10 .cshrc\n-rw-------  1 admin  wheel   6002 Jan 21 15:27 .history\n-rw-r--r--  1 admin  nobody   182 Jan 20 14:29 .login\n-rw-r--r--  1 admin  nobody    91 Jan 20 14:29 .login_conf\n-rw-------  1 admin  nobody   301 Jan 20 14:29 .mail_aliases\n-rw-r--r--  1 admin  nobody   271 Jan 20 19:04 .mailrc\n-rw-r--r--  1 admin  nobody   726 Jan 20 19:05 .profile\n-rw-------  1 admin  nobody   212 Jan 20 14:29 .rhosts\n-rw-r--r--  1 admin  nobody   911 Jan 20 19:06 .shrc\ndrwx------  2 admin  nobody   512 Jan 20 15:05 .ssh\ndrwxr-xr-x  2 admin  wheel    512 Jan 20 19:08 bin\n\nand\n% ls -i\n3611537 ?   3611534 bin\n\nI want to remove this file. I try mv and when using tab-completion it shows me:\n% mv\n^C/  bin/\n\nObviously I can't type a ^C :-/\nHow do I remove this file?",
        "top_answer": "^V (ctrl+v) works as a kind of escape sequence for the next key-press, inserting the associated value instead of taking whatever action that would normally be associated.\nMaking use of this, ^V^C (ctrl+v, ctrl+c) ought to work for entering your difficult filename in the terminal.",
        "url": "https://serverfault.com/questions/893343/freebsd-directory-called-c-really-how-to-remove"
    },
    {
        "title": "How to get pid of just started process",
        "question": "I want to start process (eg. myCommand) and get its pid (to allow to kill it later). \nI tried ps and filter by name, but I can not distinguish process by names\nmyCommand\nps ux | awk '/<myCommand>/ {print $2}' \n\nBecause processes names are not unique.\nI can run process by:\nmyCommand &\n\nI found that I can get this PID by: \necho $!\n\nIs there any simpler solution?\nI would be happy to execute myCommand and get its PID as a result of one line command.",
        "top_answer": "What can be simpler than echo $!? As one line:\nmyCommand & echo $!\n",
        "url": "https://serverfault.com/questions/205498/how-to-get-pid-of-just-started-process"
    },
    {
        "title": "Mount CIFS Host is down",
        "question": "I have an issue with a mount point that was previously configured. It shows the folder, but the mount is missing and holds \"?\" values for size, permissions, etc.\nSo I tried to remount using cifs and the same command from before:\nmount -t cifs //nas.domain.local/share /mnt/archive\n\nBut I get the error: \nHost is down.\n\nIf I ping the domain or IP I get a proper resolution and I also connected using smbclient without issue\n ping nas.domain.local\n ping ip\n smbclient //nas.domain.local/share\n\nI looked around, but cant find a solid answer. Any thoughts?",
        "top_answer": "This could also be because of a protocol mismatch.\nIn 2017 Microsoft patched Windows Servers and advised to disable the SMB1 protocol.\nFrom now on, mount.cifs might have problems with the protocol negotiation.\nThe error displayed is \"Host is down.\", but when you do debug with:\nsmbclient -L <server_ip> -U <username> -d 256\n\nyou will get the error:\nprotocol negotiation failed: NT_STATUS_CONNECTION_RESET\n\nTo overcome this use mount or smbclient with a protocol specified.\nfor smbclient: add -m SMB2 (or SMB3 for the newer version of the protocol)\nsmbclient -L <server_ip> -U <username> -m SMB2\n\nor for mount: add vers=2.0 (or vers=3.0 if you want to use version 3 of the protocol)\nmount -t cifs //<server_ip>/<share> /mnt/<mountpoint> -o vers=2.0\n",
        "url": "https://serverfault.com/questions/414074/mount-cifs-host-is-down"
    },
    {
        "title": "How to show all banned IP with fail2ban?",
        "question": "When I run this command fail2ban-client status sshd I got this:\nStatus for the jail: sshd\n|- Filter\n|  |- Currently failed: 1\n|  |- Total failed:     81\n|  `- File list:        /var/log/auth.log\n`- Actions\n   |- Currently banned: 2\n   |- Total banned:     8\n   `- Banned IP list:   218.65.30.61 116.31.116.7\n\nIt only show two IP in banned IP list instead of 8 just like Total Banned says.\nWhile I do tail -f /var/log/auth.log I got this:\nMar 29 11:08:40 DBSERVER sshd[29163]: error: maximum authentication attempts exceeded for root from 218.65.30.61 port 50935 ssh2 [preauth]\nMar 29 11:08:40 DBSERVER sshd[29163]: Disconnecting: Too many authentication failures [preauth]\nMar 29 11:08:40 DBSERVER sshd[29163]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.65.30.61  user=root\nMar 29 11:08:40 DBSERVER sshd[29163]: PAM service(sshd) ignoring max retries; 6 > 3\nMar 29 11:08:44 DBSERVER sshd[29165]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=218.65.30.61  user=root\nMar 29 11:08:46 DBSERVER sshd[29165]: Failed password for root from 218.65.30.61 port 11857 ssh2\nMar 29 11:09:01 DBSERVER CRON[29172]: pam_unix(cron:session): session opened for user root by (uid=0)\nMar 29 11:09:01 DBSERVER CRON[29172]: pam_unix(cron:session): session closed for user root\nMar 29 11:10:01 DBSERVER CRON[29226]: pam_unix(cron:session): session opened for user root by (uid=0)\nMar 29 11:10:02 DBSERVER CRON[29226]: pam_unix(cron:session): session closed for user root\nMar 29 11:10:18 DBSERVER sshd[29238]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=113.122.43.185  user=root\nMar 29 11:10:20 DBSERVER sshd[29238]: Failed password for root from 113.122.43.185 port 46017 ssh2\nMar 29 11:10:33 DBSERVER sshd[29238]: message repeated 5 times: [ Failed password for root from 113.122.43.185 port 46017 ssh2]\nMar 29 11:10:33 DBSERVER sshd[29238]: error: maximum authentication attempts exceeded for root from 113.122.43.185 port 46017 ssh2 [preauth]\nMar 29 11:10:33 DBSERVER sshd[29238]: Disconnecting: Too many authentication failures [preauth]\nMar 29 11:10:33 DBSERVER sshd[29238]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=113.122.43.185  user=root\nMar 29 11:10:33 DBSERVER sshd[29238]: PAM service(sshd) ignoring max retries; 6 > 3\nMar 29 11:11:36 DBSERVER sshd[29245]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=116.31.116.7  user=root\nMar 29 11:11:38 DBSERVER sshd[29245]: Failed password for root from 116.31.116.7 port 24892 ssh2\nMar 29 11:11:43 DBSERVER sshd[29245]: message repeated 2 times: [ Failed password for root from 116.31.116.7 port 24892 ssh2]\nMar 29 11:11:43 DBSERVER sshd[29245]: Received disconnect from 116.31.116.7 port 24892:11:  [preauth]\nMar 29 11:11:43 DBSERVER sshd[29245]: Disconnected from 116.31.116.7 port 24892 [preauth]\nMar 29 11:11:43 DBSERVER sshd[29245]: PAM 2 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=116.31.116.7  user=root\nMar 29 11:12:39 DBSERVER sshd[29247]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=116.31.116.7  user=root\nMar 29 11:12:41 DBSERVER sshd[29247]: Failed password for root from 116.31.116.7 port 26739 ssh2\nMar 29 11:12:45 DBSERVER sshd[29247]: message repeated 2 times: [ Failed password for root from 116.31.116.7 port 26739 ssh2]\nMar 29 11:12:45 DBSERVER sshd[29247]: Received disconnect from 116.31.116.7 port 26739:11:  [preauth]\nMar 29 11:12:45 DBSERVER sshd[29247]: Disconnected from 116.31.116.7 port 26739 [preauth]\nMar 29 11:12:45 DBSERVER sshd[29247]: PAM 2 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=116.31.116.7  user=root\nMar 29 11:13:41 DBSERVER sshd[29249]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=116.31.116.7  user=root\nMar 29 11:13:43 DBSERVER sshd[29249]: Failed password for root from 116.31.116.7 port 27040 ssh2\n\nbanned IP still trying.\nHowever when I check with sudo iptables -L INPUT -v -n I got this:\nChain INPUT (policy ACCEPT 228 packets, 18000 bytes)\n pkts bytes target     prot opt in     out     source               destination\n 6050  435K f2b-sshd   tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 22\n\nWhat am I doing wrong here?\nHow can I show all banned IP list?",
        "top_answer": "Please keep in mind that the fail2ban banning of IP is temporary in nature.\nThe best way to have a look at the full list of IPs that have been blocked would be to check the log file:\nsudo zgrep 'Ban' /var/log/fail2ban.log*\n\nEdit: this answer previously searched for 'Ban:', but even in 2013 the source has no colon (ref).\nThe following command can also give you a clean list of input rules:\nsudo iptables -L INPUT -v -n | less\n",
        "url": "https://serverfault.com/questions/841183/how-to-show-all-banned-ip-with-fail2ban"
    },
    {
        "title": "How to check if a port is blocked on a Windows machine?",
        "question": "On the Windows platform, what native options to I have to check if a port (3306, for example) on my local machine (as in localhost), is being blocked?",
        "top_answer": "Since you are on a Windows machine, these things can be done:\n\nExecute the following command and look for a \":3306\" listener (you did not mention UDP/TCP). This will confirm there is something running on the port.\nnetstat -a -n\n\nAfter this, if you are expecting incoming connections on this port and feel that the firewall may be blocking them, you could use start windows firewall logging and check the logs for dropped connections\n\nGo to Windows Firewall, Advanced settings\nClick on the Settings button next to \"Local Area Connection\"\nSelect \"Log dropped packets\"\nLook at the log file location (if not present, define one)\nClick OK\nNow, when the connection attempt is made (assuming you know when this is done), look at the log file for a drop on port 3306.\nIf this is seen, you will want to add an exception for this port.\n\n\nThere is one more command to check the firewall state\n(Updated for Windows 7 users -- as referred by Nick below -- use netsh advfirewall firewall)\nnetsh firewall show state\n\nthis will list the blocked ports as well as active listening ports with application associations\n\n\nThis command will dump the Windows firewall configuration detail\nnetsh firewall show config\n\n\n\nIf you have an active block (incoming connections are being dropped by firewall) after you start logging, you should see that in the log.\nIf you are running an application/service that is listening on 3306, the firewall config should show it to be Enabled. If this is not seen, you have probably missed adding an exception with the firewall to allow this app/service.\nFinally, port 3306 is typically used for MySQL. So, I presume you are running MySQL server on this windows machine. You should therefore see a listener for 3306 accepting incoming connections. If you do not see that, you need to work with your application (MySQL) to get that started first.",
        "url": "https://serverfault.com/questions/26564/how-to-check-if-a-port-is-blocked-on-a-windows-machine"
    },
    {
        "title": "How do I auto-start docker containers at system boot?",
        "question": "What is a good way to automatically start docker containers when the system boots up?\nIs there a preferred way to do this on Ubuntu 14.04?\nI've used supervisord in the past to auto start web apps. But that doesn't feel like the right thing for Docker.",
        "top_answer": "Apparently, the current method to auto-start Docker containers (from Docker 1.2) is to use restart policies. This will control how Docker should handle starting of the container upon startup and re-starting of the container when it exits. I've used the 'always' option so far, and can confirm that it makes Docker auto-start the container at system boot:\nsudo docker run --restart=always -d myimage\n\nDocumentation Excerpt\n\nRestart Policies Using the --restart flag on Docker run you can\n  specify a restart policy for how a container should or should not be\n  restarted on exit.\nno - Do not restart the container when it exits.\non-failure - Restart the container only if it exits with a non zero\n  exit status.\nalways - Always restart the container regardless of the exit status.\nYou can also specify the maximum amount of times Docker will try to\n  restart the container when using the on-failure policy. The default is\n  that Docker will try forever to restart the container.\n$ sudo docker run --restart=always redis\n\nThis will run the redis\n  container with a restart policy of always so that if the container\n  exits, Docker will restart it.\n$ sudo docker run --restart=on-failure:10 redis\n\nThis will run the\n  redis container with a restart policy of on-failure and a maximum\n  restart count of 10. If the redis container exits with a non-zero exit\n  status more than 10 times in a row Docker will abort trying to restart\n  the container. Providing a maximum restart limit is only valid for the\n  on-failure policy.\n",
        "url": "https://serverfault.com/questions/633067/how-do-i-auto-start-docker-containers-at-system-boot"
    },
    {
        "title": "Amazon EC2 terminology - AMI vs. EBS vs. Snapshot vs. Volume",
        "question": "I have been poking around Amazon EC2, and am a little confused on some of the terminology.  Specifically with regard to AMI, snapshots and volumes, and an EBS\nPlease correct me if I am wrong, or fill in any serious gaps in my following statements:\n\nAn AMI (Amazon Machine Image) is a full 'disk' capture of an operating system and configuration.  When you launch an instance, you launch it from an AMI\nAn EBS (Elastic Block Storage) is a way to persist the state of any modifications you made once booting from a given AMI.  In my mind, this is sort of like a diff on the final state of your instance vs the AMI.\nA snapshot is ... well, I'm not sure.  I can only assume it is a snapshot of a specific instance, but it is not clear to me how this differs from the state stored in an EBS.  How is a snapshot different from creating an EBS AMI from an existing instance?\nA volume is ... it would seem mounted disk space into which an AMI/EBS pair is loaded? I'm not sure on this one either.  I can see (from the AWS Console) that you can create a volume from a snapshot, and that you can attach/detach volumes, but it isn't clear to me why or when you would do that.\n",
        "top_answer": "An AMI, as you note, is a machine image. It's a total snapshot of a system stored as an image that can be launched as an instance. We'll get back to AMIs in a second.\nLets look at EBS. Your other two items are sub-items of this. EBS is a virtual block device. You can think of it as a hard drive, although it's really a bunch of software magic to link into another kind of storage device but make it look like a hard drive to an instance.\nEBS is just the name for the whole service. Inside of EBS you have what are called volumes. These are the \"unit\" amazon is selling you. You create a volume and they allocate you X number of gigabytes and you use it like a hard drive that you can plug into any of your running computers (instances). Volumes can either be created blank or from a snapshot copy of previous volume, which brings us to the next topic.\nSnapshots are ... well ... snapshots of volumes: an exact capture of what a volume looked like  at a particular moment in time, including all its data. You could have a volume, attach it to your instance, fill it up with stuff, then snapshot it, but keep using it. The  volume contents would keep changing as you used it as a file system but the snapshot would be frozen in time. You could create a new volume using this snapshot as a base. The new volume would look exactly like your first disk did when you took the snapshot. You could start using the new volume in place of the old one to roll-back your data, or maybe attach the same data set to a second machine. You can keep taking snapshots of volumes at any point in time. It's like a freeze-frame instance backup that can then easy be made into a new live disk (volume) whenever you need it.\nSo volumes can be based on new blank space or on a snapshot. Got that? Volumes can be attached and detached from any instances, but only connected to one instance at a time, just like the physical disk that they are a virtual abstraction of.\nNow back to AMIs. These are tricky because there are two types. One creates an ephemeral instances where the root files system looks like a drive to the computer but actually sits in memory somewhere and vaporizes the minute it stops being used. The other kind is called an EBS backed instance. This means that when your instances loads up, it loads its root file system onto a new EBS volume, basically layering the EC2 virtual machine technology on top of their EBS technology. A regular EBS volume is something that sits next to EC2 and can be attached, but an EBS backed instance also IS a volume itself.\nA regular AMI is just a big chunk of data that gets loaded up as a machine. An EBS backed AMI will get loaded up onto an EBS volume, so you can shut it down and it will start back up from where you left off just like a real disk would.\nNow put it all together. If an instance is EBS backed, you can also snapshot it. Basically this does exactly what a regular snapshot would ... a freeze frame of the root disk of your computer at a moment in time. In practice, it does two things different. One is it shuts down your instance so that you get a copy of the disk as it would look to an OFF computer, not an ON one. This makes it easier to boot up :) So when you snapshot an instance, it shuts it down, takes the disk picture, then starts up again. Secondly, it saves that images as an AMI instead of as a regular disk snapshot. Basically it's a bootable snapshot of a volume.",
        "url": "https://serverfault.com/questions/268719/amazon-ec2-terminology-ami-vs-ebs-vs-snapshot-vs-volume"
    },
    {
        "title": "All servers flooded by salt water, is it possible to recover data from multi-platter drives?",
        "question": "All of my servers are currently flooded by salt water.  Is it possible for each platter in a multi-platter drive to be separated, cleaned, imaged, and merged into a new virtual drive for data recovery?",
        "top_answer": "I almost cringe to do this, but if you're serious about recovering your data, and you don't have backups, you'll need a higher-end data recovery service.\nWatch out for fly-by-nights shops that will be peddling/scamming in your area, and go with an actual top-tier data recovery service.  (And be prepared to pay $$$$ for it.)\nHere's a list of what to look at:\n\nYour data.\n\nThese services are expensive, generally starting at over $1000 a drive, so you need to decide if your data's really worth the cost of recovering it.\n\nRecovery company's history.\n\nIf they haven't been around for at least several years, and in the data recovery field, they're probably not worth considering.\nTop-tier companies will have a portfolio of the more interesting or challenging recoveries they've done.  Take a look at those, and make sure the guys you choose have enough experience with water-damaged hard drives that it's just another day in the office for them.\n\nRecovery company's clientele\n\nTop tier companies are generally contracted by law enforcement and law firms for cases where someone deliberately tried to destroy a hard drive, so if you find one that's worked with a lot of police departments or on a lot of legal cases, they're probably a good one.  (Generally this becomes public record at trial too, allowing them to disclose their involvement.)\n\nRecovery company's facilities\n\nNot that you should expect a tour, but you're looking for clean rooms, labs and proprietary or custom tools they've developed to recover data from physically damaged media.\n\n\nAnd I don't generally do this, but since the other answer has a link I'd be a little wary of, I'll throw out a recommendation of sorts.  These guys are good, (expensive as hell, but good), and if you decide to go down this route you should look at them, if only for nothing more than to compare other data recovery services against.\nEDIT: And as mentioned in the comments by @Grant, DO NOT take it to a lower-quality data recovery service first, because both the data recovery process itself and the passage of time will do [additional] irreversible damage to your drives.  If you decide later to try someone better or more qualified, your earlier decision will have increased the cost and decreased the results.",
        "url": "https://serverfault.com/questions/447092/all-servers-flooded-by-salt-water-is-it-possible-to-recover-data-from-multi-pla"
    },
    {
        "title": "Nginx 1 FastCGI sent in stderr: \u201cPrimary script unknown\u201d",
        "question": "My first time using Nginx, but I am more than familiar with Apache and Linux. I am using an existing project and when ever I am trying to see the index.php I get a 404 File not found.\nHere is the access.log entry:\n2013/06/19 16:23:23 [error] 2216#0: *1 FastCGI sent in stderr: \"Primary script unknown\" while reading response header from upstream, client: 127.0.0.1, server: localhost, request: \"GET /index.php HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"www.ordercloud.lh\"\n\nAnd here is the sites-available file:\nserver {\n    set $host_path \"/home/willem/git/console/www\";\n    access_log  /www/logs/console-access.log  main;\n\n    server_name  console.ordercloud;\n    root   $host_path/htdocs;\n    set $yii_bootstrap \"index.php\";\n\n    charset utf-8;\n\n    location / {\n        index  index.html $yii_bootstrap;\n        try_files $uri $uri/ /$yii_bootstrap?$args;\n    }\n\n    location ~ ^/(protected|framework|themes/\\w+/views) {\n        deny  all;\n    }\n\n    #avoid processing of calls to unexisting static files by yii\n    location ~ \\.(js|css|png|jpg|gif|swf|ico|pdf|mov|fla|zip|rar)$ {\n        try_files $uri =404;\n    }\n\n    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n    #\n    location ~ \\.php {\n        fastcgi_split_path_info  ^(.+\\.php)(.*)$;\n\n        #let yii catch the calls to unexising PHP files\n        set $fsn /$yii_bootstrap;\n        if (-f $document_root$fastcgi_script_name){\n            set $fsn $fastcgi_script_name;\n        }\n\n        fastcgi_pass   127.0.0.1:9000;\n        include fastcgi_params;\n        fastcgi_param  SCRIPT_FILENAME  $document_root$fsn;\n\n        #PATH_INFO and PATH_TRANSLATED can be omitted, but RFC 3875 specifies them for CGI\n        fastcgi_param  PATH_INFO        $fastcgi_path_info;\n        fastcgi_param  PATH_TRANSLATED  $document_root$fsn;\n    }\n\n    location ~ /\\.ht {\n        deny  all;\n    }\n}\n\nMy /home/willem/git/console is owned by www-data:www-data (my web user running php etc) and I have given it 777 permissions out of frustration...\nMy best guess is that something is wrong with the config, but I can't figure it out...\nUPDATE\nSo I moved it to /var/www/ and used a much more basic config:\nserver {\n    #listen   80; ## listen for ipv4; this line is default and implied\n    #listen   [::]:80 default ipv6only=on; ## listen for ipv6\n\n    root /var/www/;\n    index index.html index.htm;\n\n    # Make site accessible from http://localhost/\n    server_name console.ordercloud;\n\n    location / {\n        root           /var/www/console/frontend/www/;\n                fastcgi_pass   127.0.0.1:9000;\n                fastcgi_index  index.php;\n                fastcgi_param  SCRIPT_FILENAME  /var/www;\n            include        fastcgi_params;\n    }\n\n    location ~ \\.(js|css|png|jpg|gif|swf|ico|pdf|mov|fla|zip|rar)$ {\n            try_files $uri =404;\n        }\n\n    location /doc/ {\n        alias /usr/share/doc/;\n        autoindex on;\n        allow 127.0.0.1;\n        deny all;\n    }\n\n}\n\nAlso if I call localhost/console/frontend/www/index.php I get a 500 PHP which means it is serving there. It just isn't serving off console.ordercloud ...",
        "top_answer": "The error message \u201cprimary script unknown\u201d is almost always related to a wrongly set SCRIPT_FILENAME in the nginx fastcgi_param directive (or incorrect permissions, see other answers).\nYou\u2019re using an if in the configuration you posted first. Well it should be well known by now that if is evil and often produces problems.\nSetting the root directive within a location block is bad practice, of course it works.\nYou could try something like the following:\nserver {\n    location / {\n        location ~* \\.php$ {\n            include fastcgi_params;\n            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n            fastcgi_pass 127.0.0.1:9000;\n            try_files $uri @yii =404;\n        }\n    }\n    location @yii {\n        fastcgi_param SCRIPT_FILENAME $document_root$yii_bootstrap;\n    }\n}\n\nPlease note that the above configuration is untested. You should execute nginx -t before applying it to check for problems that nginx can detect right away.",
        "url": "https://serverfault.com/questions/517190/nginx-1-fastcgi-sent-in-stderr-primary-script-unknown"
    },
    {
        "title": "How to disable timeout for nginx?",
        "question": "On a local development machine, I have a nginx reverse proxy like so:\nserver {\n  listen 80;\n  server_name myvirtualhost1.local;\n  location / {\n    proxy_pass http://127.0.0.1:8080;\n}\n\nserver {\n  listen 80;\n  server_name myvirtualhost2.local;\n  location / {\n    proxy_pass http://127.0.0.1:9090;\n}\n\nYet if I debug my application, the response may be delayed for an infinite amount of time, yet after 30 seconds I get:\n504 Gateway Time-out\nas a response.\nHow can I disable the timeout and have my reverse proxy wait forever for a response? And I like the setting to be global, so that I do not have to set it for each proxy.",
        "top_answer": "It may not be possible to disable it at all, yet a feasible workaround is to increase the execution time. On a nginx tutorial site, it was written:\n\nIf you want to increase time-limit for all-sites on your server, you\n  can edit main nginx.conf file:\n\nvim /etc/nginx/nginx.conf\n\n\nAdd following in http{..} section\n\nhttp {\n     fastcgi_read_timeout 300;\n     proxy_read_timeout 300;\n}\n\nand reload nginx' config:\nsudo service nginx reload\n\nI have used a rather large value that is unlikely to happen, i.e. 999999 or using time units, to one day via 1d.\nBeware that setting the value to 0 will cause a gateway timeout error immediately.",
        "url": "https://serverfault.com/questions/777749/how-to-disable-timeout-for-nginx"
    },
    {
        "title": "Can you run Docker natively on the new Windows 10 (Ubuntu) bash userspace?",
        "question": "My understanding was that the primary limitation of running docker on other OSs was the Linux Network containers that made it possible. (Certainly for Macs). \nRecently Microsoft announced a beta of a Ubuntu linux user mode running natively on Windows 10. This can run binaries compiled in ELF format on Windows (unlike cygwin which requires a compilation.)\nMy question is: Can you run Docker natively on the new Windows 10 (Ubuntu) bash userspace?",
        "top_answer": "You can use Docker Desktop for Windows as the engine and Docker for Linux as the client in WSL on Ubuntu / Debian on Windows. Connect them via TCP.\nInstall Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows\nIf you want to use Windows Containers instead of Linux Containers both type containers can be managed by the Linux docker client in the bash userspace.\nSince version 17.03.1-ce-win12 (12058) you must check Expose daemon on tcp://localhost:2375 without TLS to allow the Linux Docker client to continue communicating with the Windows Docker daemon by TCP\nFollow these steps:\ncd\nwget https://download.docker.com/linux/static/stable/`uname -m`/docker-19.03.1.tgz\ntar -xzvf docker-*.tgz\ncd docker\n./docker -H tcp://0.0.0.0:2375 ps\n\nor\nenv DOCKER_HOST=tcp://0.0.0.0:2375 ./docker ps\n\nTo make it permanent:\nmkdir ~/bin\nmv ~/docker/docker ~/bin\n\nAdd the corresponding variables to .bashrc\nexport DOCKER_HOST=tcp://0.0.0.0:2375\nexport PATH=$PATH:~/bin\n\nOf course, you can install docker-compose\nsudo -i\ncurl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\n\nOr using python pip\nsudo apt-get install python-pip bash-completion\nsudo pip install docker-compose\n\nAnd Bash completion. The best part:\nsudo -i\napt-get install bash-completion\ncurl -L https://raw.githubusercontent.com/docker/docker-ce/master/components/cli/contrib/completion/bash/docker > /etc/bash_completion.d/docker\ncurl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose version --short)/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n\nI've tested it using the 2.1.0.1 (37199) version of Docker Desktop using Hyper-V:\n$ docker version\nClient: Docker Engine - Community\n Version:           19.03.1\n API version:       1.40\n Go version:        go1.12.5\n Git commit:        74b1e89e8a\n Built:             Thu Jul 25 21:17:37 2019\n OS/Arch:           linux/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.1\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.12.5\n  Git commit:       74b1e89\n  Built:            Thu Jul 25 21:17:52 2019\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.2.6\n  GitCommit:        894b81a4b802e4eb2a91d1ce216b8817763c29fb\n runc:\n  Version:          1.0.0-rc8\n  GitCommit:        425e105d5a03fabd737a126ad93d62a9eeede87f\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\nLook both client and server say **OS/Arch: linux/amd64**\n\nVolumes\nTake care when adding volumes. The path C:\\dir will be visible as /mnt/c/dir on WSL and as /c/dir/ by docker engine. You can overcome it permanently:\nsudo bash -c \"echo -e '[automount] \\nroot = /'>/etc/wsl.conf\"\n\nYou must exit and reload WSL after making the change to wsl.conf so that WSL reads in your changes on launch.\nUPDATE\nfrom: What\u2019s new for the Command Line in Windows 10 version 1803\n\nUnix Sockets\n  Unix Sockets weren't supported on Windows, and now they are! You can also communicate over Unix sockets between Windows and WSL. One of the great things about this is it enables WSL to run the Linux Docker Client to interact with the Docker Daemon running on Windows. \n\nUPDATE\nThis script and the use of Unix Sockets was included in Pengwin's pengwin-setup.\nRegards",
        "url": "https://serverfault.com/questions/767994/can-you-run-docker-natively-on-the-new-windows-10-ubuntu-bash-userspace"
    },
    {
        "title": "How do I list loaded Linux module parameter values?",
        "question": "Is there a standard way to list the parameter values of a loaded Linux module? I'm essentially probing for another answer to this Linux kernel module parameters question, because the module I'm interested in doesn't have a /sys/modules/<module_name>/parameters interface.",
        "top_answer": "You can do it by using this simple one way command, which uses the /proc/modules and /sys virtual filesystems:\ncat /proc/modules | cut -f 1 -d \" \" | while read module; do \\\n echo \"Module: $module\"; \\\n if [ -d \"/sys/module/$module/parameters\" ]; then \\\n  ls /sys/module/$module/parameters/ | while read parameter; do \\\n   echo -n \"Parameter: $parameter --> \"; \\\n   cat /sys/module/$module/parameters/$parameter; \\\n  done; \\\n fi; \\\n echo; \\\ndone\n\nYou will obtain an output like this:\n...\n...\nModule: vboxnetadp\n\nModule: vboxnetflt\n\nModule: vboxdrv\nParameter: force_async_tsc --> 0\n\nModule: binfmt_misc\n\nModule: uinput\n\nModule: fuse\nParameter: max_user_bgreq --> 2047\nParameter: max_user_congthresh --> 2047\n\nModule: md_mod\nParameter: new_array --> cat: /sys/module/md_mod/parameters/new_array: Permission denied\nParameter: start_dirty_degraded --> 0\nParameter: start_ro --> 0\n\nModule: loop\nParameter: max_loop --> 0\nParameter: max_part --> 0\n\nModule: kvm_intel\nParameter: emulate_invalid_guest_state --> N\nParameter: ept --> Y\nParameter: fasteoi --> Y\nParameter: flexpriority --> Y\nParameter: nested --> N\nParameter: ple_gap --> 0\nParameter: ple_window --> 4096\nParameter: unrestricted_guest --> Y\nParameter: vmm_exclusive --> Y\nParameter: vpid --> Y\nParameter: yield_on_hlt --> Y\n\nModule: kvm\nParameter: allow_unsafe_assigned_interrupts --> N\nParameter: ignore_msrs --> N\nParameter: min_timer_period_us --> 500\n\nModule: tpm_infineon\n\nModule: joydev\n\nModule: snd_hda_codec_hdmi\nParameter: static_hdmi_pcm --> N\n...\n...\n\nHope this helps.",
        "url": "https://serverfault.com/questions/62316/how-do-i-list-loaded-linux-module-parameter-values"
    },
    {
        "title": "How does try_files work?",
        "question": "I looked at the nginx documentation and it still confuses me utterly.\nHow does try_files work? Here is what the documentation says:\nFrom NginxHttpCoreModule\n\ntry_files\nsyntax: try_files path1 [path2] uri\ndefault: none\ncontext: server, location\navailability: 0.7.27\nChecks for the existence of files in order, and returns the first file\n  that is found. A trailing slash indicates a directory - $uri /. In the\n  event that no file is found, an internal redirect to the last\n  parameter is invoked. The last parameter is the fallback URI and\n  must exist, or else an internal error will be raised. Unlike rewrite, $args are not automatically preserved if the fallback is not\n  a named location. If you need args preserved, you must do so\n  explicitly:\n\nI don't understand how it checks the paths and what if I don't want an internal error but have it resume the rest of the path in an effort to find another file?\nIf I want to try a cached file at /path/app/cache/url/index.html and if it fails to try /path/app/index.php how would I write that? If I wrote:\ntry_files /path/app/cache/ $uri\ninclude /etc/nginx/fastcgi_params;\nfastcgi_pass unix:/var/run/php-fastcgi/php-fastcgi.socket;\nfastcgi_param SCRIPT_FILENAME $document_root/index.php;\n\nI have index index.php index.html index.htm;. When I visit /urlname, will it try checking /path/app/cache/urlname/index.php then /path/app/cache/urlname/index.html? If we ignore everything after try_files is it possible for try_files to check the cache folder? I have been trying and have failed.",
        "top_answer": "try_files tries the literal path you specify in relation to the defined root directive and sets the internal file pointer. If you use for instance try_files /app/cache/ $uri @fallback; with index index.php index.html; then it will test the paths in this order:\n\n$document_root/app/cache/index.php \n$document_root/app/cache/index.html \n$document_root$uri \n\nbefore finally internally redirecting to the @fallback named location. You can also use a file or a status code (=404) as your last parameter but if using a file it must exist.\nYou should note that try_files itself will not issue an internal redirect for anything but the last parameter. Meaning you cannot do the following: try_files $uri /cache.php @fallback; as that will cause nginx to set the internal file pointer to $document_root/cache.php and serve it, but since no internal redirect takes place the locations aren't re-evaluated and as such it will be served as plain text. (The reason it works with PHP files as the index is that the index directive will issue an internal redirect)",
        "url": "https://serverfault.com/questions/329592/how-does-try-files-work"
    },
    {
        "title": "How can I kill all stopped jobs?",
        "question": "When I try to exit from my Linux server I get the message:\nThere are stopped jobs.\n: Is there a single command to kill these?",
        "top_answer": "To quickly kill all the stopped jobs under the bash, enter:\nkill -9 `jobs -ps`\n\njobs -ps lists the process IDs (-p) of the stopped (-s) jobs.\nkill -9 `jobs -ps` sends SIGKILL signals to all of them.",
        "url": "https://serverfault.com/questions/240155/how-can-i-kill-all-stopped-jobs"
    },
    {
        "title": "How can I automatically change directory on ssh login?",
        "question": "I'm trying to get ssh to automatically change to a particular directory when I log in. I tried to get that behaviour working using the following directives in ~/.ssh/config:\nHost example.net\nLocalCommand \"cd web\"\n\nbut whenever I log in, I see the following:\n/bin/bash: cd web: No such file or directory\n\nalthough though there is definitely a web folder in my home directory. Even using an absolute path gives the same message. To be clear, if I type cd web after logging in I get to the right folder.\nWhat am I missing here?\nEDIT:\nDifferent combinations of quotes/absolute paths give different error messages:\nLocalCommand \"cd web\"\n/bin/bash: cd web: No such file or directory\n\nLocalCommand cd web\n/bin/bash: line 0: cd: web: No such file or directory\n\nLocalCommand cd /home/gareth/web\n/bin/bash: line 0: cd: /home/gareth/web: Input/output error\n\nThis makes me think that the quotes shouldn't be there, and that there's another error happening.",
        "top_answer": "cd is a shell builtin.  LocalCommand is executed as:\n/bin/sh -c <localcommand>\n\nWhat you're looking to do can't really be accomplished via SSH; you need to modify the shell in some way, e.g. via bashrc/bash_profile.\n<Editing almost a decade later...>\nLocalCommand isn't what you want, anyway. That's run on your machine.\nYou want RemoteCommand. Something like this worked for me:\nHost example.net\n  RemoteCommand cd / && exec bash --login\n  RequestTTY yes\n",
        "url": "https://serverfault.com/questions/167416/how-can-i-automatically-change-directory-on-ssh-login"
    },
    {
        "title": "Turn off pager for psql's interactive output",
        "question": "We switched from PostgreSQL 8.3 to 9.0. Perhaps it's a new feature or perhaps just a configuration change, but now when output from commands (like, \\d tablename) exceeds visible vertical space, psql seem to pipe the output through something similar to less. I could not find a way to turn this behaviour off. Any advice? Thanks.\nP.S. I'm scrolling the buffer using PuTTY's Shift+PgUp/PgDn so I don't need psql's paging. Plus, when I press q in the psql's paging, its output disappears from the screen entirely (just like after running less in bash), which is wrong from the general use-cases point of view.",
        "top_answer": "TL;DR:\n\\pset pager 0\nFrom the \\pset section of the psql manual:\npager\nControls use of a pager program for query and psql help output. If the environment variable PAGER is set, the output is piped to the specified program. Otherwise a platform-dependent default (such as more) is used.\nWhen the pager option is off, the pager program is not used. When the pager option is on, the pager is used when appropriate, i.e., when the output is to a terminal and will not fit on the screen. The pager option can also be set to always, which causes the pager to be used for all terminal output regardless of whether it fits on the screen. \\pset pager without a value toggles pager use on and off.",
        "url": "https://serverfault.com/questions/235184/turn-off-pager-for-psqls-interactive-output"
    },
    {
        "title": "rm on a directory with millions of files",
        "question": "Background: physical server, about two years old, 7200-RPM SATA drives connected to a 3Ware RAID card, ext3 FS mounted noatime and data=ordered, not under crazy load, kernel 2.6.18-92.1.22.el5, uptime 545 days.  Directory doesn't contain any subdirectories, just millions of small (~100 byte) files, with some larger (a few KB) ones.\nWe have a server that has gone a bit cuckoo over the course of the last few months, but we only noticed it the other day when it started being unable to write to a directory due to it containing too many files.  Specifically, it started throwing this error in /var/log/messages:\next3_dx_add_entry: Directory index full!\n\nThe disk in question has plenty of inodes remaining:\nFilesystem            Inodes   IUsed   IFree IUse% Mounted on\n/dev/sda3            60719104 3465660 57253444    6% /\n\nSo I'm guessing that means we hit the limit of how many entries can be in the directory file itself.  No idea how many files that would be, but it can't be more, as you can see, than three million or so.  Not that that's good, mind you!  But that's part one of my question: exactly what is that upper limit?  Is it tunable?  Before I get yelled at\u2014I want to tune it down; this enormous directory caused all sorts of issues.\nAnyway, we tracked down the issue in the code that was generating all of those files, and we've corrected it.  Now I'm stuck with deleting the directory.\nA few options here:\n\nrm -rf (dir)\nI tried this first.  I gave up and killed it after it had run for a day and a half without any discernible impact.\nunlink(2) on the directory: Definitely worth consideration, but the question is whether it'd be faster to delete the files inside the directory via fsck than to delete via unlink(2).  That is, one way or another, I've got to mark those inodes as unused.  This assumes, of course, that I can tell fsck not to drop entries to the files in /lost+found; otherwise, I've just moved my problem.  In addition to all the other concerns, after reading about this a bit more, it turns out I'd probably have to call some internal FS functions, as none of the unlink(2) variants I can find would allow me to just blithely delete a directory with entries in it.  Pooh.\nwhile [ true ]; do ls -Uf | head -n 10000 | xargs rm -f 2>/dev/null; done )\nThis is actually the shortened version; the real one I'm running, which just adds some progress-reporting and a clean stop when we run out of files to delete, is:\nexport i=0;\ntime ( while [ true ]; do\n  ls -Uf | head -n 3 | grep -qF '.png' || break;\n  ls -Uf | head -n 10000 | xargs rm -f 2>/dev/null;\n  export i=$(($i+10000));\n  echo \"$i...\";\ndone )\nThis seems to be working rather well.  As I write this, it has deleted 260,000 files in the past thirty minutes or so.\nAs mentioned above, is the per-directory entry limit tunable?\nWhy did it take \"real    7m9.561s / user    0m0.001s / sys     0m0.001s\" to delete a single file which was the first one in the list returned by ls -U, and it took perhaps ten minutes to delete the first 10,000 entries with the command in #3, but now it's hauling along quite happily?  For that matter, it deleted 260,000 in about thirty minutes, but it's now taken another fifteen minutes to delete 60,000 more.  Why the huge swings in speed?\nIs there a better way to do this sort of thing?  Not store millions of files in a directory; I know that's silly, and it wouldn't have happened on my watch.  Googling the problem and looking through SF and SO offers a lot of variations on find that are not going to be significantly faster than my approach for several self-evident reasons.  But does the delete-via-fsck idea have any legs?  Or something else entirely?  I'm eager to hear out-of-the-box (or inside-the-not-well-known-box) thinking.\n\nFinal script output!:\n2970000...\n2980000...\n2990000...\n3000000...\n3010000...\n\nreal    253m59.331s\nuser    0m6.061s\nsys     5m4.019s\n\nSo, three million files deleted in a bit over four hours.",
        "top_answer": "The data=writeback mount option deserves to be tried, in order to prevent journaling of the file system. This should be done only during the deletion time, there is a risk however if the server is being shutdown or rebooted during the delete operation.\nAccording to this page,\n\nSome applications show very significant speed improvement when it is used. For example, speed improvements can be seen (...) when applications create and delete large volumes of small files.\n\nThe option is set either in fstab or during the mount operation, replacing data=ordered with data=writeback. The file system containing the files to be deleted has to be remounted.",
        "url": "https://serverfault.com/questions/183821/rm-on-a-directory-with-millions-of-files"
    },
    {
        "title": "How do you do load testing and capacity planning for web sites?",
        "question": "\nThis is a canonical question about capacity planning for web sites.\nRelated:\n\nCan you help me with my capacity planning?\nHow do you do load testing and capacity planning for databases?\n\n\nWhat are some recommended tools and methods of capacity planning for web sites and web-applications?\nPlease feel free to describe different tools and techniques for different web-servers, frameworks, etc., as well as best-practices that apply to web servers in general.",
        "top_answer": "The short answer is: Nobody can answer this question except you.\nThe long answer is that benchmarking your specific workload is something that you need to undertake yourself, because it's a bit like asking \"How long is a piece of string?\".\nA simple one-page static website could be hosted on a Pentium Pro 150 and still serve thousands of impressions every day.\nThe basic approach you need to take to answer this question is to try it and see what happens. There are plenty of tools that you can use to artificially put your system under pressure to see where it buckles.\nA brief overview of this is:\n\nPut your scenario in place\nAdd monitoring\nAdd traffic\nEvaluate results\nRemediate based on results\nRinse, repeat until reasonably happy\n\nPut your scenario in place\nBasically, in order to test some load, you need something to test against. Set up an environment to test against. This should be a fairly close guess to your production hardware if possible, otherwise you will be left extrapolating your data.\nSet up your servers, accounts, websites, bandwidth, etc. Even if you do this on VMs that's OK just as long as you're prepared to scale your results.\nSo, I'm going to set up a mid-powered virtual machine (two cores, 512\u00a0MB RAM, 4\u00a0GB HDD) and install my favourite load balancer, haproxy inside Red Hat Linux on the VM.\nI'm also going to have two web servers behind the load balancer that I'm going to use to stress test the load balancer. These two web servers are set up identically to my live systems.\nAdd Monitoring\nYou'll need some metrics to monitor, so I'm going to measure how many requests get through to my web servers, and how many requests I can squeeze through per second before users start getting a response time of over two seconds.\nI'm also going to monitor RAM, CPU and disk usage on the haproxy instance to make sure that the load balancer can handle the connections.\nHow to do this depends a lot on your platforms and is outside of the scope of this answer. You might need to review web server log files, start performance counters, or rely on the reporting ability of your stress test tool.\nA few things you always want to monitor:\n\nCPU usage\nRAM usage\nDisk usage\nDisk latency\nNetwork utilisation\n\nYou might also choose to look at SQL deadlocks, seek times, etc depending on what you're specifically testing.\nAdd traffic\nThis is where things get fun. Now you need to simulate a test load. There are plenty of tools that can do this, with configurable options:\n\nJMeter (Web, LDAP)\nApache Benchmark (Web)\nGrinder (Web)\nhttperf (Web)\nWCAT (Web)\nVisual Studio Load Test (Web)\nSQLIO (SQL Server)\n\nChoose a number, any number. Let's say you're going to see how the system responds with 10,000 hits a minute. It doesn't matter what number you choose because you're going to repeat this step many times, adjusting that number up or down to see how the system responds.\nIdeally, you should distribute these 10,000 requests over multiple load testing clients/nodes so that a single client does not become a bottleneck of requests. For example, JMeter's Remote Testing provides a central interface from which to launch several clients from a controlling Jmeter machine.\nPress the magic Go button and watch your web servers melt down and crash.\nEvaluate results\nSo, now you need to go back to your metrics you collected in step 2. You see that with 10,000 concurrent connections, your haproxy box is barely breaking a sweat, but the response time with two web servers is a touch over five seconds. That's not cool - remember, your response time is aiming for two seconds. So, we need to make some changes.\nRemediate\nNow, you need to speed up your website by more than twice. So you know that you need to either scale up, or scale out.\nTo scale up, get bigger web servers, more RAM, faster disks.\nTo scale out, get more servers.\nUse your metrics from step 2, and testing, to make this decision. For example, if you saw that the disk latency was massive during the testing, you know you need to scale up and get faster hard drives.\nIf you saw that the processor was sitting at 100% during the test, perhaps you need to scale out to add additional web servers to reduce the pressure on the existing servers.\nThere's no generic right or wrong answer, there's only what's right for you. Try scaling up, and if that doesn't work, scale out instead. Or not, it's up to you and some thinking outside the box.\nLet's say we're going to scale out. So I decide to clone my two web servers (they're VMs) and now I have four web servers.\nRinse, repeat\nStart again from Step 3. If you find that things aren't going as you expected (for example, we doubled the web servers, but the reponse times are still more than two seconds), then look into other bottlenecks. For example, you doubled the web servers, but still have a crappy database server. Or, you cloned more VMs, but because they're on the same physical host, you only achieved higher contention for the servers resources.\nYou can then use this procedure to test other parts of the system. Instead of hitting the load balancer, try hitting the web server directly, or the SQL server using an SQL benchmarking tool.",
        "url": "https://serverfault.com/questions/350454/how-do-you-do-load-testing-and-capacity-planning-for-web-sites"
    },
    {
        "title": "How to copy a large number of files quickly between two servers",
        "question": "I need to transfer a huge amount of mp3s between two serves (Ubuntu).\nBy huge I mean about a million files which are on average 300K. \nI tried with scp but it would have taken about a week. (about 500 KB/s)\nIf I transfer a single file by HTTP, I get  9-10 MB/s, but I don't know how to transfer all of them.\nIs there a way to transfer all of them quickly?",
        "top_answer": "I would recommend tar.  When the file trees are already similar, rsync performs very well.  However, since rsync will do multiple analysis passes on each file, and then copy the changes, it is much slower than tar for the initial copy.  This command will likely do what you want.  It will copy the files between the machines, as well as preserve both permissions and user/group ownerships.\ntar -c /path/to/dir | ssh remote_server 'tar -xvf - -C /absolute/path/to/remotedir'\n\nAs per Mackintosh's comment below this is the command you would use for rsync\nrsync -avW -e ssh /path/to/dir/ remote_server:/path/to/remotedir\n",
        "url": "https://serverfault.com/questions/18125/how-to-copy-a-large-number-of-files-quickly-between-two-servers"
    },
    {
        "title": "How does CTRL-ALT-DEL to log in make Windows more secure?",
        "question": "When logging into Windows, it says on that page that CTRL-ALT-DEL somehow makes Windows more secure.  I have never been able to figure a mechanism where having to press some specific key combination before logging in makes the system more secure.  I have never encountered a VMS, UNIX or related system that makes you press any key to log in -- except older terminal-based UNIXes where you press ENTER to get a login prompt.\nHow does having to press CTRL-ALT-DEL before logging in make Windows more secure?",
        "top_answer": "The Windows (NT) kernel is designed to reserve the notification of this key combination to a single process: Winlogon. So, as long as the Windows installation itself is working as it should - no third party application can respond to this key combination (if it could, it could present a fake logon window and keylog your password ;)",
        "url": "https://serverfault.com/questions/2912/how-does-ctrl-alt-del-to-log-in-make-windows-more-secure"
    },
    {
        "title": "What is the difference between unicast, anycast, broadcast and multicast traffic?",
        "question": "I have never had the privilege of working in an environment that required complicated routing or if it did require it, it was handled upstream of me. I've always used very simple static routing configurations and never needed to do any multipath routing -- hence my general confusion regarding this subject. I would like to understand multicasting and anycasting better. \n\nWhat is the difference between unicast, anycast, broadcast and multicast traffic?\nWhat situations are they generally used in and why (e.g., what applications use which method)?\nHow do you calculate how much broadcast traffic is too much for a given network segment or broadcast domain?\nWhat are the security implications of allowing broadcast and multicast traffic?\n",
        "top_answer": "Simply put:\n------------------------------------------------------------\n| TYPE      | ASSOCIATIONS     | SCOPE           | EXAMPLE |\n------------------------------------------------------------\n| Unicast   | 1 to 1           | Whole network   | HTTP    | \n------------------------------------------------------------\n| Broadcast | 1 to Many        | Subnet          | ARP     |\n------------------------------------------------------------\n| Multicast | One/Many to Many | Defined horizon | SLP     |\n------------------------------------------------------------\n| Anycast   | Many to Few      | Whole network   | 6to4    |\n------------------------------------------------------------\n\nUnicast is used when two network nodes need to talk to each other. This is pretty straight forward, so I'm not going to spend much time on it. TCP by definition is a Unicast protocol, except when there is Anycast involved (more on that below).\nWhen you need to have more than two nodes see the traffic, you have options. \nIf all of the nodes are on the same subnet, then broadcast becomes a viable solution. All nodes on the subnet will see all traffic. There is no TCP-like connection state maintained. Broadcast is a layer 2 feature in the Ethernet protocol, and also a layer 3 feature in IPv4.\nMulticast is like a broadcast that can cross subnets, but unlike broadcast does not touch all nodes. Nodes have to subscribe to a multicast group to receive information. Multicast protocols are usually UDP protocols, since by definition no connection-state can be maintained. Nodes transmitting data to a multicast group do not know what nodes are receiving. By default, Internet routers do not pass Multicast traffic. For internal use, though, it is perfectly allowed; thus, \"Defined horizon\" in the above chart. Multicast is a layer 3 feature of IPv4 & IPv6.\nTo use anycast you advertise the same network in multiple spots of the Internet, and rely on shortest-path calculations to funnel clients to your multiple locations. As far the network nodes themselves are concerned, they're using a unicast connection to talk to your anycasted nodes. For more on Anycast, try: What is \"anycast\" and how is it helpful?. Anycast is also a layer 3 feature, but is a function of how route-coalescing happens.\n\nExamples\nSome examples of how the non-Unicast methods are used in the real Internet.\nBroadcastARP is a broadcast protocol, and is used by TCP/IP stacks to determine how to send traffic to other nodes on the network. If the destination is on the same subnet, ARP is used to figure out the MAC address that goes to the stated IP address. This is a Level 2 (Ethernet) broadcast, to the reserved FF:FF:FF:FF:FF:FF MAC address.\nAlso, Microsoft's machine browsing protocol is famously broadcast based. Work-arounds like WINS were created to allow cross-subnet browsing. This involves a Level 3 (IP) broadcast, which is an IP packet with the Destination address listed as the broadcast address of the subnet (in 192.168.101.0/24, the broadcast address would be 192.168.101.255).\nThe NTP protocol allows a broadcast method for announcing time sources.\nMulticastInside a corporate network, Multicast can deliver live video to multiple nodes without having to have massive bandwidth on the part of the server delivering the video feed. This way you can have a video server feeding a 720p stream on only a 100Mb connection, and yet still serve that feed to 3000 clients. \nWhen Novell moved away from IPX and to IP, they had to pick a service-advertising protocol to replace the SAP protocol in IPX. In IPX, the Service Advertising Protocol, did a network-wide announcement every time it announced a service was available. As TCP/IP lacked such a global announcement protocol, Novell chose to use a Multicast based protocol instead: the Service Location Protocol. New servers announce their services on the SLP multicast group. Clients looking for specific types of services announce their need to the multicast group and listen for unicasted replies. \nHP printers announce their presence on a multicast group by default. With the right tools, it makes it real easy to learn what printers are available on your network.\nThe NTP protocol also allows a multicast method (IP 224.0.1.1) for announcing time sources to areas beyond just the one subnet. \nAnycast\nAnycast is a bit special since Unicast layers on top of it. Anycast is announcing the same network in different parts of the network, in order to decrease the network hops needed to get to that network. \nThe 6to4 IPv6 transition protocol uses Anycast. 6to4 gateways announce their presence on a specific IP, 192.88.99.1. Clients looking to use a 6to4 gateway send traffic to 192.88.99.1 and trust the network to deliver the connection request to a 6to4 router.\nNTP services for especially popular NTP hosts may very well be anycasted, but I don't have proof of this. There is nothing in the protocol to prevent it.\nOther services use Anycast to improve data locality to end users. Google does Anycast with its search pages in some places (and geo-IP in others). The Root DNS servers use Anycast for similar reasons. ServerFault itself just might go there, they do have datacenters in New York and Oregon, but hasn't gone there yet.\n\nNetwork concerns\nExcessive broadcast traffic can rob all nodes in that subnet of bandwidth. This is less of a concern these days with full-duplex GigE ports, but back in the half-duplex 10Mb days a broadcast storm could bring a network to a halt real fast. Those half-duplex networks with one big collision domain across all nodes were especially vulnerable to broadcast storms, which is why networking books, especially older ones, say to keep an eye on broadcast traffic. Switched/Full-Duplex networks are a lot harder to bring to a halt with a broadcast storm, but it can still happen. Broadcast is required for correct functioning of IP networks. \nMulticast has the same possibility for abuse. If one node on the multicast group starts sending huge amounts of traffic to that group, all subscribed nodes will see all of that traffic. As with broadcast, excessive Mcast traffic can increase the possibilities of collisions on such connections where that is a problem. \nMulticast is an optional feature with IPv4, but required for IPv6. The IPv4 broadcast is replaced by multicast in IPv6 (See also: Why can't IPv6 send broadcasts?). It is frequently turned off on IPv4 networks. Not coincidentally, enabling multicast is one of the many reasons network-engineers are leery of moving to IPv6 before they have to do it.\nCalculating how much traffic is too much traffic depends on a few things\n\nHalf vs Full Duplex: Half-duplex networks have much lower tolerances for bcast/mcast traffic.\nSpeed of network ports: The faster your network, the less of an issue this becomes. In the 10Mb ethernet days 5-10% of traffic on a port could be bcast traffic, if not more, but on GigE less than 1% (probably way less) is more likely. \nNumber of nodes on the network: The more nodes you have, the more unavoidable broadcast traffic you'll incur (ARP). If you have broadcast specific protocols in use, Windows browsing or other things like cluster heartbeats, where problems start will change. \nNetwork technology: Wired Ethernet is fast enough that so long as you have modern gear driving it, bcast/mcast isn't likely to cause you problems. Wireless, on the other hand, can suffer from excessive broadcast traffic as it is a shared medium amongst all nodes and therefore in a single collision domain.\n\nIn the end, Bcast and Mcast traffic rob ports of bandwidth off the top. When you start to worry is highly dependent on your individual network and tolerance for variable performance. In general, network-node counts haven't scaled as fast as network speeds so the overall broadcast percentage-as-traffic number has been dropping over time. \nSome networks disallow Multicast for specific reasons, and others have never taken the time to set it up. There are some multicast protocols that can reveal interesting information (SLP is one such) to anyone listening for the right things. Personally, I don't mind minor multicast traffic as the biggest annoyance I've seen with it is polluted network captures when I'm doing some network analysis; and for that there are filters.",
        "url": "https://serverfault.com/questions/279482/what-is-the-difference-between-unicast-anycast-broadcast-and-multicast-traffic"
    },
    {
        "title": "REJECT vs DROP when using iptables",
        "question": "Is there any reason why I would want to have \niptables -A INPUT -j REJECT\n\ninstead of \niptables -A INPUT -j DROP\n",
        "top_answer": "As a general rule, use REJECT when you want the other end to know the port is unreachable' use DROP for connections to hosts you don't want people to see.\nUsually, all rules for connections inside your LAN should use REJECT. For the Internet,  with the exception of Ident on certain servers, connections from the Internet are usually DROPPED.\nUsing DROP makes the connection appear to be to an unoccupied IP address.  Scanners may choose not to continue scanning addresses which appear unoccupied.  Given that NAT can be used to redirect a connection on the firewall, the existence of a well known service does not necessarily indicate the existence of a server on an address.\nIdent should be passed or rejected on any address providing SMTP service.  However, use of Ident look-ups by SMTP serves has fallen out of use.  There are chat protocols which also rely on a working Ident service.\nEDIT:  When using DROP rules:\n\nUDP packets will be dropped and the behavior will be the same as connecting to an unfirewalled port with no service.\nTCP packets will return an ACK/RST which is the same response that an open port with no service on it will respond with.   Some routers will respond with and ACK/RST on behalf of servers which are down.\n\nWhen using REJECT rules an ICMP packet is sent indicating the port is unavailable.",
        "url": "https://serverfault.com/questions/157375/reject-vs-drop-when-using-iptables"
    },
    {
        "title": "How can I implement ansible with per-host passwords, securely?",
        "question": "I would like to use ansible to manage a group of existing servers. I have created an ansible_hosts file, and tested successfully (with the -K option) with commands that only target a single host\nansible -i ansible_hosts host1 --sudo -K # + commands ...\n\nMy problem now is that the user passwords on each host are different, but I can't find a way of handling this in Ansible.\nUsing -K, I am only prompted for a single sudo password up-front, which then seems to be tried for all subsequent hosts without prompting:\nhost1 | ...\nhost2 | FAILED => Incorrect sudo password\nhost3 | FAILED => Incorrect sudo password\nhost4 | FAILED => Incorrect sudo password\nhost5 | FAILED => Incorrect sudo password\n\nResearch so far:\n\na StackOverflow question with one incorrect answer (\"use -K\") and one response by the author saying \"Found out I needed passwordless sudo\"\nthe Ansible docs, which say \"Use of passwordless sudo makes things easier to automate, but it\u2019s not required.\" (emphasis mine)\nthis security StackExchange question which takes it as read that NOPASSWD is required\narticle \"Scalable and Understandable Provisioning...\" which says:\n\"running sudo may require typing a password, which is a sure way of blocking Ansible forever. A simple fix is to run visudo on the target host, and make sure that the user Ansible will use to login does not have to type a password\"\narticle \"Basic Ansible Playbooks\", which says \n\"Ansible could log into the target server as root and avoid the need for sudo, or let the ansible user have sudo without a password, but the thought of doing either makes my spleen threaten to leap up my gullet and block my windpipe, so I don\u2019t\"\nMy thoughts exactly, but then how to extend beyond a single server?\nansible issue #1227, \"Ansible should ask for sudo password for all users in a playbook\", which was closed a year ago by mpdehaan with the comment \"Haven't seen much demand for this, I think most people are sudoing from only one user account or using keys most of the time.\"\n\nSo... how are people using Ansible in situations like these? Setting NOPASSWD in /etc/sudoers, reusing password across hosts or enabling root SSH login all seem rather drastic reductions in security.",
        "top_answer": "You've certainly done your research...  \nFrom all of my experience with ansible what you're looking to accomplish, isn't supported. As you mentioned, ansible states that it does not require passwordless sudo, and you are correct, it does not. But I have yet to see any method of using multiple sudo passwords within ansible, without of course running multiple configs.  \nSo, I can't offer the exact solution you are looking for, but you did ask... \n\n\"So... how are people using Ansible in situations like these? Setting\n  NOPASSWD in /etc/sudoers, reusing password across hosts or enabling\n  root SSH login all seem rather drastic reductions in security.\"\n\nI can give you one view on that. My use case is 1k nodes in multiple data centers supporting a global SaaS firm in which I have to design/implement some insanely tight security controls due to the nature of our business. Security is always balancing act, more usability less security, this process is no different if you are running 10 servers or 1,000 or 100,000. \nYou are absolutely correct not to use root logins either via password or ssh keys. In fact, root login should be disabled entirely if the servers have a network cable plugged into them. \nLets talk about password reuse, in a large enterprise, is it reasonable to ask sysadmins to have different passwords on each node? for a couple nodes, perhaps, but my admins/engineers would mutiny if they had to have different passwords on 1000 nodes. Implementing that would be near impossible as well, each user would have to store there own passwords somewhere, hopefully a keypass, not a spreadsheet. And every time you put a password in a location where it can be pulled out in plain text, you have greatly decreased your security. I would much rather them know, by heart, one or two really strong passwords than have to consult a keypass file every time they needed to log into or invoke sudo on a machine.\nSo password resuse and standardization is something that is completely acceptable and standard even in a secure environment. Otherwise ldap, keystone, and other directory services wouldn't need to exist.   \nWhen we move to automated users, ssh keys work great to get you in, but you still need to get through sudo. Your choices are a standardized password for the automated user (which is acceptable in many cases) or to enable NOPASSWD as you've pointed out. Most automated users only execute a few commands, so it's quite possible and certainly desirable to enable NOPASSWD, but only for pre-approved commands. I'd suggest using your configuration management (ansible in this case) to manage your sudoers file so that you can easily update the password-less commands list. \nNow, there are some steps you can take once you start scaling to further isolate risk. While we have 1000 or so nodes, not all of them are 'production' servers, some are test environments, etc. Not all admins can access production servers, those than can though use their same SSO user/pass|key as they would elsewhere. But automated users are a bit more secure, for instance an automated tool that non-production admins can access has a user & credentials that cannot be used in production. If you want to launch ansible on all nodes, you'd have to do it in two batches, once for non-production and once for production. \nWe also use puppet though, since it's an enforcing configuration management tool, so most changes to all environments would get pushed out through it.      \nObviously, if that feature request you cited gets reopened/completed, what you're looking to do would be entirely supported. Even then though, security is a process of risk assessment and compromise. If you only have a few nodes that you can remember the passwords for without resorting to a post-it note, separate passwords would be slightly more secure. But for most of us, it's not a feasible option.   ",
        "url": "https://serverfault.com/questions/560106/how-can-i-implement-ansible-with-per-host-passwords-securely"
    },
    {
        "title": "nginx - client request body is buffered to a temporary file",
        "question": "I get the following error in my log files every time I try to upload a large file.\na client request body is buffered to a temporary file /var/lib/nginx/body/0000000001\n\nAlthough the file uploads successfully, I always get the above error.\nI increased the client_body_buffer_size to 1000m which is what I expect the largest file uploaded to be. However, this is was just a guess and although I don't get that error anymore I am wondering if this is an appropriate value to set for the client_body_buffer_size?\nI would appreciate it if anyone can shed some light on this directive and how it should be used.",
        "top_answer": "This is a warning, not an error. That's why it was prefaced with [warn] in the log.\nIt means that the size of the uploaded file was larger than the in-memory buffer reserved for uploads.\nThe directive client_body_buffer_size controls the size of that buffer.\nIf you can afford to have 1GB of RAM always reserved for the occasional file upload, then that's fine. It's a performance optimization to buffer the upload in RAM rather than in a temporary file on disk, though with such large uploads a couple of extra seconds probably doesn't matter much. If most of your uploads are small, then it's probably a waste.\nIn the end, only you can really make the decision as to what the appropriate size is.",
        "url": "https://serverfault.com/questions/511789/nginx-client-request-body-is-buffered-to-a-temporary-file"
    },
    {
        "title": "Can I have multiple DHCP servers on one network?",
        "question": "\nThis is a Canonical Question about Redundant DHCP Servers.\n\nIs it possible to have more than one DHCP server on the same LAN? What are the implications of doing this? \n\nWhat happens if there is more than one DHCP server available? How do my clients know which one to use? \nHow can I have DHCP servers supplying addresses to more than one subnet\\network segment?\nHow can I configure multiple DHCP servers to supply addresses for the same subnet.\n",
        "top_answer": "I\u2019m assuming a basic knowledge of what DHCP does and how to configure your DHCP server of choice in this answer, but before we talk about multiple DHCP servers on the same network, let\u2019s first of all quickly re-cap how clients receive IP addresses from DHCP at the most basic level.\nDHCP on a simple network works using the DORA principle.\n\nDiscovery - the client broadcasts a message on the local network segment it's connected to, to discover available DHCP servers.\n\nOffer - a suitably configured DHCP server receives a request from a client, and offers it an address from its pool of available addresses.\n\nRequest - The client replies to the offer, requesting the address received in the Offer.\n\nAcknowledgement - The server acknowledges the request, marking the address as used in its pool of addresses, and informs the client of how long the address lease is valid for, and any other information needed.\n\n\nAny device on a network segment can be a DHCP server; it doesn't have to be the router or the domain controller or any other \"special\" device on the network.\nWhen the devices on your network first request an IP address or reach the end of their leases (or you force them to check their lease is still valid) they will simply broadcast a request for a DHCP server, and will accept an offer from the first DHCP server to reply. This is important to remember as we look at the options for multiple DHCP servers below.\nMultiple DHCP servers PT 1: Spanning multiple subnets.\nIf you have several VLANs or physical network segments that are separated into different subnets, and you want to provide a DHCP service to devices in all those subnets then there are two ways of doing this.\n\nIf the router / layer 3 switch separating them can act as a BOOTP/DHCP relay agent, then you can continue to keep all your DHCP server(s) in one or two central parts of your network and configure your DHCP server(s) to support multiple ranges of addresses. In order to support this, your router or layer 3 switch must support the BOOTP relay agent specification covered in section 4 of RFC 1542.\n\nIf your router does not support RFC 1542 BOOTP relay agents, or if some of your network segments are geographically dispersed over slow links, then you will need to place one or more DHCP server in each subnet. This \u2018local\u2019 DHCP server will only serve its own local segment\u2019s requirements, and there is no interaction between it and other DHCP servers. If this is what you want then you can simply configure each DHCP server as a standalone server, with the details of the address pool for its own subnet, and not worry about any other DHCP servers on other parts of the network. This is the most basic example of having more than one DHCP server on the same network.\n\n\nMultiple DHCP servers PT 2: DHCP servers that serve the same network segment.\nWhen most people ask about \u201cmultiple DHCP Servers on the same network\u201d, what they are usually asking for is this; they want more than one DHCP server issuing the same range of network addresses out to clients, either to split the load between multiple servers or to provide redundancy if one server is offline.\nThis is perfectly possible, though it requires some thought and planning.\nFrom a \u201cnetwork traffic\u201d point of view, the DORA process outlined at the start of this answer explains how more than one DHCP server can be present on a network segment;  the client simply broadcasts a Discovery request and the first DHCP server to respond with an Offer is the \u2018winner\u2019.\nFrom the server\u2019s point of view, each server will have a pool of addresses that it can issue to clients, known as its address scope.  DHCP servers that are serving the same subnet should not have a single \u201cshared\u201d scope, but rather they should have a \u201csplit\u201d scope.\nIn other words, if you have a range of DHCP addresses to issue to clients from 192.168.1.100 to 192.168.1.200, then both servers should be configured to serve separate parts of that range, so the first server might use parts of that scope from 192.168.1.100 to 192.168.1.150 and the second server would then issue 192.168.1.151 to 192.168.1.200.\n\nMicrosoft's more recent implementations of DHCP have a wizard to make splitting your scope like this easy to do, described in a Technet article that might be worth looking at even if you're not using the Microsoft DHCP implementation, as it illustrates the principles talked about here quite nicely and this answer is already long enough.\nSplitting the scope \u2013 best practice\nOne thing you\u2019ll hear mentioned as best practice is the 80/20 rule for splitting a DHCP scope, which means that one server will serve 80% of the addresses in that scope and the other DHCP server, which is effectively \u2018in reserve\u2019 will serve 20% of the addresses.\nThe idea behind splitting the addresses 80/20 is because 80% of the addresses available should hopefully be adequate for all the addresses needed on a subnet, and DHCP leases are typically issued for several days; so if your main DHCP server goes down for a few hours then it's unlikely that more than 20% of the machines on that subnet will need to renew their addresses during the downtime, making the 20% pool of addresses sufficient.\nThis is still reasonable advice, but it assumes two things:\n\nThat you can solve any problem with your \u201cmain\u201d DHCP server quickly enough to avoid exhausting the small pool of addresses on your reserve DHCP server.\nThat you\u2019re not interested in load balancing.\n\nThese days (as you can see from my examples) I tend to prefer 50/50 splits, which I think are a more realistic answer to the above points.\nAnother thing to consider when creating your scopes on the DHCP servers is configuring the full scope into each server and excluding the range given out by the other DHCP server. This has the benefit of \u201cself-documenting\u201d the DHCP info for the full subnet on each DHCP server which will improve clarity for anyone else trying to understand what is going on, and also in the event of one of your DHCP servers being offline for some time, you can temporarily reconfigure the exclusion range on the other server to allow it to pick up the slack.\nCombining these ideas\nLastly, it's worth remembering that you can combine the principles discussed above - you can place all your DHCP servers into one or more \"central server\" VLANs and use BOOTP relay agents on all your routers to send all DHCP requests from a very large and segmented network to a centralised DHCP service (which is what I do, see below). Or you can have DHCP servers distributed throughout your network, with a \"main\" DHCP server in its local subnet and a \"reserve\" DHCP server on a \"nearby\" network segment providing a small amount of addresses as a backup - you could even have two DHCP servers in their own network segments configured to provide an 80/20 range of addresses for each other. The most sensible choice will depend on how your physical and logical networks map to each other.\n",
        "url": "https://serverfault.com/questions/368512/can-i-have-multiple-dhcp-servers-on-one-network"
    },
    {
        "title": "How to change an EC2 instance's security group",
        "question": "I have created a new EC2 instance. It got assigned the default security group. I want to change that security group. How?",
        "top_answer": "Unless the instance is in a VPC, security groups can only be chosen before you start your instance for the first time. \nOnly VPC instances can change security group. For information on VPC see here.",
        "url": "https://serverfault.com/questions/237557/how-to-change-an-ec2-instances-security-group"
    },
    {
        "title": "How do I clear Chrome's SSL cache?",
        "question": "I have a HAProxy / stunnel server that handles SSL for our sites on AWS. During testing, I created a self-signed cert on this server and hit it from my desktop using Chrome to test that stunnel was working correctly.\nNow I have installed the legitimate cert on that server. When I hit the site from my machine in Chrome it throws the following error:\n\nError 113\n  (net::ERR_SSL_VERSION_OR_CIPHER_MISMATCH):\n  Unknown error.\n\nMy guess is that Chrome cached the key for the self-signed cert and it doesn\u2019t match that of the legitimate cert. This site works in all other browsers on my machine so it\u2019s just a Chrome problem.\nOne interesting note: When hitting the page from a incognito session (Ctrl+Shift+N), it works correctly. So it is clearly some sort of cache thing.\nI did all the things I could think of (dumped my cache, deleted certs from the Personal and Other People page in the Manage Certificates dialog, Ctrl+F5, etc.).\nMy machine is Windows 7 x64. Chrome version: 12.0.742.91.\nOn the Google Chrome Help Form, there is a description of what sounds like the same issue; however, no resolution is found.\n\nUPDATE: It seems to have \u201cfixed itself\u201d today. I hate problems like this. I still don\u2019t know what caused it or how it resolved itself. Presumably the cached cert expired or something, but I am still interested to know where this information is stored and how to verify it.",
        "top_answer": "Chrome stores SSL certificate state per host in browser history.\nSo just clear browser history (Ctrl+Shift+Del), at least the following parts:\n\nCached images and files\nHosted app data\nContent licenses\nCookies and other site data, for Chrome version 64\n\nSolution 2. If the above doesn't help, try this:\n\nClose Chrome, kill all chrome background processes\nDelete the directory:\n%USERPROFILE%/AppData/Local/Google/Chrome/User Data/CertificateTransparency\n",
        "url": "https://serverfault.com/questions/279984/how-do-i-clear-chromes-ssl-cache"
    },
    {
        "title": "Why does x86 represent 32bit when x64 represents 64bit? [closed]",
        "question": "My question is why is the shorthand for 32-bit x86 when the shorthand for 64-bit is x64?\nI'm guessing it has something to do with the days of 386s and 486s, when the 32-bit processors all ended in 86, but surely there were processors that didn't end in 86 and were still 32-bit?\nDoes anyone know the history of this or can direct me to a good source of info?",
        "top_answer": "Probably because the x86 line became synonymous with 32 bit processors for quite some time, while x64 was specifically a designation for 64 bit as applications and operating systems were transitioned over, and now there are software applications that require the 64 bit designation in order to run (like some VM software).\nIn other words, it's more marketing than technology for the reason.",
        "url": "https://serverfault.com/questions/188177/why-does-x86-represent-32bit-when-x64-represents-64bit"
    },
    {
        "title": "What ports to open for mail server?",
        "question": "I have just finished setting up a Postfix mail server on a linux (ubuntu) platform. I have it sending and receiving email and it is not an open relay. It also supports secure smtp and imap. \nNow this is a pretty beginner question but should I be leaving port 25 open? (since secure smtp is preferred). if so then why?\nAlso what about port 587?\nAlso should I require any authentication on either of these ports?\nPlease excuse my ignorance in this area :P",
        "top_answer": "Port 25 needs to be open in order for it to receive mail from the internet. All mail servers will establish a connection on port 25 and initiate TLS (encryption) on that port if necessary.\nSecure SMTP (port 465) is used only by clients connecting to your server in order to send mail out.\nPort 587 is considered a submission port. It is also what clients use to send mail out using your server. Port 587 is preferred in SMTP settings of clients over port 25 because port 25 is blocked by many ISPs. If you have port 465 open, you don't necessarily need port 587 open as well, but I believe 587 is considered a standard and 465 is considered legacy.\nPort 25 should accept anonymous connections, but not for relaying\nPorts 465 and 587 should reject anonymous connections and allow relaying.\nDon't apologize for not knowing. We all start somewhere, and nobody on here knows everything :-)",
        "url": "https://serverfault.com/questions/149903/what-ports-to-open-for-mail-server"
    },
    {
        "title": "Login without running bash_profile or bashrc",
        "question": "So let's say one typoed something in their .bashrc that prevents him (or her) from logging in via ssh (i.e. the ssh login exits because of the error in the file).  Is there any way that person could login without executing it (or .bashrc since the one runs the other), or otherwise delete/rename/invalidate the file?\nSuppose you don't have physical access to the machine, and this is the only user account with the ability to ssh in.\nFor Reference: .bash_profile includes .bashrc:\n[[ -f ~/.bashrc ]] && . ~/.bashrc\n\nEdit: Things I have tried:\nssh user@host \"rm ~/.bashrc\"\n\nscp nothing user@host:/RAID/home/tom/.bashrc\n\nssh user@host  \"/bin/bash --norc\"\n\nAll give the error:\n/RAID/home/tom/.bashrc: line 16: /usr/local/bin/file: No such file or directory\n/RAID/home/tom/.bashrc: line 16: exec: /usr/local/bin/file: cannot execute: No such file or directory\n",
        "top_answer": "I think your only options are:\n\nssh in as another user and su to your account;\nuse something like ftp or smbclient, if the relevant services are enabled on the host;\nfind an open vulnerability in an open network service and exploit it :).\nget an admin to fix the problem.\n",
        "url": "https://serverfault.com/questions/94503/login-without-running-bash-profile-or-bashrc"
    },
    {
        "title": "Engineers are using explosives to remove hard rock outside our office building. What countermeasures should we take?",
        "question": "Our building is located approx. 100 meters from the explosive charges. They happen several times per day, and really shake the entire building a lot. This is going to go on for many days and the blasts are supposed to get stronger.\nOur server rooms are nothing fancy; one of them has all the racks on hard concrete while the other one has a raised floor (the one which allows the cables to go beneath it).\nDoes anyone have any tips, countermeasures or best practices for us? \nCurrently we are thinking of the following countermeasures:\n\nDaily report of the server rooms status lights (HD lights, power supplies and so on). \nNightly check disk scan on the most important servers\nOrder in extra supply of spare harddrives\n\nEdit: Many good answers here! However one needs to be accepted. The highest voted answer at the time of this edit will get their answer accepted. ",
        "top_answer": "If I were you, I'd call the company's insurance company, and have them place out an accelerometer.\nThis way - the insurance company will know that you're not the one shaking the disks, and the insurance company will know for sure when every blast has gone off, in case your SAN dies at the exact same moment.\n(We did this.)",
        "url": "https://serverfault.com/questions/313126/engineers-are-using-explosives-to-remove-hard-rock-outside-our-office-building"
    },
    {
        "title": "GIT as a backup tool",
        "question": "On a server, install git\ncd /\ngit init\ngit add .\ngit commit -a -m \"Yes, this is server\"\n\nThen get /.git/ to point to a network drive (SAN, NFS, Samba whatever) or different disk. Use a cron job every hour/day etc. to update the changes. The .git directory would contain a versioned copy of all the server files (excluding the useless/complicated ones like /proc, /dev etc.)\nFor a non-important development server where I don't want the hassle/cost of setting it up on a proper backup system, and where backups would only be for convenience (I.E. we don't need to backup this server but it would save some time if things went wrong), could this be a valid backup solution or will it just fall over in a big pile of poop?",
        "top_answer": "You're not a silly person.  Using git as a backup mechanism can be attractive, and despite  what other folks have said, git works just fine with binary files.  Read this page from the Git Book for more information on this topic.  Basically, since git is not using a delta storage mechanism, it doesn't really care what your files look like (but the utility of git diff is pretty low for binary files with a stock configuration).\nThe biggest issue with using git for backup is that it does not preserve most filesystem metadata.  Specifically, git does not record:\n\nfile groups\nfile owners\nfile permissions (other than \"is this executable\")\nextended attributes\n\nYou can solve this by writing tools to record this information explicitly into your repository, but it can be tricky to get this right.\nA Google search for git backup metadata yields a number of results that appear to be worth reading (including some tools that already attempt to compensate for the issues I've raised here).\netckeeper was developed for backing up /etc and solves many of these problems.",
        "url": "https://serverfault.com/questions/341199/git-as-a-backup-tool"
    },
    {
        "title": "How to set default Ansible username/password for SSH connection?",
        "question": "I am using Ansible and I have this configuration in my inventory/all:\n[master]\n192.168.1.10 ansible_connection=ssh ansible_ssh_user=vagrant ansible_ssh_pass=vagrant\n\n[slave]\n192.168.1.11 ansible_connection=ssh ansible_ssh_user=vagrant ansible_ssh_pass=vagrant\n192.168.1.12 ansible_connection=ssh ansible_ssh_user=vagrant ansible_ssh_pass=vagrant\n\n[app]\n192.168.1.13 ansible_connection=ssh ansible_ssh_user=vagrant ansible_ssh_pass=vagrant\n\n[all:children]\nmaster\nslave\n\nI don't want to repeat all the parameters for each new instance. How can I configure them just in one place? Is there any file with these parameters?",
        "top_answer": "You can add following section to your inventory file:\n[all:vars]\nansible_connection=ssh\nansible_user=vagrant\nansible_ssh_pass=vagrant\n\nNote: Before Ansible 2.0 ansible_user was ansible_ssh_user.",
        "url": "https://serverfault.com/questions/628989/how-to-set-default-ansible-username-password-for-ssh-connection"
    },
    {
        "title": "How much network latency is \"typical\" for east - west coast USA?",
        "question": "At the moment we're trying to decide whether to move our datacenter from the west coast to the east coast.\nHowever, I am seeing some disturbing latency numbers from my west coast location to the east coast. Here's a sample result, retrieving a small .png logo file in Google Chrome and using the dev tools to see how long the request takes:\n\nWest coast to east coast:\n215 ms latency, 46 ms transfer time, 261 ms total\nWest coast to west coast:\n114 ms latency, 41 ms transfer time, 155 ms total\n\nIt makes sense that Corvallis, OR is geographically closer to my location in Berkeley, CA so I expect the connection to be a bit faster.. but I'm seeing an increase in latency of +100ms when I perform the same test to the NYC server. That seems .. excessive to me. Particularly since the time spent transferring the actual data only increased 10%, yet the latency increased 100%!\nThat feels... wrong... to me.\nI found a few links here that were helpful (through Google no less!) ...\n\nDoes routing distance affect performance significantly?\nHow does geography affect network latency?\nLatency in Internet connections from Europe to USA\n\n... but nothing authoritative.\nSo, is this normal? It doesn't feel normal. What is the \"typical\" latency I should expect when moving network packets from the east coast <--> west coast  of the USA?",
        "top_answer": "Speed of Light: \nYou are not going beat the speed of light as an interesting academic point.  This link works out Stanford to Boston at ~40ms best possible time. When this person did the calculation he decided the internet operates at about \"within a factor of two of the speed of light\", so there is about ~85ms transfer time.  \nTCP Window Size:\nIf you are having transfer speed issues you may need to increase the receiving window tcp size.  You might also need to enable window scaling if this is a high bandwidth connection with high latency (Called a \"Long Fat Pipe\").  So if you are transferring a large file, you need to have a big enough receiving window to fill the pipe without having to wait for window updates.  I went into some detail on how to calculate that in my answer Tuning an Elephant.  \nGeography and Latency:\nA failing point of some CDNs (Content Distribtuion Networks) is that they equate latency and geography. Google did a lot of research with their network and found flaws in this, they published the results in the white paper Moving Beyond End-to-End Path Information to Optimize CDN Performance:\n\nFirst, even though most clients are\n  served by a geographically nearby CDN\n  node, a sizeable fraction of clients\n  experience latencies several tens of\n  milliseconds higher than other clients\n  in the same region. Second, we find\n  that queueing delays often override\n  the benefits of a client interacting\n  with a nearby server.\n\nBGP Peerings:\nAlso if you start to study BGP (core internet routing protocol) and how ISPs choose peerings, you will find it is often more about finances and politics, so you might not always get the 'best' route to certain geographic locations depending on your ISP.  You can look at how your IP is connected to other ISPs (Autonomous Systems) using a looking glass router. You can also use a special whois service:\nwhois -h v4-peer.whois.cymru.com \"69.59.196.212\"\nPEER_AS | IP               | AS Name\n25899   | 69.59.196.212    | LSNET - LS Networks\n32869   | 69.59.196.212    | SILVERSTAR-NET - Silver Star Telecom, LLC\n\nIt also fun to explore these as peerings with a gui tool like linkrank, it gives you a picture of the internet around you.",
        "url": "https://serverfault.com/questions/137348/how-much-network-latency-is-typical-for-east-west-coast-usa"
    },
    {
        "title": "Do you need separate IPv4 and IPv6 listen directives in nginx?",
        "question": "I've seen various config examples for handling dual-stack IPv4 and IPv6 virtual hosts on nginx. Many suggest this pattern:\nlisten 80;\nlisten [::]:80 ipv6only=on;\n\nAs far as I can see, this achieves exactly the same thing as:\nlisten [::]:80 ipv6only=off;\n\nWhy would you use the former? The only reason I can think of is if you need additional params that are specific to each protocol, for example if you only wanted to set deferred on IPv4.",
        "top_answer": "That probably is about the only reason you would use the former construct, these days.\nThe reason you're seeing this is probably that the default of ipv6only changed in nginx 1.3.4. Prior to that, it defaulted to off; in newer versions it defaults to on.\nThis happens to interact with the IPV6_V6ONLY socket option on Linux, and similar options on other operating systems, whose defaults aren't necessarily predictable. Thus the former construct was required pre-1.3.4 to ensure that you were actually listening for connections on both IPv4 and IPv6.\nThe change to the nginx default for ipv6only ensures that the operating system default for dual stack sockets is irrelevant. Now, nginx either explicitly binds to IPv4, IPv6, or both, never depending on the OS to create a dual stack socket by default.\nIndeed, my standard nginx configs for pre-1.3.4 have the first configuration, and post-1.3.4 all have the second configuration.\nThough, since binding a dual stack socket is a Linux-only thing, my current configurations now look more like the first example, but without ipv6only set, to wit:\nlisten [::]:80;\nlisten 80;\n",
        "url": "https://serverfault.com/questions/638367/do-you-need-separate-ipv4-and-ipv6-listen-directives-in-nginx"
    },
    {
        "title": "How can I figure out my LDAP connection string?",
        "question": "We're on a corporate network thats running active directory and we'd like to test out some LDAP stuff (active directory membership provider, actually) and so far, none of us can figure out what our LDAP connection string is.  Does anyone know how we can go about finding it?  The only thing we know is the domain that we're on.",
        "top_answer": "The ASP.NET Active Directory Membership Provider does an authenticated bind to the Active Directory using a specified username, password, and \"connection string\". The connection string is made up of the LDAP server's name, and the fully-qualified path of the container object where the user specified is located.\nThe connection string begins with the URI LDAP://.\nFor the server name, you can use the name of a domain controller in that domain-- let's say \"dc1.corp.domain.com\". That gives us LDAP://dc1.corp.domain.com/ thusfar.\nThe next bit is the fully qualified path of the container object where the binding user is located. Let's say you're using the \"Administrator\" account and your domain's name is \"corp.domain.com\". The \"Administrator\" account is in a container named \"Users\" located one level below the root of the domain. Thus, the fully qualified DN of the \"Users\" container would be: CN=Users,DC=corp,DC=domain,DC=com. If the user you're binding with is in an OU, instead of a container, the path would include \"OU=ou-name\".\nSo, using an account in an OU named Service Accounts that's a sub-OU of an OU named Corp Objects that's a sub-OU of a domain named corp.domain.com would have a fully-qualified path of OU=Service Accounts,OU=Corp Objects,DC=corp,DC=domain,DC=com.\nCombine the LDAP://dc1.corp.domain.com/ with the fully qualified path to the container where the binding user is located (like, say, LDAP://dc1.corp.domain.com/OU=Service Accounts,OU=Corp Objects,DC=corp,DC=domain,DC=com) and you've got your \"connection string\".\n(You can use the domain's name in the connection string as opposed to the name of a domain controller. The difference is that the domain's name will resolve to the IP address of any domain controller in the domain. That can be both good and bad. You're not reliant on any single domain controller to be up and running for the membership provider to work, but the name happens to resolve to, say, a DC in a remote location with spotty network connectivity then you may have problems with the membership provider working.)",
        "url": "https://serverfault.com/questions/130543/how-can-i-figure-out-my-ldap-connection-string"
    },
    {
        "title": "How to install/change locale on Debian?",
        "question": "I've written a web application for which the user interface is in Dutch. I use the system's date and time routines to format date strings in the application. However, the date strings that the system formats are in English but I want them in Dutch, so I need to set the system's locale. How do I do that on Debian? I tried setting LC_ALL=nl_NL but it doesn't seem to have any effect:\n$ date\nSat Aug 15 14:31:31 UTC 2009\n$ LC_ALL=nl_NL date\nSat Aug 15 14:31:36 UTC 2009\n\nI remember that setting LC_ALL on my Ubuntu desktop system works fine. Do I need to install extra packages to make this work, or am I doing it entirely wrong?",
        "top_answer": "Edit /etc/default/locale and set the contents to:\nLANG=\"nl_NL.UTF-8\"\n\nYou can check which locales you currently have generated using:\nlocale -a\n\nYou can generate more by editing /etc/locale.gen and uncommenting the lines for the locales that you want to enable. Then you can generate them by running the command:\nlocale-gen\n\nYou can find a list of supported locales in /usr/share/i18n/SUPPORTED\nThere is more information available on the Debian wiki.",
        "url": "https://serverfault.com/questions/54591/how-to-install-change-locale-on-debian"
    },
    {
        "title": "How to check if I'm in screen session?",
        "question": "I need it to determine if hitting ctrl+d would disconnect me from server or just close current screen.\nIs it somehow possible to check if I'm right now in screen session? ",
        "top_answer": "You can look at the $STY variable (a variable set by the screen command). If it is not \"\" then you are in a screen session.\nI am in screen\n$ echo $STY \n29624.pts-1.iain-10-04\n$\n\nI am not in screen\n$ echo $STY\n\n$\n",
        "url": "https://serverfault.com/questions/257975/how-to-check-if-im-in-screen-session"
    },
    {
        "title": "Which ports do I need to open in the firewall to use NFS?",
        "question": "I'm running Ubuntu 11.10 - setting up NFS to share a directory among many other servers. Which ports are required to be opened on the firewall?",
        "top_answer": "$ rpcinfo -p | grep nfs\n\nPort 111 (TCP and UDP) and 2049 (TCP and UDP) for the NFS server.\nThere are also ports for Cluster and client status (Port 1110 TCP for the former, and 1110 UDP for the latter) as well as a port for the NFS lock manager (Port 4045 TCP and UDP). Only you can determine which ports you need to allow depending on which services are needed cross-gateway.",
        "url": "https://serverfault.com/questions/377170/which-ports-do-i-need-to-open-in-the-firewall-to-use-nfs"
    },
    {
        "title": "Using wget to recursively download whole FTP directories",
        "question": "I want to copy all of the files and folders from one host to another.\nThe files on the old host sit at /var/www/html and I only have FTP access to that server, and I can't TAR all the files.\nRegular connection to the old host through FTP brings me to the /home/admin folder.\nI tried running the following command form my new server:\nwget -r ftp://username:[email\u00a0protected]\n\nBut all I get is a made up index.html file.\nWhat the right syntax for using wget recursively over FTP?",
        "top_answer": "Try -m for --mirror\nwget -m ftp://username:[email\u00a0protected]\n",
        "url": "https://serverfault.com/questions/25199/using-wget-to-recursively-download-whole-ftp-directories"
    },
    {
        "title": "Postgres equivalent to MySQL's \\G?",
        "question": "Does anyone know if Postgres has a way to display query results \"prettily\", like how MySQL does when ending a query with \\G on the command line?  For instance, \"select * from sometable\\G\" as opposed to \"select * from sometable;\"\nMany thanks!",
        "top_answer": "I'm not familiar enough with MySQL to know what the \\G option does, but based on the documentation it looks like the psql \\x option might do what you want.\nIt's a toggle, though, so you do it before you submit the query.\n\\x\nselect * from sometable;\n",
        "url": "https://serverfault.com/questions/34741/postgres-equivalent-to-mysqls-g"
    },
    {
        "title": "Should I use tap or tun for openvpn?",
        "question": "What are the differences between using dev tap and dev tun for openvpn? I know the different modes cannot inter-operate.  What is the technical differences, other then just layer 2 vs 3 operation.  Are there different performance characteristics, or different levels of overhead.  Which mode is better.  What functionality is exclusively available in each mode.",
        "top_answer": "if it's ok to create vpn on layer 3 (one more hop between subnets) - go for tun.\nif you need to bridge two ethernet segments in two different locations - then use tap. in such setup you can have computers in the same ip subnet (eg 10.0.0.0/24) on both ends of vpn, and they'll be able to 'talk' to each other directly without any changes in their routing tables. vpn will act like ethernet switch. this might sound cool and is useful in some cases but i would advice not to go for it unless you really need it. if you choose such layer 2 bridging setup - there will be a bit of 'garbage' (that is broadcast packets) going across your vpn. \nusing tap you'll have slightly more overhead - besides ip headers also 38B or more of ethernet headers are going to be sent via the tunnel (depending on the type of your traffic - it'll possibly introduce more fragmentation). ",
        "url": "https://serverfault.com/questions/21157/should-i-use-tap-or-tun-for-openvpn"
    },
    {
        "title": "What's the difference between the single dash and double dash flags on shell commands?",
        "question": "I'm new to working in the shell and the usage of these commands seems arbitrary. Is there a reason one flag has a single dash and another might have a double dash?",
        "top_answer": "A single hyphen can be followed by multiple single-character flags.  A double hyphen prefixes a single, multicharacter option.\nConsider this example:\ntar -czf\n\nIn this example, -czf specifies three single-character flags: c, z, and f.  \nNow consider another example:\ntar --exclude\n\nIn this case, --exclude specifies a single, multicharacter option named exclude.  The double hyphen disambiguates the command-line argument, ensuring that tar interprets it as exclude rather than a combination of e, x, c, l, u, d, and e.",
        "url": "https://serverfault.com/questions/387935/whats-the-difference-between-the-single-dash-and-double-dash-flags-on-shell-com"
    },
    {
        "title": "What firewall ports need to be open to allow access to external git repositories?",
        "question": "What firewall port(s) need to be open to allow access to external git repositories?",
        "top_answer": "Specifically TCP 9418, no need for UDP.\nReference.",
        "url": "https://serverfault.com/questions/189070/what-firewall-ports-need-to-be-open-to-allow-access-to-external-git-repositories"
    },
    {
        "title": "Where does email sent to *@example.com go? [closed]",
        "question": "So I've wondered this for a long time.\nWhere does email sent to *@example.com go? If I accidentally sent sensitive information to *@example.com would some evil person (potentially at the IANA) be able to retrieve it someday?",
        "top_answer": "If you attempt to send an email to *@example.com\n\nYour SMTP will check the domain exists.\nYour SMTP server will lookup for a MX record at  example.com.\nThere is none: Your SMTP will fall back on the A record.  The IP is 174.137.125.92 (as of today)\nThe IANA has registered the domain, but has not set up a SMTP server listening on port 25 on 174.137.125.92.\nThen the behaviour depends on your SMTP.  Most servers will send you a warning, and try again later.  Eventually (usually in 3 days), the SMTP will discard the message and send you a notification of failure.\n\nBottom line: It depends on your own configuration.  But if IANA set up a server today, they might be able to receive messages you tried to send 3 days ago.",
        "url": "https://serverfault.com/questions/333548/where-does-email-sent-to-example-com-go"
    },
    {
        "title": "How do I create a symbolic link in Windows?",
        "question": "Windows Vista added the ability to create symbolic links to files and directories.  How do I create a symbolic link and what are the current consumer and server versions of Windows that support it?",
        "top_answer": "You can create a symbolic link with the command line utility mklink.\nMKLINK [[/D] | [/H] | [/J]] Link Target\n\n        /D      Creates a directory symbolic link.  Default is a file\n                symbolic link.\n        /H      Creates a hard link instead of a symbolic link.\n        /J      Creates a Directory Junction.\n        Link    specifies the new symbolic link name.\n        Target  specifies the path (relative or absolute) that the new link\n                refers to.\n\nSymbolic links via mklink are available since Windows Vista and Windows Server 2008. On Windows XP and Windows Server 2003 you can use\nfsutil hardlink create <destination filename> <source filename>\n\nAccording to msdn.microsoft, Symbolic Links are NOT supported on FAT16/32 and exFAT. It seems Windows only supports them from or to NTFS-Partitions. Future Windows operating systems are likely to continue support for mklink.\nYou can read further information about this new feature on Microsoft TechNet, Junfeng Zhang's blog or howtogeek.com.",
        "url": "https://serverfault.com/questions/7109/how-do-i-create-a-symbolic-link-in-windows"
    },
    {
        "title": "Where can I find data stored by a Windows Service running as \"Local System Account\"?",
        "question": "I'm using a service which stores data on disk.\nThe service is running as \"local system account\".\nWhere is the stored data for that system user?\nI'm thinking about C:\\Documents and Settings\\Default User but I'm not sure about that.\nCan someone confirm that?",
        "top_answer": "The data you are looking should not, by default, be located in \"C:\\Documents and Settings\\Default User\".  That is the location of the default user profile, which is the template for new user profiles.  Its only function is to be copied to a new folder for use as a user profile when a user logs onto the computer for the first time.\nIf the service is following Microsoft's guidelines, it will be storing data in \nthe application data folder (%APPDATA%) or the local application data folder (%LOCALAPPDATA% on Windows Vista and later).  It should not use the My Documents or Documents folders, but you might want to check there as well.\nOn a typical installation of Windows XP or Windows Server 2003, check the following locations for application data for programs running as Local System (NT AUTHORITY\\SYSTEM):\n\nC:\\Windows\\system32\\config\\systemprofile\\Application Data\\Vendor\\Program\nC:\\Windows\\system32\\config\\systemprofile\\Local Settings\\Application    Data\\Vendor\\Program\nC:\\Windows\\system32\\config\\systemprofile\\My Documents\n\nOn a typical installation of Windows Vista and later versions, check the following locations for application data for programs running as Local System (NT AUTHORITY\\SYSTEM):\n\nC:\\Windows\\system32\\config\\systemprofile\\AppData\\Roaming\\Vendor\\Program\nC:\\Windows\\system32\\config\\systemprofile\\AppData\\Local\\Vendor\\Program\nC:\\Windows\\system32\\config\\systemprofile\\AppData\\LocalLow\\Vendor\\Program\nC:\\Windows\\system32\\config\\systemprofile\\Documents\n\nOf course, substitute the appropriate vendor name and program name for Vendor and Program.\n[Edit - for bricelam]\nFor 32 bit processes running on 64 bit windows, it would be in SysWOW64. \n\nC:\\Windows\\SysWOW64\\config\\systemprofile\\AppData\n",
        "url": "https://serverfault.com/questions/9325/where-can-i-find-data-stored-by-a-windows-service-running-as-local-system-accou"
    },
    {
        "title": "How to run command as user who has /usr/sbin/nologin as Shell?",
        "question": "All I need to do is to run a specific script as a particular user who does have the nologin/false shell indicated in /etc/passwd.\nI would run the script as root and this should run as another user.\nRunning:\n~# su -c \"/bin/touch /tmp/test\" testuser\n\nwould work, but I need a valid shell for the testuser.\nI know I can disable the password with passwd -d testuser and leave the shell to /bin/bash this way would secure a little bit but I need to have nologin/false shell.\nBasically what I need is what crontab does when we set jobs to be running as a particular user, regardless this one has nologin/false shell.\np.s I found this thread Executing a command as a nologin user, but I have no idea how to concatenate the command su -s /bin/sh $user to the script I need to run.",
        "top_answer": "You can use the -s switch to su to run a particular shell\nsu -s /bin/bash -c '/path/to/your/script' testuser\n\n(Prepend sudo to the above if testuser is a passwordless user.)",
        "url": "https://serverfault.com/questions/351046/how-to-run-command-as-user-who-has-usr-sbin-nologin-as-shell"
    },
    {
        "title": "Why does the MySQL command line tool ignore the --port parameter?",
        "question": "This is what I'm doing:\nmysql --host=localhost --port=9999 mysql -u root -p --execute=\"show tables;\"\n\nThe command works (connecting to port 3306) no matter what I provide in --port argument. I have two mysql servers running on one machine, and want to connect to the second one by explicitly providing its port number. What's going on? Why does mysql ignore this parameter?",
        "top_answer": "When localhost parameter given, MySQL uses sockets. Use 127.0.0.1 instead.",
        "url": "https://serverfault.com/questions/306421/why-does-the-mysql-command-line-tool-ignore-the-port-parameter"
    },
    {
        "title": "How to force nginx to resolve DNS (of a dynamic hostname) everytime when doing proxy_pass?",
        "question": "I am using nginx/0.7.68, running on CentOS, with the following configuration:\nserver {\n    listen       80;\n    server_name ***;\n    index index.html index.htm index.php default.html default.htm default.php;\n\n    location / {\n            root   /***;\n            proxy_pass   http://***:8888;\n            index  index.html index.htm;\n    }\n    # where *** is my variables\n\nThe proxy_pass is to a DNS record whose IP changes frequently. Nginx caches the outdated IP address, resulting in a request to the wrong IP address.\nHow can I stop nginx from caching the IP address, when it is outdated?",
        "top_answer": "It's an intriguing question and AFAIK that's not going to work well. You can try to use the upstream module and use the directives for failover to see if it works as a hack.\n2018 edit: a lot of things changed. Check the answer by @ohaal to get real information about this.",
        "url": "https://serverfault.com/questions/240476/how-to-force-nginx-to-resolve-dns-of-a-dynamic-hostname-everytime-when-doing-p"
    },
    {
        "title": "SFTP logging: is there a way?",
        "question": "I'm wondering if there is a way to log commands received by the server. It can be all SSH commands, as long as it includes information on commands related to file transfer.\nI'm having issues with an SFTP client and the creator is asking for logs, but I am unable to find any existing logs.\nI'm looking to log on both or either CentOS or OS X (although I suspect if it's possible, it'd be similar on both).",
        "top_answer": "OpenSSH versions 4.4p1 and up (which should include the latest version with CentOS 5) have SFTP logging capability built in - you just need to configure it. \nFind this in your sshd_config (in centos, file /etc/ssh/sshd_config):\nSubsystem       sftp    /usr/libexec/openssh/sftp-server\n\nand change it to:\nSubsystem       sftp    /usr/libexec/openssh/sftp-server -l INFO\n\nINFO is just one level of detail over what you're seeing by default - it provides detailed information regarding file transfers, permission changes, etc.  If you need more info, you can adjust the log level accordingly.  The various levels (in order of detail) are:\nQUIET, FATAL, ERROR, INFO, VERBOSE, DEBUG, DEBUG1, DEBUG2, and DEBUG3\n\nAnything over VERBOSE is probably more information than you're looking for, but it might be useful.\nFinally restart the SSH service to update the changes (centos):\nsystemctl restart sshd\n",
        "url": "https://serverfault.com/questions/73319/sftp-logging-is-there-a-way"
    },
    {
        "title": "What is the purpose of the \".well-known\"-folder?",
        "question": "If've found a new error message in our log files and would like to know,  for what this .well_known folder is used for.\nWhich application/client would need to access such a folder and which application would create files inside it?\nHere are some entries of the PHP error log of one of my domain. (I removed the date, IP and target domains.)\n0000/00/00 00:00:00 [error] 851#0: *88611 access forbidden by rule, client: xxx.xxx.xxx.xxx, server: example.com, request: \"GET /.well-known/apple-app-site-association HTTP/1.1\", host: \"exampleA.com\"\n0000/00/00 00:00:00 [error] 850#0: *89749 access forbidden by rule, client: xxx.xxx.xxx.xxx, server: example.com, request: \"GET /.well-known/assetlinks.json HTTP/1.1\", host: \"exampleA.com\"\n0000/00/00 00:00:00 [error] 850#0: *89767 access forbidden by rule, client: xxx.xxx.xxx.xxx, server: example.com, request: \"GET /.well-known/assetlinks.json HTTP/1.1\", host: \"exampleB.com\"\n0000/00/00 00:00:00 [error] 853#0: *90120 access forbidden by rule, client: xxx.xxx.xxx.xxx, server: example.com, request: \"GET /.well-known/apple-app-site-association HTTP/1.1\", host: \"exampleB.com\"\n0000/00/00 00:00:00 [error] 853#0: *90622 access forbidden by rule, client: xxx.xxx.xxx.xxx, server: example.com, request: \"GET /.well-known/apple-app-site-association HTTP/1.1\", host: \"www.exampleB.com\"\n0000/00/00 00:00:00 [error] 853#0: *90926 access forbidden by rule, client: xxx.xxx.xxx.xxx, server: example.com, request: \"GET /.well-known/assetlinks.json HTTP/1.1\", host: \"www.exampleA.com\"\n0000/00/00 00:00:00 [error] 854#0: *91780 access forbidden by rule, client: xxx.xxx.xxx.xxx, server: example.com, request: \"GET /.well-known/apple-app-site-association HTTP/1.1\", host: \"exampleA.com\"\n\nFirst I thought that I could be the one who generated this, but at the times I wasn't accessing/working these domains. And these access requests comes from 3 of our domains. (with different web-applications)\n\n\nINFO1: It seems the IP is from the Googlebot (crawler) But why is it\nso important to access these files? (We don't have these files in the folders. I checked for hidden files in all domain root directories.)\n",
        "top_answer": "That /.well-known/ subdirectory is defined by RFC 5785 RFC 8615\n\nIt is increasingly common for Web-based protocols to require the\ndiscovery of policy or other information about a host (\"site-wide\nmetadata\") before making a request.  For example, the Robots\nExclusion Protocol http://www.robotstxt.org/ specifies a way for\nautomated processes to obtain permission to access resources;\nlikewise, the Platform for Privacy Preferences [W3C.REC-P3P-20020416]\ntells user-agents how to discover privacy policy beforehand.\nWhile there are several ways to access per-resource metadata (e.g.,\nHTTP headers, WebDAV's PROPFIND [RFC4918]), the perceived overhead\n(either in terms of client-perceived latency and/or deployment\ndifficulties) associated with them often precludes their use in these\nscenarios.\nWhen this happens, it is common to designate a \"well-known\nlocation\"    for such data, so that it can be easily located.\nHowever, this    approach has the drawback of risking collisions, both\nwith other such    designated \"well-known locations\" and with\npre-existing resources.\nTo address this, this memo defines a path prefix in HTTP(S) URIs\nfor    these \"well-known locations\", /.well-known/.  Future\nspecifications    that need to define a resource for such site-wide\nmetadata can    register their use to avoid collisions and minimise\nimpingement upon    sites' URI space.\n\nThe reason that you see access forbidden errors might be the result of a blanket block on requests for hidden files/folders (paths starting with a dot .).\nIn case you do have useful content in /.well-known, this Q&A may be of interest.\nLocations within that directory are then used for specific purposes,\n\n/.well-known/apple-app-site-association is for IOS universal links\n/.well-known/assetlinks.json is an (Android) Digital Asset Link\n\nBoth of which support a similar purpose, they allow the site operator to instruct a visitor to open the site in an associated app, rather than in the (mobile) browser.\n\n/.well-known/acme-challenge/ is the location for the proposed Automatic Certificate Management Environment standard as the location for HTTP Identifier Validation as used by Let's Encrypt.\n\nIANA maintains a comprehensive list of assigned well-known locations on www.iana.org/assignments/well-known-uris/well-known-uris.xhtml and a similar list on Wikipedia also includes a few different URI's that are not officially assigned and registered by IANA.",
        "url": "https://serverfault.com/questions/795467/what-is-the-purpose-of-the-well-known-folder"
    },
    {
        "title": "How to start/stop iptables on Ubuntu?",
        "question": "How can I start/stop the iptables service on Ubuntu?\nI have tried \n service iptables stop\n\nbut it is giving \"unrecognized service\".\nWhy is it doing so? Is there any other method?",
        "top_answer": "I don't know about \"Ubuntu\", but in Linux generally, \"iptables\" isn't a service - it's a command to manipulate the netfilter kernel firewall. You can \"disable\" (or stop) the firewall by setting the default policies on all standard chains to \"ACCEPT\", and flushing the rules.\niptables -P INPUT ACCEPT\niptables -P OUTPUT ACCEPT\niptables -P FORWARD ACCEPT\niptables -F\n\n(You  may need to flush other tables, too, such as \"nat\", if you've used them)\nThe following article on the Ubuntu website describes setting up iptables for use with NetworkManager: https://help.ubuntu.com/community/IptablesHowTo",
        "url": "https://serverfault.com/questions/129086/how-to-start-stop-iptables-on-ubuntu"
    },
    {
        "title": "How can I prevent the warning No xauth data; using fake authentication data for X11 forwarding?",
        "question": "Every time I initiate an ssh connection from my Mac to a Linux (Debian) I do get this warning:\nNo xauth data; using fake authentication data for X11 forwarding.\n\nThis also happens for tools that are using ssh, like git or mercurial.\nI just want to make a local change to my system in order to prevent this from appearing. \nNote: I do have X11 server (XQuartz 2.7.3 (xorg-server 1.12.4)) on my Mac OS X (10.8.1) and it is working properly, I can successfully start clock locally or remotely.",
        "top_answer": "None of the posted solutions worked for me.  My client (desktop) system is running macOS 10.12.5 (Sierra).  I added -v to the options for the ssh command and it told me,\ndebug1: No xauth program.\n\nwhich means it doesn't have a correct path to the xauth program.  (On this version of macOS the path to xauth is nonstandard.)  The solution was to add this line to /etc/ssh/ssh_config (may be /etc/ssh/config in some setups) or in ~/.ssh/config (if you don't have admin rights):\nXAuthLocation /opt/X11/bin/xauth\n\nNow the warning message is gone.",
        "url": "https://serverfault.com/questions/422908/how-can-i-prevent-the-warning-no-xauth-data-using-fake-authentication-data-for"
    },
    {
        "title": "How to re-order windows, change the scroll shortcut, and modify the status bar contents in GNU Screen?",
        "question": "I learned to use gnu-screen and have been using it for the past several weeks. I got a grip of the basics now, and would be very helpful if I get these queries clarified:\n\nHow do I rename / reorder / move windows in Linux screen utility?\nFor eg., after a few days of use, I come to a state where the window-numbers are 2, 3, 6, 8. Now I want a new-window to be created at #9, or #7. Is this possible? If yes, how?\nAlso, is it possible to 'move' the window #6 to #4 or #7? If yes, how?\nScrolling shortcut: Scroll takes Ctrl-A + Esc, and then Ctrl-u / Ctrl-d. Is there a way to map PgUp / PgDn to do these directly? Or, is there a way to map some key (like F5) to take me to copy mode, and then PgUp / PgDn for scrolling?\nI have enabled \"caption always\", so the current window title is displayed always at the bottom. Is there a way to display the current window's log-file-name and logging status (on/off) in the same caption bar?\n",
        "top_answer": "You can renumber the current window with ctrl+a :number x where x is a numeric argument.\nYou can rename the current window with ctrl+a A",
        "url": "https://serverfault.com/questions/244294/how-to-re-order-windows-change-the-scroll-shortcut-and-modify-the-status-bar-c"
    },
    {
        "title": "How to let 'cp' command don't fire an error when source file does not exist?",
        "question": "I'm using Mac OS X.\nI'm trying to copying some files with cp command for a build script like this.\ncp ./src/*/*.h ./aaa\n\nBut this command fires an error if there is no .h file in ./src directory.\nHow to make the command don't fire the error? (silent failure) The error makes build result fail, but I just want to copy when only there are some header file.",
        "top_answer": "If you're talking about the error message, you can suppress that by sending it to the bit bucket:\ncp ./src/*/*.h ./aaa 2>/dev/null\n\nIf you want to suppress the exit code and the error message:\ncp ./src/*/*.h ./aaa 2>/dev/null || :\n",
        "url": "https://serverfault.com/questions/153875/how-to-let-cp-command-dont-fire-an-error-when-source-file-does-not-exist"
    },
    {
        "title": "What does this nginx error \"rewrite or internal redirection cycle\" mean?",
        "question": "tail -f /var/log/nginx/error.log\n2013/05/04 23:43:35 [error] 733#0: *3662 rewrite or internal redirection cycle while internally redirecting to \"/index.html\", client: 127.0.0.1, server: _, request: \"GET /robots.txt HTTP/1.1\", host: \"kowol.mysite.net\"\nHTTP/1.1\", host: \"www.joesfitness.net\"\n2013/05/05 00:49:14 [error] 733#0: *3783 rewrite or internal redirection cycle while internally redirecting to \"/index.html\", client: 127.0.0.1, server: _, request: \"GET / http://www.qq.com/ HTTP/1.1\", host: \"www.qq.com\"\n2013/05/05 03:12:33 [error] 733#0: *4232 rewrite or internal redirection cycle while internally redirecting to \"/index.html\", client: 127.0.0.1, server: _, request: \"GET / HTTP/1.1\", host: \"joesfitness.net\"\n\nI am getting these from nginx error log, I don't have a \"kowol\" sub domain, I don't have any links to qq.com or joesfitness.net on my site. Whats going on?\nEdit: Nginx default config:\nserver {\n    listen   8080; ## listen for ipv4; this line is default and implied\n    listen   [::]:8080 default ipv6only=on; ## listen for ipv6\n\n    root /usr/share/nginx/www;\n    index index.php index.html index.htm;\n\n    # Make site accessible from http://localhost/\n    server_name _;\n\n    location / {\n        # First attempt to serve request as file, then\n        # as directory, then fall back to index.html\n        try_files $uri $uri/ /index.html;\n        # Uncomment to enable naxsi on this location\n        # include /etc/nginx/naxsi.rules\n    }\n\n    location /doc/ {\n        alias /usr/share/doc/;\n        autoindex on;\n        allow 127.0.0.1;\n        deny all;\n    }\n\n    # Only for nginx-naxsi : process denied requests\n    #location /RequestDenied {\n        # For example, return an error code\n        #return 418;\n    #}\n\n    #error_page 404 /404.html;\n\n    # redirect server error pages to the static page /50x.html\n    #\n    #error_page 500 502 503 504 /50x.html;\n    #location = /50x.html {\n    #   root /usr/share/nginx/www;\n    #}\n\n    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n    #\n    location ~ \\.php$ {\n        fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n        # NOTE: You should have \"cgi.fix_pathinfo = 0;\" in php.ini\n\n        # With php5-cgi alone:\n        fastcgi_pass 127.0.0.1:9000;\n        #With php5-fpm:\n        #fastcgi_pass unix:/var/run/php5-fpm.sock;\n        fastcgi_index index.php;\n        include fastcgi_params;\n    }\n\n    # deny access to .htaccess files, if Apache's document root\n    # concurs with nginx's one\n    #\n    #location ~ /\\.ht {\n    #   deny all;\n    #}\n}\n",
        "top_answer": "It's a strange one all right, though I'm going to bet the problem is with:\n        try_files $uri $uri/ /index.html;\n\nThe problem here is that the second parameter here, $uri/, causes each of the files in your index directive to be tried in turn. If none are found, it then moves on to /index.html, which causes the same location block to be re-entered, and since it still doesn't exist, you get an endless loop.\nI would rewrite this as:\n        try_files $uri $uri/ =404;\n\nto return a 404 error if none of the index files you specified in the index directive exist.\n\nBTW, those requests you are seeing are Internet background noise. In particular, they are probes to determine whether your web server is an open proxy and can be abused to hide a malicious user's origin when he goes to perform malicious activity. Your server isn't an open proxy in this configuration, so you don't really need to worry about it.",
        "url": "https://serverfault.com/questions/505098/what-does-this-nginx-error-rewrite-or-internal-redirection-cycle-mean"
    },
    {
        "title": "How to do the port forwarding from one ip to another ip in same network?",
        "question": "I would like do some NAT in iptables. So that, all the packets coming to 192.168.12.87 and port 80 will be forwarded to 192.168.12.77 port 80.\nHow to do this with iptables?\nOr \nAny other ways to achieve the same?",
        "top_answer": "These rules should work, assuming that iptables is running on server 192.168.12.87 :\n#!/bin/sh\n\necho 1 > /proc/sys/net/ipv4/ip_forward\n\niptables -F # DO NOT FLUSH IF YOU'RE SSHING, YOU WILL LOCK YOUR SELF OUT\niptables -t nat -F\niptables -X\n\niptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination 192.168.12.77:80\niptables -t nat -A POSTROUTING -p tcp -d 192.168.12.77 --dport 80 -j SNAT --to-source 192.168.12.87\n\nYou have to DNAT incoming traffic on port 80, but you will also need to SNAT the traffic back.\n\nAlternative (and best approach IMHO) :\nDepending on what your Web Server is (Apache, NGinx) you should consider an HTTP Proxy on your front-end server (192.168.12.87) :\n\nmod_proxy (Apache)\n\nproxy_pass (NGinx)\n\n",
        "url": "https://serverfault.com/questions/586486/how-to-do-the-port-forwarding-from-one-ip-to-another-ip-in-same-network"
    },
    {
        "title": "How long before an s3 bucket can be created with same name after deletion?",
        "question": "Title is quite explanatory, but I have just deleted an s3 bucket as it was in the wrong region and am wanting to recreate it in the correct region with same name as the just deleted one.\nIs there any documentation of this or user experience?",
        "top_answer": "The S3 docs used to say:\n\nWhen you delete a bucket, there may be a delay of up to one hour before the bucket name is available for reuse in a new region or by a new bucket owner. If you re-create the bucket in the same region or with the same bucket owner, there is no delay.\n\nBut now they just say:\n\n... it might take some time before the name can be reused ...\n",
        "url": "https://serverfault.com/questions/554374/how-long-before-an-s3-bucket-can-be-created-with-same-name-after-deletion"
    },
    {
        "title": "Can you mount a network switch upside down?",
        "question": "We just took delivery of a new Avaya 2500 48-port switch, that has 24 PoE ports. The problem is that all the PoE ports are on the left-hand size of the switch, and our PoE device cables can only reach the right-hand side of the switch (we're upgrading from an old switch to a new one, and the old one had them on the right-hand side. This is the problem with doing neat cabling).\n\nCan I just mount the switch upside down? This would move the left-hand ports to the right-hand side and problem solved.\nMy largest concern is that airflow or cooling might not work, but I can't see any visible breathing holes in the bottom or top of the switch which leads me to believe it will be OK, but better safe than sorry.",
        "top_answer": "\u02d9s\u026f\u01ddlqo\u0279d \u028eu\u0250 p\u0250\u0265 \u0279\u01dd\u028c\u01ddu \u01dd\u028c,I pu\u0250 u\u028dop \u01ddp\u0131sdn p\u01dd\u0287uno\u026f \u01dd\u0279\u0250 s\u01dd\u0265\u0254\u0287\u0131\u028ds \u029e\u0279o\u028d\u0287\u01ddu \u028e\u026f \u025fo ll\u2200\n(Seriously: you should have no problem mounting a switch upside-down - just make sure you don't create any ventilation issues)",
        "url": "https://serverfault.com/questions/384397/can-you-mount-a-network-switch-upside-down"
    },
    {
        "title": "how to create a cron job that runs on the first day of month [duplicate]",
        "question": "There are fields on my server's control panel like this\nMinute -  Hour -  Day of month - Month - Day of the week - Command\nHow can I create a cron job runs on first day of the month with this fields?",
        "top_answer": "This will run the command foo at 12:00AM on the first of every month\n0 0 1 * * /usr/bin/foo\n\nThis article describes the various fields, look to the bottom of the page: http://en.wikipedia.org/wiki/Cron\nTo add this to your cron file, just use the command\ncrontab -e\n",
        "url": "https://serverfault.com/questions/87472/how-to-create-a-cron-job-that-runs-on-the-first-day-of-month"
    },
    {
        "title": "Can I send some text to the STDIN of an active process running in a screen session?",
        "question": "I have a long-running server process inside a screen session on my Linux server. It's a bit unstable (and sadly not my software so I can't fix that!), so I want to script a nightly restart of the process to help stability. The only way to make it do a graceful shutdown is to go to the screen process, switch to the window it's running in, and enter the string \"stop\" on its control console.\nAre there any smart redirection contortions I can do to make a cronjob send that stop command at a fixed time every day?",
        "top_answer": "This answer doesn't solve the problem, but it's left here because 30+ people found it useful, otherwise I would have deleted it long time ago.\nWrite to /proc/*pid of the program*/fd/0. The fd subdirectory contains the descriptors of all the opened files and file descriptor 0 is the standard input (1 is stdout and 2 is stderr).\nYou can use this to output messages on the tty where a program is running, though it does not allow you to write to the program itself.\nExample\nTerminal 1:\n[ciupicri@hermes ~]$ cat\nshows on the tty but bypasses cat\n\nTerminal 2:\n[ciupicri@hermes ~]$ pidof cat\n7417\n[ciupicri@hermes ~]$ echo \"shows on the tty but bypasses cat\" > /proc/7417/fd/0\n",
        "url": "https://serverfault.com/questions/178457/can-i-send-some-text-to-the-stdin-of-an-active-process-running-in-a-screen-sessi"
    },
    {
        "title": "Tell Jenkins to run a specific project on a particular slave node",
        "question": "How do I tell Jenkins to run a specific project on a particular slave? \nI've set up a Jenkins master node, and a slave node that I want to use for staging an application. But I can't figure out how to configure the project to run on the slave node I created.",
        "top_answer": "Set the \"Restrict where this job can be run\" check box in your job configuration and specify the name of your slave.  \nIf you add more slaves later, you can set labels for each slave and specify those in your job configs.\nSee this reference documentation:\n\nhttps://wiki.jenkins-ci.org/display/JENKINS/Distributed+builds\n",
        "url": "https://serverfault.com/questions/359793/tell-jenkins-to-run-a-specific-project-on-a-particular-slave-node"
    },
    {
        "title": "How can I tell what version of IIS is installed?",
        "question": "I've got a box running Win2k3 and some directions from Microsoft KB about SSL certificates, for IIS 5.0 and 6.0. How can I tell which version of IIS is currently installed?",
        "top_answer": "As a more general answer, not specifically aimed at your question, Microsoft has a support article which lists all old versions and the operating systems that provide each one.\nIIS version   Built-in  \n5.0           Windows 2000\n5.1           Windows XP Pro\n6.0           Windows Server 2003\n7.0           Windows Vista and Windows Server 2008\n7.5           Windows 7 and Windows Server 2008 R2\n8.0           Windows 8 and Windows Server 2012\n\nCurrent versions are on Wikipedia\n8.5           Windows 8.1 and Windows Server 2012 R2\n10.0 v1607    Windows Server 2016 and Windows 10.*\n10.0 v1709    Windows Server 2016 v1709 and Windows 10.*\n10.0 v1809    Windows Server 2019  and Windows 10.* October\n",
        "url": "https://serverfault.com/questions/10518/how-can-i-tell-what-version-of-iis-is-installed"
    },
    {
        "title": "Allow SFTP but disallow SSH?",
        "question": "I'm starting a very little hosting company for a few friends and little clients, nothing big.\nI want to give my \"clients\" the right to manage their files on the server. I hate FTP as it is not secure and it's in my opinion obsolete.\nSo I'd like to allow my users to connect through SFTP but not allow them to connect through SSH. (I know, I know, SFTP is using SSH). But I was just wondering, is it possible?\nSo I wouldn't have to install a FTP service on the server and everything would be awesome!",
        "top_answer": "Starting with version 4.9 OpenSSH (not available in centos 5.x but ChrootDirectory feature was backported) has an internal-sftp subsystem:\nSubsystem sftp internal-sftp\n\nAnd then block other uses:\nMatch group sftponly\n     ChrootDirectory /upload\n     X11Forwarding no\n     AllowTcpForwarding no\n     AllowAgentForwarding no\n     ForceCommand internal-sftp -d /%u\n\nAdd your users to the sftponly group. The chroot directory must be owned by root, and cannot be group-writeable, so create a subdirectory for each user that's owned by the appropriate user (if you match their home directory, it will be the default working directory when connecting). I'd also set /bin/false as the user's shell.\nAs an example, users can then upload single files with:\nsftp username@hostname <<< 'put filename.ext /'\n\n(scp will hopefully soon be modified to use sftp so this will become easier)",
        "url": "https://serverfault.com/questions/354615/allow-sftp-but-disallow-ssh"
    },
    {
        "title": "Why is Enterprise Storage so expensive?",
        "question": "\nThis is a Canonical Question about the Cost of Enterprise Storage.\n  See also the following question:\n\nWhat's the best way to explain storage issues to developers and other users \n\n\nRegarding general questions like:\n\nWhy do I have to pay 50 bucks a month per extra gigabyte of storage?\nOur file server is always running out of space, why doesn't our sysadmin just throw an extra 1TB drive in there?\nWhy is SAN equipment so expensive?\n\nThe Answers here will attempt to provide a better understanding of how enterprise-level storage works and what influences the price. If you can expand on the Question or provide insight as to the Answer, please post.",
        "top_answer": "\nServer hard-drive capacities are miniscule compared to desktop hard-drive capacities. 450 and 600GB are not uncommon sizes to see in brand new servers, and you could buy many 4TB SATA desktop drives for the price of one 600GB SAS (server) hard drive.\nYour SATA hard-drive in your desktop PC at home is like a muscle car from Ford, or GM or Mercedes or any other manufacturer of cars for every-day people (large capacity V8 or V12, 5 or 6 litres). Because they need to be driven by people who don't have a racing license, or understand how an internal combusion engine works, they have very large tolerances. They have rev limiters, they're designed to run on any oil of a certain rating, they have service intervals say 10,000km apart, but if you miss a service interval by a few weeks it won't explode in your face. They don't catch fire when you drive long distances.   \nThe SAS drive in a server is more akin to a Formula 1 engine. They're really small (2.4 litres) but have immense power outputs because of their tiny tolerances. They rev higher, and often have no rev limiter (which means they suffer serious damage if driven incorrectly), and if you miss a service interval (which is every few hours) they explode.  \nYou're basically comparing chalk and cheese. Numbers and a full breakdown are discussed in the Intel Whitepaper Enterprise-class versus Desktop-class Hard Drives\nLet's talk some hard numbers here. Let's say you request 1MB of additional data (a nice round number). How much data is that really?  Well, your 1MB of data is going to go into a RAID array. Let's say they're being safe and making that into RAID1. Your 1MB of data is mirrored, so it's actually 2MB of data.\nLet's say your data is inside a SAN. In case of a SAN node failure, your data is synchronized at a byte-level to a 2nd SAN node. So it's duplicated, and your 2MB of data is now 4MB.\nYou expect your provider to keep on-site backups, so your data can be restored in the case of a non-disaster emergency? Any decent provider is going to provide you with at least 1 on-site backup, perhaps more. Let's say they take snapshots once a week for three weeks on-site. That's an extra 3MB of data, so you're now up to 7MB.\nIf there is a critical disaster, your provider had better have a copy kept off-site somewhere. Even if it's a month old, it should exist. So now you're up to 8MB.\nIf it's a really high-level provider, they may even have a disaster recovery site that's synchronized live. These disks will be RAIDed as well, so that's an extra 2MB, and thus you're up to 10MB of data.\nYou're going to have to transfer that data eventually. What? Transfer it? Yes, data transfer costs money. It costs money when you download it, access it over the internet, it even costs money to back it up (someone has to take those tapes out of the office, and it could be that your 1MB of data means they have to purchase an extra set of tapes and transfer them somewhere).\nWhen your SATA home drive fails you get to call tech support and convince them your drive is dead. Then send your drive in to the manufacturer (on your own dime most times). Wait a week. Get a replacement drive back and have to reinstall it (it almost certainly isn't hot swappable or in a drive sled already).\nWhen that SAS drive fails you call the tech support. They almost never question your opinion that the drive needs immediate replacement and drop ship a new drive; usually the new drive is delivered later that same day, otherwise the next day is very common too. Commonly the manufacturer will send a representative out to actually install the drive if you don't know how (very handy if you plan on taking a vacation ever and need for things to keep working while you are away).\nEnterprise drives have tight tolerances, see #2 above, and tend to last about 10 times longer than Consumer grade drives (MTBF). Enterprise drives almost always support advanced error and failure detection, which a Google report found works about 40% of the time, but that's something anyone would prefer to a computer suddenly dying.\nWhen you have a single drive in your home computer, its statistical chances of failure are simply that of the drive. Drives used to be rated in MTBF (where SAS drives still enjoy ~50% higher ratings or more), now it's more common to see error rates. A typical SAS drive is 10 to 1,000 times less likely to have an unrecoverable error (with 100x the most common that I found recently). (error rates according to manufacturer documentation supplied by Seagate, Western Digital, and Hitachi; no bias intended; expressly disclaim indemnification).\nError rates are particularly important not when you run across an unrecoverable error on a drive, but when another drive in the same array fails and you are not relying on all the drives in an array to be readable in order to recover the failed disk.\nSAS is a derivative of SCSI, which is a storage protocol. SATA is based on ATA, which is itself based on the ISA bus (that 8/16-bit bus in computers from the dinosaur age). The SCSI storage protocol has more extensive commands for optimizing the manner in which data is transferred from drives to controllers and back. This uptick in efficiency would make an otherwise equal SAS drive inherently faster, especially under extreme work loads, than a SATA drive; it also increases the cost.\nThere are fewer SAS drives produced, economies of scale dictate that they will be more expensive all else being equal.\nSAS drives typically come in 10k or 15k rotational speeds; while SATA typically come in 5.4k or 7.2k. SAS drives, particularly the 2.5\" size which are becoming increasingly popular, have faster seek times. The two combined dramatically increase the IOps a drive can perform, typically a SAS drive is ~3x faster. When multiple users are demanding disparate data, the IOps capacity of the drive/array becomes a critical performance indicator. \nThe drives in a data center are typically powered up all the time. Studies have found that drive failure is influenced by the number of heating/cooling cycles it goes through (from running vs turned off). Keeping them running all the time typically increases the drive's life. The consequence of this is that the drives consume electricity. This electricity has to be supplied by something (in the case of a large DC the drives alone might take more power than a small neighborhood of houses). They also need to dissipate that heat somewhere, requiring cooling systems (which themselves take more power to operate).\nInfrastructure and staffing costs. Those drives are in high-end NAS or SAN units. Those units are expensive, even without the expensive drives in them. They require expensive staff to deploy and maintain them. The buildings that those NAS and SAN units are in are expensive to operate (see the point about cooling, above, but there's a lot more going on there.) The backup software is typically not free (nor are the licenses for things like mirroring), and the staff to deploy and maintain backups are usually pricey too. The cost of renting off-site tape delivery and storage is just one more of the many things that start to pile up when you need more storage.\n\nKeeping in mind that the capacity of their drives may well be 1/10th the size of a desktop drive, and five times the price, your 1MB of data is actually 10, and all the other differences, there's no way you can draw any meaningful conclusions between the price of your desktop storage and the price of enterprise level storage.",
        "url": "https://serverfault.com/questions/263694/why-is-enterprise-storage-so-expensive"
    },
    {
        "title": "How do I prevent apache from serving the .git directory?",
        "question": "I have started using git for deployment of websites for testing. How do I prevent apache from serving the .git directory contents? \nI tried\n<Directorymatch \"^/.*/\\.svn/\">\nOrder deny,allow\nDeny from all\n</Directorymatch>\n\nwith no success.\nI know that I can create a .htaccess file in each .git directory and deny access, but I wanted something I could put into the main config file that makes this global across all websites. ",
        "top_answer": "This has the same effect as many of the other answers but is much simpler:\nRedirectMatch 404 /\\.git\n\nThis can go into .htaccess or your server config file. It hides any file or directory whose name begins with .git (e.g. a .git directory or .gitignore file) by returning a 404. So not only are the contents of your Git repo hidden, its very existence is hidden too.",
        "url": "https://serverfault.com/questions/128069/how-do-i-prevent-apache-from-serving-the-git-directory"
    },
    {
        "title": "Best practice: Should I always install a fresh OS for new employees?",
        "question": "I had an argument with a superior about this. Though at first glance the prior user of a laptop only did work in his own documents-folders, should I always install a new OS for the next user or is deleting the old profile enough? The software that is installed is mostly also needed by the next user.\nI think an install is needed, but except my own argument of viruses and private data, what reasons are there for doing so?\nAt our company it is allowed to use the PC for e.g. private mail, on some PCs are even games installed. We have kinda mobile users, that are often on site at a customer, so I don't really blame them.\nAlso because of that we have a lot of local admins out there.\nI know both the private use and the availability of local admin-accounts aren't good ideas, but that's how it was handled before I worked here and I can only change this once I am out of traineeship ;)\nEdit: I think all of the answers posted are relevant, and I also know that a couple of the practices we have at my company aren't the best to begin with (local admin for too many people for example ;).\nAs of now, I think the most usable answer for a discussion would be the one from Ryder. Although the example he gave in his answer may be exaggerated, it has happened before that a former employee forgot private data. I recently found a retail copy of the game Runaway in a old laptop and we had a couple of cases of remaining private images, too.",
        "top_answer": "Absolutely you should. It's not just common sense from a security POV, it should also be practice as matter of business ethics. \nLet's imagine the following scenario: Alice leaves, and her computer is transferred to Bob. Bob didn't know it, but Alice was into illegal shota porn and left several files tucked away outside of her profile. IT wipes her profile and nothing else, which included only her browsing history and local files.\nOne day, Bob is checking out the bells and whistles on his shiny new work machine, while sitting at a Starbucks\u2122 and sipping at a latte. He stumbles across Alice's cache and innocently clicks on a file that looks strange. Suddenly, every head in the store whips around to watch in horror as Bob's PC flouts several state and federal regulations at full volume. One little girl in the corner starts crying. \nBob is mortified. After six months of depression and after having been fired for his unintentional act of public indecency (and possible criminal charges), he finds himself a really crackin' legal team and lays waste to his former employer with an outrageously damaging lawsuit. Alice is in Thailand and escapes extradition.\n\nMaybe all this is a little beyond the pale, but it absolutely could happen if you don't take the time to scour through a former employee's every action. Or you could save time, and reinstall from scratch.",
        "url": "https://serverfault.com/questions/783556/best-practice-should-i-always-install-a-fresh-os-for-new-employees"
    },
    {
        "title": "Setting the \"Expires\" HTTP header for static content served from Nginx?",
        "question": "I'm using nginx to server my static content, is there a way that I can set the expires headers for every file that meets a specific rule?  For example can I set the expires header for all files that have an extension of '.css'?",
        "top_answer": "I prefer to do a more complete cache header, in addition to some more file extensions.  The '?' prefix is a 'non-capturing' mark, nginx won't create a $1. It helps to reduce unnecessary load.\nlocation ~* \\.(?:ico|css|js|gif|jpe?g|png)$ {\n    expires 30d;\n    add_header Pragma public;\n    add_header Cache-Control \"public\";\n}\n",
        "url": "https://serverfault.com/questions/23157/setting-the-expires-http-header-for-static-content-served-from-nginx"
    },
    {
        "title": "How to display certain lines from a text file in Linux?",
        "question": "I guess everyone knows the useful Linux cmd line utilities head and tail. head allows you to print the first X lines of a file, tail does the same but prints the end of the file. What is a good command to print the middle of a file? something like middle --start 10000000 --count 20 (print the 10\u2019000\u2019000th till th 10\u2019000\u2019010th lines).\nI'm looking for something that will deal with large files efficiently. I tried tail -n 10000000 | head 10 and it's horrifically slow.",
        "top_answer": "sed -n '10000000,10000020p' filename\n\nYou might be able to speed that up a little like this:\nsed -n '10000000,10000020p; 10000021q' filename\n\nIn those commands, the option -n causes sed to \"suppress automatic printing of pattern space\". The p command \"print[s] the current pattern space\" and the q command \"Immediately quit[s] the sed script without processing any more input...\" The quotes are from the sed man page.\nBy the way, your command\ntail -n 10000000 filename | head -n 10\n\nstarts at the ten millionth line from the end of the file, while your \"middle\" command would seem to start at the ten millionth from the beginning which would be equivalent to:\nhead -n 10000010 filename | tail -n 10\n\nThe problem is that for unsorted files with variable length lines any process is going to have to go through the file counting newlines. There's no way to shortcut that.\nIf, however, the file is sorted (a log file with timestamps, for example) or has fixed length lines, then you can seek into the file based on a byte position. In the log file example, you could do a binary search for a range of times as my Python script here* does. In the case of the fixed record length file, it's really easy. You just seek linelength * linecount characters into the file.\n* I keep meaning to post yet another update to that script. Maybe I'll get around to it one of these days.",
        "url": "https://serverfault.com/questions/133692/how-to-display-certain-lines-from-a-text-file-in-linux"
    },
    {
        "title": "Windows Server restart / shutdown history",
        "question": "How can I easily see a history of every time my Windows Server has restarted or shutdown and the reason why, including user-initiated, system-initiated, and system crashed? \nThe Windows Event Log is an obvious answer but what is the complete list of events that I should view?\nI found these posts that partially answer my question:\n\nWindows server last reboot time includes several answers that partially address the full restart history\nView Shutdown Event Tracker logs under Windows Server 2008 R2 includes an additional event id\nEvent Log time when Computer Start up / boot up includes some of the same event ids\n\nbut those don't cover every scenario AFAIK and the info is hard to understand because it is spread across multiple answers.\nI have several versions of Windows Server so a solution that works for at least versions 2008, 2008 R2, 2012, and 2012 R2 would be ideal.",
        "top_answer": "The clearest most succinct answer I could find is:\n\nHow To See PC Startup And Shutdown History In Windows\n\nwhich lists these event ids to monitor (quoted but edited and reformatted from article):\n\nEvent ID 6005 (alternate): \u201cThe event log service was started.\u201d This is synonymous to system startup.\nEvent ID 6006 (alternate): \u201cThe event log service was stopped.\u201d This is synonymous to system shutdown.\nEvent ID 6008 (alternate): \"The previous system shutdown was unexpected.\" Records that the system started after it was not shut down properly.\nEvent ID 6009 (alternate): Indicates the Windows product name, version, build number, service pack number, and operating system type detected at boot time.\nEvent ID 6013: Displays the uptime of the computer. There is no TechNet page for this id.\n\nAdd to that a couple more from the Server Fault answers listed in my OP:\n\nEvent ID 1074 (alternate): \"The process X has initiated the restart / shutdown of computer on behalf of user Y for the following reason: Z.\" Indicates that an application or a user initiated a restart or shutdown.\nEvent ID 1076 (alternate): \"The reason supplied by user X for the last unexpected shutdown of this computer is: Y.\"  Records when the first user with shutdown privileges logs on to the computer after an unexpected restart or shutdown and supplies a reason for the occurrence.\n\nDid I miss any?",
        "url": "https://serverfault.com/questions/702828/windows-server-restart-shutdown-history"
    },
    {
        "title": "What is the difference between service and systemctl?",
        "question": "Maybe this is a trivial question, but it is not totally clear to me. On one of our servers we have some background processes running which were started with service and some others which were started with systemctl, like this:\n$ service nginx start\n$ systemctl start gunicorn\n\nWhat is the difference between the two commands? Which one is the preferred way to deal with background services? How to configure the preferred command?",
        "top_answer": "service is an \"high-level\" command used for starting and stopping services in different unixes and linuxes. Depending on the \"lower-level\" service manager, service redirects on different binaries.\nFor example, on CentOS 7 it redirects to systemctl, while on CentOS 6 it directly calls the relative /etc/init.d script. On the other hand, in older Ubuntu releases it redirects to upstart\nservice is adequate for basic service management, while directly calling systemctl give greater control options.",
        "url": "https://serverfault.com/questions/867322/what-is-the-difference-between-service-and-systemctl"
    },
    {
        "title": "nmap find all alive hostnames and IPs in LAN",
        "question": "How can I issue a nmap command that shows me all the alive machines' IP addresses and corresponding hostname s in the LAN that I am connected?\n(if this can be done in another way/tool you surely are welcome to answer)",
        "top_answer": "nmap versions lower than 5.30BETA1:\nnmap -sP 192.168.1.*\n\nnewer nmap versions:\nnmap -sn 192.168.1.*\n\nThis gives me hostnames along with IP adresses, and only pings the hosts to discover them. This will only give you the hostnames if you run it as root.\nEDIT: As of Nmap 5.30BETA1 [2010-03-29] -sP has been replaced with -sn as the preferred way to do ping scans, while skipping port scanning, just like the comments indicate:\n\nPreviously the -PN and -sP options were recommended. This establishes a more regular syntax for some options that disable phases of a scan:\n\n-n  no reverse DNS\n-Pn no host discovery\n-sn no port scan\n\n",
        "url": "https://serverfault.com/questions/153776/nmap-find-all-alive-hostnames-and-ips-in-lan"
    },
    {
        "title": "Linux command line best practices and tips?",
        "question": "I would like to open a discussion that would accumulate your Linux command line (CLI) best practices and tips.\nI've searched for such a discussion to share the below comment but haven't found one, hence this post.\nI hope we all could learn from this.\nYou are welcome to share your Bash tips, grep, sed, AWK, /proc and all other related Linux/Unix system administration, shell programming best practices for the benefit of us all.",
        "top_answer": "Use screen, a free terminal multiplexer developed by the GNU Project that will allow you to have several terminals in one.\nYou can start a session and your terminals will be saved even when you connection is lost, so you can resume later or from home.",
        "url": "https://serverfault.com/questions/68883/linux-command-line-best-practices-and-tips"
    },
    {
        "title": "df says disk is full, but it is not",
        "question": "On a virtualized server running Ubuntu 10.04, df reports the following:\n# df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/sda1             7.4G  7.0G     0 100% /\nnone                  498M  160K  498M   1% /dev\nnone                  500M     0  500M   0% /dev/shm\nnone                  500M   92K  500M   1% /var/run\nnone                  500M     0  500M   0% /var/lock\nnone                  500M     0  500M   0% /lib/init/rw\n/dev/sda3             917G  305G  566G  36% /home\n\nThis is puzzling me for two reasons: 1.) df says that /dev/sda1, mounted at /, has a 7.4 gigabyte capacity, of which only 7.0 gigabytes are in use, yet it reports / being 100 percent full; and 2.) I can create files on / so it clearly does have space left.\nPossibly relevant is that the directory /www is a symbolic link to /home/www, which is on a different partition (/dev/sda3, mounted at /home).\nCan anyone offer suggestions on what might be going on here?  The server appears to be working without issue, but I want to make sure there's not a problem with the partition table, file systems or something else which might result in implosion (or explosion) later.",
        "top_answer": "It's possible that a process has opened a large file which has since been deleted. You'll have to kill that process to free up the space. You may be able to identify the process by using lsof. On Linux deleted yet open files are known to lsof and marked as (deleted) in lsof's output.\nYou can check this with sudo lsof +L1",
        "url": "https://serverfault.com/questions/315181/df-says-disk-is-full-but-it-is-not"
    },
    {
        "title": "How to restart nginx?",
        "question": "For me, I run \"killall nginx\" and start it by \"sbin/nginx\", anyone has a better restart script?\nBTW: I install nginx from source, i do not find 'service nginx' command or /etc/init.d/nginx",
        "top_answer": "The nginx package supplies a /etc/init.d/nginx script that provides the usual start|stop|restart|reload ... functionality. \n/etc/init.d/nginx restart \nwill restart nginx \nas will \nservice nginx restart\nEdit\nHere is a link to a script you can use as /etc/init.d/nginx.",
        "url": "https://serverfault.com/questions/213185/how-to-restart-nginx"
    },
    {
        "title": "How do I find if there is a rogue DHCP server on my Network?",
        "question": "What's the best approach towards determining if I have a rogue DHCP server inside my network?\nI'm wondering how most admins approach these kinds of problems.  I found DHCP Probe through searching, and thought about trying it out.  Has anyone had experience with it? (I would like to know before taking the time to compile it and install).\nDo you know any useful tools or best-practices towards finding rogue DHCP servers?  ",
        "top_answer": "One simple method is to simply run a sniffer like tcpdump/wireshark on a computer and send out a DHCP request.  If you see any offers other then from your real DHCP server then you know you have a problem.",
        "url": "https://serverfault.com/questions/8526/how-do-i-find-if-there-is-a-rogue-dhcp-server-on-my-network"
    },
    {
        "title": "Properly setting up a \"default\" nginx server for https",
        "question": "I have several servers running on the same machine, some with http only, some with both http and https.  There are several server blocks defined in separate files which are included from the main config file.\nI have set up a \"default\" server for http which will serve a generic \"maintenance page\" to requests that don't match any of the other server_names in the other config files.  The http default server works as expected, it uses the server_name \"_\" and it appears first in the list of includes (because I have observed that in the case of duplicate server_names across servers, the one appearing first is used).  This works great.\nI would expect the same exact server block (only switching \"listen 80 default_server\" to \"listen 443 default_server\" and also instead of serving page \"return 444\") however it does not.  Instead, it appears that the new default https server is actually grabbing all incoming https connections and causing them to fail, although the other server blocks have more appropriate server_names for the incoming requests.  Removing the new default https server will cause semi-correct behavior to resume: the websites with https will all load correctly; but the websites without https will all be routed to the first https server in the include files (which according to the docs, if no \"default_server\" appears, then the first server block to appear will be \"default\").\nSo my question is, what is the correct way to define a \"default server\" in nginx for ssl connections?  Why is it that when I explicitly set a \"default_server\" it gets greedy and grabs all connections whereas when I implicitly let nginx decide the \"default server\" it works like I would expect (with the incorrect server set as default and the other real servers behaving correctly)?\nHere are my \"default servers\".  Http works without breaking other servers.  Https breaks other servers and consumes all.\nserver {\n    listen 443 ssl default_server;\n    server_name _;\n\n    access_log /var/log/nginx/maintenance.access.log;\n    error_log /var/log/nginx/maintenance.error.log error;\n\n    return 444;\n}\n\nserver {\n    listen *:80 default_server;\n    server_name _;\n    charset utf-8;\n\n    access_log /var/log/nginx/maintenance.access.log;\n    error_log /var/log/nginx/maintenance.error.log error;\n\n    root /home/path/to/templates;\n\n    location / {\n        return 503;\n    }\n\n    error_page 503 @maintenance;\n\n    location @maintenance {\n        rewrite ^(.*)$ /maintenance.html break;\n    }\n}\n\nAny of you see what might be wrong here?",
        "top_answer": "I managed to configure a shared dedicated hosting on a single IP with nginx. Default HTTP and HTTPS serving a 404 for unknown domains incoming.\n1 - Create a default zone\nAs nginx is loading vhosts in ascii order, you should create a 00-default file/symbolic link into your /etc/nginx/sites-enabled.\n2 - Fill the default zone\nFill your 00-default with default vhosts. Here is the zone i am using:\nserver {\n    server_name _;\n    listen       80  default_server;\n    return       404;\n}\n\n\nserver {\n    listen 443 ssl;\n    server_name _;\n    ssl_certificate /etc/nginx/ssl/nginx.crt;\n    ssl_certificate_key /etc/nginx/ssl/nginx.key;\n    return       404;\n}\n\n3 - Create self signed certif, test, and reload\nYou will need to create a self signed certificate into /etc/nginx/ssl/nginx.crt.  \nCreate a default self signed certificate: \nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt\n\nJust a reminder:\n\nTest the nginx configuration before reloading/restarting : nginx -t\nReload a enjoy: sudo service nginx reload\n\nHope it helps.",
        "url": "https://serverfault.com/questions/578648/properly-setting-up-a-default-nginx-server-for-https"
    },
    {
        "title": "What is a good SSH server to use on Windows? [closed]",
        "question": "In my spare time I remotely support my wife's office via VPN into a Windows Server. I am about to purchase a wireless broadband service which doesn't support VPN.\nI don't want to open up the remote desktop ports directly, and I would like to set up an SSH tunnel into the network, and if necessary then VPN over the top of that.\nWhat is the best windows SSH Server implementation to use on a Windows 2003 Server, or should I just be using sshwindows?",
        "top_answer": "I've been using FreeSSHd on my home Windows box, and have not run into any limitations.  Highly recommended.  ",
        "url": "https://serverfault.com/questions/8411/what-is-a-good-ssh-server-to-use-on-windows"
    },
    {
        "title": "Is a wildcard CNAME DNS record valid?",
        "question": "I know it's valid to have a DNS A record that's a wildcard (e.g. *.mysite.com).  Is it possible/valid/advised to have a wildcard CNAME record?",
        "top_answer": "It is possible to do this. At one point it was up in the air a bit until 4592 clarified that it should be supported.\nJust because it is possible doesn't mean it is supported by all DNS providers. For example, GoDaddy won't let you set up a wildcard in a CNAME record.\nIn terms of whether it is advisable or not to do this, it depends on your usage. Usually CNAMES are used for convenience when you are pointing to an \"outside\" domain name that you don't control the DNS on. \nFor example, let's say you set up a CMS system that allows you to have *.mycms.com as the site name (it uses host headers). You want customers to be able to easily set up *.cms.customer.com, without worrying that you might change your IP address at some point. In that case, you could advise them to set up a wildcard CNAME called *.cms.customer.com to www.mycms.com. \nBecause wildcard CNAMES aren't supported by all providers (such as GoDaddy), I wouldn't advise using it in a case where you suggested it for various customers (where you don't know their provider's capabilities).",
        "url": "https://serverfault.com/questions/44618/is-a-wildcard-cname-dns-record-valid"
    },
    {
        "title": "How to stop people from using my domain to send spam? [duplicate]",
        "question": "I receive Mailer Daemon messages saying certain emails fail. My domain is itaccess.org which is administered by Google apps. Is there any way I can identify who is sending emails from my domain, and how they are doing it without me creating an account for them?\nDelivered-To: [email\u00a0protected]\nReceived: by 10.142.152.34 with SMTP id z34csp12042wfd;\n        Wed, 8 Aug 2012 07:12:46 -0700 (PDT)\nReceived: by 10.152.112.34 with SMTP id in2mr18229790lab.6.1344435165782;\n        Wed, 08 Aug 2012 07:12:45 -0700 (PDT)\nReturn-Path: <[email\u00a0protected]>\nReceived: from smtp-gw.fsdata.se (smtp-gw.fsdata.se. [195.35.82.145])\n        by mx.google.com with ESMTP id b9si24888989lbg.77.2012.08.08.07.12.44;\n        Wed, 08 Aug 2012 07:12:45 -0700 (PDT)\nReceived-SPF: neutral (google.com: 195.35.82.145 is neither permitted nor denied by best guess record for domain of [email\u00a0protected]) client-ip=195.35.82.145;\nAuthentication-Results: mx.google.com; spf=neutral (google.com: 195.35.82.145 is neither permitted nor denied by best guess record for domain of [email\u00a0protected]) [email\u00a0protected]\nReceived: from www20.aname.net (www20.aname.net [89.221.250.20])\n    by smtp-gw.fsdata.se (8.14.3/8.13.8) with ESMTP id q78EChia020085\n    for <[email\u00a0protected]>; Wed, 8 Aug 2012 16:12:43 +0200\nReceived: from www20.aname.net (localhost [127.0.0.1])\n    by www20.aname.net (8.14.3/8.14.3) with ESMTP id q78ECgQ1013882\n    for <[email\u00a0protected]>; Wed, 8 Aug 2012 16:12:42 +0200\nReceived: (from whao@localhost)\n    by www20.aname.net (8.14.3/8.12.0/Submit) id q78ECgKn013879;\n    Wed, 8 Aug 2012 16:12:42 +0200\nDate: Wed, 8 Aug 2012 16:12:42 +0200\nMessage-Id: <[email\u00a0protected]>\nTo: [email\u00a0protected]\nReferences: <20120808171231.CAC5128A79D815BC08430@USER-PC>\nIn-Reply-To: <20120808171231.CAC5128A79D815BC08430@USER-PC>\nX-Loop: [email\u00a0protected]\nFrom: [email\u00a0protected]\nSubject: whao.se:  kontot avst\u00e4ngt - account closed\nX-FS-SpamAssassinScore: 1.8\nX-FS-SpamAssassinRules: ALL_TRUSTED,DCC_CHECK,FRT_CONTACT,SUBJECT_NEEDS_ENCODING\n\n    Detta \u00e4r ett automatiskt svar fr\u00e5n F S Data - http://www.fsdata.se\n\n    Kontot f\u00f6r dom\u00e4nen whao.se \u00e4r tillsvidare avst\u00e4ngt.\n    F\u00f6r mer information, kontakta [email\u00a0protected]\n\n    Mvh,\n    /F S Data\n\n    -----\n\n  This is an automatic reply from F S Data - http://www.fsdata.se\n\n  The domain account \"whao.se\" is closed.\n  For further information, please contact [email\u00a0protected]\n\n  Best regards,\n  /F S Data\n",
        "top_answer": "Since it hasn't been explicitly stated yet, I'll state it.\nNo one's using your domain to send spam.\nThey're using spoofed sender data to generate an email that looks like it's from your domain.  It's about as easy as putting a fake return address on a piece of postal mail, so no, there's really no way to stop it.  SPF (as suggested) can make it easier for other mail servers to identify email that actually comes from your domain and email that doesn't, but just like you can't stop me from putting your postal address as the return address on all the death threats I mail, you can't stop someone from putting your domain as the reply-to address on their spam.\nSMTP just wasn't designed to be secure, and it isn't.",
        "url": "https://serverfault.com/questions/415533/how-to-stop-people-from-using-my-domain-to-send-spam"
    },
    {
        "title": "Why does my OpenSSH key fingerprint not match the AWS EC2 console keypair fingerprint?",
        "question": "When I import my OpenSSH public key into AWS EC2's keyring the fingerprint that AWS shows doesn't match what I see from:\nssh-keygen -l -f my_key\n\nIt is a different length and has different bytes.\nWhy? I'm sure I uploaded the correct key.",
        "top_answer": "AWS EC2 shows the SSH2 fingerprint, not the OpenSSH fingerprint everyone expects. It doesn't say this in the UI.\nIt also shows two completely different kinds of fingerprints depending on whether the key was generated on AWS and downloaded, or whether you uploaded your own public key.\nFingerprints generated with\nssh-keygen -l -f id_rsa\n\nwill not match what EC2 shows. You can either use the AWS API tools to generate a fingerprint with the ec2-fingerprint-key command, or use OpenSSL to do it.\nNote that if you originally generated a key on AWS, but then uploaded it again (say, to another region) then you'll get a different fingerprint because it'll take the SSH2 RSA fingerprint, rather than the sha1 it shows for keys you generated on AWS.\nFun, hey? \nIn the above, test-generated was generated using AWS EC2. test-generated-reuploaded is the public key from the private key AWS generated, extracted with ssh-keygen -y and uploaded again. The third key, test-uploaded, is a locally generated key ... but the local ssh-keygen -l fingerprint is b2:2c:86:d6:1e:58:c0:b0:15:97:ab:9b:93:e7:4e:ea.\n$ ssh-keygen -l -f theprivatekey\n2048 b2:2c:86:d6:1e:58:c0:b0:15:97:ab:9b:93:e7:4e:ea\n$ openssl pkey -in theprivatekey -pubout -outform DER | openssl md5 -c\nEnter pass phrase for id_landp:\n(stdin)= 91:bc:58:1f:ea:5d:51:2d:83:d3:6b:d7:6d:63:06:d2\n\nKeys uploaded to AWS\nWhen you upload a key to AWS, you upload the public key only, and AWS shows the MD5 hash of the public key.\nYou can use OpenSSL, as demonstrated by Daniel on the AWS forums, to generate the fingerprint in the form used by AWS to show fingerprints for uploaded public keys (SSH2 MD5), like:\n7a:58:3a:a3:df:ba:a3:09:be:b5:b4:0b:f5:5b:09:a0\n\nIf you have the private key, you can generate the fingerprint by extracting the public part from the private key and hashing it using:\nopenssl pkey -in id_rsa -pubout -outform DER | openssl md5 -c\n\nIf you only have the public key, and it is in OpenSSH format, you need to first convert it to PEM and then DER and then hash, using:\nssh-keygen -f id_rsa.pub -e -m PKCS8 | openssl pkey -pubin -outform DER | openssl md5 -c\n\nKeys generated on AWS\nWhen you generate a keypair on AWS, AWS shows the SHA1 hash of the private key, which is longer, like:\nea:47:42:52:2c:25:43:76:65:f4:67:76:b9:70:b4:64:12:00:e4:5a\n\nIn this case you need to use the following command, also shown by Daniel on the AWS forums, to generate a sha1 hash based on the private key:\nopenssl pkcs8 -in aws_private.pem -nocrypt -topk8 -outform DER | openssl sha1 -c\n\non the downloaded AWS-generated private key/certificate file. It'll work on keys you converted to OpenSSH format too. This does, however, require that you have the private key, since the hash is of the private key. You cannot generate the hash locally if all you have is the public key.\nReferences\nSee:\n\nAWS developers forum discussion\nhttps://stackoverflow.com/q/19251562/398670\nBug report on AWS forums - please chime in\n",
        "url": "https://serverfault.com/questions/603982/why-does-my-openssh-key-fingerprint-not-match-the-aws-ec2-console-keypair-finger"
    },
    {
        "title": "Is there a way to see the execution tree of systemd?",
        "question": "What I mean under the question is: is there a way to dump the ordered list (like pstree does for processes) to see how systemd executed the supplied set of units, i.e. the tree after the dependencies were resolved and jobs were queued for the execution?  I know that you can do it by analysing systemd state data, but is there a quick way to see such a tree?  It would help a lot in failure investigation (e.g. if you see that the boot process was stuck on some unit you would be able to pinpoint the approximate location for your deeper investigation.",
        "top_answer": "systemd-analyze is your friend.\nFor example systemd-analyze critical-chain outputs blocking tree of daemons.\nMine for example:\ngraphical.target @20.211s\n\u2514\u2500multi-user.target @20.211s\n  \u2514\u2500nginx.service @19.348s +862ms\n    \u2514\u2500network.target @19.347s\n      \u2514\u2500NetworkManager.service @10.315s +9.031s\n        \u2514\u2500basic.target @10.312s\n          \u2514\u2500timers.target @10.311s\n            \u2514\u2500systemd-tmpfiles-clean.timer @10.311s\n              \u2514\u2500sysinit.target @10.295s\n                \u2514\u2500systemd-update-utmp.service @10.167s +127ms\n                  \u2514\u2500systemd-tmpfiles-setup.service @10.124s +41ms\n                    \u2514\u2500local-fs.target @10.097s\n                      \u2514\u2500home-entd-Downloads.mount @10.093s +2ms\n                        \u2514\u2500home.mount @9.326s +672ms\n                          \u2514\u2500[email\u00a0protected] @8.472s +696ms\n                            \u2514\u2500dev-sda6.device @8.471s\n\nNetworkManager in example basically holding entire bootup.\nIf you want to have more detailed view you can render entire execution chain in a svg file.\nsystemd-analyze plot > something.svg outputs entire chain (120+ modules) as progress bars to high-res svg file which show states, that are blocked and another problems.\nFinally you have systemd-analyze dot tool which outputs dot file which outputs entire hierarchy:\nsystemd-analyze dot | dot -Tpng -o stuff.png\nwith dot tool you can output it as ps and svg files too. \nAll of above tools are built-in in systemd-analyze tool which comes by default with systemd in archlinux at least. I think there is some 3rd party projects dealing with it too.",
        "url": "https://serverfault.com/questions/617398/is-there-a-way-to-see-the-execution-tree-of-systemd"
    },
    {
        "title": "Is it possible to alias a hostname in Linux?",
        "question": "Is it possible to alias a hostname in Linux?\nIt has been asked by jmillikin at various Ubuntu forums as follows:\n\n\nIs it possible to create a hostname alias? Sort of like /etc/hosts,\nbut with other hostnames rather than IP addresses. So that with some\nfile like this, you could ping \"fakehost1\", and it would be re-mapped\nto \"realhost\", and then \"realhost\" would be resolved to an IP address.\n# Real host        # Aliases\nrealhost           fakehost1 fakehost2 fakehost3\n\n\n\nSomebody has answered about ssh, but not about ping, etc. My main\npurpose is to use it as an alias for a Subversion server. In my case, realhost\nis under a dynamic IP address. So, the \"/etc/hosts\" alias doesn't work. I want to\naccess my Subversion server as svn://my_svnserver/my_repos instead of\nsvn://realhost/my_repos.",
        "top_answer": "For those who don't have an account on the forums (or don't wish to login):\n\nif your main issue is not to ping but to ssh, you can create/edit your\n  ~/.ssh/config adding lines like these:\n\nHost fakehost1\n  Hostname real-hostname\n\nHost fakehost2\n  Hostname real-hostname2\n\nHost fakehost3\n  Hostname real-hostname3\n",
        "url": "https://serverfault.com/questions/65199/is-it-possible-to-alias-a-hostname-in-linux"
    },
    {
        "title": "Fighting Spam - What can I do as an: Email Administrator, Domain Owner, or User?",
        "question": "\nThis is a Canonical Question about Fighting Spam.\n  Also related: \n\nHow to stop people from using my domain to send spam? \nWhat are SPF records, and how do I configure them?\n\n\nThere are so many techniques and so much to know about fighting SPAM. What widely used techniques and technologies are available to Administrator, Domain Owners, and End Users to help keep the junk out of our inboxes?\nWe're looking for an answer that covers different tech from various angles. The accepted answer should include a variety of technologies (eg SPF/SenderID, DomainKeys/DKIM, Graylisting, DNS RBLs, Reputation Services, Filtering Software [SpamAssassin, etc]); best practices (eg mail on Port 25 should never be allowed to relay, Port 587 should be used; etc), terminology (eg, Open Relay, Backscatter, MSA/MTA/MUA, Spam/Ham), and possibly other techniques.",
        "top_answer": "To defeat your enemy, you must know your enemy.\nWhat is spam?\nFor our purposes, spam is any unsolicited bulk electronic message. Spam these days is intended to lure unsuspecting users into visiting a (usually shady) web site where they will be asked to buy products, or have malware delivered to their computers, or both. Some spam will deliver malware directly.\nIt may surprise you to learn that the first spam was sent in 1864. It was an advertisement for dental services, sent via Western Union telegram. The word itself is a reference to a scene in Monty Python's Flying Circus.\nSpam, in this case, does not refer to mailing list traffic a user subscribed to, even if they changed their minds later (or forgot about it) but have not actually unsubscribed yet.\nWhy is spam a problem?\nSpam is a problem because it works for the spammers. Spam typically generates more than enough sales (or malware delivery, or both) to cover the costs -- to the spammer -- of sending it. The spammer does not consider the costs to the recipient, you and your users. Even when a tiny minority of users receiving spam respond to it, it's enough.\nSo you get to pay the bills for bandwidth, servers, and administrator time to deal with incoming spam.\nWe block spam for these reasons: we don't want to see it, to reduce our costs of handling email, and to make spamming more expensive for the spammers.\nHow does spam work?\nSpam typically is delivered in different ways from normal, legitimate email.\nSpammers almost always want to obscure the origin of the email, so a typical spam will contain fake header information. The From: address is usually fake. Some spam includes fake Received: lines in an attempt to disguise the trail. A lot of spam is delivered via open SMTP relays, open proxy servers and botnets. All of these methods make it more difficult to determine who originated the spam.\nOnce in the user's inbox, the purpose of the spam is to entice the user to visit the advertised web site. There, the user will be enticed to make a purchase, or the site will attempt to install malware on the user's computer, or both. Or, the spam will ask the user to open an attachment which contains malware.\nHow do I stop spam?\nAs a system administrator of a mail server, you will configure your mail server and domain to make it more difficult for spammers to deliver their spam to your users.\nI will be covering issues specifically focused on spam and may skip over things not directly related to spam (such as encryption).\nDon't run an open relay\nThe big mail server sin is to run an open relay, a SMTP server which will accept mail for any destination and deliver it onward. Spammers love open relays because they virtually guarantee delivery. They take on the load of delivering messages (and retrying!) while the spammer does something else. They make spamming cheap.\nOpen relays also contribute to the problem of backscatter. These are messages which were accepted by the relay but then found to be undeliverable. The open relay will then send a bounce message to the From: address which contains a copy of the spam.\n\nConfigure your mail server to accept incoming mail on port 25 only for your own domain(s). For most mail servers, this is the default behavior, but you at least need to tell the mail server what your domains are.\nTest your system by sending your SMTP server a mail from outside your network where both the From: and To: addresses are not within your domain. The message should be rejected. (Or, use an online service like MX Toolbox to perform the test, but be aware that some online services will submit your IP address to blacklists if your mail server fails the test.)\n\nReject anything that looks too suspicious\nVarious misconfigurations and errors can be a tip-off that an incoming message is likely to be spam or otherwise illegitimate.\n\nMark as spam or reject messages for which the IP address has no reverse DNS (PTR record). Treat the lack of a PTR record more harshly for IPv4 connections than for IPv6 connections, as many IPv6 addresses do not yet have reverse DNS, and may not for several years, until DNS server software is better able to handle these potentially very large zones.\nReject messages for which the domain name in the sender or recipient addresses does not exist.\nReject messages which do not use fully qualified domain names for the sender or recipient domains, unless they originate within your domain and are meant to be delivered within your domain (e.g. monitoring services).\nReject connections where the other end does not send a HELO/EHLO.\nReject connections where the HELO/EHLO is:\n\nnot a fully qualified domain name and not an IP address\nblatantly wrong (e.g. your own IP address space)\n\n\nReject connections which use pipelining without being authorized to do so.\n\nAuthenticate your users\nMail arriving at your servers should be thought of in terms of inbound mail and outbound mail.  Inbound mail is any mail arriving at your SMTP server which is ultimately destined for your domain; outbound mail is any mail arriving at your SMTP server which will be transferred elsewhere before being delivered (eg. it's going to another domain). Inbound mail can be handled by your spam filters, and may come from anywhere but must always be destined for your users. This mail can't be authenticated, because it is not possible to give credentials to every site which might send you mail.\nOutbound mail, that is, mail which will be relayed, must be authenticated. This is the case whether it comes from the Internet or from inside your network (though you should restrict the IP address ranges allowed to use your mailserver if operationally possible); this is because spambots might be running inside your network. So, configure your SMTP server such that mail bound for other networks will be dropped (relay access will be denied) unless that mail is authenticated. Better still, use separate mail servers for inbound and outbound mail, allow no relaying at all for the inbound ones, and allow no unauthenticated access to the outbound ones.\nIf your software allows this, you should also filter messages according to the authenticated user; if the from address of the mail does not match the user who authenticated, it should be rejected. Do not silently update the from address; the user should be aware of the configuration error.\nYou should also log the username which is used to send mail, or add an identifying header to it. This way, if abuse does occur, you have evidence and know which account was used to do it. This allows you to isolate compromised accounts and problem users, and is especially valuable for shared hosting providers.\nFilter traffic\nYou want to be certain that mail leaving your network is actually being sent by your (authenticated) users, not by bots or people from outside. The specifics of how you do this depend on exactly what kind of system you are administering.\nGenerally, blocking egress traffic on ports 25, 465, and 587 (SMTP, SMTP/SSL, and Submission) for everything but your outbound mailservers is a good idea if you are a corporate network. This is so that malware-running bots on your network cannot send spam from your network either to open relays on the Internet or directly to the final MTA for an address.\nHotspots are a special case because legitimate mail from them originates from many different domains, but (because of SPF, among other things) a \"forced\" mailserver is inappropriate and users should be using their own domain's SMTP server to submit mail.  This case is much harder, but using a specific public IP or IP range for Internet traffic from these hosts (to protect your site's reputation), throttling SMTP traffic, and deep packet inspection are solutions to consider.\nHistorically, spambots have issued spam mainly on port 25, but nothing prevents them from using port 587 for the same purpose, so changing the port used for inbound mail is of dubious value. However, using port 587 for mail submission is recommended by RFC 2476, and allows for a separation between mail submission (to the first MTA) and mail transfer (between MTAs) where that is not obvious from network topology; if you require such separation, you should do this.\nIf you are an ISP, VPS host, colocation provider, or similar, or are providing a hotspot for use by visitors, blocking egress SMTP traffic can be problematic for users who are sending mail using their own domains.  In all cases except a public hotspot, you should require users who need outbound SMTP access because they are running a mailserver to specifically request it. Let them know that abuse complaints will ultimately result in that access being terminated to protect your reputation.\nDynamic IPs, and those used for virtual desktop infrastructure, should never have outbound SMTP access except to the specific mailserver those nodes are expected to use. These types of IPs should also appear on blacklists and you should not attempt to build reputation for them. This is because they are extremely unlikely to be running a legitimate MTA.\nConsider using SpamAssassin\nSpamAssassin is a mail filter which can be used to identify spam based on the message headers and content. It uses a rules-based scoring system to determine the likelihood that a message is spam. The higher the score, the more likely the message is spam.\nSpamAssassin also has a Bayesian engine which can analyze spam and ham (legitimate email) samples fed back into it.\nBest practice for SpamAssassin is not to reject the mail, but to put it in a Junk or Spam folder. MUAs (mail user agents) such as Outlook and Thunderbird can be set up to recognize the headers that SpamAssassin adds to email messages and to file them appropriately. False positives can and do happen, and while they're rare, when it happens to the CEO, you will hear about it. That conversation will go much better if the message was simply delivered to the Junk folder rather than rejected outright.\nSpamAssassin is almost one-of-a-kind, though a few alternatives exist.\n\nInstall SpamAssassin and configure automatic update for its rules using sa-update.\nConsider using custom rules where appropriate.\nConsider setting up Bayesian filtering.\n\nConsider using DNS-based blackhole lists and reputation services\nDNSBLs (formerly known as RBLs, or realtime blackhole lists) provide lists of IP addresses associated with spam or other malicious activity. These are run by independent third parties based on their own criteria, so research carefully whether the listing and delisting criteria used by a DNSBL is compatible with your organization's need to receive email. For instance, a few DNSBLs have draconian delisting policies which make it very difficult for someone who was accidentally listed to be removed. Others automatically delist after the IP address has not sent spam for a period of time, which is safer. Most DNSBLs are free to use.\nReputation services are similar, but claim to provide better results by analyzing more data relevant to any given IP address. Most reputation services require a subscription payment or hardware purchase or both.\nThere are dozens of DNSBLs and reputation services available, though some of the better known and useful ones I use and recommend are:\nConservative lists:\n\nSpamhaus ZEN\nBarracuda Reputation Database (no purchase necessary)\nSpamCop\n\nAggressive lists:\n\nUCEPROTECT\nBackscatterer\n\nAs mentioned before, many dozens of others are available and may suit your needs. One of my favorite tricks is to look up the IP address which delivered a spam that got through against multiple DNSBLs to see which of them would have rejected it.\n\nFor each DNSBL and reputation service, examine its policies for listing and delisting of IP addresses and determine whether these are compatible with your organization's needs.\nAdd the DNSBL to your SMTP server when you have decided it is appropriate to use that service.\nConsider assigning each DNSBL a score and configuring it into SpamAssassin rather than your SMTP server. This reduces the impact of a false positive; such a message would be delivered (possibly to Junk/Spam) instead of bounced. The tradeoff is that you will deliver a lot of spam.\nOr, reject outright when the IP address is on one of the more conservative lists, and configure the more aggressive lists in SpamAssassin.\n\nUse SPF\nSPF (Sender Policy Framework; RFC 4408 and RFC 6652) is a means to prevent email address spoofing by declaring which Internet hosts are authorized to deliver mail for a given domain name.\n\nConfigure your DNS to declare an SPF record with your authorized outgoing mail servers and -all to reject all others.\nConfigure your mail server to check the SPF records of incoming mail, if they exist, and reject mail which fails SPF validation. Skip this check if the domain does not have SPF records.\n\nInvestigate DKIM\nDKIM (DomainKeys Identified Mail; RFC 6376) is a method of embedding digital signatures in mail messages which can be verified using public keys published in the DNS. It is patent-encumbered in the US, which has slowed its adoption. DKIM signatures can also break if a message is modified in transit (e.g. SMTP servers occasionally may repack MIME messages).\n\nConsider signing your outgoing mail with DKIM signatures, but be aware that the signatures may not always verify correctly even on legitimate mail.\n\nConsider using greylisting\nGreylisting is a technique where the SMTP server issues a temporary rejection for an incoming message, rather than a permanent rejection. When the delivery is retried in a few minutes or hours, the SMTP server will then accept the message.\nGreylisting can stop some spam software which is not robust enough to differentiate between temporary and permanent rejections, but does not help with spam that was sent to an open relay or with more robust spam software. It also introduces delivery delays which users may not always tolerate.\n\nConsider using greylisting only in extreme cases, since it is highly disruptive to legitimate email traffic.\n\nConsider using nolisting\nNolisting is a method of configuring your MX records such that the highest priority (lowest preference number) record does not have a running SMTP server. This relies on the fact that a lot of spam software will only try the first MX record, while legitimate SMTP servers try all MX records in ascending order of preference. Some spam software also attempts to send directly to the lowest priority (highest preference number) MX record in violation of RFC 5321, so that could also be set to an IP address without an SMTP server. This is reported to be safe, though as with anything, you should test carefully first.\n\nConsider setting your highest-priority MX record to point to a host which does not answer on port 25.\nConsider setting your lowest-priority MX record to point to a host which does not answer on port 25.\n\nConsider a spam filtering appliance\nPlace a spam filtering appliance such as Cisco IronPort or Barracuda Spam & Virus Firewall (or other similar appliances) in front of your existing SMTP server to take much of the work out of reducing the spam you receive. These appliances are pre-configured with DNSBLs, reputation services, Bayesian filters and the other features I've covered, and are updated regularly by their manufacturers.\n\nResearch spam filtering appliance hardware and subscription costs.\n\nConsider hosted email services\nIf it's all too much for you (or your overworked IT staff) you can always have a third party service provider handle your email for you. Services such as Google's Postini, Symantec MessageLabs Email Security (or others) will filter messages for you. Some of these services can also handle regulatory and legal requirements.\n\nResearch hosted email service subscription costs.\n\nWhat guidance should sysadmins give to end users regarding fighting spam?\nThe absolute #1 thing that end users should do to fight spam is:\n\nDO NOT RESPOND TO THE SPAM.\nIf it looks funny, don't click the website link and don't open the attachment. No matter how attractive the offer seems. That viagra isn't that cheap, you aren't really going to get naked pictures of anybody, and there is no $15 million dollars in Nigeria or elsewhere except for the money taken from people who did respond to the spam.\n\nIf you see a spam message, mark it as Junk or Spam depending on your mail client.\n\nDO NOT mark a message as Junk/Spam if you actually signed up to receive the messages and just want to stop receiving them. Instead, unsubscribe from the mailing list using the unsubscribe method provided.\n\nCheck your Junk/Spam folder regularly to see if any legitimate messages got through. Mark these as Not Junk/Not Spam and add the sender to your contacts to prevent their messages from being marked as spam in the future.\n\n",
        "url": "https://serverfault.com/questions/419407/fighting-spam-what-can-i-do-as-an-email-administrator-domain-owner-or-user"
    },
    {
        "title": "Does getting disconnected from an SSH session kill your programs?",
        "question": "So, say I get disconnected from an SSH-session after I've started rsync or cp or any other command that can be long running. Does that command keep running until it's finished after I get disconnected or does it just get killed?\nAlways wondered this.  ",
        "top_answer": "Edit for 2016:\nThis Q&A predates the systemd v230 debacle. As of systemd v230, the new default is to kill all children of a terminating login session, regardless of what historically valid precautions were taken to prevent this. The behavior can be changed by setting KillUserProcesses=no in /etc/systemd/logind.conf, or circumvented using the systemd-specific mechanisms for starting a daemon in userspace. Those mechanisms are outside the scope of this question.\nThe text below describes how things have traditionally worked in UNIX designspace for longer than Linux has existed.\n\nThey will get killed, but not necessarily immediately. It depends on how long it takes for the SSH daemon to decide that your connection is dead. What follows is a longer explanation that will help you understand how it actually works.\nWhen you logged in, the SSH daemon allocated a pseudo-terminal for you and attached it to your user's configured login shell. This is called the controlling terminal. Every program you start normally at that point, no matter how many layers of shells deep, will ultimately trace its ancestry back to that shell. You can observe this with the pstree command.\nWhen the SSH daemon process associated with your connection decides that your connection is dead, it sends a hangup signal (SIGHUP) to the login shell. This notifies the shell that you've vanished and that it should begin cleaning up after itself. What happens at this point is shell specific (search its documentation page for \"HUP\"), but for the most part it will start sending SIGHUP to running jobs associated with it before terminating. Each of those processes, in turn, will do whatever they're configured to do on receipt of that signal. Usually that means terminating. If those jobs have jobs of their own, the signal will often get passed along as well.\nThe processes that survive a hangup of the controlling terminal are ones that either disassociated themselves from having a terminal (daemon processes that you started inside of it), or ones that were invoked with a prefixed nohup command. (i.e. \"don't hang up on this\") Daemons interpret the HUP signal differently; since they do not have a controlling terminal and do not automatically receive a HUP signal, it is instead repurposed as a manual request from the administrator to reload the configuration. Ironically this means that most admins don't learn the \"hangup\" usage of this signal for non-daemons until much, much later. That's why you're reading this!\nTerminal multiplexers are a common way of keeping your shell environment intact between disconnections. They allow you to detach from your shell processes in a way that you can reattach to them later, regardless of whether that disconnection was accidental or deliberate. tmux and screen are the more popular ones; syntax for using them is beyond the scope of your question, but they're worth looking into.\n\nIt was requested that I elaborate on how long it takes for the SSH daemon to decide that your connection is dead. This is a behavior which is specific to every implementation of a SSH daemon, but you can count on all of them to terminate when either side resets the TCP connection. This will happen quickly if the server attempts to write to the socket and the TCP packets are not acknowledged, or slowly if nothing is attempting to write to the PTY.\nIn this particular context, the factors most likely to trigger a write are:\n\nA process (typically the one in the foreground) attempting to write to the PTY on the server side. (server->client)\nThe user attempting to write to the PTY on the client side. (client->server)\nKeepalives of any sort. These are usually not enabled by default, either by the client or the server, and there are typically two flavors: application level and TCP based (i.e. SO_KEEPALIVE). Keepalives amount to either the server or the client infrequently sending packets to the other side, even when nothing would otherwise have a reason to write to the socket. While this is typically intended to skirt firewalls that time out connections too quickly, it has the added side effect of causing the sender to notice when the other side isn't responding that much more quickly.\n\nThe usual rules for TCP sessions apply here: if there is an interruption in connectivity between the client and server, but neither side attempts to send a packet during the problem, the connection will survive provided that both sides are responsive afterwards and receiving the expected TCP sequence numbers.\nIf one side has decided that the socket is dead, the effects are typically immediate: the sshd process will send HUP and self-terminate (as described earlier), or the client will notify the user of the detected problem. It's worth noting that just because one side thinks the other is dead does not mean that the other is has been notified of this. The orphaned side of the connection will typically remain open until either it attempts to write to it and times out, or receives a TCP reset from the other side. (if connectivity was available at the time) The cleanup described in this answer only happens once the server has noticed.",
        "url": "https://serverfault.com/questions/463366/does-getting-disconnected-from-an-ssh-session-kill-your-programs"
    },
    {
        "title": "an upstream response is buffered to a temporary file",
        "question": "I have a rather large and slow (complex data, complex frontend) web application build in RoR and served by Puma with nginx as reverse proxy.\nLooking at the nginx error log, I see quite a few entries like:\n2014/04/08 09:46:08 [warn] 20058#0: *819237 an upstream response is buffered to a temporary file \n    /var/lib/nginx/proxy/8/47/0000038478 while reading upstream, \n    client: 5.144.169.242, server: engagement-console.foo.it, \n    request: \"GET /elements/pending?customer_id=2&page=2 HTTP/1.0\", \n    upstream: \"http://unix:///home/deployer/apps/conversationflow/shared/sockets/puma.sock:/elements/pending?customer_id=2&page=2\", \n    host: \"ec.reputationmonitor.it\", \n    referrer: \"http://ec.foo.it/elements/pending?customer_id=2&page=3\"\n\nI am rather curious as it's very unlikely that the page remains the same for different users and different user interactions, and I would not think that buffering the response on disk is necessary/useful.\nI know about proxy_max_temp_file_size and setting it to 0, but it seems to me a little bit awkward (my proxy tries to buffer but has no file where to buffer to... how can that be faster?).\nMy questions are:\n\nHow can I remove the [warn] and avoid buffering responses? Is it better to turn off proxy_buffering or to set proxy_max_temp_file_size to 0? Why?\nIf nginx buffers a response: When does it serve the buffered response, to whom, and why?\nWhy nginx turns proxy_buffering on by default and then [warn]s you if it actually buffers a response?\nWhen does a response trigger that option? When it takes > some seconds (how many?) to serve the response? Is this configurable?\n\nTIA, ngw.",
        "top_answer": "\n\nHow can I remove the [warn] and avoid buffering responses? Is it better to turn off proxy_buffering or set proxy_max_temp_file_size to 0? Why?\n\n\nYou should set proxy_max_temp_file_size to 0 in order to remove it. The proxy_buffering directive isn't directly related to the warning. You can switch it off to stop any buffering at all but that isn't recommended in general (unless it's needed for Comet).\n\n\nIf nginx buffers a response when does it serve the buffered response, to whom and why?\n\n\nIt serves the response immediately, but a client usually has a much slower connection and can't consume the response data as fast as it is produced by your application. Nginx tries to buffer the whole response in order to release your application ASAP.\nSee also: http://aosabook.org/en/nginx.html\n\n\nWhy nginx turns proxy_buffering on by default and then [warn]s you if it actually buffers a response?\n\n\nAs I already mentioned, the proxy_buffering isn't directly related to the warning. It's generally needed for optimized proxy operations and turning it off degrades performance and throughput.\nNginx only warns you when a response doesn't fit into configured memory buffers. You may ignore the warning if it's ok for you.\n\n\nWhen does a response triggers that option? When it takes > than some seconds (how many?) to serve the response? Is this configurable?\n\n\nIt triggers when memory buffers are full. Please, look at the docs, the whole mechanism is explained: http://nginx.org/r/proxy_max_temp_file_size\nYou may want to increase memory buffers.",
        "url": "https://serverfault.com/questions/587386/an-upstream-response-is-buffered-to-a-temporary-file"
    },
    {
        "title": "Why should I firewall servers?",
        "question": "PLEASE NOTE: I'm not interested in making this into a flame war! I understand that many people have strongly-held beliefs about this subject, in no small part because they've put a lot of effort into their firewalling solutions, and also because they've been indoctrinated to believe in their necessity.\nHowever, I'm looking for answers from people who are experts in security. I believe that this is an important question, and the answer will benefit more than just myself and the company I work for. I've been running our server network for several years without a compromise, without any firewalling at all. None of the security compromises that we have had could have been prevented with a firewall.\nI guess I've been working here too long, because when I say \"servers\", I always mean \"services offered to the public\", not \"secret internal billing databases\". As such, any rules we would have in any firewalls would have to allow access to the whole Internet. Also, our public-access servers are all in a dedicated datacenter separate from our office.\nSomeone else asked a similar question, and my answer was voted into negative numbers. This leads me to believe that either the people voting it down didn't really understand my answer, or I don't understand security enough to be doing what I'm currently doing.\nThis is my approach to server security:\n\nFollow my operating system's security guidelines before connecting my server to the Internet.\nUse TCP wrappers to restrict access to SSH (and other management services) to a small number of IP addresses.\nMonitor the state of this server with Munin. And fix the egregious security problems inherent to Munin-node in its default configuration.\nNmap my new server (also before connecting my server to the Internet). If I were to firewall this server, this should be the exact set of ports incoming connections should be restricted to.\nInstall the server in the server room and give it a public IP address.\nKeep the system secure by using my operating system's security updates feature.\n\nMy philosophy (and the basis of the question) is that strong host-based security removes the necessity of a firewall. The overall security philosophy says that strong host-based security is still required even if you have a firewall (see security guidelines). The reason for this is that a firewall that forwards public services to a server enables an attacker just as much as no firewall at all. It is the service itself that is vulnerable, and since offering that service to the entire Internet is a requirement of its operation, restricting access to it is not the point.\nIf there are ports available on the server that do not need to be accessed by the whole Internet, then that software needed to be shut down in step 1, and was verified by step 4. Should an attacker successfully break into the server through vulnerable software and open a port themselves, the attacker can (and do) just as easily defeat any firewall by making an outbound connection on a random port instead. The point of security isn't to defend yourself after a successful attack - that's already proven to be impossible - it's to keep the attackers out in the first place.\nIt's been suggested that there are other security considerations besides open ports - but to me that just sounds like defending one's faith. Any operating system/TCP stack vulnerabilities should be equally vulnerable whether or not a firewall exists - based on the fact that ports are being forwarded directly to that operating system/TCP stack. Likewise, running your firewall on the server itself as opposed to having it on the router (or worse, in both places) seems to be adding unnecessary layers of complexity. I understand the philosophy \"security comes in layers\", but there comes a point where it's like building a roof by stacking X number of layers of plywood on top of each other and then drilling a hole through all of them. Another layer of plywood isn't going to stop the leaks through that hole you're making on purpose.\nTo be honest, the only way I see a firewall being any use for servers is if it has dynamic rules preventing all connections to all servers from known attackers - like the RBLs for spam (which coincidentally, is pretty much what our mail server does). Unfortunately, I can't find any firewalls that do that. The next best thing is an IDS server, but that assumes that the attacker doesn't attack your real servers first, and that attackers bother to probe your entire network before attacking. Besides, these have been known to produce large numbers of false positives.",
        "top_answer": "Advantages of firewall:\n\nYou can filter outbound traffic.\nLayer 7 firewalls (IPS) can protect against known application vulnerabilities.\nYou can block a certain IP address range and/or port centrally rather than trying to ensure that there is no service listening on that port on each individual machine or denying access using TCP Wrappers.\nFirewalls can help if you have to deal with less security aware users/administrators as they would provide second line of defence. Without them one has to be absolutely sure that hosts are secure, which requires good security understanding from all administrators.\nFirewall logs would provide central logs and help in detecting vertical scans. Firewall logs can help in determining whether some user/client is trying to connect to same port of all your servers periodically. To do this without a firewall one would have to combine logs from various servers/hosts to get a centralized view.\nFirewalls also come with anti-spam / anti-virus modules which also add to protection.\nOS independent security. Based on host OS, different techniques / methods are required to make the host secure. For example, TCP Wrappers may not be available on Windows machines.\n\nAbove all this if you do not have firewall and system is compromised then how would you detect it? Trying to run some command 'ps', 'netstat', etc. on local system can't be trusted as those binaries can be replaced. 'nmap' from a remote system is not guaranteed protection as an attacker can ensure that root-kit accepts connections only from selected source IP address(es) at selected times.\nHardware firewalls help in such scenarios as it is extremely difficult to change firewall OS/files as compared to host OS/files.\nDisadvantages of firewall:\n\nPeople feel that firewall will take care of security and do not update systems regularly and stop unwanted services.\nThey cost. Sometimes yearly license fee needs to be paid. Especially if the firewall has anti-virus and anti-spam modules.\nAdditional single point of failure. If all traffic passes through a firewall and the firewall fails then network would stop. We can have redundant firewalls, but then previous point on cost gets further amplified.\nStateful tracking provides no value on public-facing systems that accept all incoming connections.\nStateful firewalls are a massive bottleneck during a DDoS attack and are often the first thing to fail, because they attempt to hold state and inspect all incoming connections.\nFirewalls cannot see inside encrypted traffic. Since all traffic should be encrypted end-to-end, most firewalls add little value in front of public servers. Some next-generation firewalls can be given private keys to terminate TLS and see inside the traffic, however this increases the firewall's susceptibility to DDoS even more, and breaks the  end-to-end security model of TLS.\nOperating systems and applications are patched against vulnerabilities much more quickly than firewalls. Firewall vendors often sit on known issues for years without patching, and patching a firewall cluster typically requires downtime for many services and outbound connections.\nFirewalls are far from perfect, and many are notoriously buggy. Firewalls are just software running on some form of operating system, perhaps with an extra ASIC or FPGA in addition to a (usually slow) CPU. Firewalls have bugs, but they seem to provide few tools to address them. Therefore firewalls add complexity and an additional source of hard-to-diagnose errors to an application stack.\n",
        "url": "https://serverfault.com/questions/201298/why-should-i-firewall-servers"
    },
    {
        "title": "InnoDB: Error: log file ./ib_logfile0 is of different size",
        "question": "I just added the following lines in /etc/mysql/my.cnf after I converted one database to use InnoDB engine.\ninnodb_buffer_pool_size = 2560M\ninnodb_log_file_size    = 256M\ninnodb_log_buffer_size  = 8M\ninnodb_flush_log_at_trx_commit  = 2\ninnodb_thread_concurrency   = 16\ninnodb_flush_method = O_DIRECT\n\nBut it raise \"ERROR 2013 (HY000) at line 2: Lost connection to MySQL server during query\" error restarting mysqld. And mysql error log shows the following\nInnoDB: Error: log file ./ib_logfile0 is of different size 0 5242880 bytes\nInnoDB: than specified in the .cnf file 0 268435456 bytes!\n100118 20:52:52 [ERROR] Plugin 'InnoDB' init function returned error.\n100118 20:52:52 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.\n100118 20:52:52 [ERROR] Unknown/unsupported table type: InnoDB\n100118 20:52:52 [ERROR] Aborting\n\nSo I commented out this line\n# innodb_log_file_size  = 256M\n\nAnd it restarted mysql successfully.\nI wonder what's the \"5242880 bytes of log file\" showed in mysql error? It's the first database on InnoDB engine on this server so when and where is that log file created? In this case, how can I enable innodb_log_file_size directive in my.cnf?\nEDIT\nI tried to delete /var/lib/mysql/ib_logfile0 and restart mysqld but it still failed. It now shows the following in error log.\n100118 21:27:11  InnoDB: Log file ./ib_logfile0 did not exist: new to be created\nInnoDB: Setting log file ./ib_logfile0 size to 256 MB\nInnoDB: Database physically writes the file full: wait...\nInnoDB: Progress in MB: 100 200\nInnoDB: Error: log file ./ib_logfile1 is of different size 0 5242880 bytes\nInnoDB: than specified in the .cnf file 0 268435456 bytes!\n\nResolution\nIt works now after deleted both ib_logfile0 and ib_logfile1 in /var/lib/mysql",
        "top_answer": "InnoDB is insanely picky about it's config; if something's not right, it'll just give up and go home.  To make a change to the log file size without data loss:\n\nRevert any config changes you've made to the log file size and start MySQL again. \nIn your running MySQL: SET GLOBAL innodb_fast_shutdown=0;\nStop MySQL\nMake the configuration change to the log file size.\nDelete both log files.\nStart MySQL.   It will complain about the lack of log files, but it'll create them and all will be well.\n",
        "url": "https://serverfault.com/questions/104014/innodb-error-log-file-ib-logfile0-is-of-different-size"
    },
    {
        "title": "How do I update a CentOS server's time from an authoritative time server?",
        "question": "I have CentOS machine and each time I've noticed that the server loses correct time after a while. It is usually behind by several minutes after time passes from having manually set the correct time. Is there a mechanism whereby I can update the server with the time from a specific time server?",
        "top_answer": "Use the ntp daemon. Run yum install ntp and ensure that the service is started via ntsysv or chkconfig ntpd on.\nTo get an immediate sync, run ntpdate time.apple.com (or something similar).",
        "url": "https://serverfault.com/questions/368602/how-do-i-update-a-centos-servers-time-from-an-authoritative-time-server"
    },
    {
        "title": "Multiple SSL domains on the same IP address and same port?",
        "question": "\nThis is a Canonical Question about Hosting multiple SSL websites on the same IP.\n\nI was under the impression that each SSL Certificate required it's own unique IP Address/Port combination. But the answer to a previous question I posted is at odds with this claim.\nUsing information from that Question, I was able to get multiple SSL certificates to work on the same IP address and on port 443. I am very confused as to why this works given the assumption above and reinforced by others that each SSL domain website on the same server requires its own IP/Port.\nI am suspicious that I did something wrong.  Can multiple SSL Certificates be used this way?",
        "top_answer": "\nFor the most up-to-date information on Apache and SNI, including additional HTTP-Specific RFCs, please refer to the Apache Wiki\n\n\nFYsI: \"Multiple (different) SSL certificates on one IP\" is brought to you by the magic of TLS Upgrading.\nIt works with newer Apache servers (2.2.x) and reasonably recent browsers (don't know versions off the top of my head).\nRFC 2817 (upgrading to TLS within HTTP/1.1) has the gory details, but basically it works for a lot of people (if not the majority).You can reproduce the old funky behavior with openssl's s_client command (or any \"old enough\" browser) though.\nEdit to add: apparently curl can show you what's happening here better than openssl:\n\nSSLv3\nmikeg@flexo% curl -v -v -v -3 https://www.yummyskin.com\n* About to connect() to www.yummyskin.com port 443 (#0)\n*   Trying 69.164.214.79... connected\n* Connected to www.yummyskin.com (69.164.214.79) port 443 (#0)\n* successfully set certificate verify locations:\n*   CAfile: /usr/local/share/certs/ca-root-nss.crt\n  CApath: none\n* SSLv3, TLS handshake, Client hello (1):\n* SSLv3, TLS handshake, Server hello (2):\n* SSLv3, TLS handshake, CERT (11):\n* SSLv3, TLS handshake, Server key exchange (12):\n* SSLv3, TLS handshake, Server finished (14):\n* SSLv3, TLS handshake, Client key exchange (16):\n* SSLv3, TLS change cipher, Client hello (1):\n* SSLv3, TLS handshake, Finished (20):\n* SSLv3, TLS change cipher, Client hello (1):\n* SSLv3, TLS handshake, Finished (20):\n* SSL connection using DHE-RSA-AES256-SHA\n* Server certificate:\n*    subject: serialNumber=wq8O9mhOSp9fY9JcmaJUrFNWWrANURzJ; C=CA; \n              O=staging.bossystem.org; OU=GT07932874;\n              OU=See www.rapidssl.com/resources/cps (c)10;\n              OU=Domain Control Validated - RapidSSL(R);\n              CN=staging.bossystem.org\n*    start date: 2010-02-03 18:53:53 GMT\n*    expire date: 2011-02-06 13:21:08 GMT\n* SSL: certificate subject name 'staging.bossystem.org'\n       does not match target host name 'www.yummyskin.com'\n* Closing connection #0\n* SSLv3, TLS alert, Client hello (1):\ncurl: (51) SSL: certificate subject name 'staging.bossystem.org'\ndoes not match target host name 'www.yummyskin.com'\n\n\nTLSv1\nmikeg@flexo% curl -v -v -v -1 https://www.yummyskin.com\n* About to connect() to www.yummyskin.com port 443 (#0)\n*   Trying 69.164.214.79... connected\n* Connected to www.yummyskin.com (69.164.214.79) port 443 (#0)\n* successfully set certificate verify locations:\n*   CAfile: /usr/local/share/certs/ca-root-nss.crt\n  CApath: none\n* SSLv3, TLS handshake, Client hello (1):\n* SSLv3, TLS handshake, Server hello (2):\n* SSLv3, TLS handshake, CERT (11):\n* SSLv3, TLS handshake, Server key exchange (12):\n* SSLv3, TLS handshake, Server finished (14):\n* SSLv3, TLS handshake, Client key exchange (16):\n* SSLv3, TLS change cipher, Client hello (1):\n* SSLv3, TLS handshake, Finished (20):\n* SSLv3, TLS change cipher, Client hello (1):\n* SSLv3, TLS handshake, Finished (20):\n* SSL connection using DHE-RSA-AES256-SHA\n* Server certificate:\n*    subject: C=CA; O=www.yummyskin.com; OU=GT13670640;\n              OU=See www.rapidssl.com/resources/cps (c)09;\n              OU=Domain Control Validated - RapidSSL(R);\n              CN=www.yummyskin.com\n*    start date: 2009-04-24 15:48:15 GMT\n*    expire date: 2010-04-25 15:48:15 GMT\n*    common name: www.yummyskin.com (matched)\n*    issuer: C=US; O=Equifax Secure Inc.; CN=Equifax Secure Global eBusiness CA-1\n*    SSL certificate verify ok.\n",
        "url": "https://serverfault.com/questions/109800/multiple-ssl-domains-on-the-same-ip-address-and-same-port"
    },
    {
        "title": "Send Ctrl-Alt-Del to nested RDP session",
        "question": "Is there a way to send the Ctrl-Alt-Del command to an RDP session (Windows Server 2008 R2) inside another RDP session (also Windows Server 2008 R2) without the first session catching it?\nCtrl+Alt+End and Ctrl+Alt+Shift+End do not reach the 2nd level session.\nTop-level environment is  Windows 7 Enterprise.",
        "top_answer": "Use the On-Screen Keyboard (osk.exe). You can press Ctrl-Alt-Del virtually!\n(Note: you may need to hold the CTRL and ALT keys on your physical keyboard (Windows Server 2012-R2))\n",
        "url": "https://serverfault.com/questions/359976/send-ctrl-alt-del-to-nested-rdp-session"
    },
    {
        "title": "Is there a way to do a remote \"ls\" much like \"scp\" does a remote copy?",
        "question": "Is there a way to do a remote \"ls\" much like \"scp\" does a remote copy in a standard linux shell?",
        "top_answer": "You could always do this:\nssh user@host ls -l /some/directory\n\nThat will SSH to the host, run ls, dump the output back to you and immediately disconnect. ",
        "url": "https://serverfault.com/questions/242176/is-there-a-way-to-do-a-remote-ls-much-like-scp-does-a-remote-copy"
    },
    {
        "title": "How can the little guys effectively learn and use Puppet? [closed]",
        "question": "Six months ago, in our not-for-profit project we decided to start migrating our system management to a Puppet-controlled environment because we are expecting our number of servers to grow substantially between now and a year from now.\nSince the decision has been made our IT guys have become a bit too annoyed a bit too often.  Their biggest objections are:\n\n\"We're not programmers, we're sysadmins\";\nModules are available online but many differ from one another; wheels are being reinvented too often, how do you decide which one fits the bill;\nCode in our repo is not transparent enough, to find how something works they have to recurse through manifests and modules they might have even written themselves a while ago;\nOne new daemon requires writing a new module, conventions have to be similar to other modules, a difficult process;\n\"Let's just run it and see how it works\"\nTons of hardly known 'extensions' in community modules: 'trocla', 'augeas', 'hiera'... how can our sysadmins keep track?\n\nI can see why a large organisation would dispatch their sysadmins to Puppet courses to become Puppet masters. But how would smaller players get to learn Puppet to a professional level if they do not go to courses and basically learn it via their browser and editor?",
        "top_answer": "I started using Puppet ahead of deploying a new infrastructure and simply bought a (well-regarded) book on the topic. I don't think most people actually obtain professional Puppet training. I worked on examples until I could mold the process to my environment. This was December 2011, so within a few weeks, I was able to understand the basics and get a production framework in place. I wasn't new to configuration management, having a CFEngine background, but many of your sysadmins' concerns resonate. I made mistakes and had to refactor several times, but I did get things working satisfactorily.\nA few notes on your points...\n\nThe traditional systems administration role is changing. Adapt or be left behind. I've been a successful systems engineer, but am having to retool as well (learning Python, for example). The focus on individual servers is diminished as hardware abstraction through virtualization and public and private cloud services gain traction. This means automation of systems tasks and the use of configuration management to wrest control of a larger number of servers. Add DevOps concepts to the mix, and you'll see that the customer/end-user expectations and requirements are changing.\nPuppet modules available online differ in style and structure and yes, I saw lots of overlap, redundancy and duplicated efforts. One developer I worked with said, \"you could have developed your own tools in the time you spent looking online for something that works!\" That gave me pause as I realized that Puppet seems to appeal more to developer types than admins looking for a best-practices or the right way approach.\nDocument heavily in order to get a feel for how things are connected. Given the shaky definitions and lack of a standard way of doing things, your configuration management structure really is unique to your environment. That transparency is going to have to be developed within. \nI'd argue that it's reasonably easy to duplicate a module to accommodate a new daemon or add a service to an existing manifest, depending on how you've organized your servers and roles.\nI spent a lot of time testing on a single target before pushing changes to larger groups of servers. Running puppetd by hand on a representative server allowed me to debug changes and assess their impact. Maybe that's a bit conservative, but it was necessary.\nI'm not sure how much I'd depend on community modules. I did have to start using Augeas for some work, and lamented the fact that it was a functionality I took for granted in CFEngine. \n\nIn all, I feel like there isn't a well-defined standard when it comes to Puppet. I had trouble figuring out how to organize directory structure on my Puppetmaster, understanding how to manage certificate signing, getting proper reverse DNS in place everywhere, getting Puppet to scale appropriately for the environment and understanding when to leverage community modules versus building my own. It's a shift in thinking and I see how that would make a sysadmin panic. However, this was also solution built from scratch, so I had the luxury of evaluating tools. The decision to go this way was based on mindshare and the momentum behind Puppet. It was worth the effort to learn something new. \nRemember, this site is a good resource, too.",
        "url": "https://serverfault.com/questions/395990/how-can-the-little-guys-effectively-learn-and-use-puppet"
    },
    {
        "title": "ssh tunnel refusing connections with \"channel 2: open failed\"",
        "question": "All of a sudden (read: without changing any parameters) my netbsd virtualmachine started acting oddly. The symptoms concern ssh tunneling.\nFrom my laptop I launch:\n$ ssh -L 7000:localhost:7000 user@host -N -v\n\nThen, in another shell:\n$ irssi -c localhost -p 7000\n\nThe ssh debug says:\ndebug1: Connection to port 7000 forwarding to localhost port 7000 requested.\ndebug1: channel 2: new [direct-tcpip]\nchannel 2: open failed: connect failed: Connection refused\ndebug1: channel 2: free: direct-tcpip: listening port 7000 for localhost port 7000, connect from 127.0.0.1 port 53954, nchannels 3\n\nI tried also with localhost:80 to connect to the (remote) web server, with identical results.\nThe remote host runs NetBSD:\nbash-4.2# uname -a\nNetBSD host 5.1_STABLE NetBSD 5.1_STABLE (XEN3PAE_DOMU) #6: Fri Nov  4 16:56:31 MET 2011  root@youll-thank-me-later:/m/obj/m/src/sys/arch/i386/compile/XEN3PAE_DOMU i386\n\nI am a bit lost. I tried running tcpdump on the remote host, and I spotted these 'bad chksum':\n09:25:55.823849 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 67, bad cksum 0 (->3cb3)!) 127.0.0.1.54381 > 127.0.0.1.7000: P, cksum 0xfe37 (incorrect (-> 0xa801), 1622402406:1622402421(15) ack 1635127887 win 4096 <nop,nop,timestamp 5002727 5002603>\n\nI tried restarting the ssh daemon to no avail. I haven't rebooted yet - perhaps somebody here can suggest other diagnostics. I think it might either be the virtual network card driver, or somebody rooted our ssh. \nIdeas..?",
        "top_answer": "Problem solved: \n$ ssh -L 7000:127.0.0.1:7000 user@host -N -v -v\n\n...apparently, 'localhost' was not liked by the remote host. Yet, remote /etc/hosts contains:\n::1                     localhost localhost.\n127.0.0.1               localhost localhost.\n\nwhile the local network interface is\nlo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> mtu 33184\n        inet 127.0.0.1 netmask 0xff000000\n        inet6 ::1 prefixlen 128\n        inet6 fe80::1%lo0 prefixlen 64 scopeid 0x2\n\nSigh. so much for the bounty of 100rp I put on :)",
        "url": "https://serverfault.com/questions/489192/ssh-tunnel-refusing-connections-with-channel-2-open-failed"
    },
    {
        "title": "Should a wildcard SSL certificate secure both the root domain as well as the sub-domains?",
        "question": "I ask this question, because Comodo are telling me that a wildcard certificate for *.example.com will also secure the root domain example.com. So with a single certificate, both my.example.com and example.com are secured without warning from a browser.\nHowever, this is not the case with the certificate I've been provided. My sub-domains are secured fine and do not give an error, but the root domain throws up an error in the browser, saying the identify can't be verified.\nWhen I compare this certificate to other similar scenarios, I see that in the scenarios that work without error, the Subject Alternative Name (SAN) lists both *.example.com and example.com, whereas the recent certificate from Comodo only lists *.example.com as the Common Name and NOT example.com as the Subject Alternative Name.\nCan anyone confirm/clarify that the root domain should be listed in SAN details if it is also to be secured correctly?\nWhen I read this: http://www.digicert.com/subject-alternative-name.htm It seems that the SAN must list both in order to work as I need it to. What's your experience?\nThanks very much.",
        "top_answer": "There's some inconsistency between SSL implementations on how they match wildcards, however you'll need the root as an alternate name for that to work with most clients.\nFor a *.example.com cert,\n\na.example.com should pass\nwww.example.com should pass\nexample.com should not pass\na.b.example.com may pass depending on implementation (but probably not).\n\nEssentially, the standards say that the * should match 1 or more non-dot characters, but some implementations allow a dot.\nThe canonical answer should be in RFC 2818 (HTTP Over TLS):\n\nMatching is performed using the matching rules specified by\n[RFC2459].  If more than one identity of a given type is present in\nthe certificate (e.g., more than one dNSName name, a match in any one\nof the set is considered acceptable.) Names may contain the wildcard\ncharacter * which is considered to match any single domain name\ncomponent or component fragment. E.g., *.a.com matches foo.a.com but\nnot bar.foo.a.com. f*.com matches foo.com but not bar.com.\n\nRFC 2459 says:\n\n\nA \"*\" wildcard character MAY be used as the left-most name\ncomponent in the certificate.  For example, *.example.com would\nmatch a.example.com, foo.example.com, etc. but would not match\nexample.com.\n\n\nIf you need a cert to work for example.com, www.example.com and foo.example.com, you need a certificate with subjectAltNames so that you have \"example.com\" and \"*.example.com\" (or example.com and all the other names you might need to match).",
        "url": "https://serverfault.com/questions/310530/should-a-wildcard-ssl-certificate-secure-both-the-root-domain-as-well-as-the-sub"
    },
    {
        "title": "How can a single disk in a hardware SATA RAID-10 array bring the entire array to a screeching halt?",
        "question": "Prelude:\nI'm a code-monkey that's increasingly taken on SysAdmin duties for my small company.  My code is our product, and increasingly we provide the same app as SaaS.\nAbout 18 months ago I moved our servers from a premium hosting centric vendor to a barebones rack pusher in a tier IV data center.  (Literally across the street.)  This ment doing much more ourselves--things like networking, storage and monitoring.\nAs part the big move, to replace our leased direct attached storage from the hosting company, I built a 9TB two-node NAS based on SuperMicro chassises, 3ware RAID cards, Ubuntu 10.04, two dozen SATA disks, DRBD and .  It's all lovingly documented in three blog posts: Building up & testing a new 9TB SATA RAID10 NFSv4 NAS: Part I, Part II and Part III.\nWe also setup a Cacti monitoring system.  Recently we've been adding more and more data points, like SMART values.\nI could not have done all this without the awesome boffins at ServerFault.  It's been a fun and educational experience.  My boss is happy (we saved bucket loads of $$$), our customers are happy (storage costs are down), I'm happy (fun, fun, fun).\nUntil yesterday.\nOutage & Recovery:\nSome time after lunch we started getting reports of sluggish performance from our application, an on-demand streaming media CMS.  About the same time our Cacti monitoring system sent a blizzard of emails.  One of the more telling alerts was a graph of iostat await.\n\nPerformance became so degraded that Pingdom began sending \"server down\" notifications.  The overall load was moderate, there was not traffic spike.\nAfter logging onto the application servers, NFS clients of the NAS, I confirmed that just about everything was experiencing highly intermittent and insanely long IO wait times.  And once I hopped onto the primary NAS node itself, the same delays were evident when trying to navigate the problem array's file system.\nTime to fail over, that went well.  Within 20 minuts everything was confirmed to be back up and running perfectly.\nPost-Mortem:\nAfter any and all system failures I perform a post-mortem to determine the cause of the failure.  First thing I did was ssh back into the box and start reviewing logs.  It was offline, completely.  Time for a trip to the data center.  Hardware reset, backup an and running.\nIn /var/syslog I found this scary looking entry:\nNov 15 06:49:44 umbilo smartd[2827]: Device: /dev/twa0 [3ware_disk_00], 6 Currently unreadable (pending) sectors\nNov 15 06:49:44 umbilo smartd[2827]: Device: /dev/twa0 [3ware_disk_07], SMART Prefailure Attribute: 1 Raw_Read_Error_Rate changed from 171 to 170\nNov 15 06:49:45 umbilo smartd[2827]: Device: /dev/twa0 [3ware_disk_10], 16 Currently unreadable (pending) sectors\nNov 15 06:49:45 umbilo smartd[2827]: Device: /dev/twa0 [3ware_disk_10], 4 Offline uncorrectable sectors\nNov 15 06:49:45 umbilo smartd[2827]: Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\nNov 15 06:49:45 umbilo smartd[2827]: # 1  Short offline       Completed: read failure       90%      6576         3421766910\nNov 15 06:49:45 umbilo smartd[2827]: # 2  Short offline       Completed: read failure       90%      6087         3421766910\nNov 15 06:49:45 umbilo smartd[2827]: # 3  Short offline       Completed: read failure       10%      5901         656821791\nNov 15 06:49:45 umbilo smartd[2827]: # 4  Short offline       Completed: read failure       90%      5818         651637856\nNov 15 06:49:45 umbilo smartd[2827]:\n\nSo I went to check the Cacti graphs for the disks in the array.  Here we see that, yes, disk 7 is slipping away just like syslog says it is.  But we also see that disk 8's SMART Read Erros are fluctuating.\n\nThere are no messages about disk 8 in syslog.  More interesting is that the fluctuating values for disk 8 directly correlate to the high IO wait times!  My interpretation is that:\n\nDisk 8 is experiencing an odd hardware fault that results in intermittent long operation times.\nSomehow this fault condition on the disk is locking up the entire array\n\nMaybe there is a more accurate or correct description, but the net result has been that the one disk is impacting the performance of the whole array.\nThe Question(s)\n\nHow can a single disk in a hardware SATA RAID-10 array bring the entire array to a screeching halt?\nAm I being na\u00efve to think that the RAID card should have dealt with this?\nHow can I prevent a single misbehaving disk from impacting the entire array?\nAm I missing something?\n",
        "top_answer": "I hate to say \"don't use SATA\" in critical production environments, but I've seen this situation quite often. SATA drives are not generally meant for the duty cycle you describe, although you did spec drives specifically rated for 24x7 operation in your setup.  My experience has been that SATA drives can fail in unpredictable ways, often times affecting the entire storage array, even when using RAID 1+0, as you've done. Sometimes the drives fail in a manner that can stall the entire bus. One thing to note is whether you're using SAS expanders in your setup. That can make a difference in how the remaining disks are impacted by a drive failure. \nBut it may have made more sense to go with midline/nearline (7200 RPM) SAS drives versus SATA. There's a small price premium over SATA, but the drives will operate/fail more predictably. The error-correction and reporting in the SAS interface/protocol is more robust than the SATA set. So even with drives whose mechanics are the same, the SAS protocol difference may have prevented the pain you experienced during your drive failure.",
        "url": "https://serverfault.com/questions/331499/how-can-a-single-disk-in-a-hardware-sata-raid-10-array-bring-the-entire-array-to"
    },
    {
        "title": "Can I create a top level TLD? (.michael for instance)",
        "question": "It appears that ICANN is allowing the creation of top level domains.  Instead of 'registering' a domain, you would essentially be signing up to be a registrar (you'd be giving out registrations on your TLD).\n\nHow do they decide whether to accept/reject applications? (i.e. is notability a requirement precluding .michael for instance)\nCan an existing business register a TLD, or is it only a more general organization (i.e. \"the museum society\" instead of the \"NYC Natural History Museum\")\nHow much does it cost?\n",
        "top_answer": "There are various considerations for accepting an application, covered in the guidebook (PDF). Part of the process will involve the application going through several panels, including:\n\nString Similarity Panel \u2013 assesses whether a proposed gTLD string is likely to result in user confusion due to similarity with any reserved name, any existing TLD, any requested IDN ccTLD, or any new gTLD string applied for in the current application round. This occurs during the String Similarity review in Initial Evaluation. The panel may also review IDN tables submitted by applicants as part of its work.\nDNS Stability Panel \u2013 reviews each applied-for string to determine whether the proposed string might adversely affect the security or stability of the DNS. This occurs during the DNS Stability String Review in Initial Evaluation.\nGeographical Names Panel \u2013 reviews each application to determine whether the applied-for gTLD represents a geographic name, as defined in the Applicant Guidebook. In the event that the string represents a geographic name and requires government support, the panel will review and verify that the documentation provided with the application is from the relevant governments or public authorities and is authentic.\nTechnical Evaluation Panel \u2013 reviews the technical components of each application against the criteria in the Applicant Guidebook, along with proposed registry operations, in order to determine whether the applicant is technically and operationally capable of operating a gTLD registry as proposed in the application. This occurs during the Technical/Operational Reviews in Initial Evaluation, and may also occur in Extended Evaluation if necessary and if elected by the applicant.\nFinancial Evaluation Panel \u2013 reviews each application against the relevant business, financial and organizational criteria contained in the Applicant Guidebook, to determine whether the applicant is financially capable of maintaining a gTLD registry as proposed in the application. This occurs during the Financial Review in Initial Evaluation, and may also occur in Extended Evaluation if necessary and if elected by the applicant.\nRegistry Services Panel \u2013 reviews the proposed registry services in the application to determine if any registry services pose a risk of a meaningful adverse impact on security or stability. This occurs, if applicable, during the Extended Evaluation period.\n\nAnyone can register a TLD, though you'd better own the trademark, and others can file objections to your TLD application. Also, you'll have to be able to operate as a registry.\nThe costs start at $185,000. From the FAQ to which you linked:\n\n7.2 How much is the evaluation fee?\nThe evaluation fee is estimated at US$185,000. Applicants will be required to pay a US$5,000 deposit fee per application request slot when registering. The US$5,000 will be credited against the evaluation fee. Other fees may apply depending on the specific application path. See the section 1.5 of the Applicant Guidebook for details about the methods of payment, additional fees and refund schedules.\n7.3 Are there any additional costs I should be aware of in applying for a new gTLD?\nYes. Applicants may be required to pay additional fees in certain cases where specialized process steps are applicable, and should expect to account for their own business start up costs. See Section 1.5.2 of the Applicant Guidebook.\n7.5 Are there any ongoing fees once a gTLD is approved by ICANN?\nYes. Once an application has successfully passed all the evaluation steps, the applicant is required to sign a New gTLD Agreement (also called Registry Agreement) with ICANN. Under the agreement, there are two fees: (a) a fixed fee of US$6,250 per calendar quarter; (b) and a transaction fee of US$0.25. The latter does not apply until and unless more than 50,000 domain names are registered in the gTLD.\n",
        "url": "https://serverfault.com/questions/243316/can-i-create-a-top-level-tld-michael-for-instance"
    },
    {
        "title": "Perform rsync while following sym links",
        "question": "I have a directory that contains symbolic links to other directories located on different media on my system:\n/opt/lun1/2011\n/opt/lun1/2010\n/opt/lun2/2009\n/opt/lun2/2008\n/opt/lun3/2007\n\nBut the symbolic links show up as:\n/files/2011\n/files/2010\n/files/2009\n/files/2008\n/files/2007\n\nHow can I perform an rsync that follows the symbolic links?\ne.g.:\nrsync -XXX /files/ user@server:/files/",
        "top_answer": "The -L flag to rsync will sync the contents of files or directories linked to, rather than the symbolic link.",
        "url": "https://serverfault.com/questions/245774/perform-rsync-while-following-sym-links"
    },
    {
        "title": "Is X.Y.Z.0 a valid IP address?",
        "question": "Are IP addresses with a 0 in the last octet valid?\n10.6.43.0\n\nIn my case, I have the the following netmask\n255.255.252.0\n\nWhat about a 0 for the other octets?",
        "top_answer": "It depends on the subnet of the IP address in question. In general, the first and last addresses in a subnet are used as the network identifier and broadcast address, respectively. All other addresses in the subnet can be assigned to hosts on that subnet.\nFor example, IP addresses of networks with subnet masks of at least 24 bits ending in .0 or .255 can never be assigned to hosts. Such \"last\" addresses of a subnet are considered \"broadcast\" addresses and all hosts on the corresponding subnet will respond to it.\nTheoretically, there could be situations where you can assign an address ending in .0: for example, if you have a subnet like  192.168.0.0/255.255.0.0, you are allowed to assign a host the address 192.168.1.0. It could create confusion though, so it's not a very common practice.\nIn your example\n 10.6.43.0 with subnet 255.255.252.0 (22 bit subnet mask)\n\nmeans subnet ID 10.6.40.0, a host address range from 10.6.40.1 to 10.6.43.254 and a broadcast address 10.6.43.255. So in theory, your example 10.6.43.0 would be allowed as a valid host address.",
        "url": "https://serverfault.com/questions/10985/is-x-y-z-0-a-valid-ip-address"
    },
    {
        "title": "How to retrieve the last modification date of all files in a Git repository",
        "question": "I know how to retrieve the last modification date of a single file in a Git repository:\ngit log -1 --format=\"%ad\" -- path/to/file\n\nIs there a simple and efficient way to do the same for all the files currently present in the repository?",
        "top_answer": "A simple answer would be to iterate through each file and display its modification time, i.e.:\ngit ls-tree -r --name-only HEAD | while read filename; do\n  echo \"$(git log -1 --format=\"%ad\" -- $filename) $filename\"\ndone\n\nThis will yield output like so:\nFri Dec 23 19:01:01 2011 +0000 Config\nFri Dec 23 19:01:01 2011 +0000 Makefile\n\nObviously, you can control this since its just a bash script at this point--so feel free to customize to your heart's content!",
        "url": "https://serverfault.com/questions/401437/how-to-retrieve-the-last-modification-date-of-all-files-in-a-git-repository"
    },
    {
        "title": "escaping double quotes and percent signs (%) in cron",
        "question": "The following command works from prompt but not from crontab.\ngrep abc /var/log/messages | grep \"`date '+%B %d'`\" | mail -s\"abc log of `hostname`\" s.o+`hostname`@gmail.com\n\nI need to add it to daily cron.",
        "top_answer": "You have to escape the % signs. They have a special meaning in crontabs:\nman (5) crontab:\n\nPercent-signs (%) in the command, unless escaped with backslash (\\), \nwill be changed into newline characters, and all data after the \nfirst % will be sent to the command as standard input.\n",
        "url": "https://serverfault.com/questions/274475/escaping-double-quotes-and-percent-signs-in-cron"
    },
    {
        "title": "How do I set the global PATH environment variable on OS X?",
        "question": "I'd like to append to the global PATH environment variable on OS\u00a0X so that all user shells and GUI applications get the same PATH environment.\nI know I can append to the path in shell startup scripts, but those settings are not inherited by GUI applications.\nThe only way I found so far is to redefine the PATH environment variable in /etc/launchd.conf:\nsetenv PATH /usr/bin:/bin:/usr/sbin:/sbin:/my/path\n\nI couldn't figure out a way to actually append to PATH in launchd.conf.\nI'm a bit worried about this method, but so far this is the only thing that works. Is there a better way?",
        "top_answer": "palmer's GUI information is correct, but there is a more maintainable way to modify the path seen by the shell. Like mediaslave said, you can edit /etc/paths, but even better you can drop a text file in /etc/paths.d/ that has a path in it and all shells will construct the path correctly.\nFor example, on my system:\n$ cat /etc/paths\n/usr/bin\n/bin\n/usr/sbin\n/sbin\n/usr/local/bin\n$ ls /etc/paths.d\nX11       git      postgres\n$ cat /etc/paths.d/postgres\n/Library/PostgreSQL/8.4/bin\n$ echo $PATH\n/opt/local/bin:/opt/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/usr/local/git/bin:/Library/PostgreSQL/8.4/bin:/usr/X11/bin:/usr/local/mysql/bin\n",
        "url": "https://serverfault.com/questions/16355/how-do-i-set-the-global-path-environment-variable-on-os-x"
    },
    {
        "title": "\"POSSIBLE BREAK-IN ATTEMPT!\" in /var/log/secure \u2014 what does this mean?",
        "question": "I've got a CentOS 5.x box running on a VPS platform. My VPS host misinterpreted a support inquiry I had about connectivity and effectively flushed some iptables rules.  This resulted in ssh listening on the standard port and acknowledging port connectivity tests. Annoying. \nThe good news is that I require SSH Authorized keys.  As far as I can tell, I don't think there was any successful breach.  I'm still very concerned about what I'm seeing in /var/log/secure though: \n\nApr 10 06:39:27 echo sshd[22297]: reverse mapping checking getaddrinfo for 222-237-78-139.tongkni.co.kr failed - POSSIBLE BREAK-IN ATTEMPT!\nApr 10 13:39:27 echo sshd[22298]: Received disconnect from 222.237.78.139: 11: Bye Bye\nApr 10 06:39:31 echo sshd[22324]: Invalid user edu1 from 222.237.78.139\nApr 10 06:39:31 echo sshd[22324]: reverse mapping checking getaddrinfo for 222-237-78-139.tongkni.co.kr failed - POSSIBLE BREAK-IN ATTEMPT!\nApr 10 13:39:31 echo sshd[22330]: input_userauth_request: invalid user edu1\nApr 10 13:39:31 echo sshd[22330]: Received disconnect from 222.237.78.139: 11: Bye Bye\nApr 10 06:39:35 echo sshd[22336]: Invalid user test1 from 222.237.78.139\nApr 10 06:39:35 echo sshd[22336]: reverse mapping checking getaddrinfo for 222-237-78-139.tongkni.co.kr failed - POSSIBLE BREAK-IN ATTEMPT!\nApr 10 13:39:35 echo sshd[22338]: input_userauth_request: invalid user test1\nApr 10 13:39:35 echo sshd[22338]: Received disconnect from 222.237.78.139: 11: Bye Bye\nApr 10 06:39:39 echo sshd[22377]: Invalid user test from 222.237.78.139\nApr 10 06:39:39 echo sshd[22377]: reverse mapping checking getaddrinfo for 222-237-78-139.tongkni.co.kr failed - POSSIBLE BREAK-IN ATTEMPT!\nApr 10 13:39:39 echo sshd[22378]: input_userauth_request: invalid user test\nApr 10 13:39:39 echo sshd[22378]: Received disconnect from 222.237.78.139: 11: Bye Bye\n\n\nWhat exactly does \"POSSIBLE BREAK-IN ATTEMPT\" mean?  That it was successful? Or that it didn't like the IP the request was coming from? ",
        "top_answer": "Unfortunately this in now a very common occurrence. It is an automated attack on SSH which is using 'common' usernames to try and break into your system. The message means exactly what it says, it does not mean that you have been hacked, just that someone tried.",
        "url": "https://serverfault.com/questions/260706/possible-break-in-attempt-in-var-log-secure-what-does-this-mean"
    },
    {
        "title": "How to make Windows 7 USB flash install media from Linux?",
        "question": "I have: \n\nISO image of Windows 7 install media\n4 GB USB flash drive\nno DVD drive\nLinux installed\n",
        "top_answer": "OK, after unsuccessfully trying all methods mentioned here, I finally got it working. Basically, the missing step was to write a proper boot sector to the USB stick, which can be done from Linux with ms-sys or lilo -M. This works with the Windows 7 retail version.\nHere is the complete rundown again:\nInstall ms-sys - if it is not in your repositories, get it here. Or alternatively, make sure lilo is installed (but do not run the liloconfig step on your local box if e.g. Grub is installed there!)\nCheck what device your USB media is assigned - here we will assume it is /dev/sdb. Delete all partitions, create a new one taking up all the space, set type to NTFS (7), and remember to set it bootable:\n# cfdisk /dev/sdb \u00a0 or \u00a0 fdisk /dev/sdb  (partition type 7, and bootable flag)\nCreate an NTFS filesystem:\n# mkfs.ntfs -f /dev/sdb1\nWrite Windows 7 MBR on the USB stick (also works for windows 8), multiple options here:\n\n# ms-sys -7 /dev/sdb\nor (e.g. on newer Ubuntu installs) sudo lilo -M  /dev/sdb mbr (info)\nor (if syslinux is installed), you can run sudo dd if=/usr/lib/syslinux/mbr/mbr.bin of=/dev/sdb\n\nMount ISO and USB media:\n# mount -o loop win7.iso /mnt/iso\n# mount /dev/sdb1 /mnt/usb\nCopy over all files:\n# cp -r /mnt/iso/* /mnt/usb/  \u00a0 ...or use the standard GUI file-browser of your system\nCall sync to make sure all files are written.\nOpen gparted, select the USB drive, right-click on the file system, then click on \"Manage Flags\". Check the boot checkbox, then close.\n...and you're done.\nAfter all that, you probably want to back up your USB media for further installations and get rid of the ISO file... Just use dd:\n# dd if=/dev/sdb of=win7.img\nNote, this copies the whole device! \u2014 which is usually (much) bigger than the files copied to it. So instead I propose\n# dd count=[(size of the ISO file in MB plus some extra MB for boot block) divided by default dd blocksize] if=/dev/sdb of=win7.img\n\nThus for example with 8 M extra bytes:\n# dd count=$(((`stat -c '%s' win7.iso` + 8*1024*1024) / 512)) if=/dev/sdb of=win7.img status=progress\n\nAs always, double check the device names very carefully when working with dd.\nThe method creating a bootable USB presented above works also with Win10 installer iso. I tried it running Ubuntu 16.04 copying  Win10_1703_SingleLang_English_x64.iso (size 4,241,291,264 bytes) onto an 8 GB USB-stick \u2014 in non-UEFI [non-secure] boot only. After execution dd reports:\n        8300156+0 records in\n        8300156+0 records out\n        4249679872 bytes (4.2 GB, 4.0 GiB) copied, 412.807 s, 10.3 MB/s\nReverse if/of next time you want to put the Windows 7 installer onto USB.",
        "url": "https://serverfault.com/questions/6714/how-to-make-windows-7-usb-flash-install-media-from-linux"
    },
    {
        "title": "Differences and dis/advanages between: Fast-CGI, CGI, Mod-PHP, SuPHP, PHP-FPM",
        "question": "There are few different php \"wrappers\"(?). What are differences between them? Tried to google some, but cant seem to find informations. (mod-php is not googleable).\nWhy might I choose one over another?",
        "top_answer": "CGI and FastCGI are two protocols not specific to PHP:\n\nCGI scripts is a way how to run a server side script (not only PHP!) when a HTTP request arrives. In this setup, the web server launches a new CGI process for every incoming request, causing significant performance overhead.\n\nFastCGI is a \"better CGI\" - to address the limitations of CGI, the FastCGI runs as a server (TCP or UNIX), so that resources can be reused across requests.\n\n\nPHP-enabled webserver can be configured as following:\n\nmod_php is an Apache module to run PHP. In this setup, PHP request is handled under Apache process with everything that goes with it: PHP processes are defined in Apache configuration, PHP runs under Apache user and permissions etc.\n\nPHP-FPM is PHP's FastCGI implementation. In this setup, PHP-FPM runs as a standalone FastCGI server and Apache connects to it using FastCGI modules, such as mod_fcgid, mod_fastcgi or mod_proxy_fcgi (Apache 2.4+). In this configuration, permissions, processes related stuff & everything else is controlled by the PHP-FPM server. Performance is comparable with mod_php.\n\nSuPHP - this was used to address some shortcomings of mod_php related to permissions: with mod_php PHP scripts are run under the Apache user/group, but mod_suphp can run the scripts as a different user. suPHP is not maintained anymore and should not be used.\n\nCGI/FastCGI - I have added this one based on a question in the comments. Without knowing the details of the setup, PHP can be run as FastCGI server using any other FastCGI implementation - as explained in another question. I don't use this setup and don't see any benefit over PHP-FPM.\n\nCGI - PHP can also be run as the good-ol' CGI script, but I can't imagine a single good use case for that, apart for compatibility with some very outdated environments.\n\n\nRegarding advantages and disadvantages of those different approaches, I stick only to mod_php and PHP-FPM, covering two main use cases:\n\nmod_php can be useful in certain Docker setups where you want to deliver a single container running a PHP-enabled web server. The fact that everything runs as a single process makes the Docker container configuration easier. On the other hand, running PHP-FPM server in a single container with a webserver would require process orchestration either with supervisord, advanced bash scripting or some other approach and goes against best practices writing Docker containers.\n\nPHP-FPM is a more powerful approach that separates the concerns better, so the PHP-FPM server can be configured, (performance-)tuned and maintained separately from the webserver. This also allows to run the PHP-FPM server in a pool or on a different machine than the webserver. As implied above, for Docker containers, a separate PHP-FPM and webserver containers are recommended in this case, making the configuration more complex (and more powerful). PHP-FPM approach is also the only way with nginx webserver as the PHP module for it AFAIK does not exist.\n\n\nMy Docker implementation of the two aforementioned approaches can be found here:\n\nhttps://gitlab.com/craynic.com/docker/lap/ - a single container approach, running PHP 7.4/8.0 as an Apache module\nhttps://gitlab.com/craynic.com/craynic.net/mvh - a multi container approach, separating PHP-FPM from Apache webserver\n\nThe implementation is designed to work with some of my legacy and new projects in my Kubernetes cluster. Feel free to use it.\nSo, TLDR:\n\nCGI, FastCGI are protocols; CGI is slow, FastCGI is much faster\nmod_php and PHP-FPM are two main ways how to run PHP\nmod_SuPHP was an approach that was used to address mod_php shortcomings. It is outdated and PHP-FPM should be used instead.\n",
        "url": "https://serverfault.com/questions/645755/differences-and-dis-advanages-between-fast-cgi-cgi-mod-php-suphp-php-fpm"
    },
    {
        "title": "How do high traffic sites service more than 65535 TCP connections?",
        "question": "If there is a limit on the number of ports one machine can have and a socket can only bind to an unused port number, how do servers experiencing extremely high amounts (more than the max port number) of requests handle this? Is it just done by making the system distributed, i.e., many servers on many machines?",
        "top_answer": "You misunderstand port numbers: a server listens only on one port and can have large numbers of open sockets from clients connecting to that one port.\nOn the TCP level the tuple (source ip, source port, destination ip, destination port) must be unique for each simultaneous connection. That means a single client cannot open more than 65535 simultaneous connections to a single server. But a server can (theoretically) serve 65535 simultaneous connections per client.\nSo in practice the server is only limited by how much CPU power, memory etc. it has to serve requests, not by the number of TCP connections to the server.",
        "url": "https://serverfault.com/questions/533611/how-do-high-traffic-sites-service-more-than-65535-tcp-connections"
    },
    {
        "title": "SSH from A through B to C, using private key on B [closed]",
        "question": "I'm looking for a simple way to SSH from my local machine, A, through a proxy, B, to a destination host, C. The private key that goes with the public key on C is on B, and I can't put that key on my local machine. Any tips? \nAlso, I'd like to be able to do this using ~/.ssh/config.\nThanks!",
        "top_answer": "Schematic:\n    ssh       ssh\nA ------> B ------> C\n    ^          ^\n using A's   using B's\n ssh key     ssh key\n\nPreconditions:\n\nA is running ssh-agent;\nA can access B;\nB can access C;\nA's ssh public key is present in B:~/.ssh/authorized_keys\nB's ssh public key is present in C:~/.ssh/authorized_keys\n\nIn ~/.ssh/config on A, add\nHost C\n    ProxyCommand ssh -o 'ForwardAgent yes' B 'ssh-add && nc %h %p'\n\nIf your ssh private key on B is in a nonstandard location, add its path after ssh-add.\nYou should now be able to access C from A:\nA$ ssh C\nC$\n",
        "url": "https://serverfault.com/questions/337274/ssh-from-a-through-b-to-c-using-private-key-on-b"
    },
    {
        "title": "High load average, low CPU usage - why?",
        "question": "We're seeing huge performance problems on a web application and we're trying to find the bottleneck.  I am not a sysadmin so there is some stuff I don't quite get.  Some basic investigation shows the CPU to be idle, lots of memory to be available, no swapping, no I/O, but a high average load.\nThe software stack on this server looks like this:\n\nSolaris 10\nJava 1.6\nWebLogic 10.3.5 (8 domains)\n\nThe applications running on this server talk with an Oracle database on a different server.\nThis server has 32GB of RAM and 10 CPUs (I think).\nRunning prstat -Z gives something like this:\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP\n  3836 ducm0101 2119M 2074M cpu348  58    0   8:41:56 0.5% java/225\n 24196 ducm0101 1974M 1910M sleep   59    0   4:04:33 0.4% java/209\n  6765 ducm0102 1580M 1513M cpu330   1    0   1:21:48 0.1% java/291\n 16922 ducm0102 2115M 1961M sleep   58    0   6:37:08 0.0% java/193\n 18048 root     3048K 2440K sleep   59    0   0:06:02 0.0% sa_comm/4\n 26619 ducm0101 2588M 2368M sleep   59    0   8:21:17 0.0% java/231\n 19904 ducm0104 1713M 1390M sleep   59    0   1:15:29 0.0% java/151\n 27809 ducm0102 1547M 1426M sleep   59    0   0:38:19 0.0% java/186\n  2409 root       15M   11M sleep   59    0   0:00:00 0.0% pkgserv/3\n 27204 root       58M   54M sleep   59    0   9:11:38 0.0% stat_daemon/1\n 27256 root       12M 8312K sleep   59    0   7:16:40 0.0% kux_vmstat/1\n 29367 root      297M  286M sleep   59    0  11:02:13 0.0% dsmc/2\n 22128 root       13M 6768K sleep   59    0   0:10:51 0.0% sendmail/1\n 22133 smmsp      13M 1144K sleep   59    0   0:01:22 0.0% sendmail/1\n 22003 root     5896K  240K sleep   59    0   0:00:01 0.0% automountd/2\n 22074 root     4776K 1992K sleep   59    0   0:00:19 0.0% sshd/1\n 22005 root     6184K 2728K sleep   59    0   0:00:31 0.0% automountd/2\n 27201 root     6248K  344K sleep   59    0   0:00:01 0.0% mount_stat/1\n 20964 root     2912K  160K sleep   59    0   0:00:01 0.0% ttymon/1\n 20947 root     1784K  864K sleep   59    0   0:02:22 0.0% utmpd/1\n 20900 root     3048K  608K sleep   59    0   0:00:03 0.0% ttymon/1\n 20979 root       77M   18M sleep   59    0   0:14:13 0.0% inetd/4\n 20849 daemon   2856K  864K sleep   59    0   0:00:03 0.0% lockd/2\n 17794 root       80M 1232K sleep   59    0   0:06:19 0.0% svc.startd/12\n 17645 root     3080K  728K sleep   59    0   0:00:12 0.0% init/1\n 17849 root       13M 6800K sleep   59    0   0:13:04 0.0% svc.configd/15\n 20213 root       84M   81M sleep   59    0   0:47:17 0.0% nscd/46\n 20871 root     2568K  600K sleep   59    0   0:00:04 0.0% sac/1\n  3683 ducm0101 1904K 1640K sleep   56    0   0:00:00 0.0% startWebLogic.s/1\n 23937 ducm0101 1904K 1640K sleep   59    0   0:00:00 0.0% startWebLogic.s/1\n 20766 daemon   5328K 1536K sleep   59    0   0:00:36 0.0% nfsmapid/3\n 20141 daemon   5968K 3520K sleep   59    0   0:01:14 0.0% kcfd/4\n 20093 ducm0101 2000K  376K sleep   59    0   0:00:01 0.0% pfksh/1\n 20797 daemon   3256K  240K sleep   59    0   0:00:01 0.0% statd/1\n  6181 root     4864K 2872K sleep   59    0   0:01:34 0.0% syslogd/17\n  7220 ducm0104 1268M 1101M sleep   59    0   0:36:35 0.0% java/138\n 27597 ducm0102 1904K 1640K sleep   59    0   0:00:00 0.0% startWebLogic.s/1\n 27867 root       37M 4568K sleep   59    0   0:13:56 0.0% kcawd/7\n 12685 ducm0101 4080K  208K sleep   59    0   0:00:01 0.0% vncconfig/1\nZONEID    NPROC  SWAP   RSS MEMORY      TIME  CPU ZONE\n    42      135   22G   19G    59%  87:27:59 1.2% dsuniucm01\n\nTotal: 135 processes, 3167 lwps, load averages: 54.48, 62.50, 63.11\n\nI understand that CPU is mostly idle, but the load average is high, which is quite strange to me.  Memory doesn't seem to be a problem.\nRunning vmstat 15 gives something like this:\n kthr      memory            page            disk          faults      cpu\n r b w   swap  free  re  mf pi po fr de sr s0 s1 s4 sd   in   sy   cs us sy id\n 0 0 0 32531400 105702272 317 1052 126 0 0 0 0 13 13 -0 8 9602 107680 10964 1 1 98\n 0 0 0 15053368 95930224 411 2323 0 0 0 0 0 0  0  0  0 23207 47679 29958 3 2 95\n 0 0 0 14498568 95801960 3072 3583 0 2 2 0 0 3 3  0 21 22648 66367 28587 4 4 92\n 0 0 0 14343008 95656752 3080 2857 0 0 0 0 0 3 3  0 18 22338 44374 29085 3 4 94\n 0 0 0 14646016 95485472 1726 3306 0 0 0 0 0 0 0  0  0 24702 47499 33034 3 3 94\n\nI understand that the CPU is mostly idle, no processes are waiting in the queue to be executed, little swapping is happening.\nRunning iostat 15 gives this:\n   tty        sd0           sd1           sd4           ssd0           cpu\n tin tout kps tps serv  kps tps serv  kps tps serv  kps tps serv   us sy wt id\n   0  676 324  13    8  322  13    8    0   0    0  159   8    0    1  1  0 98\n   1 1385   0   0    0    0   0    0    0   0    0    0   0    0    3  4  0 94\n   0  584  89   6   24   89   6   25    0   0    0  332  19    0    2  1  0 97\n   0  296   0   0    0    0   0    0    0   0    0    0   0    0    2  2  0 97\n   1 1290  43   5   24   43   5   22    0   0    0  297  20    1    3  3  0 94\n\nRunning netstat -i 15 gives the following:\n    input   aggr26    output       input  (Total)    output\npackets errs  packets errs  colls  packets errs  packets errs  colls\n1500233798 0     1489316495 0     0      3608008314 0     3586173708 0     0\n10646   0     10234   0     0      26206   0     25382   0     0\n11227   0     10670   0     0      28562   0     27448   0     0\n10353   0     9998    0     0      29117   0     28418   0     0\n11443   0     12003   0     0      30385   0     31494   0     0\n\nWhat am I missing?",
        "top_answer": "With some further investigation, it appears that the performance problem is mostly due to a high number of network calls between two systems (Oracle SSXA and UCM).  The calls are quick but plenty and serialized, hence the low CPU usage (mostly waiting for I/O), the high load average (many calls waiting to be processed) and especially the long response times (by accumulation of small response times).\nThanks for your insight on this problem!",
        "url": "https://serverfault.com/questions/365061/high-load-average-low-cpu-usage-why"
    },
    {
        "title": "What is .crt and .key files and how to generate them?",
        "question": "I've the following configuration:\nSSLEngine on\nSSLCertificateFile /etc/httpd/conf/login.domain.com.crt\nSSLCertificateKeyFile /etc/httpd/conf/login.domain.com.key\nSSLCipherSuite ALL:-ADH:+HIGH:+MEDIUM:-LOW:-SSLv2:-EXP\n\nbut I don't know how to generate .crt and .key files.",
        "top_answer": "crt and key files represent both parts of a certificate, key being the private key to the certificate and crt being the signed certificate.\nIt's only one of the ways to generate certs, another way would be having both inside a pem file or another in a p12 container.\nYou have several ways to generate those files, if you want to self-sign the certificate you can just issue this commands\nopenssl genrsa 2048 > host.key\nchmod 400 host.key\nopenssl req -new -x509 -nodes -sha256 -days 365 -key host.key -out host.cert\n\nNote that with self-signed certificates your browser will warn you that the certificate is not \"trusted\" because it hasn't been signed by a certification authority that is in the trust list of your browser.\nFrom there onwards you can either generate your own chain of trust by making your CA or buy a certificate from a company like Verisign or Thawte.",
        "url": "https://serverfault.com/questions/224122/what-is-crt-and-key-files-and-how-to-generate-them"
    },
    {
        "title": "list all packages from a repository in ubuntu / debian",
        "question": "is there a command to see what packages are available from a certain ppa repository?",
        "top_answer": "Simple:\n grep -h -P -o \"^Package: \\K.*\" /var/lib/apt/lists/ppa.launchpad.net_*_Packages | sort -u\n\nOr more flexible:\ngrep-dctrl -sPackage . /var/lib/apt/lists/ppa.launchpad.net_*_Packages \n\nFor fancier querying, use apt-cache policy and aptitude as described here:\naptitude search '~O LP-PPA-gstreamer-developers'\n",
        "url": "https://serverfault.com/questions/252333/list-all-packages-from-a-repository-in-ubuntu-debian"
    },
    {
        "title": "Can Mac OS X be run inside Docker? [closed]",
        "question": "Can Mac OS X be run inside Docker? If so, any suggestion as to how?\nAnd would it be running headless, or there would be a possibility to connect to the GUI remotely?",
        "top_answer": "Docker provides methods for managing OS-level containers and is built on top of Linux's native features for OS-level containerization.  All containers running on a system share the same kernel; Mac OS X does not use the Linux kernel, but rather a mach kernel, so it cannot be run inside a Docker container at this time.\nYou can run Docker on your Mac using a virtual machine, but containers running on that instance would need to run Linux.\nNow that Docker uses libcontainer rather than LXC as its basis, it is possible that porting of libcontainer in the future could one day allow for running Windows and Mac OS Docker containers on those systems respectively, but it would depend on appropriate OS features being available to allow for containerization.",
        "url": "https://serverfault.com/questions/607443/can-mac-os-x-be-run-inside-docker"
    },
    {
        "title": "SSH keypair generation: RSA or DSA?",
        "question": "SSH supports two signature algorithms for key pairs: RSA and DSA. Which is preferred, if any?  For RSA, what is the minimum acceptable key length?",
        "top_answer": "RSA is generally preferred (now that the patent issue is over with) because it can go up to 4096 bits, where DSA has to be exactly 1024 bits (in the opinion of ssh-keygen).  2048 bits is ssh-keygen's default length for RSA keys, and I don't see any particular reason to use shorter ones.  (The minimum possible is 768 bits; whether that's \"acceptable\" is situational, I suppose.)",
        "url": "https://serverfault.com/questions/40071/ssh-keypair-generation-rsa-or-dsa"
    },
    {
        "title": "Supervisor not loading new configuration files",
        "question": "I have a problem deploying Django app using Gunicorn and Supervisor. While I can make Gunicorn serving my app (by setting proper PYTHONPATH and running apropriate command, the one from supervisord config) I can't make supervisor to run it. It just won't see my app. I don't know how to make sure if the config file is ok. \nHere's what supervisorctl says:\n# supervisorctl start myapp_live\nmyapp_live: ERROR (no such process)\n\nI'm running it on Ubuntu 10.04 with following config:\nFile /home/myapp/live/deploy/supervisord_live.ini:\n[program:myapp_live]\ncommand=/usr/local/bin/gunicorn_django --log-file /home/myapp/logs/gunicorn_live.log --log-level info --workers 2 -t 120 -b 127.0.0.1:10000 -p deploy/gunicorn_live.pid webapp/settings_live.py\ndirectory=/home/myapp/live\nenvironment=PYTHONPATH='/home/myapp/live/eco/lib'\nuser=myapp\nautostart=true\nautorestart=true\n\nIn /etc/supervisor/supervisord.conf, at the end of the file, there is:\n[include]\nfiles = /etc/supervisor/conf.d/*.conf\n\nand here's a symlink to my config file:\n# ls -la /etc/supervisor/conf.d\nlrwxrwxrwx 1 root root   48 Dec  4 18:02 myapp-live.conf -> /home/myapp/live/deploy/supervisord_live.ini\n\neverything looks fine for me but supervisorctl just keep saying myapp_live: ERROR (no such process). Any solution for this?",
        "top_answer": "I had the same issue, a \nsudo service supervisord reload\n\ndid the trick, though I don't know if that is the answer to your question.",
        "url": "https://serverfault.com/questions/211525/supervisor-not-loading-new-configuration-files"
    },
    {
        "title": "How can I edit the welcome message when ssh start?",
        "question": "I have a VPS for my website hosting. It is running a Ubuntu server. Every time I logged in my  server by ssh, it displays a lengthy welcome message in my terminal. \n\nLinux node61.buyvm.net 2.6.18-pony6-3 #1 SMP Tue Mar 13 07:31:44 PDT\n  2012 x86_64\nThe programs included with the Debian GNU/Linux system are free\n  software; the exact distribution terms for each program are described\n  in the individual files in /usr/share/doc/*/copyright.\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n  permitted by applicable law. Last login: Wed Jul 11 12:08:19 2012 from\n  113.72.193.52 Linux node61.buyvm.net 2.6.18-pony6-3 #1 SMP Tue Mar 13 07:31:44 PDT 2012 x86_64\nThe programs included with the Debian GNU/Linux system are free\n  software; the exact distribution terms for each program are described\n  in the individual files in /usr/share/doc/*/copyright.\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n  permitted by applicable law. entered into CT 17323\n  -bash-4.2#\n\nAfter doing some researches about this(yes i was just googling around), I realized that my server should have a .bashrc and .bash_profile (or .profile) controlling this. I use vim to open my .bashrc and .profile and I couldn't seem to find any line of codes that would display message in my terminal. Therefore I am wondering if there is like another file for this? \nI want to comment out those welcome message because my SFTP is not working with an error (Received message too long 761422195). I am pretty sure that this error is caused by my server's welcome message. ",
        "top_answer": "You need to edit two files:\n\n/etc/motd (Message of the Day)\n/etc/ssh/sshd_config: Change the setting PrintLastLog to \"no\", this will disable the \"Last login\" message.\n\nAnd then restart your sshd.",
        "url": "https://serverfault.com/questions/407033/how-can-i-edit-the-welcome-message-when-ssh-start"
    },
    {
        "title": "Is Ping a reliable way to check if a server is available?",
        "question": "In my application I am pinging a server and waiting for a response. I am using this to determine whether the server is available and responsive or not.\nIs this a reliable way of determining availability? I assume a firewall could be filtering icmp traffic... Are there any other drawbacks? Is there a more reliable method?",
        "top_answer": "The best way to tell if any given remote service is alive is to ask it to service a request in the way it's meant to - in fact it's the only way to truly know something is working correctly.\nAs an example I always get my load-balancers to get an actual 'head' response back from our web servers, you could do the same for a small select on a DB box if you wanted to, or whatever your actual server serves. As a tip you can create an 'online.txt' (or whatever name you want to give it) on your web servers, have your LBs try to get that file and if it fails then it removes the server from the VIP, this is a nice way of manually taking individual servers out of your VIPs simply by renaming a single file.\nPing only tests for the ability to respond to pings, so that's base OS, parts of the IP stack and the physical links - but that's all, everything else could be down and you'd not know.\nI know this is mentioned below, but it bears repeating again and again. \nICMP Echo Requests (aka \"Pings\") (aka ICMP Type 8) are built onto the IP stack spec, yes, but are not required to be either implemented or used. As a matter of fact, there are a large number of internet providers who refuse to forward those and silently drop those requests, as they are a form of network attack (called a pingflood).\nAs mentioned above, this is handled by the OS (specifically at the network stack level) and so it is up to the OS configuration to respond to those or not. If this is turned off (a security precaution?), you can't do anything about receiving ping replies from the other end. This is why it's not reliable.",
        "url": "https://serverfault.com/questions/312930/is-ping-a-reliable-way-to-check-if-a-server-is-available"
    },
    {
        "title": "Multiple commands in Docker CMD directive",
        "question": "Not understanding what is happening when I try to execute two commands at runtime via CMD directive in `Dockerfile. I assumed that this should work:\nCMD [\"/etc/init.d/nullmailer\", \"start\", \";\", \"/usr/sbin/php5-fpm\"]\n\nBut it's not working. Container has not started. So I had to do it like this:\nCMD [\"sh\", \"-c\", \"/etc/init.d/nullmailer start ; /usr/sbin/php5-fpm\"]\n\nI don't understand. Why is that? Why first line is not the right way? Can somebody explain me these \"CMD shell format vs JSON format, etc\" stuff. In simple words.\nJust to note - the same was with command: directive in docker-compose.yml, as expected.",
        "top_answer": "I believe the difference might be because the second command does shell processing while the first does not. Per the official documentation, there are the exec and shell forms. Your first command is an exec form. The exec form does not expand environment variables while the shell form does. It is possible that by using the exec form the command is failing due to its dependence on shell processing. You can check this by running docker logs CONTAINERID\nYour second command, the shell form, is equivalent to -\nCMD /etc/init.d/nullmailer start ; /usr/sbin/php5-fpm\nExcerpts from the documentation -\n\nNote: Unlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, CMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ \"sh\", \"-c\", \"echo\", \"$HOME\" ].\n",
        "url": "https://serverfault.com/questions/685697/multiple-commands-in-docker-cmd-directive"
    },
    {
        "title": "How to disable everything in crontab -l?",
        "question": "I just want to pause everything.  Don't execute anything listed on crontab -l.",
        "top_answer": "crontab -e then comment out each line you don't want to run with #.",
        "url": "https://serverfault.com/questions/94351/how-to-disable-everything-in-crontab-l"
    },
    {
        "title": "How to delete cached temporarily credentials for a network share on a Windows machine without rebooting or logging off [duplicate]",
        "question": "This is more of a curiosity then a real problem, I am just to lazy to reboot or log off my laptop.\nI have connected to a network share on a Windows server with domain credentials from a non-domain Windows 7 machine, I didn't mark the option to remember the password.\nThe share is let's say \\\\10.10.10.10\\folder.\nI have changed the password for that domain account in the meantime, and now when I try to access that share I get the following error:\n\nLogon failure: unknown user name or bad password\n\nI have tried the following on the client side:\n\ndeleting cached credentials in Credential Manager\nrunning net use delete\nrunning net session \\\\ip.of.the.server /delete gives me \"A session does not exist with that computer name.\"\nrunning net use \\\\10.10.10.10\\folder /u:DOMAIN\\USER password gives me \"The command completed successfully.\", but I still get the same unknown user name or bad password when trying to access the share from Windows Explorer\nmapping the share as a network drive from GUI, but then I get\n\n\nThe network folder specified is currently mapped using a different user name and password.\n  To connect using a different user name and password, first disconnect any existing mappings to this network share.\n\nrunning net use to see connections, I get that there are no connections in the list\nkilling explorer.exe and starting it again.\n\nI have tried the following on the server side:\n\ngoing to Computer Management > Shared folders > Sessions to kill the session with my username\nrebooting the server\n\nI have managed to access the share using the domain name instead of the IP address, but I am curios.\nDoes anybody know any way how to delete the cached credentials in this case? Where are credentials cached when you don't mark the remember password option when accessing the share, they are not shown in Credential Manager and there is no mapping shown when you run net use.",
        "top_answer": "NOT FOR WINDOWS 10 (I am answering for WINDOWS 7)\nTo delete all network authentication\nC:\\> net use * /d\n\nTo view the current network connection\nC:\\> net use \n\nIMPORTANT NOTE\nI tested this successfully in Windows 7 SP1 64 Bits:\nAfter run the command, you need to go to task manager, terminate the explorer.exe task, then reopen the application again.\nTo reopen the application, go to RUN, enter explorer.exe\nNow you are fully clear the connection information in the session.",
        "url": "https://serverfault.com/questions/451387/how-to-delete-cached-temporarily-credentials-for-a-network-share-on-a-windows-ma"
    },
    {
        "title": "How can I get diff to show only added and deleted lines? If diff can't do it, what tool can?",
        "question": "How can I get diff to show only added and deleted lines? If diff can't do it, what tool can?",
        "top_answer": "Try comm\nAnother way to look at it:\n\nShow lines that only exist in file a: (i.e. what was deleted from a)\n  comm -23 a b\n\n\nShow lines that only exist in file b: (i.e. what was added to b)\n  comm -13 a b\n\n\nShow lines that only exist in one file or the other: (but not both)\n  comm -3 a b | sed 's/^\\t//'\n\n\n\n(Warning: If file a has lines that start with TAB, it (the first TAB) will be removed from the output.)\nSorted files only\nNOTE: Both files need to be sorted for comm to work properly.  If they aren't already sorted, you should sort them:\nsort <a >a.sorted\nsort <b >b.sorted\ncomm -12 a.sorted b.sorted\n\nIf the files are extremely long, this may be quite a burden as it requires an extra copy and therefore twice as much disk space.\nOr if you use a modern shell:\ncomm -12 <(sort a) <(sort b)\n",
        "url": "https://serverfault.com/questions/68684/how-can-i-get-diff-to-show-only-added-and-deleted-lines-if-diff-cant-do-it-wh"
    },
    {
        "title": "Best way to gracefully restart CentOS?",
        "question": "I always used the command:\nshutdown -r now\n\nHowever, sometimes that causes MySQL issues.\nWhat's the most graceful way to restart CentOS?\nI've seen:\nreboot\n\nand\nhalt\n\nHow can I gently reboot the machine?",
        "top_answer": "Systems using systemd (CentOS >=7) will have the reboot, shutdown and halt commands symlinked to systemctl to handle the reboot. The systemctl program will detect the use of the symlink and run the systemctl command with the correstponing arguments. For the difference between the commands see the manpage for systemctl (man systemctl) for it is quite nicely documented.\nFor CentOS 6, there is no better way to restart your server by using anything else than any those commands stated in the original question:\n\nshutdown is the most common way to stop your system. Adding the argument -r and a specific time (or 'now') will reboot your system instead of halting it after the shutdown sequence.\nreboot is a wrapper round shutdown which does some harddisk maintenance (syncing and/or putting in standby mode and not really relevant).\nNew versions of reboot (>2.74) will initiate shutdown if not in runlevel 0 or 6.\nMost init scripts call halt to make a log in utmp.\n\nModern distributions will have all tasks covered regardless of the command you are using. Basically they all initiate the shutdown run-time of your SysV (CentOS <7) or systemd (CentOS >=7) scripts (I will call them init scripts for ease of reading).\nShutting down using init scripts step by step stop all your services registered under usually runlevel 'S'. Individual init scripts can have a timeout, like the MySQL init script in CentOS. When the stop argument is given and the daemon will not be shutdown in a fair amount of time, the script will stop and exit giving a failure. The shutdown process will continue as if nothing was wrong, only taking a bit longer and probably print a warning. At the end, when all init scripts are executed, the inevitable will happen: all processes still running will get a SIGTERM signal and, after a few seconds (2 or 5), a SIGKILL. This will clean up the rest before an ACPI call is done to really reboot or shutdown your system.\nOne exception is using the reboot command with the -f option, this will skip executing init scripts and will reboot the system directly.\nYou will be better off fixing the root-cause of your worries: MySQL not shutting down properly.\nOften this is due to the massive load of work that needs to be done before the daemon can be exited safely. I once had a MySQL instance with +300.000 tables that took over an hour to exit. Similar issues can be found with systems using huge buffers and sparse I/O availability.",
        "url": "https://serverfault.com/questions/620595/best-way-to-gracefully-restart-centos"
    },
    {
        "title": "I have a keypair. How do I determine the key length?",
        "question": "Using OpenSSL from the command line in Linux, is there some way to examine a key (either public or private) to determine the key size?",
        "top_answer": "openssl rsa -in private.key -text -noout\n\nThe top line of the output will display the key size.\nFor example:\nPrivate-Key: (2048 bit)\n\n\nTo view the key size from a certificate:\nopenssl x509 -in public.pem -text -noout | grep \"Public Key\"\nRSA Public Key: (2048 bit)\n",
        "url": "https://serverfault.com/questions/325467/i-have-a-keypair-how-do-i-determine-the-key-length"
    },
    {
        "title": "How to determine JAVA_HOME on Debian/Ubuntu?",
        "question": "On Ubuntu it is possible to have multiple JVMs at the same time. The default one is selected with update-alternatives. But this does not set the JAVA_HOME environment variable, due to a debian policy.\nI am writing a launcher script (bash), which starts a java application. This java application needs the JAVA_HOME environment variable. So how to get the path of the JVM which is currently selected by update-alternatives?",
        "top_answer": "For the JRE, something like this should do the trick:\nJAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\n",
        "url": "https://serverfault.com/questions/143786/how-to-determine-java-home-on-debian-ubuntu"
    },
    {
        "title": "How to get a .pem file from ssh key pair?",
        "question": "I created a key pair using ssh-keygen and get the two clasic id_rsa and id_rsa.pub.\nI imported the public key into my AWS EC2 account.\nNow I created a windows instance and to decrypt that instance password, AWS console is asking me for a .pem file. How I can get that .pem file from my two id_rsa and id_rsa.pub files?",
        "top_answer": "According to this, this command can be used:\nssh-keygen -f id_rsa -e -m pem\n\nThis will convert your public key to an OpenSSL compatible format.\nYour private key is already in PEM format and can be used as is (as Michael Hampton stated).\nDouble check if AWS isn't asking for a (X.509) certificate in PEM format, which would\nbe a different thing than your SSH keys.",
        "url": "https://serverfault.com/questions/706336/how-to-get-a-pem-file-from-ssh-key-pair"
    },
    {
        "title": "How to reload screenrc without restarting screen?",
        "question": "After modified screenrc, how to see the changes without restarting screen?",
        "top_answer": "You could try Ctrl-a : source ~/.screenrc.",
        "url": "https://serverfault.com/questions/194597/how-to-reload-screenrc-without-restarting-screen"
    },
    {
        "title": "How to make PuTTY settings persistent? [closed]",
        "question": "Some PuTTY settings are valid only for the current session, and when I start it again, they are at the default value again. How can I change the default values?",
        "top_answer": "Make your settings changes and then click on \"Default Settings\" under \"Load, save or delete a stored session\" (This is in the \"Session\" category) to select it.  Then click \"Save.\" ",
        "url": "https://serverfault.com/questions/12295/how-to-make-putty-settings-persistent"
    },
    {
        "title": "Why can't MX records point to an IP address?",
        "question": "I understand you should not point a MX record at an IP address directly, but should instead point it to an A record, which, in turns, points to the IP address of your mail server.\nBut, in principle, why is this required?",
        "top_answer": "The whole idea behind the MX record is to specify a host or hosts which can accept mail for a domain. As specified in RFC 1035, the MX record contains a domain name. It must therefore point to a host which itself can be resolved in the DNS. An IP address could not be used as it would be interpreted as an unqualified domain name, which cannot be resolved.\nThe reasons for this in the 1980s, when the specs were originally written, are almost the same as the reasons for it today: A host may be connected to multiple networks and use multiple protocols.\nBack in the 80s, it was not uncommon to have mail gateways which connected both to the (relatively new) Internet which used TCP/IP and to other legacy networks, which often used other protocols. Specifying MX in this way allowed for DNS records which could identify how to reach such a host on a network other than the Internet, such as Chaosnet. In practice, though, this almost never happened; virtually everyone re-engineered their networks to become part of the Internet instead.\nToday, the situation is that a host may be reached by multiple protocols (IPv4 and IPv6) and by multiple IP addresses in each protocol. A single MX record can't possibly list more than one address, so the only option is to point to a host, where all of that host's addresses can then be looked up. (As a performance optimization, the DNS server will send along the address records for the host in the response additional section if it has authoritative records for them, saving a round trip.)\nThere is also the situation that arises when your mail exchangers are provided by a third party (e.g. Google Apps or Office 365). You point your MX records to their hostnames, but it may occur that the service provider needs to change the mail servers' IP addresses. Since you have pointed to a host, the service provider can do this transparently and you don't have to make any changes to your records.",
        "url": "https://serverfault.com/questions/663112/why-cant-mx-records-point-to-an-ip-address"
    },
    {
        "title": "What limits the maximum number of connections on a Linux server?",
        "question": "What kernel parameter or other settings control the maximum number of TCP sockets that can be open on a Linux server? What are the tradeoffs of allowing more connections?\nI noticed while load testing an Apache server with ab that it's pretty easy to max out the open connections on the server. If you leave off ab's -k option, which allows connection reuse, and have it send more than about 10,000 requests then Apache serves the first 11,000 or so requests and then halts for 60 seconds. A look at netstat output shows 11,000 connections in the TIME_WAIT state. Apparently, this is normal. Connections are kept open a default of 60 seconds even after the client is done with them for TCP reliability reasons.\nIt seems like this would be an easy way to DoS a server and I'm wondering what the usual tunings and precautions for it are.\nHere's my test output:\n# ab -c 5 -n 50000 http://localhost/\nThis is ApacheBench, Version 2.0.40-dev <$Revision: 1.146 $> apache-2.0\nCopyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/\nCopyright 2006 The Apache Software Foundation, http://www.apache.org/\n\nBenchmarking localhost (be patient)\nCompleted 5000 requests\nCompleted 10000 requests\napr_poll: The timeout specified has expired (70007)\nTotal of 11655 requests completed\n\nHere's the netstat command I run during the test:\n # netstat --inet -p | grep \"localhost:www\" | sed -e 's/ \\+/ /g' | cut -d' ' -f 1-4,6-7 | sort | uniq -c \n  11651 tcp 0 0 localhost:www TIME_WAIT -\n      1 tcp 0 1 localhost:44423 SYN_SENT 7831/ab\n      1 tcp 0 1 localhost:44424 SYN_SENT 7831/ab\n      1 tcp 0 1 localhost:44425 SYN_SENT 7831/ab\n      1 tcp 0 1 localhost:44426 SYN_SENT 7831/ab\n      1 tcp 0 1 localhost:44428 SYN_SENT 7831/ab\n",
        "top_answer": "I finally found the setting that was really limiting the number of connections: net.ipv4.netfilter.ip_conntrack_max. This was set to 11,776 and whatever I set it to is the number of requests I can serve in my test before having to wait tcp_fin_timeout seconds for more connections to become available. The conntrack table is what the kernel uses to track the state of connections so once it's full, the kernel starts dropping packets and printing this in the log:\nJun  2 20:39:14 XXXX-XXX kernel: ip_conntrack: table full, dropping packet.\n\nThe next step was getting the kernel to recycle all those connections in the TIME_WAIT state rather than dropping packets. I could get that to happen either by turning on tcp_tw_recycle or increasing ip_conntrack_max to be larger than the number of local ports made available for connections by ip_local_port_range. I guess once the kernel is out of local ports it starts recycling connections. This uses more memory tracking connections but it seems like the better solution than turning on tcp_tw_recycle since the docs imply that that is dangerous.\nWith this configuration I can run ab all day and never run out of connections:\nnet.ipv4.netfilter.ip_conntrack_max = 32768\nnet.ipv4.tcp_tw_recycle = 0\nnet.ipv4.tcp_tw_reuse = 0\nnet.ipv4.tcp_orphan_retries = 1\nnet.ipv4.tcp_fin_timeout = 25\nnet.ipv4.tcp_max_orphans = 8192\nnet.ipv4.ip_local_port_range = 32768    61000\n\nThe tcp_max_orphans setting didn't have any effect on my tests and I don't know why. I would think it would close the connections in TIME_WAIT state once there were 8192 of them but it doesn't do that for me.",
        "url": "https://serverfault.com/questions/10852/what-limits-the-maximum-number-of-connections-on-a-linux-server"
    },
    {
        "title": "CentOS vs. Ubuntu [closed]",
        "question": "I had a web server that ran Ubuntu, but the hard drive failed recently and everything was erased. I decided to try CentOS on the machine instead of Ubuntu, since it's based on Red Hat. That association meant a lot to me because Red Hat is a commercial server product and is officially supported by my server's manufacturer. However, after a few days I'm starting to miss Ubuntu. I have trouble finding some of the packages I want in the CentOS repositories, and the third-party packages I've tried have been a hassle to deal with.\nMy question is, what are the advantages of using CentOS as a server over Ubuntu? CentOS is ostensibly designed for this purpose, but so far I would prefer to use a desktop edition of Ubuntu over CentOS. Are there any killer features of CentOS which make it a better server OS? Is there any reason I shouldn't switch back to Ubuntu Server or Xubuntu?",
        "top_answer": "There are no benefits that I can discern for using CentOS (or RHEL) over Ubuntu if you are equally familiar with using both OSes.\nWe use RHEL and CentOS heavily at work, and it's just painful -- we're building custom packages left and right because the OS doesn't come with them, and paid RedHat support is worse than useless, being chock full of \"pillars of intransigence\" who see it as their duty to make sure you never get to speak to anyone who can actually answer your question.  (I've heard that if you spend enough money with them their support improves markedly, so if you're a fortune 500 you'll probably have better luck than we do -- but then again, if you're fortune 500 you're probably chock full of useless oxygen thieves internally anyway, so it feels natural to deal with another bunch of them)\nThat much-vaunted \"hardware support\" pretty much always comes in the form of puke-worthy binary-only drivers and utilities that I'd prefer to avoid by almost any means necessary.  Just choosing hardware that has proper support to begin with is much less hassle than trying to deal with the crap utilities.\nThe long-term stability of the OS platform isn't a differentiating factor -- Ubuntu has LTS (long-term support) releases that are around for five years (and which are coming out more often than RHEL releases, so if you want the latest and greatest you're not waiting as long), so there's no benefit there either.\nProprietary software doesn't get much of a benefit, either -- installing Oracle on RedHat is just as much of a \"genitals in the shredder\" experience as installing it on Debian, and you won't get any useful help from Oracle either (proprietary software support is near-universally worthless in my long and painful experience).\nThe only benefit to running CentOS is if you are more comfortable working in that environment and have your processes and tools tuned that way.",
        "url": "https://serverfault.com/questions/53954/centos-vs-ubuntu"
    },
    {
        "title": "Adding a user to an additional group using ansible",
        "question": "How can I add a user to additional groups with Ansible? For example, I would like to add a user to the sudo group without replacing the user's existing set of groups.",
        "top_answer": "According to the User module you can use this:\n- name: Adding user {{ user }}  \n  user: name={{ user }}\n        group={{ user }}\n        shell=/bin/bash\n        password=${password}\n        groups=sudo\n        append=yes\n\nYou can just add the groups=groupname and append=yes to add them to an existing user when you're creating them",
        "url": "https://serverfault.com/questions/542910/adding-a-user-to-an-additional-group-using-ansible"
    },
    {
        "title": "Windows server last reboot time",
        "question": "How can I find a Windows server's last reboot time, apart from 'net statistics server/workstation'?",
        "top_answer": "Start \u2192 Run \u2192 cmd.exe:\nsysteminfo | find \"System Boot Time\"\n\nOr for older OS versions (see comment):\nsysteminfo | find \"System Up Time\"\n",
        "url": "https://serverfault.com/questions/159612/windows-server-last-reboot-time"
    },
    {
        "title": "scp without known_hosts check",
        "question": "Is there any chance to skip the known_hosts check without clearing known_hosts or disable it in ssh.conf? I neither have access to known_hosts nor ssh.conf yet.\nDon't find any suitable in man.",
        "top_answer": "scp is supposed to take the same command line options as ssh, try:\n-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\n\nMaybe add -q to disable the warnings as well.",
        "url": "https://serverfault.com/questions/330503/scp-without-known-hosts-check"
    },
    {
        "title": "Add comment to existing SSH public key",
        "question": "I have an existing SSH key (public and private), that was created with ssh-keygen. How can I add a comment to this existing key?",
        "top_answer": "Just add a space after the key and put in the comment, e.g.:\nssh-dss AAAAB3NzaC1kc3MAAACBAN+NX/rmUkRW7Xn7faglC/pxqbVIohbcVOt41VThMYORtMQr\nQSqMZugxew2s9iX4qRowHWLBRci6404nSydLiDe1q6/NmpK+oQ8zD1yXekl+fruBAYeno7f6dM7c\n2swwwXY6knp4umXkLItxIUki6SXM0WfabJ8BwuNDyA8IrbFAAAAFQCynEN3MYXbs4AA7E/1I03jb\nB1rewAAAIAztzZUygrUI8XX6eE4zEHdTbv89AHYsAsf7fSAWnPxWc63dV0P5lCPNk58nze6+N+MD\nX7ZQADT6710fvbOmEFLciTwBGHHLxIV+1iTApJSsQp9T+pdkbFzBZ+mqQamZpSN1hC8fXe/Uty0D\nSbhnQ1qanwrOdKP1JV7DUgzehSfAAAAIEAwAyNYxUsGil46gZQea6sfhUnrBwyM6JnEbA6ogfGdS\nT2TDn1U5rfTV9UuNHzfoZ4CplVHclXyUPPhbKqcedpuRPJhHN/lp5MH7Q2tI/UxHvmePNHrXKk86\nXYt7RzKHjWbHRxf84GIyTlKa8yfNfFlf9oNXdtBXcsJjHIvNsBk= ThisIsAComment\n\nThe man page for sshd has a section on the authorized_keys format, where it states that the comment extends to the end of the line.  While I haven't tried it, you should be able to put spaces into the comment.",
        "url": "https://serverfault.com/questions/442933/add-comment-to-existing-ssh-public-key"
    },
    {
        "title": "How to add a security group to a running EC2 Instance?",
        "question": "I have an Amazon EC2 instance running and I will like to add another security group to that instance and then remove the current security group from that instance.  Is this possible?",
        "top_answer": "Update 2015-02-27:\nThis is now possible, see the answer below.\nOld reply:\nAmazon's FAQ says it's not possible to define a security group anywhere but at launch time.",
        "url": "https://serverfault.com/questions/37088/how-to-add-a-security-group-to-a-running-ec2-instance"
    },
    {
        "title": "How to get the url of the current svn repo?",
        "question": "I have 2 svn checkouts that someone setup for me. Now I need to check these same files on another computer, but since I didn't check them out initially I don't know the urls to use when running the svn checkout command:\nsvn co WHAT_GOES_HERE?\n\nSince these 2 checkouts already exist on one of my servers, is there a way to get the url of the repo from which they were initially checked out from?",
        "top_answer": "You can get the URL of the directory you are in, as well as the Repository Root and other info by running the following command in any of the checked out directories:\nsvn info\n\nIf you want a command that returns only the URL of the repository, perhaps for use in a script, then you can pass the following parameter:\nsvn info --show-item repos-root-url \n\nIt is worth noting that --show-item is available in Subversion 1.9+. In older versions you can use the following snippet the achieve similar result:\nsvn info | grep 'Repository Root' | awk '{print $NF}'\n",
        "url": "https://serverfault.com/questions/310300/how-to-get-the-url-of-the-current-svn-repo"
    },
    {
        "title": "How does ServerName and ServerAlias work?",
        "question": "It's the following part of a virtual host config that I need further clarification on:\n<VirtualHost *:80>\n  # Admin email, Server Name (domain name), and any aliases\n  ServerAdmin [email\u00a0protected]\n  ServerName  141.29.495.999\n  ServerAlias example.com\n...\n\nThis is and example config, similar to what I currently have (I don't have a domain name at the moment).\n<VirtualHost *:80> - Allow the following settings for all HTTP requests made on port 80 to IPs that this server can be contacted on. For instance, if the server could be accessed on more than one IP, you could restrict this directive to just one instead of both.\nServerName - If the host part of the HTTP request matches this name, then allow the request. Normally this would be a domain name that maps to an IP, but in this case the HTTP request host must match this IP.\nServerAlias - Alternate names accepted by the server.\nThe confusing part for me is, in the above scenario, if I set ServerAlias mytestname.com and then made an HTTP request to mytestname.com, there would have to be a DNS record pointing to the server's IP for this to work? In which case, is ServerAlias just basically EXTRA ServerName entries?\nSay I had a DNS entry such that foobar.com = 141.29.495.999 but then I had ServerName = 141.29.495.999 and ServerAlias was empty, would that mean that although foobar.com gets resolved to the right IP, because there is no reference to accept foobar.com in ServerName or ServerAlias?\nOr something. Man I'm confused.",
        "top_answer": "Think of it like this:\nDNS is the phone directory/yellow pages. When someone wants to call your phone, they can look up your name and get your phone number and call that phone. DNS does the same but for computers - when someone wants to go to www.example.com they ask DNS for the IP address and then they can contact the computer that has that IP address. That is what resolve means. Resolving an IP address has nothing at all to do with Apache; it is strictly a DNS question.\nThe ServerName and ServerAlias is more like a company's internal phone list. Your webserver is the switchboard; it will accept all incoming connections to the server. Then the client/caller will tell them what name they're looking for, and it will look in the Apache configuration for how to handle that name.\nIf the name isn't listed as a ServerName/ServerAlias in the apache configuration, apache will always give them the first VirtualHost listed. Or, if there's no VirtualHost at all, it will give the same content no matter what hostname is given in the request.\nETA: So, step by step for a normal connection:\n\nYou type http://www.example.com into your browser.\nYour computer asks its DNS resolver which IP address it should use when it wants to talk to www.example.com.\nYour computer connects to that IP address, and says that it wants to talk to www.example.com (that's the Host:header in HTTP).\nThe webserver looks at its configuration to figure out what to do with a request for content from www.example.com. Any one of the following may happen:\n\n\nwww.example.com is listed as a ServerName or ServerAlias for a VirtualHost - if so, then it will use the configuration for that VirtualHost to deliver the content.\nThe server doesn't have any VirtualHosts at all - if so, then it will use the configuration in its httpd.conf to deliver the content.\nThe server has VirtualHosts but www.example.com isn't listed in any of them - if so, the first Virtualhost in the list will be used to deliver the content.\n\n",
        "url": "https://serverfault.com/questions/520195/how-does-servername-and-serveralias-work"
    },
    {
        "title": "Adding a directory to $PATH in CentOS?",
        "question": "We just got our new server(s) up and we're running CentOS on them all. After successfully installing Ruby Enterprise Edition, I would now like to add the REE /bin (located at /usr/lib/ruby-enterprise/bin) directory to make it the default Ruby interpreter on the server.\nI have tried the following, which only adds it to the current shell session:\nexport PATH=/usr/lib/ruby-enterprise/bin:$PATH\n\nWhat would be the correct approach to permanently adding this directory to $PATH for all users? I'm currently logged in as root.",
        "top_answer": "It's not a good idea to edit /etc/profile for things like this, because you'll lose all your changes whenever CentOS publishes an update for this file. This is exactly what /etc/profile.d is for:\necho 'pathmunge /usr/lib/ruby-enterprise/bin' > /etc/profile.d/ree.sh\nchmod +x /etc/profile.d/ree.sh\n\nLog back in and enjoy your (safely) updated $PATH:\necho $PATH\n/usr/lib/ruby-enterprise/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\n\nwhich ruby\n/usr/lib/ruby-enterprise/bin/ruby\n\nInstead of logging back in, you could reload the profile:\n. /etc/profile\n\nThis will update the $PATH variable.",
        "url": "https://serverfault.com/questions/102932/adding-a-directory-to-path-in-centos"
    },
    {
        "title": "How can I connect to a Windows server using a Command Line Interface? (CLI)",
        "question": "Especially with the option to install Server Core in Server 2008 and above, connecting to Windows servers over a CLI is increasingly useful ability, if not one that's very widespread amongst Windows administrators.\nPractically every Windows GUI management tool has an option to connect to a remote computer, but there is no such option present in the built-in Windows CLI (cmd.exe), which gives the initial impression that this might not be possible.\nIs it possible to remotely management or administer a Windows Server using a CLI?  And if so, what options are there to achieve this?",
        "top_answer": "There are several fairly easy options available for remotely managing a remote Windows Server using a command line, including a few native options.\nNative Options:\n\nWinRS/WinRM\n\nWindows Remote Shell/Management tool is the easiest way to remotely manage a remote Windows server in a command line utility, and as with most Windows command line utilities, ss64 has a good page on its options and syntax.\nAlthough not explicitly stated in the Microsoft documentation, this can be used to launch a remote instance of cmd.exe, which creates an interactive command line on the remote system, rather than as command line option to execute a single command on a remote server.\n\n\nAs with: winrs -r:myserver.mydomain.tld cmd\n\nThis is also the natively-supported option that will probably be most familiar to administrators of other systems (*nix, BSD, etc.) that are primarily CLI-based.\n\nPowerShell\n\nHopefully PowerShell needs no introduction, and can be used to manage remote computers from a CLI using WMI (Windows Management Instrumentation).\nPowerShell remoting allows the execution of Powershell scripts and commands on remote computers.\nThere are a number of good resources on using WMI + PowerShell for remote management, such as The Scripting Guy's blog, the MSDN WMI Reference and ss64.com, which has an index of PowerShell 2.0 commands.\n\nRemote Desktop\n\nProbably not exactly the first thing to come to mind as a Window CLI option, but of course, using mstsc.exe to connect to a server over Remote Desktop Protocl (RDP) does enable the use of a command line on the remote server.\nConnecting to a Server Core installation over RDP, is actually possible and will give the same interface as connecting to the console - an instance of cmd.exe.\n\n\nThis may be somewhat counter-intuitive, as Server Core lacks a desktop, or the other normal Windows shell options, but there's a quick article over at petri.co.il about how to manage Server Core over RDP, should one be so inclined.\n\n\n\nPopular, Non-Native Options:\nEven though Windows now provides a few native options for accessing a remote sever over aCLI, this was not always the case, and as a result, a number of fairly popular 3rd party solutions were created.  The three most notable are below.\n\nInstall SSH on your Windows Server\n\nIf you just must have SSH, that's an option too, and there's a guide on social.technet for how to install OpenSSH on Server 2008.\nProbably most useful for administrators of other systems (*nix, BSD, etc.) that make heavy use of SSH for this purpose, though there are advantages to even Windows-only administrators for having a single terminal emulator client (like PuTTY) store a number of target computers and customized (or standardized) settings for each.\n\nPSExec\n\nThe original option for executing remote commands on a Windows box through the Windows CLI, this is part of the excellent SysInternals suite.  One of the very few \"must have\" packages for Windows admins, the SysInternals tools were so widely respected and used that SyInternals was bought out by Microsoft, and the tools are now somewhat officially supported by Microsoft.\nJust as with WinRS/RM, PSExec can be used to issue single commands to a remote server, or to launch an interactive instance of cmd.exe on a remote computer.\n\n\nAs with: psexec \\\\myserver.mydomain.tld cmd\n\nAs with the other options, there are steps one must take first to ensure PSExec is actually able to connect to the target machine.\n\nAdd a utilities folder to the server and store its value in the %PATH% system variable\n\nAs has been noted in the comments there are many a good SysInternals program that can be executed on the command line and targeted at a remote system, and this is true of more than just SysInternals.\nBasically, package up a bundle of your favorite Windows utilities into a folder you push to all your servers and add that folder to the %PATH% environmental variable of your systems.  Both are easily done through GPO.\n\n\n(I include the SysInternals Suite, PuTTY, WinDirStat and a bunch of custom scripts I find myself reusing) into a folder that gets pushed to all my servers\n\nObviously, this is useful for more than just managing Windows systems via CLI, but I find it so useful I think it's worth including anyway.\n\n",
        "url": "https://serverfault.com/questions/429426/how-can-i-connect-to-a-windows-server-using-a-command-line-interface-cli"
    },
    {
        "title": "Difference between `curl -I` and `curl -X HEAD`",
        "question": "I was watching the funny server type from http://www.reddit.com with curl -I http://www.reddit.com when I guessed that curl -X HEAD http://www.reddit.com would do the same. But, in fact, it doesn't.\nI'm curious about why.\nThis is what I observe running the two commands:\n\ncurl -I: works as expected, outputs the header and exists.\ncurl -X HEAD: does not show anything and seems to wait for user input.\n\nBut, sniffing with tshark I see the second command actually sends the same HTML query and receives the correct answer, but it does not show it and it doesn't close the connection.\ncurl -I\n0.000000 333.33.33.33 -> 213.248.111.106 TCP 59675 > http [SYN] Seq=0 Win=5840 Len=0 MSS=1460 TSV=47267342 TSER=0 WS=6\n0.045392 213.248.111.106 -> 333.33.33.33 TCP http > 59675 [SYN, ACK] Seq=0 Ack=1 Win=5792 Len=0 MSS=1460 TSV=2552532839 TSER=47267342 WS=1\n0.045441 333.33.33.33 -> 213.248.111.106 TCP 59675 > http [ACK] Seq=1 Ack=1 Win=5888 Len=0 TSV=47267353 TSER=2552532839\n0.045623 333.33.33.33 -> 213.248.111.106 HTTP HEAD / HTTP/1.1\n0.091665 213.248.111.106 -> 333.33.33.33 TCP http > 59675 [ACK] Seq=1 Ack=155 Win=6432 Len=0 TSV=2552532886 TSER=47267353\n0.861782 213.248.111.106 -> 333.33.33.33 HTTP HTTP/1.1 200 OK\n0.861830 333.33.33.33 -> 213.248.111.106 TCP 59675 > http [ACK] Seq=155 Ack=321 Win=6912 Len=0 TSV=47267557 TSER=2552533656\n0.862127 333.33.33.33 -> 213.248.111.106 TCP 59675 > http [FIN, ACK] Seq=155 Ack=321 Win=6912 Len=0 TSV=47267557 TSER=2552533656\n0.910810 213.248.111.106 -> 333.33.33.33 TCP http > 59675 [FIN, ACK] Seq=321 Ack=156 Win=6432 Len=0 TSV=2552533705 TSER=47267557\n0.910880 333.33.33.33 -> 213.248.111.106 TCP 59675 > http [ACK] Seq=156 Ack=322 Win=6912 Len=0 TSV=47267570 TSER=2552533705\n\ncurl -X HEAD\n34.106389 333.33.33.33 -> 213.248.111.90 TCP 51690 > http [SYN] Seq=0 Win=5840 Len=0 MSS=1460 TSV=47275868 TSER=0 WS=6\n34.149507 213.248.111.90 -> 333.33.33.33 TCP http > 51690 [SYN, ACK] Seq=0 Ack=1 Win=5792 Len=0 MSS=1460 TSV=3920268348 TSER=47275868 WS=1\n34.149560 333.33.33.33 -> 213.248.111.90 TCP 51690 > http [ACK] Seq=1 Ack=1 Win=5888 Len=0 TSV=47275879 TSER=3920268348\n34.149646 333.33.33.33 -> 213.248.111.90 HTTP HEAD / HTTP/1.1\n34.191484 213.248.111.90 -> 333.33.33.33 TCP http > 51690 [ACK] Seq=1 Ack=155 Win=6432 Len=0 TSV=3920268390 TSER=47275879\n34.192657 213.248.111.90 -> 333.33.33.33 TCP [TCP Dup ACK 15#1] http > 51690 [ACK] Seq=1 Ack=155 Win=6432 Len=0 TSV=3920268390 TSER=47275879\n34.823399 213.248.111.90 -> 333.33.33.33 HTTP HTTP/1.1 200 OK\n34.823453 333.33.33.33 -> 213.248.111.90 TCP 51690 > http [ACK] Seq=155 Ack=321 Win=6912 Len=0 TSV=47276048 TSER=3920269022\n\nAny idea about why this difference in behaviour?",
        "top_answer": "It seems the difference has to do with the Content-Length header and how it is treated by both commands.\nBut before going into that, curl -X HEAD does not give any output because, by default, curl does not print headers if switch -i is not provided (not needed on -I though).\nIn any case, curl -I is the proper way to fetch the headers. It just ask for the header and close the connection.\nOn the other hand curl -X HEAD -i will wait for the transmission of the number of bytes stated by Content-Length. In the case no Content-Length is not specified, I guess it will wait for some data or for that particular header.\nSome examples that shows this behaviour:\n$ curl -X HEAD -i http://www.elpais.es\nHTTP/1.1 301 Moved Permanently\nServer: AkamaiGHost\nContent-Length: 0\nLocation: http://www.elpais.com/\nDate: Wed, 12 May 2010 06:35:57 GMT\nConnection: keep-alive\n\nBecause Content-Length is 0, in this case both commands behave the same. And the connection is closed afterwards.\n$ curl -X HEAD -i http://slashdot.org\nHTTP/1.1 200 OK\nServer: Apache/1.3.41 (Unix) mod_perl/1.31-rc4\nSLASH_LOG_DATA: shtml\nX-Powered-By: Slash 2.005001296\nX-Bender: Since I love you all so much, I'd like to give everyone hugs.\nX-XRDS-Location: http://slashdot.org/slashdot.xrds\nCache-Control: no-cache\nPragma: no-cache\nContent-Type: text/html; charset=iso-8859-1\nContent-Length: 115224\nDate: Wed, 12 May 2010 06:37:20 GMT\nX-Varnish: 1649060825 1649060810\nAge: 1\nConnection: keep-alive\n\ncurl: (18) transfer closed with 115224 bytes remaining to read\n\nIn this case, there seems to be a timeout (probably by Varnish), so curl protests that the connection was closed before having received the Content-Length number of bytes.\nBy the way, look at the funny X-Bender (shown in the example) and X-Fry (try it for yourself) headers :).",
        "url": "https://serverfault.com/questions/140149/difference-between-curl-i-and-curl-x-head"
    },
    {
        "title": "How to zip/unzip files in Powershell?",
        "question": "Is there a one-liner that will zip/unzip files (*.zip) in PowerShell?",
        "top_answer": "DotNetZip will allow you to do this from PowerShell. It is not a one-liner, but the library will allow you to write the PowerShell script you need.  \nYou can also use the COM interface, see Compress Files with Windows PowerShell then package a Windows Vista Sidebar Gadget. \nGoogling \"zip powershell\" or \"unzip powershell\" might also turn up useful results.",
        "url": "https://serverfault.com/questions/18872/how-to-zip-unzip-files-in-powershell"
    },
    {
        "title": "How can I fully log all bash scripts actions?",
        "question": "From my script output I want to capture ALL the logs data with error messages and redirect them all to log file. \nI have script like below:\n#!/bin/bash\n(\necho \" `date` : part 1 - start \"\nssh -f [email\u00a0protected] 'bash /www/htdocs/server.com/scripts/part1.sh logout exit'\necho \" `date` : sleep 120\"\nsleep 120\necho \" `date` : part 2 - start\"\nssh [email\u00a0protected] 'bash /www/htdocs/server.com/scripts/part2.sh logout exit'\necho \" `date` : part 3 - start\"\nssh [email\u00a0protected] 'bash /www/htdocs/server.com/scripts/part3.sh logout exit'\necho \" `date` : END\"\n) | tee -a /home/scripts/cron/logs\n\nI want to see all actions in file /home/scripts/cron/logs\nBut I only see this what I put after echo command.\nHow to check in logs that SSH command was successful?\nI need to gather all logs. I need this to monitor result of every command in my script, to better analyse what's going on while script fails. ",
        "top_answer": "I generally put something similar to the following at the beginning of every script (especially if it'll run as a daemon):\n#!/bin/bash\nexec 3>&1 4>&2\ntrap 'exec 2>&4 1>&3' 0 1 2 3\nexec 1>log.out 2>&1\n# Everything below will go to the file 'log.out':\n\nExplanation:\n\nexec 3>&1 4>&2\nSaves file descriptors so they can be restored to whatever they were before redirection or used themselves to output to whatever they were before the following redirect.\ntrap 'exec 2>&4 1>&3' 0 1 2 3\nRestore file descriptors for particular signals. Not generally necessary since they should be restored when the sub-shell exits.\nexec 1>log.out 2>&1\nRedirect stdout to file log.out then redirect stderr to stdout. Note that the order is important when you want them going to the same file. stdout must be redirected before stderr is redirected to stdout.\n\nFrom then on, to see output on the console (maybe), you can simply redirect to &3. For example,\necho \"$(date) : part 1 - start\" >&3\n\nwill go to wherever stdout was directed, presumably the console, prior to executing line 3 above.",
        "url": "https://serverfault.com/questions/103501/how-can-i-fully-log-all-bash-scripts-actions"
    },
    {
        "title": "Can you have more than one ~/.ssh/config file?",
        "question": "We have a bastion server that we use to connect to multiple hosts, and our .ssh/config has grown to over a thousand lines (we have hundreds of hosts that we connect to). This is beginning to get a little unwieldy and I'd like to know if there is a way to break the .ssh/config file up into multiple files. Ideally, we'd specify somewhere that other files would be treated as an .ssh/config file, possibly like:\n~/.ssh/config\n  ~/.ssh/config_1\n  ~/.ssh/config_2\n  ~/.ssh/config_3\n  ...\n\nI have read the documentation on ssh/config, and I don't see that this is possible. But maybe someone else has had a similar issue and has found a solution.",
        "top_answer": "The ~/.ssh/config file don't have a directive for including other files, possibly related to SSH's check for file permissions.\nSuggestions around this can include a script to cat several changes together either on the system or via checkin hooks on a repository. One might also look into tools such as Puppet or Augeas.\nHowever you approach it, though, you'll have to concatenate individual files to be a single file from outside of the file.\n$ cat ~/.ssh/config_* >> ~/.ssh/config\n\nnote: overwrite: > v.s. append: >>\nUpdate December 2017:\nFrom 7.3p1 and up, there is the Include option. Which allows you to include configuration files.\nInclude\n    Include the specified configuration file(s).  Mul\u2010\n    tiple pathnames may be specified and each pathname\n    may contain glob(3) wildcards and, for user config\u2010\n    urations, shell-like \u201c~\u201d references to user home\n    directories.  Files without absolute paths are\n    assumed to be in ~/.ssh if included in a user con\u2010\n    figuration file or /etc/ssh if included from the\n    system configuration file.  Include directive may\n    appear inside a Match or Host block to perform con\u2010\n    ditional inclusion.\n",
        "url": "https://serverfault.com/questions/375525/can-you-have-more-than-one-ssh-config-file"
    },
    {
        "title": "Caching/preloading files on Linux into RAM",
        "question": "I have a rather old server that has 4GB of RAM and it is pretty much serving the same files all day, but it is doing so from the hard drive while 3GBs of RAM are \"free\".\nAnyone who has ever tried running a ram-drive can witness that It's awesome in terms of speed. The memory usage of this system is usually never higher than 1GB/4GB so I want to know if there is a way to use that extra memory for something good.\n\nIs it possible to tell the filesystem to always serve certain files out of RAM?\nAre there any other methods I can use to improve file reading capabilities by use of RAM?\n\nMore specifically, I am not looking for a 'hack' here. I want file system calls to serve the files from RAM without needing to create a ram-drive and copy the files there manually. Or at least a script that does this for me.\nPossible applications here are:\n\nWeb servers with static files that get read alot\nApplication servers with large libraries\nDesktop computers with too much RAM\n\nAny ideas?\nEdit:\n\nFound this very informative: The Linux Page Cache and pdflush\nAs Zan pointed out, the memory isn't actually free. What I mean is that it's not being used by applications and I want to control what should be cached in memory.\n",
        "top_answer": "vmtouch seems like a good tool for the job.\nHighlights:\n\nquery how much of a directory is cached\nquery how much of a file is cached (also which pages, graphical representation)\nload file into cache\nremove file from cache\nlock files in cache\nrun as daemon\n\nvmtouch manual\nEDIT:\nUsage as asked in the question is listed in example 5 on vmtouch Hompage\nExample 5\n\nDaemonise and lock all files in a directory into physical memory:\n\nvmtouch -dl /var/www/htdocs/critical/\n\nEDIT2:\nAs noted in the comments, there is now a git repository available.",
        "url": "https://serverfault.com/questions/43383/caching-preloading-files-on-linux-into-ram"
    },
    {
        "title": "Configuring Docker to not use the 172.17.0.0 range",
        "question": "Due to problems with captive portals and the default Docker IP range I am trying to make Docker use the 198.18.0.0 range, instead of 172.17.0.0, which clashes with the captive portals used on the trains where I live.\nFollowing the docs, I created /etc/docker/daemon.json, and put the following in it:\n{\n    \"bip\":\"198.18.0.0/16\"\n}\n\nThis worked for docker0, but it seems to not have affected any of the other networks, and using docker compose the first network created is 172.17.0.0, which recreates the clash.\nWhat can I do to change the default subnet for all docker networks (preferably without having to state my custom IP range in every compose file)?",
        "top_answer": "It is possible to redefine default range.\n$ docker -v\nDocker version 18.06.0-ce, build 0ffa825\n\nEdit or create config file for docker daemon:\n# nano /etc/docker/daemon.json\n\nAdd lines:\n{\n  \"default-address-pools\":\n  [\n    {\"base\":\"10.10.0.0/16\",\"size\":24}\n  ]\n}\n\nRestart dockerd:\n# service docker restart\n\nCheck the result:\n$ docker network create foo\n$ docker network inspect foo | grep Subnet\n                    \"Subnet\": \"10.10.1.0/24\"\n\nIt works for docker-compose too. More info here https://github.com/moby/moby/pull/29376 (merged)",
        "url": "https://serverfault.com/questions/916941/configuring-docker-to-not-use-the-172-17-0-0-range"
    },
    {
        "title": "What range of MAC addresses can I safely use for my virtual machines?",
        "question": "I want to assign my virtual machines MAC addresses so that I can configure DHCP reservations for them so that they always get the same IP address regardless of which host hypervisor they are running on or operating system they are running.\nWhat I need to know is what range of MAC addresses can I use without fear that one day some device may be connected to our network with that MAC?\nI have read the Wikipedia article on MAC addresses and this section seems to indicate that if I create an address with the form 02-XX-XX-XX-XX-XX then it is considered a locally administered address. \nI would assume this means that no hardware manufacturer would ever use an address starting with 02 so I should be safe to use anything that starts with 02 for my virtual machines?\nThanks for the help.",
        "top_answer": "There are actually 4 sets of Locally Administered Address Ranges that can be used on your network without fear of conflict, assuming no one else has assigned these on your network:\nx2-xx-xx-xx-xx-xx\nx6-xx-xx-xx-xx-xx\nxA-xx-xx-xx-xx-xx\nxE-xx-xx-xx-xx-xx\n\nReplacing x with any hex value.",
        "url": "https://serverfault.com/questions/40712/what-range-of-mac-addresses-can-i-safely-use-for-my-virtual-machines"
    },
    {
        "title": "How to make bash scripts print out every command before it executes?",
        "question": "For example, I have a simple bash file \n#!/bin/bash\ncd ~/hello\nls\n\nHow can I make it display every command before executing it? Just the opposite effect of \"@echo off\" in windows batch scripting.",
        "top_answer": "bash -x script\n\nor\nset -x\n\nin the script.\nYou can unset the option again with set +x. If you just want to do it for a few commands you can use a subshell: `(set -x; command1; command; ...;)",
        "url": "https://serverfault.com/questions/16204/how-to-make-bash-scripts-print-out-every-command-before-it-executes"
    },
    {
        "title": "Ansible: Execute task only when a tag is specified",
        "question": "Ansible tags can be used to run only a subset of tasks/roles. This means that by default all tasks are executed and we can only prevent some tasks to execute.\nCan we limit a task to be exectued only when \"foo\" tag is specified? Can we use current tags in when section of a task?",
        "top_answer": "Ansible 2.5 comes with special tags never and always. Tag never can be used exactly for this purpose. E.g:\ntasks:\n  - debug: msg='{{ showmevar}}'\n    tags: [ 'never', 'debug' ]\n\nIn this example, the task will only run when the debug (or never) tag is explicitly requested. [Reference on ansible docs]",
        "url": "https://serverfault.com/questions/623634/ansible-execute-task-only-when-a-tag-is-specified"
    },
    {
        "title": "IPTABLES - Limit rate of a specific incoming IP",
        "question": "I do not wish to limit the rate of a specific service.  My goals is to limit rate based solely on the incoming IP address.  For example using a pseudo-rule:\njohn.domain.local (192.168.1.100) can only download from our httpd/ftp servers at \"10KB/s\" (instead of 1MB/s)\nHow could I rate limit using IPTables based on incoming IP addresses?",
        "top_answer": "IPTables isn't made for this kind of work, where lots and lots of packets need to be analyzed to make these decisions. IPTables is partly the answer though!\nThe real answer to this is the awesome and underused traffic control facilities in Linux. Note that mucking around with this without knowing what is going on may lead to you losing network connectivity to the machine! You have been warned!\nAssuming eth0 is the outgoing device you will need to create a class-based traffic control queue which will by default output most traffic through the 'fast' queue and put a specific list of people into the 'slow' queue.\nThe beauty of this is you can create a situation whereby you allow lots of outbound traffic for the slow user unless an overriding class wants the bandwidth, but this example does not do this (will always provide 10kbps to the slow users). The queuing system will look something like this:\n                         Inbound traffic\n                              +\n                              |\n                              |\n                              v\n                     +------------------+\n                     |   Class 1:1      |\n                     |------------------|\n                     |  Root (all flows)|\n                     |       100mbit    |\n                     +-----+-----+------+\n                           |     |\n                           |     |\n                           |     |\n                           |     |\n                           |     |\n          +----------+     |     |     +----------+\n          |    1:11  +-----+     +-----+    1:12  |\n          |----------|                 |----------|\n          | Default  |                 | Slow     |\n          |100mb-80kb|                 |   80kb   |\n          +----------+                 +----------+\n\nTo do this, first  you'll need to setup the queuing discipline in the kernel. The following will do this for you.. you must run this as one whole script\n#!/bin/bash\ntc qdisc add dev eth0 parent root handle 1: hfsc default 11\ntc class add dev eth0 parent 1: classid 1:1 hfsc sc rate 100mbit ul rate 100mbit\ntc class add dev eth0 parent 1:1 classid 1:11 hfsc sc rate 99920kbit ul rate 100000kbit\ntc class add dev eth0 parent 1:1 classid 1:12 hfsc sc rate 80kbit ul rate 80kbit\n\ntc qdisc add dev eth0 parent 1:11 handle 11:1 pfifo\ntc qdisc add dev eth0 parent 1:12 handle 12:1 pfifo\n\nThe \"default 11\" is important as it tells the kernel what to do with traffic not classified.\nOnce this is done, you can then setup an iptables rule to classify packets that match a certain criteria. If you plan on putting lots and lots of people into this slow rule an ipset rule is more appropriate (which should be available on rhel6 I believe).\nSo, create an ipset database to do the matching against...\nipset create slowips hash:ip,port\n\nThen create the iptables rule to do the match..\niptables -t mangle -I OUTPUT -m set --match-set slowips dst,src -j CLASSIFY --set-class 1:12\n\nThis instructs the kernel that if you match the destination IP with the source port from the set, classify it into the slow queue you setup with traffic control.\nNow, finally whenever you want to slow an IP down you can use the ipset command to add the ip to the set such as this:\nipset add slowips 192.168.1.1,80\nipset add slowips 192.168.1.1,21\n...\n\nYou can test it works using the command \"tc -s class show dev eth0\" and you will see stats in there indicating packets being redirected to the slow queue.\nNote the only real downside to this is making it survive reboots. I don't think there are any  init scripts available to create the ipsets from dumps on reboot (and they also must be created before iptables rules) and I'm certain there's no init scripts to resetup traffic control rules on reboot. If your not bothered, you can just recreate the whole thing from invoking a script in rc.local.",
        "url": "https://serverfault.com/questions/384132/iptables-limit-rate-of-a-specific-incoming-ip"
    },
    {
        "title": "How to prevent a user from login in, but allow \"su - user\" in Linux?",
        "question": "How do you allow a user to log in using \"su - user\" but prevent the user from login in using SSH?\nI tried to set the shell to /bin/false but the when I try to su it doesn't work.\nAre there several ways to only allow logins by su?\nIs SSH's AllowUser the way to go? (how would I do this if it's the way to go)",
        "top_answer": "You can use AllowUsers / AllowGroups if you have only a few users/groups that are allowed to login via ssh or DenyUsers / DenyGroups if you have only a few users/groups that are not allowed to login. Note that this only restricts login via ssh, other ways of login (console, ftp, ...) are still possible. You need to add these options to your /etc/ssh/sshd_config file for most ssh installations.\nIf you have set the login shell to /bin/false you can use su -s /bin/bash user (replace /bin/bash with the shell of your choice)",
        "url": "https://serverfault.com/questions/149673/how-to-prevent-a-user-from-login-in-but-allow-su-user-in-linux"
    },
    {
        "title": "Cooling Server Closet - No A/C Is Possible",
        "question": "We're moving into a new office in an old building in London (that's England :) and are walling off a 2m x 1.3m area where the router & telephone equipment currently terminates to use as a server closet. The closet will contain: \n\n2 24-port switches \n1 router \n1 VSDL modem \n1 Dell desktop \n1 4-bay NAS \n1 HP micro-server \n1 UPS \nMiscellaneous minor telephony boxes.\n\nThere is no central A/C in the office and there never will be. We can install ducting to the outside quite easily - it's only a couple of metres to the windows, which face a courtyard. \nMy question is whether installing an extractor fan with ducting to the window should be sufficient for cooling? Would an intake fan and intake duct (from the window, too) be required? We don't want to leave a gap in the closet door as that'll let noise out into the office. If we don't have to put a portable A/C unit into the closet, that'd be perfect. \nThe office has about 12 people; London is temperate, average maximum in August is 31 Celsius, 25 Celsius is more typical. The same equipment runs fine in our current office (same building as new office, also no A/C) but it isn't in an enclosed space.\nI can see us putting say one Dell 2950 tower server into the closet, but no more than that. So, sustained power consumption in the closet would currently be about 800w (I'm guessing); possibly in the future 2kw. \nThe closet will have a ceiling and no windows and be well-insulated. We don't care if the equipment runs hot, so long as it runs and we don't hear it. ",
        "top_answer": "Well, let's work this out;\n\n2 x 24 port switches (say Cisco 3750-E's) can output 344 BTU/hr each so that's 688 in total\n1 x router (say a Cisco 2921) can output 1260 BTU/hr\n1 x VDSL modem (say a Draytek Vigor 2750) can output 120 BTU/hr\n1 x Desktop (say a Dell Optiplex 790, with monitor switched off) can output 850 BTU/hr\n1 x 4-Bay NAS (say a Netgear ReadyNAS Ultra 4 with 4 x 2TB disks) can output ~600 BTU/hr\n1 x HP Microserver can output 511 BTU/hr\n1 x UPS (say an APC Smart-UPS 2200VA that can handle the ~1.2Kw you may be drawing) can output 275 BTU/hr\n\nThat's 4300 BTU/hr.\nYou've got 5.2 cubic metres of space (minus the items inside it), so not including natural heat loss you're going to have to install a minimum 29cm fan with a 900 cubic metre per hour rating with 29cm conduit all the way to the room if you don't want to hit 42 degrees C (the lowest recommended highest temp of the kit listed above) from a nominal of 20C in 17 minutes.\nBasically get an external A/C unit that can scrub 5k BTU/hr ok - a fan's going to literally and figuratively suck :)",
        "url": "https://serverfault.com/questions/351021/cooling-server-closet-no-a-c-is-possible"
    },
    {
        "title": "mysqldump to a tar.gz",
        "question": "Usually after dumping a MySQL database with mysqldump command I immediately tar/gzip the resultant file. I'm looking for a way to do this in one command:\nSo from this:\nmysqldump dbname -u root -p > dbname.sql\ntar czvf dbname.sql.tgz dbname.sql\nrm dbname.sql\n\nTo something like this:\nmysqldump dbname -u root -p > some wizardry > dbname.sql.tgz\n\nOr even better (since I'm usually scp'ing the dump file to another server):\nmysqldump dbname -u root -p > send dbname.sql.tgz to user@host\n\nI'm running bash on debian.",
        "top_answer": "mysqldump --opt <database> | gzip -c | ssh user@wherever 'cat > /tmp/yourfile.sql.gz'\n\nYou can't use tar in a pipe like this, and you don't need it anyway, as you're only outputting a single file. tar is only useful if you have multiple files.",
        "url": "https://serverfault.com/questions/106595/mysqldump-to-a-tar-gz"
    },
    {
        "title": "What is the maximum port number in Linux?",
        "question": "I'd like to set some Linux services to non-standard ports - what's the highest valid port number?",
        "top_answer": "(2^16)-1, or 0-65,535 (the -1 is because port 0 is reserved and unavailable).  (edited because o_O Tync reminded me that we can't use port 0, and Steve Folly reminded me that you asked for the highest port, not the number of ports)\nBut you're probably going about this the wrong way. There are people who argue for and against non-standard ports. I say they're irrelevant except to the most casual scanner, and the most casual scanner can be kept at bay by using up-to-date software and proper firewall techniques, along with strong passwords. In other words, security best practices. ",
        "url": "https://serverfault.com/questions/103626/what-is-the-maximum-port-number-in-linux"
    },
    {
        "title": "How to inspect remote SMTP server's TLS certificate?",
        "question": "We have an Exchange 2007 server running on Windows Server 2008.  Our client uses another vendor's mail server.  Their security policies require us to use enforced TLS.  This was working fine until recently.\nNow, when Exchange tries to deliver mail to the client's server, it logs the following:\n\nA secure connection to domain-secured domain 'ourclient.com' on connector 'Default external mail' could not be established because the validation of the Transport Layer Security (TLS) certificate for ourclient.com failed with status 'UntrustedRoot. Contact the administrator of ourclient.com to resolve the problem, or remove the domain from the domain-secured list.\n\nRemoving ourclient.com from the TLSSendDomainSecureList causes messages to be delivered successfully using opportunistic TLS, but this is a temporary workaround at best.\nThe client is an extremely large, security-sensitive international corporation.  Our IT contact there claims to be unaware of any changes to their TLS certificate.  I have asked him repeatedly to please identify the authority that generated the certificate so that I can troubleshoot the validation error, but so far he has been unable to provide an answer.  For all I know, our client could have replaced their valid TLS certificate with one from an in-house certificate authority.\nDoes anyone know a way to manually inspect a remote SMTP server's TLS certificate, as one can do for a remote HTTPS server's certificate in a web browser?  It could be very helpful to determine who issued the certificate and compare that information against the list of trusted root certificates on our Exchange server.",
        "top_answer": "You can use OpenSSL. If you have to check the certificate with STARTTLS, then just do\nopenssl s_client -connect mail.example.com:25 -starttls smtp\n\nor for a standard secure smtp port:\nopenssl s_client -connect mail.example.com:465\n",
        "url": "https://serverfault.com/questions/131627/how-to-inspect-remote-smtp-servers-tls-certificate"
    },
    {
        "title": "How do I do Multihop SCP transfers between machines?",
        "question": "I want to copy a file from my machine A to server C, but only have access to server C through server B.\nInstead of first transferring to server B, log in and then transfer to server C, Is is possible to transfer the file directly with SCP or similar programs? \n(Emacs tramp-mode has this feature for editing files remotely).",
        "top_answer": "You can add -o options to scp instead of .ssh/config.\nscp -o ProxyCommand=\"ssh $jump_host nc $host 22\" $local_path $host:$destination_path\n\n$jump_host is your \"server B\" in this case.",
        "url": "https://serverfault.com/questions/37629/how-do-i-do-multihop-scp-transfers-between-machines"
    },
    {
        "title": "\"413 Request Entity Too Large\" in Nginx with \"client_max_body_size\" set",
        "question": "I am uploading a 26Gb file, but I am getting:\n413 Request Entity Too Large\nI know, this is related to client_max_body_size, so I have this parameter set to 30000M.\n  location /supercap {\n    root  /media/ss/synology_office/server_Seq-Cap/;\n    index index.html;\n    proxy_pass  http://api/supercap;\n  }\n\n  location /supercap/pipe {\n    client_max_body_size 30000M;\n    client_body_buffer_size 200000k;\n    proxy_pass  http://api/supercap/pipe;\n    client_body_temp_path /media/ss/synology_office/server_Seq-Cap/tmp_nginx;\n  }\n\nBut I still get this error when the whole file has been uploaded.",
        "top_answer": "Modify NGINX Configuration File\nsudo nano /etc/nginx/nginx.conf\n\nSearch for this variable: client_max_body_size. If you find it, just increase its size to 100M, for example. If it doesn\u2019t exist, then you can add it inside and at the end of http\nclient_max_body_size 100M;\n\nTest your nginx config changes.\nsudo service nginx configtest\n\nRestart nginx to apply the changes.\nsudo service nginx restart\n\nModify PHP.ini File for Upload Limits\nIt\u2019s not needed on all configurations, but you may also have to modify the PHP upload settings as well to ensure that nothing is going out of limit by php configurations.\nIf you are using PHP5-FPM use following command,\nsudo nano /etc/php5/fpm/php.ini\n\nIf you are using PHP7.0-FPM use following command,\nsudo nano /etc/php/7.0/fpm/php.ini\n\nNow find following directives one by one\nupload_max_filesize\npost_max_size\n\nand increase its limit to 100M, by default they are 8M and 2M.\nupload_max_filesize = 100M\npost_max_size = 100M\n\nFinally save it and restart PHP.\nPHP5-FPM users use this,\nsudo service php5-fpm restart\n\nPHP7.0-FPM users use this,\nsudo service php7.0-fpm restart\n\nIt will work fine !!!",
        "url": "https://serverfault.com/questions/814767/413-request-entity-too-large-in-nginx-with-client-max-body-size-set"
    },
    {
        "title": "How do I validate an RSA SSH public key file (id_rsa.pub)?",
        "question": "Is there a command I can use to verify the public key (id_rsa.pub), just the format only.\nSometimes I have added a new linebreak, sometimes the file missed the ssh-rsa prefix, so is there a command to validate with?",
        "top_answer": "You can use ssh-keygen for this. Despite its name it can do more than just generate keys:\nssh-keygen -l -f .ssh/id_rsa.pub \n\n#=>\n\n2048 68:df:b2:22:d8:43:5d:36:75:c1:d8:59:c0:8c:22:e8  Dennis Kaarsemaker (RSA)\n\nssh-keygen -l -f foo.txt\n\n#=>\n\nfoo.txt is not a public key file.\n",
        "url": "https://serverfault.com/questions/453296/how-do-i-validate-an-rsa-ssh-public-key-file-id-rsa-pub"
    },
    {
        "title": "Tips for Securing a LAMP Server",
        "question": "\nThis is a Canonical Question about Securing a LAMP stack\n\nWhat are the absolute guidelines for securing a LAMP server?",
        "top_answer": "David's answer is a good baseline of the general principles of server hardening. As David indicated, this is a huge question. The specific techniques you take could depend highly on your environment and how your server will be used. Warning, this can take a lot of work in a test environment to build out and get done right. Followed by a lot of work to integrate into your production environment, and more importantly, business process.\nFirst, however, check to see if your organization has any hardening policies, as those might be the most directly relevant. If not, depending on your role, this might be a great time to build them out. I would also recommend tackling each component separately from the bottom up.\nThe L\nThere are lots of good guides available to help you out. This list may or may not help you depending on your distribution.\n\nCenter for Internet Security Benchmarks - Distribution specific for the major flavors\nCentOS Hardening HowTo - Follows closely to the CIS RHEL5 guide, but is a much easier read\nNIST SP800-123 - Guide to General Server Security\nNSA Hardening Factsheets - Not as recently updated as CIS, but still mostly applicable\nTiger - Live System Security Auditing Software\n\nThe A\nApache can be fun to secure.  I find it easier to harden the OS and maintain usability than either Apache or PHP.\n\nApache Server Hardening - This question on the IT Security sister site has lots of good information.\nCenter for Internet Security Benchmarks - Again, Apache benchmarks.\nApache Security Tips - Straight from the Apache project, it looks like it covers the basics\nDISA Hardening Checklist - Checklist from the DoD Information Assurance guys\n\nThe M\n\nCenter for Internet Security Benchmarks - Again, but for MySQL benchmarks\nOWASP MySQL Hardening\nGeneral Security Guidelines - Basic checklist from the project devs\n\nThe P\nThis runs headlong into the whole idea of Secure Programming Practices, which is an entire discipline of its own. SANS and OWASP have a ridiculous amount of information on the subject, so I won't try to replicate it here. I will focus on the runtime configuration and let your developers worry about the rest. Sometimes the 'P' in LAMP refers to Perl, but usually PHP. I am assuming the latter.\n\nHardening PHP - Some minor discussion, also on IT Security SE site.\nHardened PHP Project - Main project that produces Suhosin, an attempt to patch the PHP application to project against certain types of attacks.\nHardening PHP With Suhosin - A brief HowTo specifically for Suhosin\nHardening PHP from php.ini - Short, but not bad discussion on some of the security related runtime options\n",
        "url": "https://serverfault.com/questions/212269/tips-for-securing-a-lamp-server"
    },
    {
        "title": "How to filter http traffic in Wireshark?",
        "question": "I suspect my server has a huge load of http requests from its clients.\nI want to measure the volume of http traffic.\nHow can I do it with Wireshark?\nOr probably there is an alternative solution using another tool?\nThis is how a single http request/response traffic looks in Wireshark.\nThe ping is generated by WinAPI funciton ::InternetCheckConnection()\nalt text http://yowindow.com/shared/ping.png\nThanks!",
        "top_answer": "Ping packets should use an ICMP type of 8 (echo) or 0 (echo reply), so you could use a capture filter of:\nicmp\n\nand a display filter of:\nicmp.type == 8 || icmp.type == 0\n\nFor HTTP, you can use a capture filter of:\ntcp port 80\n\nor a display filter of:\ntcp.port == 80\n\nor:\nhttp\n\nNote that a filter of http is not equivalent to the other two, which will include handshake and termination packets. \nIf you want to measure the number of connections rather than the amount of data, you can limit the capture or display filters to one side of the communication. For example, to capture only packets sent to port 80, use:\ndst tcp port 80 \n\nCouple that with an http display filter, or use:\ntcp.dstport == 80 && http\n\nFor more on capture filters, read \"Filtering while capturing\" from the Wireshark user guide, the capture filters page on the Wireshark wiki, or pcap-filter (7) man page. For display filters, try the display filters page on the Wireshark wiki. The \"Filter Expression\" dialog box can help you build display filters.",
        "url": "https://serverfault.com/questions/96272/how-to-filter-http-traffic-in-wireshark"
    },
    {
        "title": "Testing UDP port connectivity",
        "question": "I am trying to test whether I can get to a particular port on a remote server (both of which I have access to) through UDP.\nBoth servers are internet facing.\nI am using netcat to have a certain port listening.\nI then use nmap to check for that port to see if it is open, but it doesn't appear to be.\nIptables is turned off.\nAny suggestions why this could be? I am eventually going to setup a VPN tunnel, but because I'm very new to tunnels, I want to make sure I have connectivity on port UDP 1194 before advancing.",
        "top_answer": "There is no such thing as an \"open\" UDP port, at least not in the sense most people are used to think (which is answering something like \"OK, I've accepted your connection\").  UDP is session-less, so \"a port\" (read: the UDP protocol in the operating system IP stack) will never respond \"success\" on its own.\nUDP ports only have two states: listening or not. That usually translates to \"having a socket open on it by a process\" or \"not having any socket open\". The latter case should be easy to detect since the system should respond with an ICMP Destination Unreachable packet with code=3 (Port unreachable). Unfortunately many firewalls could drop those packets so if you don't get anything back you don't know for sure if the port is in this state or not.\nAnd let's not forget that ICMP is session-less too and doesn't do retransmissions: the Port Unreachable packet could very well be lost somewhere on the net.\nA UDP port in the \"listening\" state may not respond at all (the process listening on it just receives the packet and doesn't transmit anything) or it could send something back (if the process does act upon reception and if it acts by responding via UDP to the original sender IP:port). So again, you never know for sure what's the state if you don't get anything back.\nYou say you can have control of the receiving host: that makes you able to construct your own protocol to check UDP port reachability: just put a process on the receiving host that'll listen on the given UDP port and respond back (or send you an email, or just freak out and unlink() everything on the host file system... anything that'll trigger your attention will do).",
        "url": "https://serverfault.com/questions/416205/testing-udp-port-connectivity"
    },
    {
        "title": "How to make a modification take affect without restarting nginx?",
        "question": "Apache has a graceful option which can scan for modification in http.conf without restarting Apache. What about nginx?",
        "top_answer": "nginx supports the following signals : \nTERM, INT - Quick shutdown\nQUIT - Graceful shutdown\nHUP - Configuration reload: Start the new worker processes with a new configuration, Gracefully shutdown the old worker processes\nUSR1 - Reopen the log files\nUSR2 - Upgrade Executable on the fly\nWINCH - Gracefully shutdown the worker processes\n\nHUP is what you are looking for, so sudo kill -HUP pid (nginx pid)\nsource : \nhttp://nginx.org/en/docs/control.html",
        "url": "https://serverfault.com/questions/108261/how-to-make-a-modification-take-affect-without-restarting-nginx"
    },
    {
        "title": "Should I install Linux applications in /var or /opt?",
        "question": "I run a lot of open source applications including java and tomcat. It seems like most instructions have my applications running from the /var directory. But every once in a while, I also see the /opt directory. While I'm at it, I also see /usr/local/ and even /etc as well.\nWhen should I install applications in one folder or the other? Are there pros and cons of each one? Does it have to do with the flavor history (Solaris vs Linux or Red Hat vs Ubuntu)?",
        "top_answer": "The standard for these issues is the Filesystem Hierarchy Standard. It's a rather big document. Basically (and very roughly), the standard paths on Linux are:\n\n/bin & /sbin are for vital programs for the OS, sbin being for administrators only ;\n/usr/bin & /usr/sbin are for not vital programs, sbin being for administrators only ;\n/var is for living data for programs. It can be cache data, spool data, temporary data (unless it's in /tmp, which is wiped at every reboot), etc. ;\n/usr/local is for locally installed programs. Typically, it hosts programs that follow the standards but were not packaged for the OS, but rather installed manually by the administrator (using for example ./configure && make && make install) as well as administrator scripts ;\n/opt is for programs that are not packaged and don't follow the standards. You'd just put all the libraries there together with the program. It's often a quick & dirty solution, but it can also be used for programs that are made by yourself and for which you wish to have a specific path. You can make your own path (e.g. /opt/yourcompany) within it, and in this case you are encouraged to register it as part of the standard paths ;\n/etc should not contain programs, but rather configurations.\n\nIf your programs are specific to the services provided by the service, /srv can also be a good location for them. For example, I prefer to use /srv/www for websites rather than /var/www to make sure the directory will only contain data I added myself, and nothing that comes from software packages.\nThere are some differences between distributions. For example, RedHat systems use libexec directories when Debian/Ubuntu systems don't.\nThe FHS is mostly used by Linux distributions (I actually don't know any other OS that really complies to it). Other Unix systems don't follow it. For example, BSD systems tend to use /usr/local for packaged programs, which is not the case for Linux. Solaris has very different standard paths.\nI strongly encourage you to read the FHS document I linked above if you wish to know more about this.",
        "url": "https://serverfault.com/questions/96416/should-i-install-linux-applications-in-var-or-opt"
    },
    {
        "title": "How to send emails and avoid them being classified as spam?",
        "question": "\nThis is a canonical question about how to handle email sent from your server being misclassified as spam.  For additional information you may find these similar questions helpful:\n\nBest Practices for preventing you from looking like a spammer \nFighting Spam - What can I do as an: Email Administrator, Domain Owner, or User?\n\n\nSometimes I want to send newsletters to my customers. The problem is, that some of the emails get caught as spam messages. Mostly by Outlook at the client (even in my own Outlook 2007).\nNow I want to know what should be done to create \"good\" emails. I know about reverse lookup etc., but (for example), what about a unsubscribe link with an unique ID?  Does that increase a spam rating?",
        "top_answer": "Be sure that your emails don\u2019t look like typical spam emails: don\u2019t insert only a large image; check that the character-set is set correctly; don\u2019t insert \u201cIP-address only\u201d links. Write your communication as you would write a normal email. Make it really easy to unsubscribe or opt-out. Otherwise, your users will unsubscribe by pressing the \u201cspam\u201d button, and that will affect your reputation.\nOn the technical side: if you can choose your SMTP server, be sure it is a \u201cclean\u201d SMTP server. IP addresses of spamming SMTP servers are often blacklisted by other providers. If you don\u2019t know your SMTP servers in advance, it\u2019s a good practice to provide configuration options in your application for controlling batch sizes and delay between batches. Some mail servers don\u2019t accept large sending batches or continuous activity.\nUse email authentication methods, such as SPF, and DKIM to prove that your emails and your domain name belong together. The nice side-effect is you help in preventing that your email domain is spoofed. Also check your reverse DNS to make sure the IP address of your mail server points to the domain name that you use for sending mail.\nMake sure that the reply-to address of your emails are a valid, existing addresses. Use the full, real name of the addressee in the To field, not just the email-address (e.g. \"John Doe\" <[email\u00a0protected]> ) and monitor your abuse accounts, such as [email\u00a0protected] and [email\u00a0protected].",
        "url": "https://serverfault.com/questions/48428/how-to-send-emails-and-avoid-them-being-classified-as-spam"
    },
    {
        "title": "Show all users and their groups/vice versa",
        "question": "I know I have existing groups and users but I'm not sure about their association. Is there an shell command I can use to list all users or all groups and a command to list all groups/users for a specified user/group?\nSo something like showusers would list all users, and showgroups -u thisuser would show all the groups that have thisuser in it.",
        "top_answer": "All users:\n$ getent passwd\n\nAll groups:\n$ getent group\n\nAll groups with a specific user:\n$ getent group | grep username\n",
        "url": "https://serverfault.com/questions/355292/show-all-users-and-their-groups-vice-versa"
    },
    {
        "title": "Is there a directory equivalent of /dev/null in Linux?",
        "question": "When configuring an application, you can often use /dev/null as config file if you want the application to read an empty file. But, if the application reads a list of files from a directory, you cannot use this trick. You would need to give it an empty directory to read.\nI was wondering: does Linux have a default empty directory that can be used for such purposes? I know OpenSSH used /var/empty for a while, and I can of course create an empty dir myself, but maybe the FHS has specified a standard directory for this?",
        "top_answer": "The FHS provides no \"standard\" empty directory.\nIt is common for Linux systems to provide a directory /var/empty, but this directory is not defined in FHS and may not actually be empty. Instead, certain daemons will create their own empty directories in here. For instance, openssh uses the empty directory /var/empty/sshd for privilege separation.\nIf your need for an empty directory is transient, you can create an empty directory yourself, as a subdirectory of /run or /tmp. If you're doing this outside the program, you can use mktemp -d for this, or use the mkdtemp(3) C function inside your program. Though if you always need the empty directory to be present, consider creating one under /var/empty as openssh does.\nFor this use case, creating a directory under /tmp is probably the best fit, though in practice it doesn't matter very much where you put it.",
        "url": "https://serverfault.com/questions/819226/is-there-a-directory-equivalent-of-dev-null-in-linux"
    },
    {
        "title": "How can one distinguish the host and the port in an IPv6 URL?",
        "question": "URLs always have this format:\n<protocol>://<host>[:<port>]/[<path>][#<hash>]\n\nThe problem is that IPv6 uses colons, just like the separator of port and host, e.g:\n2001:db8:1f70::999:de8:7648:6e8\n\nBut what if this is the host, and I want to connect to it with HTTP on port 100?\nhttp://2001:db8:1f70::999:de8:7648:6e8:100/\n\nThe problem is the last colon. Since zero's are omitted with double colons (between 1f70 and 999), it's unknown if ':100' belongs to the IP or the port number. How can we know this?",
        "top_answer": "The notation in that case is to encode the IPv6 IP number in square brackets:\nhttp://[2001:db8:1f70::999:de8:7648:6e8]:100/\n\nThat's RFC 3986, section 3.2.2: Host\n\nA host identified by an Internet Protocol literal address, version 6\n      [RFC3513] or later, is distinguished by enclosing the IP literal\n      within square brackets (\"[\" and \"]\").  This is the only place where\n      square bracket characters are allowed in the URI syntax.  In\n      anticipation of future, as-yet-undefined IP literal address formats,\n      an implementation may use an optional version flag to indicate such a\n      format explicitly rather than rely on heuristic determination.\n",
        "url": "https://serverfault.com/questions/205793/how-can-one-distinguish-the-host-and-the-port-in-an-ipv6-url"
    },
    {
        "title": "Windows Active Directory naming best practices?",
        "question": "\nThis is a Canonical Question about Active Directory domain naming.\n\nAfter experimenting with Windows domains and domain controllers in a virtual environment, I've realized that having an active directory domain named identically to a DNS domain is bad idea (Meaning that having example.com as an Active Directory name is no good when we have the example.com domain name registered for use as our website). \nThis related question seems to support that conclusion, but I'm still not sure about what other rules there are around naming Active Directory domains.\nAre there any best practices on what an Active Directory name should or shouldn't be?",
        "top_answer": "This has been a fun topic of discussion on Server Fault. There appear to be varying \"religious views\" on the topic.\nI agree with Microsoft's recommendation: Use a sub-domain of the company's already-registered Internet domain name.\nSo, if you own foo.com, use ad.foo.com or some such.\nThe most vile thing, as I see it, is using the registered Internet domain name, verbatim, for the Active Directory domain name. This causes you to be forced to manually copy records from the Internet DNS (like www) into the Active Directory DNS zone to allow \"external\" names to resolve. I've seen utterly silly things like IIS installed on every DC in an organization running a web site that does a redirect such that someone entering foo.com into their browser would be redirected to www.foo.com by these IIS installations. Utter silliness!\nUsing the Internet domain name gains you no advantages, but creates \"make work\" every time you change the IP addresses that external host names refer to. (Try using geographically load-balanced DNS for the external hosts and integrating that with such a \"split DNS\" situation, too! Gee-- that would be fun...)\nUsing such a subdomain has no effect on things like Exchange email delivery or User Principal Name (UPN) suffixes, BTW. (I often see those both cited as excuses for using the Internet domain name as the AD domain name.)\nI also see the excuse \"lots of big companies do it\". Large companies can make boneheaded decisions as easily (if not moreso) than small companies. I don't buy that just because a large company makes a bad decision that somehow causes it to be a good decision.",
        "url": "https://serverfault.com/questions/76715/windows-active-directory-naming-best-practices"
    },
    {
        "title": "SSHFS mount that survives disconnect",
        "question": "I'm using SSHFS mounts from my laptop to a central server.\nObviously, the SSHFS mount is broken after a longer disconnect (eg. during suspend), cause the underlying SSH connection timed out.\nIs there a way to get SSHFS mounts surviving long lasting disconnections (> 5 min) or even a re-dialin with a different IP?",
        "top_answer": "Use  -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3\nThe combination ServerAliveInterval=15,ServerAliveCountMax=3 causes the I/O errors to pop out after one minute of network outage. This is important but largely undocumented. If ServerAliveInterval option is left at default (so without the alive check), processes which experience I/O hang seem to sleep indefinitely, even after the sshfs gets reconnect'ed. I regard this a useless behavior.\nIn other words what happens on -o reconnect without assigning ServerAliveInterval is that any I/O will either succeed, or hang the application indefinitely if the ssh reconnects underneath. A typical application becomes entirely hung as a result. If you'd wish to allow I/O to return an error and resume the application, you need ServerAliveInterval=1 or greater.\nThe ServerAliveCountMax=3 is the default anyway, but I like to specify it for readability.",
        "url": "https://serverfault.com/questions/6709/sshfs-mount-that-survives-disconnect"
    },
    {
        "title": "Does each subdomain need its own SSL certificate?",
        "question": "I am creating a websocket server which will live on ws.mysite.example. I want the web socket server to be SSL encrypted as well as domain.example to be SSL encrypted. Do I need to purchase a new certificate for each subdomain I create? Do I need a dedicated IP address for each subdomain I create? I will likely have more than one subdomain.\nI am using NGINX and Gunicorn running on Ubuntu.",
        "top_answer": "I'll answer this in two steps...\nDo You Need an SSL Cert for Each Subdomain ?\nYes and no, it depends.  Your standard SSL certificate will be for a single domain, say www.domain.example.  There are different types of certs you can get aside from the standard single domain cert: wildcard and multi domain certs.\n\nA wild card cert will be issued for something like *.domain.example and clients will treat this as valid for any domain that ends with domain.example, such as www.domain.example or ws.domain.example.\n\nA multi domain cert is valid for a predefined list of domain names.  It does this by using the Subject Alternative Name field of the cert.  For example, you could tell an CA that you want a multi domain cert for domain.example and ws.mysite.example.  This would allow it to be used for both domain names.\n\n\nIf neither of these options work for you, then you would need to have two different SSL certs.\nDo I Need a Dedicated IP for Each Subdomain ?\nAgain, this is a yes and no...it all depends on your web/application server.  I am a Windows guy, so I will answer with IIS examples.\n\nIf you are running IIS7 or older, then you are forced to bind SSL certs to an IP and you can not have multiple certs assigned to a single IP.  This causes you to need to have a different IP for each subdomain if you are using a dedicated SSL cert for each subdomain.  If you are using a multi domain cert or a wildcard cert, then you can get away with the single IP as you only have one SSL cert to begin with.\n\nIf you are running IIS8 or later, then the same applies.  However, IIS8+ includes support for something called Server Name Indication (SNI).  SNI allows you to bind an SSL cert to a hostname, not to an IP.  So the hostname (Server Name) that is used to make the request is used to indicate which SSL cert that IIS should use to for the request.\n\nIf you use a single IP, then you can configure websites to respond to requests for specific hostnames.\n\n\nI know that Apache and Tomcat also have support for SNI, but I am not familiar them enough to know what versions support it.\nBottom Line\nDepending on your application/web server and what type of SSL certs you are able to obtain will dictate your options.",
        "url": "https://serverfault.com/questions/566426/does-each-subdomain-need-its-own-ssl-certificate"
    },
    {
        "title": "How can I export the privileges from MySQL and then import to a new server?",
        "question": "I know how to export/import the databases using mysqldump & that's fine but how do I get the privileges into the new server. \nFor extra points, there are a couple of existing databases on the new one already, how do I import the old servers privileges without nuking the couple existing of ones.\nOld server: 5.0.67-community\nNew server: 5.0.51a-24+lenny1\nEDIT: I've got a dump of the db 'mysql' from the Old Server & now want to know the proper way to merge with the 'mysql' db on the New Server.\nI tried a straight 'Import' using phpMyAdmin and ended up with an error regarding a duplicate (one that I've already migrated manually).\nAnyone got an elegant way of merging the two 'mysql' databases?",
        "top_answer": "Do not mess with the mysql db. There is a lot more going on there than just the users table. Your best bet is the \"SHOW GRANTS FOR\" command. I have a lot of CLI maintenance aliases and functions in my .bashrc (actually my .bash_aliases that I source in my .bashrc). This function:\nmygrants()\n{\n  mysql -B -N $@ -e \"SELECT DISTINCT CONCAT(\n    'SHOW GRANTS FOR \\'', user, '\\'@\\'', host, '\\';'\n    ) AS query FROM mysql.user\" | \\\n  mysql $@ | \\\n  sed 's/\\(GRANT .*\\)/\\1;/;s/^\\(Grants for .*\\)/## \\1 ##/;/##/{x;p;x;}'\n}\n\nThe first mysql command uses SQL to generate valid SQL which is piped to the second mysql command. The output is then piped through sed to add pretty comments.\nThe $@ in the command will allow you to call it as:\nmygrants --host=prod-db1 --user=admin --password=secret\nYou can use your full unix tool kit on this like so:\nmygrants --host=prod-db1 --user=admin --password=secret | grep rails_admin | mysql --host=staging-db1 --user=admin --password=secret\n\nThat is THE right way to move users. Your MySQL ACL is modified with pure SQL.",
        "url": "https://serverfault.com/questions/8860/how-can-i-export-the-privileges-from-mysql-and-then-import-to-a-new-server"
    },
    {
        "title": "Route 53 doesn't allow adding DKIM keys because length is too long",
        "question": "Here is how I enter the value for DKIM key:\n\"v=DKIM1; k=rsa; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwztXzIUqic95qSESmnqX U5v4W4ENbciFWyBkymsmmSNOhLlEtzp/mnyhf50ApwCTGLK9U7goo/ijX/wr5roy XhReVrvcqtIo3+63a1Et58C1J2o4xCvp0K2/lM6hla4B9jSph7QzjYdtWlOJqLRs o0nzcut7DSq/xYcVqvrFDNbutCfG//0wcRVUtGEyLX/a/7mAAkW6H8UEYMPglQ9c eEDfTT6pzIlqaK9cHGOsSCg4r0N8YxnHFMRzKaZwmudaXTorSbCs7e681g125/vJ e82VV7DE0uvKW/jquZYtgMn7+0rm+2FDYcDx/7lzoByl91rx37MAJaUx/2JHi1EA nwIDAQAB\"\n\nThere are no new lines in this value (I specifically copy pasted and tested it in a text editor). But for some reason I keep getting TXT is too long error:\n\nTXTRDATATooLong encountered at \"v=DKIM1; k=rsa; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwztXzIUqic95qSESmnqX U5v4W4ENbciFWyBkymsmmSNOhLlEtzp/mnyhf50ApwCTGLK9U7goo/ijX/wr5roy XhReVrvcqtIo3+63a1Et58C1J2o4xCvp0K2/lM6hla4B9jSph7QzjYdtWlOJqLRs o0nzcut7DSq/xYcVqvrFDNbutCfG//0wcRVUtGEyLX/a/7mAAkW6H8UEYMPglQ9c eEDfTT6pzIlqaK9cHGOsSCg4r0N8YxnHFMRzKaZwmudaXTorSbCs7e681g125/vJ e82VV7DE0uvKW/jquZYtgMn7+0rm+2FDYcDx/7lzoByl91rx37MAJaUx/2JHi1EA nwIDAQAB\"\n\nI really don't know what I should do to fix this issue.",
        "top_answer": "See a similar issue in Route 53 forum: \n\nUnfortunately the 255 character limit per string on TXT records is not a Route53 limit but rather one imposed by the DNS protocol itself. However, each TXT record can have multiple strings, each 255 characters long. You will need to split your DKIM into multiple strings for your TXT record. You can do this via the console by entering each string encapsulated in quotes, one string per line.\n\nImportant note: Do not use \"one string per line\" as the instructions say -- separate strings with a single space, eg. \"foo\" \"bar\" not \"foo\"\\n\"bar\". Use DKIMValidator to validate the signature is being read correctly.",
        "url": "https://serverfault.com/questions/763815/route-53-doesnt-allow-adding-dkim-keys-because-length-is-too-long"
    },
    {
        "title": "Reread partition table without rebooting?",
        "question": "Sometimes, when resizing or otherwise mucking about with partitions on a disk, cfdisk will say:\n\nWrote partition table, but re-read table failed.  Reboot to update table.\n\n(This also happens with other partitioning tools, so I'm thinking this is a Linux issue rather than a cfdisk issue.) Why is this, and why does it only happens sometimes, and what can I do to avoid it?\nNote: Please assume that none of the partitions I am actually editing are opened, mounted or otherwise in use.\n\nUpdate:\ncfdisk uses ioctl(fd, BLKRRPART, NULL) to tell Linux to reread the partition table.  Two of the other tools recommended so far (hdparm -z DEVICE, sfdisk -R DEVICE) does exactly the same thing. The partprobe DEVICE command, on the other hand, seems to use a new ioctl called BLKPG, which might be better; I don't know.  (It also falls back on BLKRRPART if BLKPG fails.)\nBLKPG seems to be a \"this partition has changed; here is the new size\" operation, and it looked like partprobe called it individually on all the partitions on the device passed, so it should work if the individual partitions are unused.  However, I have not had the opportunity to try it.",
        "top_answer": "IMHO the most reliable/best answer is\npartprobe /dev/sdX\n",
        "url": "https://serverfault.com/questions/36038/reread-partition-table-without-rebooting"
    },
    {
        "title": "Should we host our own nameservers?",
        "question": "\nThis is a Canonical Question about whether to outsource DNS resolution for ones own domains\n\nI currently have my ISP providing DNS for my domain, but they impose limitations on adding records. Therefore, I am thinking about running my own DNS.\nDo you prefer to host your own DNS, or is it better to have your ISP do this?\nAre there alternatives which I can look into?",
        "top_answer": "I wouldn't run my own DNS server - in my case, the hosting company that hosts my website provides free DNS service. There are also alternatives, companies that do nothing but DNS hosting (DNS Made Easy comes to mind, but there are many others) which are the kind of thing you should probably look into.\nThe reason I wouldn't do it myself is that DNS is supposed to be fairly reliable, and unless you have a geographically distributed network of servers of your own, you'd be putting all your eggs in one basket, so to speak. Also, there are plenty of dedicated DNS servers out there, enough that you wouldn't need to start up a new one.",
        "url": "https://serverfault.com/questions/23744/should-we-host-our-own-nameservers"
    },
    {
        "title": "dpkg-reconfigure: unable to re-open stdin: No file or directory",
        "question": "I am building a provisioning script for a ubuntu vagrant vm, on a ubuntu host, both 12.10 64bit When installing the following packages:\nsudo apt-get -y install php5-xsl graphviz php-pear unison\n\nI get the warning:\ndpkg-reconfigure: unable to re-open stdin: No file or directory\n\nhave tried searching but results are throwing up every other error with apt-get possible, can't find out how to supress the warning above. The installs work, but the warning above is causing error lines in the vagrant up stdout.\nAnybody any idea what could be the cause or how to suppress the warning",
        "top_answer": "I got the error message to go away by putting the following in my provisioning script, prior to any apt-get calls:\nexport DEBIAN_FRONTEND=noninteractive\n\nThis makes debconf use a frontend that expects no interactive input at all, preventing it from even trying to access stdin.",
        "url": "https://serverfault.com/questions/500764/dpkg-reconfigure-unable-to-re-open-stdin-no-file-or-directory"
    },
    {
        "title": "Smell of rotten eggs in the server room",
        "question": "A week ago I got the following error on my APC Smart-UPS 1000 which I muted.\nWarning State:\nConnect battery\nLoad: 55%\nBatt: 100%\n\nToday, I could smell a sort of sulfur/sulphur/rotten egg smell when I came into the office and the UPS is alarming again.  There isn't a burning smell.\nI have vented the office & server room and shutdown the UPS.\nGot any other advice?\nUPDATE: This is what I found in the UPS.\n",
        "top_answer": "To answer the question: This is almost always a lead-acid battery failure causing the battery to vent hydrogen sulfide (H2S). The battery needs to be replaced as soon as possible.\nAs an additional note, H2S can be extremely dangerous at higher concentrations. If you experience eye irritation or difficulty breathing or your ability to smell the odor deteriorates noticeably, the concentration of the gas is dangerously high and you should see a doctor. At that point, you may need to hire a hazmat cleanup service to remove the battery and clean up the area.\nWikipedia says this on H2S toxicity:\n\n\n0.00047 ppm or 0.47 ppb is the odor threshold, the point at which 50% of a human panel can detect the presence of an odor without being able to identify it.\n10 ppm is the OSHA permissible exposure limit (PEL) (8 hour time-weighted average).\n10\u201320 ppm is the borderline concentration for eye irritation.\n20 ppm is the acceptable ceiling concentration established by OSHA.\n50 ppm is the acceptable maximum peak above the ceiling concentration for an 8-hour shift, with a maximum duration of 10 minutes.\n50\u2013100 ppm leads to eye damage.\nAt 100\u2013150 ppm the olfactory nerve is paralyzed after a few inhalations, and the sense of smell disappears, often together with awareness of danger.\n320\u2013530 ppm leads to pulmonary edema with the possibility of death.\n530\u20131000 ppm causes strong stimulation of the central nervous system and rapid breathing, leading to loss of breathing.\n800 ppm is the lethal concentration for 50% of humans for 5 minutes exposure (LC50).\nConcentrations over 1000 ppm cause immediate collapse with loss of breathing, even after inhalation of a single breath.\n\n",
        "url": "https://serverfault.com/questions/767572/smell-of-rotten-eggs-in-the-server-room"
    },
    {
        "title": "How can I send a message to the systemd journal from the command line?",
        "question": "In older Linux systems, the logger command can be used to send a log message to syslog.\nReading where does logger log its messages to in Arch Linux?, it seems that syslog messages and the logger command line app only talk to the systemd journal if a socket for message forwarding is set up.\nSo what's the modern equivalent of the logger command? How can I send a message directly to the systemd journal from the command line?",
        "top_answer": "systemd-cat is the equivalent to logger:\necho 'hello' | systemd-cat\n\nIn another terminal, running journalctl -f:\nFeb 07 13:38:33 localhost.localdomain cat[15162]: hello\n\nPriorities are specified just by part of the string:\necho 'hello' | systemd-cat -p info\necho 'hello' | systemd-cat -p warning\necho 'hello' | systemd-cat -p emerg\n\nWarnings are bold, emergencies are bold and red. Scary stuff.\nYou can also use an 'identifier' which is arbitrary, to specify the app name. These are like syslog's old facilities, but you're not stuck with ancient things like lpr uucp nntp or the ever-descriptive local0 through local7.\necho 'hello' | systemd-cat -t someapp -p emerg\n\nIs logged as:\nFeb 07 13:48:56 localhost.localdomain someapp[15278]: hello\n",
        "url": "https://serverfault.com/questions/573946/how-can-i-send-a-message-to-the-systemd-journal-from-the-command-line"
    },
    {
        "title": "Does drilling a hole into a hard drive suffice to make its data unrecoverable?",
        "question": "We have a lot of PCs in the company and nobody wants to wipe a multitude of hard drives. We also have many apprentice toolmakers who really want to destroy things. Thus, every couple of months, our apprentices receive two heavy baskets of hard drives to drill through.\nSome of my coworkers believe that this is absolutely overkill. I, however, believe that not wiping the drives before drilling through them might make some data recoverable.\nAccording to this question, wiping with DBAN will make data completely unrecoverable. \n\nDBAN is just fine. Here's the dirty little secret--any program that\n  overwrites every byte of the drive will have wiped everything\n  permanently. You don't need to do multiple passes with different write\n  patterns, etc.\n\nHow about drilling a hole?",
        "top_answer": "Drilling a hole in the drive enclosure which passes through all the platters will make it impossible to run the drive.  Most modern HDDs don't have air inside the enclosure, and you've let what was in there escape.  You've filled the cavity with tiny pieces of drill swarf, which will be on everything including the platters, and will crash the heads if someone tries to lower them onto the rotating platters.  You've also unbalanced the platters, though I don't have an estimate for whether this will be fatal.  The drill bit will likely pass through the controller board on the way, which though not fatal will certainly not help anyone trying to hook the drive up.\nYou have not prevented someone from putting the platter under a magnetic force microscope and reading most of the data off that way.  We can be fairly sure this is possible, because the SANS paper linked from the linked SF article demonstrates that you can't recover data from a platter with an MFM after a single overwriting pass, and such a test would be completely meaningless if you couldn't recover non-overwritten data using the same procedure.\nSo drilling through the platters will very likely prevent data from being read off the HDD by normal means.  It won't prevent much of the data being recoverable by a determined, well-funded opponent.\nAll security is meaningless without a threat model.  So decide what you're securing against.  If you're worried about someone hooking up your old company HDDs and reading them, after they found them on ebay / the local rubbish dump / the WEEE recycling bin, then drilling is good.  Against state-level actors, drilling is probably insufficient.  If it helps, I drill most of my old drives, too, because I am worried about casual data leakage, but I doubt the security services are interested in most of my data.  For the few drives I have which hold data that Simply Must Not Leak, I encrypt them using passphrases of known strength, and drill them at the end of their lives.",
        "url": "https://serverfault.com/questions/868863/does-drilling-a-hole-into-a-hard-drive-suffice-to-make-its-data-unrecoverable"
    },
    {
        "title": "How to fix PuTTY showing garbled characters? [closed]",
        "question": "Connecting from a Windows 7 PC via SSH to an Ubuntu server using PuTTY, I get some screen errors:\n\nI.e. it:\n\n\"Double-draws\" the selection inside Midnight Commander (MC).\nOther characters like line elements are drawn as the wrong characters (e.g. \"\u00e2\" instead of \"|\").\n\nI connected to the same Ubuntu server with a terminal and SHH from a Mac OS X and do not get these screen garbling (i.e. everything looks and works correctly). I've already tried to play with the font settings inside PuTTY, changing it from Courier New to Consolas but without luck.\nMy question therefore is:\nHow to configure PuTTY to correctly display special characters and not double-draw/overwrite screen lines?",
        "top_answer": "You almost certainly have set the wrong character set in your PuTTY settings.\nVerify the character set on the remote system by running the command:\nlocale\n\nThis should return something like:\nLANG=de_DE.UTF-8\nLC_CTYPE=\"de_DE.UTF-8\"\nLC_NUMERIC=\"de_DE.UTF-8\"\nLC_TIME=\"de_DE.UTF-8\"\nLC_COLLATE=\"de_DE.UTF-8\"\nLC_MONETARY=\"de_DE.UTF-8\"\nLC_MESSAGES=\"de_DE.UTF-8\"\nLC_PAPER=\"de_DE.UTF-8\"\nLC_NAME=\"de_DE.UTF-8\"\nLC_ADDRESS=\"de_DE.UTF-8\"\nLC_TELEPHONE=\"de_DE.UTF-8\"\nLC_MEASUREMENT=\"de_DE.UTF-8\"\nLC_IDENTIFICATION=\"de_DE.UTF-8\"\nLC_ALL=\n\nSo check your PuTTY settings under Translation and ensure that you have UTF-8 set as the character set.\n\nYou may need to tweak the line drawing setting as well, but it is probably not likely.",
        "url": "https://serverfault.com/questions/475925/how-to-fix-putty-showing-garbled-characters"
    },
    {
        "title": "How can I get processor/RAM/disk specs from the Linux command Line? [duplicate]",
        "question": "\nPossible Duplicate:\nHow to find out details about hardware on the Linux machine? \n\nHow can I get processor/RAM/disk specs from the Linux command Line?",
        "top_answer": "CPU \n\n$ cat /proc/cpuinfo\n\nMemory : \n\n$ free\n$ cat /proc/meminfo\n\nHDD:\n\n$ df -h\n$ sudo fdisk -l\n$ hdparm -i /dev/device (for example sda1, hda3...)\n",
        "url": "https://serverfault.com/questions/112542/how-can-i-get-processor-ram-disk-specs-from-the-linux-command-line"
    },
    {
        "title": "Standard place for user defined bash_completion.d scripts?",
        "question": "I have user access (no root) into a Linux (Suse) machine where I developed some bash scripts and the corresponding bash autocompletion rules.\nSince the scripts belong only to my user and therefore I need the complete rules only \"active\" for me (a part from the fact that I have no root write acces), placing my bash_completion script into /etc/bash_completion.d/ folder is not an option.\nAt the moment I named my file .bash_completion.myscript and source it directly from my .bashrc, but I just wonder if there is any other \"standard\" way of achieving these results, already considered in the bash implementation.\nFor example, creating a folder /home/myuser/.bash_completion.d/?",
        "top_answer": "Use a ~/.bash_completion file.\nFrom the Bash Completion FAQ:\n\nQ. How can I insert my own local completions without having to\n  reinsert them every time you issue a new release?\nA. Put them in ~/.bash_completion, which is parsed at the end of the\n  main completion script. See also the next question.\nQ. I author/maintain package X and would like to maintain my own\n  completion code for this package. Where should I put it to be sure\n  that interactive bash shells will find it and source it?\nA. Install it in one of the directories pointed to by\n  bash-completion's pkgconfig file variables.  There are two\n  alternatives: the recommended one is 'completionsdir' (get it with\n  \"pkg-config --variable=completionsdir bash-completion\") from which\n  completions are loaded on demand based on invoked commands' names,\n  so be sure to name your completion file accordingly, and to include\n  for example symbolic links in case the file provides completions\n  for more than one command.  The other one which is present for\n  backwards compatibility reasons is 'compatdir' (get it with\n  \"pkg-config --variable=compatdir bash-completion\") from which files\n  are loaded when bash_completion is loaded.\n",
        "url": "https://serverfault.com/questions/506612/standard-place-for-user-defined-bash-completion-d-scripts"
    },
    {
        "title": "Best way to disable swap in Linux",
        "question": "I am running a custom compiled 3.18.9 kernel and I am wondering about the best way to disable swap on the system. I also use init if it makes a difference.\nIs it enough to comment or remove the swap line in /etc/fstab to prevent swap from working/mounting at boot or should I recompile the kernel without Support for paging of anonymous memory (swap) to be 100% sure it does not get enabled?\nI run encrypted partitions and want to prevent accidental leakage to the hard disk. My system specifications are also great enough that I can survive in a swap-less environment.",
        "top_answer": "\nIdentify configured swap devices and files with cat /proc/swaps.\nTurn off all swap devices and files with swapoff -a.\nRemove any matching reference found in /etc/fstab.\nOptional: Destroy any swap devices or files found in step 1 to prevent their reuse. Due to your concerns about leaking sensitive information, you may wish to consider performing some sort of secure wipe.\n\nman swapoff",
        "url": "https://serverfault.com/questions/684771/best-way-to-disable-swap-in-linux"
    },
    {
        "title": "My /var/log/btmp file is huge! What should I do?",
        "question": "My /var/log/btmp file is 1.3 GB in size. I've read that the file is \"Used to store information about failed login\".\nWhat does this mean for my server? And can I delete this file?",
        "top_answer": "This means people are trying to brute-force your passwords (common on any public-facing server).\nIt shouldn't cause any harm to clear out this file.\nOne way to reduce this is to change the port for SSH from 22 to something arbitrary. For some additional security, DenyHosts can block login attempts after a certain number of failures. I'd highly recommend installing and configuring it.",
        "url": "https://serverfault.com/questions/119299/my-var-log-btmp-file-is-huge-what-should-i-do"
    },
    {
        "title": "How bad is it really to install Linux on one big partition?",
        "question": "We will be running CentOS 7 on our new server. We have 6 x 300GB drives in raid6 internal to the server. (Storage is largely external in the form of a 40TB raid box.) The internal volume comes to about 1.3TB if formatted as a single volume. Our sysadmin thinks it is a really bad idea to install the OS on one big 1.3TB partition.\nI am a biologist. We constantly install new software to run and test, most of which lands in /usr/local. However, because we have about 12 non-computer savvy biologists using the system, we also collect a lot cruft in /home as well. Our last server had a 200GB partition for /, and after 2.5 years it was 90% full. I don't want that to happen again, but I also don't want to go against expert advice!\nHow can we best use the 1.3TB available to make sure that space is available when and where it's needed but not create a maintenance nightmare for the sysadmin??",
        "top_answer": "The primary (historical) reasons for partitioning are: \n\nto separate the operating system from your user and application data. Until the release of RHEL 7 there was no supported upgrade path and a major version upgrade would require a re-install and then having for instance /home and other (application) data on separate partitions (or LVM volumes) allows you to easily preserve the user data and application data and wipe the OS partition(s). \nUsers can't log in properly and your system starts to fail in interesting ways when you completely run out of disk space. Multiple partitions allow you to assign hard reserved disk space for the OS and keep that separate from the area's where users and/or specific applications are allowed to write (eg /home /tmp/ /var/tmp/ /var/spool/ /oradata/ etc.) , mitigating operational risk of badly behaved users and/or applications.  \nQuota. Disk quota allow the administrator to prevent an individual user of using up all available space, disrupting service to all other users of the system. Individual disk quota is assigned per file system, so a single partition and thus a single file-system means only 1 disk quotum. Multiple (LVM) partitions means multiple file-systems allowing for more granular quota management. Depending on you usage scenario you may want for instance allow each user 10 GB in their home directory, 2TB in the /data directory on the external storage array and set up a large shared scratch area where anyone can dump datasets too large for their home directory and where the policy becomes \"full is full\" but when that happens nothing breaks either. \nProviding dedicated IO paths. You may have a combination of SSD's and spinning disks and would do well to address them differently. Not so much an issue in a general purpose server, but quite common in database setups is to also assign certain spindles (disks) to different purposes to prevent IO contention, e.g. seperate disk for the transaction logs, separate disks for actual database data and separate disks for temp space. .\nBoot You may have a need for a separate /boot partition. Historically to address BIOS problems with booting beyond the 1024 cylinder limit,  nowadays more often a requirement to support encrypted volumes, to support certain RAID controllers, HBA's that don't support booting from SAN or file-systems not immediately supported by the installer etc.  \nTuning You may have a need for different tuning options or even completely different file-systems.  \n\nIf you use hard partitions you more or less have to get it right at install time and then a single large partition isn't the worst, but it does come with some of the restrictions above. \nTypically I recommend to partition your main volume as a single large Linux LVM physical volume and then create logical volumes that fit your current needs and for the remainder of your disk space, leave unassigned until needed. \nYou can than expand those volumes and their file-systems as needed (which is a trivial operation that can be done on a live system), or create additional ones as well. \nShrinking LVM volumes is trivial but often shrinking the file-systems on them is not supported very well and should probably be avoided. ",
        "url": "https://serverfault.com/questions/629439/how-bad-is-it-really-to-install-linux-on-one-big-partition"
    },
    {
        "title": "Nested Nginx location (prefix blocks in regex blocks) not working",
        "question": "I'm having trouble understanding nested locations in nginx.  I have two locations with some configuration directives in common, so rather than repeat the directives, I'd prefer to repeat the URI using a regular-expression:\nlocation ~ /a|/b {\n        location /a {\n        }\n        location /b {\n        }\n}\n\nHowever, this gives the error \nnginx: [emerg] location \"/a\" is outside location \"/a|/b\" in /etc/nginx/nginx.conf:36\n\nWithout the or | it seems okay, though.\nlocation ~ /a {\n        location /a {\n        }\n}\n\nWhat am I doing wrong?  Is there a better way to do this without repeating anything?  Can I group locations in another way?",
        "top_answer": "Old question, but the issue is because the parent location is a regex location while the nested locations are prefix locations.\nWhen a parent location is defined by a regex, any nested locations must also be defined by regexes:\nlocation ~ ^/(a|b) {\n        location ~ ^/a {\n        ...\n        }\n        location ~ ^/b {\n        ...\n        }\n}\n\nYou may only define nested prefix locations when the parent location is also a prefix location:\nlocation /a {\n        location /a {\n               # You can also skip this location and just write\n               # your code directly under the parent location\n        }\n        location /a/b {\n        ...\n        }\n}\n\nHowever, you may define nested regex locations when the parent location is a prefix location:\nlocation /a/b {\n        location ~ /a {\n        ...\n        }\n        location ~ /b {\n        ...\n        }\n}\n",
        "url": "https://serverfault.com/questions/496371/nested-nginx-location-prefix-blocks-in-regex-blocks-not-working"
    },
    {
        "title": "Is it better practice to buy RAID disks individually vs. in bulk?",
        "question": "This may sound like an odd question, but it's generated some spirited discussion with some of my colleagues. Consider a moderately sized RAID array consisting of something like eight or twelve disks. When buying the initial batch of disks, or buying replacements to enlarge the array or refresh the hardware, there are two broad approaches one could take:\n\nBuy all the drives in one order from one vendor, and receive one large box containing all the disks.\nOrder one disk apiece from a variety of vendors, and/or spread out (over a period of days or weeks) several orders of one disk apiece.\n\nThere's some middle ground, obviously, but these are the main opposing mindsets. I've been genuinely curious which approach is more sensible in terms of reducing the risk of catastrophic failure of the array. (Let's define that as \"25% of the disks fail within a time window equal to how long it takes to resilver the array once.\") The logic being, if all the disks came from the same place, they might all have the same underlying defects waiting to strike. The same timebomb with the same initial countdown on the clock, if you will.\nI've collected a couple of the more common pros and cons for each approach, but some of them feel like conjecture and gut instinct instead of hard evidence-based data.\nBuy all at once, pros\n\nLess time spent in research/ordering phase.\nMinimizes shipping cost if the vendor charges for it.\nDisks are pretty much guaranteed to have the same firmware version and the same \"quirks\" in their operational characteristics (temperature, vibration, etc.)\nPrice increases/stock shortages unlikely stall the project midway.\nEach next disk is on-hand the moment it's required to be installed.\nSerial numbers are all known upfront, disks can be installed in the enclosure in order of increasing serial number. Seems overly fussy, but some folks seem to value that. (I guess their management interface sorts the disks by serial number instead of hardware port order...?)\n\nBuy all at once, cons\n\nAll disks (probably) came from the same factory, made at the same time, of the same materials. They were stored in the same environment, and subject to the same potential abuses during transit. Any defect or damage present in one is likely present in all.\nIf the drives are being replaced one-at-a-time into an existing array and each new disk needs to be resilvered individually, it could be potentially weeks before the last disk from the order is installed and discovered to be faulty. The return/replacement window with the vendor may expire during this time.\nCan't take advantage of near-future price decreases that may occur during the project.\n\nBuy individually, pros\n\nIf one disk fails, it shares very little manufacturing/transit history with any of the other disks. If the failure was caused by something in manufacturing or transit, the root cause likely did not occur in any other disk.\nIf a disk is dead on arrival or fails during the first hours of use, that will be detected shortly after the shipment arrives and the return process may go more smoothly.\n\nBuy individually, cons\n\nTakes a significant amount of time to find enough vendors with agreeable prices. Order tracking, delivery failure, damaged item returns, and other issues can be time-consuming to resolve.\nPotentially higher shipping costs.\nA very real possibility exists that a new disk will be required but none will be on-hand, stalling the project.\nImagined benefit. Regardless of the vendor or date purchased, all the disks came from the same place and are really the same. Manufacturing defects would have been detected by quality control and substandard disks would not have been sold. Shipping damage would have to be so egregious (and plainly visible to the naked eye) that damaged drives would be obvious upon unpacking.\n\nIf we're going simply by bullet point count, \"buy in bulk\" wins pretty clearly. But some of the pros are weak, and some of the cons are strong. Many of the bullet points simply state the logical inverse of some of the others. Some of these things may be absurd superstition. But if superstition does a better job at maintaining array integrity, I guess I'd be willing to go along with it.\nWhich group is most sensible here?\nUPDATE: I have data relevant to this discussion. The last array I personally built (about four years ago) had eight disks. I ordered from one single vendor, but split the purchase into two orders of four disks each, about one month apart. One disk of the array failed within the first hours of running. It was from the first batch, and the return window for that order had closed in the time it took to spin everything up.\nFour years later, the seven original disks plus one replacement are still running error-free. (knock on wood.)",
        "top_answer": "In practice, people who buy from enterprise vendors (HPE, Dell, etc.) do not worry about this.\nDrives sourced by these vendors are already spread across multiple manufacturers under the same part number. \nAn HP disk under a particular SKU may be HGST or Seagate or Western Digital.\nSame HP part number, variation on manufacturer, lot number and firmware\n\nYou shouldn't try to outsmart/outwit the probability of batch failure, though. You're welcome to try if it gives peace of mind, but it may not be worth the effort. \nGood practices like clustering, replication and solid backups are the real protection for batch failures. Add hot and cold spares. Monitor your systems closely. Take advantage of smart filesystems like ZFS :)\nAnd remember, hard drive failures aren't always mechanical...",
        "url": "https://serverfault.com/questions/870095/is-it-better-practice-to-buy-raid-disks-individually-vs-in-bulk"
    },
    {
        "title": "How do I list the files associated with a Python package installed using pip or easy_install?",
        "question": "I've installed a Python package using pip, which is a replacement for easy_install. How do I get a list of which installed files are associated with this package? \nBasically, I'm looking for the Python package equivalent of \ndpkg -L\n\nor \nrpm -ql\n",
        "top_answer": "You could do that by using command:\npip show -f <package>\n",
        "url": "https://serverfault.com/questions/153843/how-do-i-list-the-files-associated-with-a-python-package-installed-using-pip-or"
    },
    {
        "title": "Ubuntu displays \"There is 1 zombie process\" upon login",
        "question": "This message appears when I login to my machine:\nThere is 1 zombie process.\n\n\nWhat is it telling me?\nIs this anything I should worry about?\nIf yes, then what should I do, and how?\n",
        "top_answer": "There's nothing to worry about :\n\nZombie\nOn Unix operating systems, a zombie process or defunct process is a\n  process that has completed execution but still has an entry in the\n  process table, allowing the process that started it to read its exit\n  status. In the term's colorful metaphor, the child process has died\n  but has not yet been reaped. \nWhen a process ends, all of the memory and resources associated with\n  it are deallocated so they can be used by other processes. However,\n  the process's entry in the process table remains. The parent is sent a\n  SIGCHLD signal indicating that a child has died; the handler for this\n  signal will typically execute the wait system call, which reads the\n  exit status and removes the zombie. The zombie's process ID and entry\n  in the process table can then be reused. However, if a parent ignores\n  the SIGCHLD, the zombie will be left in the process table. In some\n  situations this may be desirable, for example if the parent creates\n  another child process it ensures that it will not be allocated the\n  same process ID. \n\nSource : http://wiki.answers.com/Q/What_is_Zombie_Process_and_Orphan_Process",
        "url": "https://serverfault.com/questions/390202/ubuntu-displays-there-is-1-zombie-process-upon-login"
    },
    {
        "title": "Assign Public IP (not Elastic IP) after instance launched",
        "question": "When I launch instance in AWS console I can set \"Auto-assign Public IP\" to true and newly created instance will be assigned with public IP address from pool.\nNow assume I have launched instance with this setting set to false and want to assign public IP to this instance. The same public IP as in first case, not Elastic IP.\nPS I know I can launch new instance and shut down old one. I'm particularly interested in assigning to one already running.",
        "top_answer": "The instance that you launched without a public IP will stay without one as it is only assignable when you launch the instance.  Even having a subnet with auto assign public IP switched on will not assign a public IP to your instance if, when you launched the instance you chose not to have a public IP.\nThe only way I know is to select assign a public IP before launching the instance or having the subnet set up to auto assign public IPs which will do that only when you launch a new instance.\nSo to summarize:\nIt is not possible to assign a public IP after launching that instance unless you use EIPs.",
        "url": "https://serverfault.com/questions/706560/assign-public-ip-not-elastic-ip-after-instance-launched"
    },
    {
        "title": "Should CNAME Be Used For Subdomains?",
        "question": "I manage multiple websites that currently have the following DNS configuration:\nexample.com      - A Record - Production Server IP\ntest.example.com - A Record - Test Server IP\nwww.example.com  - CNAME    - example.com\nbeta.example.com - CNAME    - test.example.com\ndev.example.com  - CNAME    - test.example.com\n\nIs this an appropriate use of CNAME records? I've looked online and have not found a clear answer. Some people claim that CNAME records are bad (they are not, however, clear on why this is) and propose the following setup:\nexample.com      - A Record - Production Server IP\ntest.example.com - A Record - Test Server IP\nwww.example.com  - A Record - Production Server IP\nbeta.example.com - A Record - Test Server IP\ndev.example.com  - A Record - Test Server IP\n\nWhich one of these is the better approach (and why)?\nNote: The subdomains do not require their own MX records, so that is not an issue here.",
        "top_answer": "Yes, that's an appropriate use of CNAMEs. In the discussions I've been part of, the arguments tend to go like this:\nAgainst CNAMEs:\n\nThere is a (tiny) performance penalty, as the downstream DNS caches need to perform 2 DNS lookups, one for the CNAME and one for the A-Record the CNAME points to.\nVague, bogus arguments about CNAMEs having less \"authority\" or compatibility issues.\n\nIn favor of CNAMEs:\n\nThey provide a clean abstraction between hardware (physical servers) and services.\nThey simplify DNS management -- when a server moves, you only need to change one record.\n\nAfter trying a couple of different ways to do this, I now have a personal favorite style. It is:\n\nOne A Record for each physical server; with a fairly low TTL (perhaps 30 minutes); giving the server a human-friendly name.\nOne CNAME for each service; with a high TTL (perhaps 24 hours); pointing to the above server names.\nAs the sole exeption to the rules above, the domain root is an A-Record, pointing to the webserver / web load balancer. (The @ is required to be an A-record.)\n\nI find that this setup works well. It keeps extra DNS lookups for the CNAMES down; and if a server crashes I can still change public DNS around fairly fast.\nHere's a (improvised) example in BIND syntax:\n;name     ttl   class rr     value \nserver01  30m   IN    A      192.168.0.3\nserver02  30m   IN    A      192.168.0.4\n\nwebmail   24h   IN    CNAME  server01\nextranet  24h   IN    CNAME  server02\nftp       24h   IN    CNAME  server02\n",
        "url": "https://serverfault.com/questions/181927/should-cname-be-used-for-subdomains"
    },
    {
        "title": "How do I extract login history?",
        "question": "I need to know the login history for specific user (i.e. login and logout time),\nHow do I extract this history for a specific date range in Linux ?",
        "top_answer": "You can try the last command:\nlast john \n\nIt prints out the login/out history of user john. Whereas running just\nlast\n\nprints out the login/out history of all users.",
        "url": "https://serverfault.com/questions/305738/how-do-i-extract-login-history"
    },
    {
        "title": "Ansible stuck on gathering facts",
        "question": "I'm having some odd issues with my ansible box(vagrant).\nEverything worked yesterday and my playbook worked fine.\nToday, ansible hangs on \"gathering facts\"?\nHere is the verbose output:\n<5.xxx.xxx.xxx> ESTABLISH CONNECTION FOR USER: deploy\n<5.xxx.xxx.xxx> REMOTE_MODULE setup\n<5.xxx.xxx.xxx> EXEC ['ssh', '-C', '-tt', '-vvv', '-o', 'ControlMaster=auto', '-\no', 'ControlPersist=60s', '-o', 'ControlPath=/home/vagrant/.ansible/cp/ansible-s\nsh-%h-%p-%r', '-o', 'Port=2221', '-o', 'KbdInteractiveAuthentication=no', '-o',\n'PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey', '-o\n', 'PasswordAuthentication=no', '-o', 'User=deploy', '-o', 'ConnectTimeout=10',\n'5.xxx.xxx.xxx', \"/bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1411372677\n.18-251130781588968 && chmod a+rx $HOME/.ansible/tmp/ansible-tmp-1411372677.18-2\n51130781588968 && echo $HOME/.ansible/tmp/ansible-tmp-1411372677.18-251130781588\n968'\"]\n",
        "top_answer": "I was having a similar issue with Ansible  ping on Vagrant, it just suddenly stuck for no reason and has previously worked absolutely fine. Unlike any other issue like ssh or connective issue, it just forever die with no timeout. \nOne thing I did to resolve this issue is to clean ~/.ansible directory and it just works again. I can't find out why, but it did get resolved.\nIf you got change to have it again try clean the ~/.ansible folder before you refresh your Vagrant.",
        "url": "https://serverfault.com/questions/630253/ansible-stuck-on-gathering-facts"
    },
    {
        "title": "Is it possible to reboot a Linux OS without rebooting the hardware?",
        "question": "Is there a way to reboot a Linux system (Debian in particular) without rebooting the hardware?\nI have a RAID controller that takes a bit to get itself running before the OS starts up, and I would like it if there was a way to quickly reboot the Linux OS without having to go through the whole reboot process of restarting the RAID controller, etc.",
        "top_answer": "I use kexec-reboot on nearly all of my production systems. \nIt works incredibly well, allowing me to bypass the long POST time on HP ProLiant servers and reduce the boot cycle from 5 minutes to ~45 seconds. \nSee: https://github.com/error10/kexec-reboot\nThe only caveat is that it doesn't seem to work on RHEL/CentOS 6.x systems booting UEFI. But most sane OS/hardware combinations work.",
        "url": "https://serverfault.com/questions/678024/is-it-possible-to-reboot-a-linux-os-without-rebooting-the-hardware"
    },
    {
        "title": "Service start request repeated too quickly, refusing to start limit",
        "question": "I have a systemd service that displays the following error service start request repeated too quickly, refusing to start \nI understand that the service is configured to restart on failure and it is restarting again and again. But when exactly does it refuse to restart ?\nIs there a limit or number that defines it ?\nMoreover, what does too quickly exactly mean, is it a limit of number of restarts in a given period of time ?",
        "top_answer": "The default limit is to allow 5 restarts in a 10sec period. If a service goes over that threshold due to the Restart= config option in the service definition, it will not attempt to restart any further. \nThe rates are configured with the StartLimitIntervalSec= and StartLimitBurst= options and the Restart= option controls when SystemD tries to restart a service. \nMore info in man systemd.unit and man systemd.service. \nThen use systemctl daemon-reload to reload unit configuration.",
        "url": "https://serverfault.com/questions/845471/service-start-request-repeated-too-quickly-refusing-to-start-limit"
    },
    {
        "title": "Proxy Error 502 \"Reason: Error reading from remote server\" with Apache 2.2.3 (Debian) mod_proxy and Jetty 6.1.18",
        "question": "Apache is receiving requests at port :80 and proxying them to Jetty at port :8080\nThe proxy server received an invalid response from an upstream server\nThe proxy server could not handle the request GET /.\n\nMy dilemma: Everything works fine normally (fast requests, few seconds or few tens of seconds long requests are processed ok). Problems occur when request processing takes long (few minutes?). \nIf I issue request instead directly to Jetty at port :8080 the request is processed OK. So problem is likely to sit somewhere between Apache and Jetty where I am using mod_proxy. How to solve this? \nI have already tried some \"tricks\" related to KeepAlive settings, without luck. Here is my current configuration, any suggestions?\n#keepalive Off                     ## I have tried this, does not help\n#SetEnv force-proxy-request-1.0 1  ## I have tried this, does not help\n#SetEnv proxy-nokeepalive 1        ## I have tried this, does not help\n#SetEnv proxy-initial-not-pooled 1 ## I have tried this, does not help\nKeepAlive 20                       ## I have tried this, does not help\nKeepAliveTimeout 600               ## I have tried this, does not help\nProxyTimeout 600                   ## I have tried this, does not help\n\nNameVirtualHost *:80\n<VirtualHost _default_:80>\n    ServerAdmin [email\u00a0protected]\n\n    ServerName www.mydomain.fi\n\n    ServerAlias mydomain.fi mydomain.com mydomain www.mydomain.com\n\n    ProxyRequests On\n    ProxyVia On\n    <Proxy *>\n            Order deny,allow\n            Allow from all\n    </Proxy>\n\n    ProxyRequests Off\n    ProxyPass / http://www.mydomain.fi:8080/ retry=1 acquire=3000 timeout=600\n    ProxyPassReverse / http://www.mydomain.fi:8080/\n\n    RewriteEngine On\n    RewriteCond %{SERVER_NAME} !^www\\.mydomain\\.fi\n    RewriteRule /(.*) http://www.mydomain.fi/$1 [redirect=301L]\n\n    ErrorLog /var/log/apache2/error.log\n\n    # Possible values include: debug, info, notice, warn, error, crit,\n    # alert, emerg.\n    LogLevel warn\n\n    CustomLog /var/log/apache2/access.log combined\n    ServerSignature On\n\n</VirtualHost>\n\nHere is also the debug log from a failing request:\n74.125.43.99 - - [29/Sep/2010:20:15:40 +0300] \"GET /?wicket:bookmarkablePage=newWindow:com.mydomain.view.application.reports.SaveReportPage HTTP/1.1\" 502 355 \"https://www.mydomain.fi/?wicket:interface=:0:2:::\" \"Mozilla/5.0 (Windows; U; Windows NT 6.1; fi; rv:1.9.2.10) Gecko/20100914 Firefox/3.6.10\"\n[Wed Sep 29 20:20:40 2010] [error] [client 74.125.43.99] proxy: error reading status line from remote server www.mydomain.fi, referer: https://www.mydomain.fi/?wicket:interface=:0:2:::\n[Wed Sep 29 20:20:40 2010] [error] [client 74.125.43.99] proxy: Error reading from remote server returned by /, referer: https://www.mydomain.fi/?wicket:interface=:0:2:::\n",
        "top_answer": "I have solved the problem. The Keepalive=On should be inserted into ProxyPass config line:\nProxyPass / http://www.dom.fi:8080/ retry=1 acquire=3000 timeout=600 Keepalive=On\n\nSee that \nKeepalive=On\n\nthere? It is critical ;)",
        "url": "https://serverfault.com/questions/185894/proxy-error-502-reason-error-reading-from-remote-server-with-apache-2-2-3-de"
    },
    {
        "title": "Disable caching when serving static files with Nginx (for development)",
        "question": "We are using Nginx to serve static files on a development platform. As it is a development platform, we'd like to disable caching so that each change is propagated to the server. The configuration of the VHost is quite simple:\nserver {\n  server_name  static.server.local;\n  root /var/www/static;\n\n  ## Default location\n  location / {\n    access_log        off;\n    expires           0;\n    add_header        Cache-Control private;\n  } \n}\n\nWhen we access an HTML file (http://static.server.local/test.html), we have no issue: the server returns a code 304 Not Modified as long as the file is not changed, and a 200 OK response with the modified file when the file is changed.\nHowever, it seems to behave differently with a Javascript or a CSS file. Once the file is changed, we get a 200 OK response as expected, but with the old text.\nIs there an internal cache mechanism in Nginx that could explain this behaviour? Or some configuration that we should add?  \nAs a side note, here is the header returned by Nginx when the file has been modified (it seems correct):\nAccept-Ranges:bytes\nCache-Control:max-age=0\nprivate\nConnection:keep-alive\nContent-Length:309\nContent-Type:text/css\nDate:Fri, 13 May 2011 14:13:13 GMT\nExpires:Fri, 13 May 2011 14:13:13 GMT\nLast-Modified:Fri, 13 May 2011 14:13:05 GMT\nServer:nginx/0.8.54\n\nEdit\nAfter trying different settings with the expires directive and Cache-Controlheader, I made some further investigations. In fact, the server is installed on a VirtualBox guest Ubuntu, and data are read from a shared folder that is on the Mac OSX host.\nIf the file is edited from an IDE (NetBeans) on the host, it seems that changes do not appear whereas if I edit it directly on the guest (using VIM), it is refreshed.\nThe strange thing is it does not behave similarly with HTML files.\nQuite puzzling.\nEdit 2 (ANSWER)\nIndeed, the origin of the issue was more on the VirtualBox side. Or rather a conflict between VirtualBox and the \"sendfile\" option of the server.\nThis link VirtualBox Hates Sendfile gave me the solution: switch the sendfile flag in the server configuration to off:\nsendfile  off;\n\nHope this could also help other person using VirtualBox for development. :)\nThere are some additional information on the VirtualBox forum.",
        "top_answer": "Since the answer is somehow hidden in the question - here is the solution for nginx in a VirtualBox environment as standalone answer.\nIn your nginx config (usally /etc/nginx/nginx.conf) or vhost config file change the sendfile parameter to off:\nsendfile  off;\n\nWhile sendfile is at the heart of Nginx's fame (blazing-fast low-level static file serving efficiency) it might be a bane for local development, e.g. Javascripts that change often and need to be reloaded. Nonetheless Nginx sendfile is smart and probably isn't most people's issue; check your browser's \"disable cache\" options as well!",
        "url": "https://serverfault.com/questions/269420/disable-caching-when-serving-static-files-with-nginx-for-development"
    },
    {
        "title": "Transfer 10 TB of files from USA to UK datacenter",
        "question": "I am migrating my server from the USA to the UK from one data center to another. My host said I should be able to achieve 11 megabytes per second.\nThe operating system is Windows Server 2008 at both ends.\nMy average file size is around 100 MB and the data is split across five 2\u00a0TB drives.\nWhat would be the recommended way to transfer these files?\n\nFTP\nSMB\nRsync / Robocopy\nOther?\n\nI'm not too bothered about security as these are public files anyway, but I just want a solution that can push the full 11\u00a0MB/s transfer rate to minimize the total transfer time.",
        "top_answer": "Ship hard drives across the ocean instead.\nAt 11 Mbps with full utilization, you're looking at just shy of 90 days to transfer 10 TB.\n\n11 Mbps = 1.375 MBps = 116.015 GB/day.\n10240 GB / 116.015 GB/day = ~88.3 days.",
        "url": "https://serverfault.com/questions/318091/transfer-10-tb-of-files-from-usa-to-uk-datacenter"
    },
    {
        "title": "Nginx - root versus alias, for serving single files?",
        "question": "After many hours getting nginx to serve single files such as robots.txt (hint: clear your browser cache each time), I wound up with two different ways, one using the alias directive, and one using the root directive, like so:\nlocation /robots.txt { alias /home/www/static/robots.txt; }\nlocation /robots.txt { root /home/www/static/;  }\n\nIs there any functional difference between the two? Or security issues? Any conflicts with other directives? (Both seemed fine with another /static location). Or any reason to pick one over the other?\nNote - I didn't use both at the same time :) Rather I tried each, one at a time, and both worked. I'm not asking how they both interact together in the same file, but which one would be better to use.",
        "top_answer": "Well, these two directives are slightly functional different because you do not use exact match in the latter case. So, /robots.txt1111 will match your second location too.\nlocation =/robots.txt { root /home/www/static/;  } is an exact functional equivalent of your first directive.",
        "url": "https://serverfault.com/questions/278351/nginx-root-versus-alias-for-serving-single-files"
    },
    {
        "title": "Why hasn't rsync caught on in the Windows world? [closed]",
        "question": "It seems that rsync is the de-facto standard for efficient file backup and sync in Unix/Linux.\nDoes anyone have any thoughts on why it wouldn't have caught on in the Windows world? \nWhy hasn't it become a universal 'protocol' for file sync?",
        "top_answer": "I would say mostly because people in windows are unaware of it. Rsync is a command-line utility that is consistent with the unix philosophy of having lots of small tools preinstalled. The windows philosophy is based around GUI applications that are all downloaded and installed separately. There is not a smooth integration spot where rsync would be obvious or make much sense, and running commands on a windows system is tedious at best. \nAlso, rsync really shines when its part of a larger application (say for consolidating and parsing logs), or as an automated archival system (implemented easily with a cronjob). Windows simply doesnt have the other tools in its ecosystem to make using rsync actually viable.\nFinally, I would say that rsync is just too freaking complicated. Anyone I know who uses it regularly has a pre-set group of flags (mine is -avuz) that generally does what they want, but the man pages / documentation lists dozens of command-line switches, some of them amalgamations of other switches. For example (from the [man page][1]):\n\n-a, --archive: archive mode; equals -rlptgoD (no -H,-A,-X)\n\nIt is a quick way of saying you want recursion and want to preserve almost everything (with -H being a notable omission).  The only exception to  the  above equivalence is when --files-from is specified, in which case -r is not implied.\nWindows users generally expect, well, windows, and menus, and to have a single app be an all-in-one solution, not just an independent piece of a tool chain.",
        "url": "https://serverfault.com/questions/35336/why-hasnt-rsync-caught-on-in-the-windows-world"
    },
    {
        "title": "Getting the last match in a file using grep",
        "question": "What's the best way of getting only the final match of a regular expression in a file using grep?\nAlso, is it possible to begin grepping from the end of the file instead of the beginning and stop when it finds the first match?",
        "top_answer": "You could try\ngrep pattern file | tail -1\n\nor\ntac file | grep pattern | head -1\n\nor\ntac file | grep -m1 pattern\n",
        "url": "https://serverfault.com/questions/197123/getting-the-last-match-in-a-file-using-grep"
    },
    {
        "title": "What should I do to make sure that IIS does not recycle my application?",
        "question": "I have a WCF service app hosted in IIS.  On startup, it goes and fetches a really expensive (in terms of time and cpu) resource to use as local cache.\nUnfortunately, IIS seems to recycle the process on a fairly regular basis.  So I am trying to change the settings on the Application Pool to make sure that IIS does not recycle the application.  So far, I've change the following:\n\nLimit Interval under CPU from 5 to 0.\nIdle Time-out under Process Model from 20 to 0.\nRegular Time Interval under Recycling from 1740 to 0.\n\nWill this be enough?  And I have specific questions about the items I changed:\n\nWhat specifically does Limit Interval setting under CPU mean?  Does it mean that if a certain CPU usage is exceeded, the application pool will be recycled?  \nWhat exactly does \"recycled\" mean?  Is the application completely torn down and started up again?\nWhat is the difference between \"Worker Process shutdown\" and \"Application Pool recycling\"?  The documentation for the Idle Time-out under Process Model talks about shutting down the worker process.  While the docs for Regular Time Interval under Recycling talk about application pool recycling.  I don't quite grok the difference between the two.  I thought the w3wp.exe is the worker process which runs the application pool.  Can someone explain the difference to the application between the two?\n\nThe reason for having IIS7 and IIS7.5 tags is because the app will run in both and hope the answers are the same between the versions.\nImage for reference: ",
        "top_answer": "Recycling\nRecycling is usually* where IIS starts up a new process as a container for your application, and then gives the old one up to ShutdownTimeLimit to go away of its own volition before it's killed.\n*- usually: see DisallowOverlappingRotation / \"Disable overlapped recycle\" setting\nIt is destructive, in that the original process and all its state information are discarded. Using out-of-process session state (eg, State Server or a database, or even a cookie if your state is tiny) can allow you to work around this.\nBut it is by default overlapped - meaning the duration of an outage is minimized because the new process starts and is hooked up to the request queue, before the old one is told \"you have [ShutdownTimeLimit] seconds to go away. Please comply.\"\nSettings\nTo your question: all the settings on that page control recycling in some way. \"Shutdown\" might be described as \"proactive recycling\" - where the process itself decides it's time to go, and exits in an orderly manner.\nReactive recycling is where WAS detects a problem and shoots the process (after establishing a suitable replacement W3WP).\nNow, here's some stuff that can cause recycling of one form or another:\n\nan ISAPI deciding it's unhealthy\nany module crashing\nidle timeout\ncpu limiting\nadjusting App Pool properties\n\nas your mum may have screamed at one point: \"Stop picking at it, or it'll never get better!\"\n\n\n\"ping\" failure * not actually pinging per se, cos it uses a named pipe - more \"life detection\"\nall of the settings in the screenshot above\n\nWhat To Do:\nGenerally:\n\nDisable Idle timeouts. 20 minutes of inactivity = boom! Old process gone! New process on the next incoming request. Set that to zero.\n\nDisable Regular time interval - the 29 hour default has been described as \"insane\", \"annoying\" and \"clever\" by various parties. Actually, only two of those are true.\n\nOptionally Turn on DisallowRotationOnConfigChange (above, Disable Reycling for configuration changes) if you just can't stop playing with it - this allows you to change any app pool setting without it instantly signaling to the worker processes that it needs to be killed. You need to manually recycle the App Pool to get the settings to take effect, which lets you pre-set settings and then use a change window to apply them via your recycle process.\n\nAs a general principle, leave pinging enabled. That's your safety net. I've seen people turn it off, and then the site hangs indefinitely sometimes, leading to panic... so if the settings are too aggressive for your apparently-very-very-slow-to-respond app, back them off a bit and see what you get, rather than turning it off. (Unless you've got auto-crash-mode dumping set up for hung W3WPs through your own monitoring process)\n\n\nThat's enough to cause a well-behaved process to live forever. If it dies, sure, it'll be replaced. If it hangs, pinging should pick that up and a new one should start within 2 minutes (by default; worst-case calc should be: up to ping frequency + ping timeout + startup time limit before requests start working again).\nCPU limiting isn't normally interesting, because by default it's turned off, and it's also configured to do nothing anyway; if it were configured to kill the process, sure, that'd be a recycling trigger. Leave it off. Note for IIS 8.x, CPU Throttling becomes an option too.\nAn (IIS) AppPool isn't a (.Net) AppDomain (but may contain one/some)\nBut... then we get into .Net land, and AppDomain recycling, which can also cause a loss of state. (See: https://blogs.msdn.microsoft.com/tess/2006/08/02/asp-net-case-study-lost-session-variables-and-appdomain-recycles/)\nShort version, you do that by touching a web.config file in your content folder (again with the picking!), or by creating a folder in that folder, or an ASPX file, or.. other things... and that's about as destructive as an App Pool recycle, minus the native-code startup costs (it's purely a managed code (.Net) concept, so only managed code initialization stuff happens here).\nAntivirus can also trigger this as it scans web.config files, causing a change notification, causing....",
        "url": "https://serverfault.com/questions/333907/what-should-i-do-to-make-sure-that-iis-does-not-recycle-my-application"
    },
    {
        "title": "Boot and Install Windows from a USB thumb drive",
        "question": "Installing Windows from a thumb drive is vastly superior to burning a copy to a DVD which will fill some landfill somewhere with toxic stuff. Not to mention it's about 50x faster to install Windows from a USB Thumb Drive.\nHow do you get the bits onto the thumb drive so that you can boot from it and do a clean install?",
        "top_answer": "Update: Microsoft has created the Windows 7 USB/DVD Download tool to make this very easy.\nI used this guide as a set of directions - http://kurtsh.spaces.live.com/blog/cns!DA410C7F7E038D!1665.entry\n1. Get a USB Thumbdrive between 4-32GB.\nIf the drive is larger than 32GB, Windows cannot format it as FAT32, so an alternate utility must be used. Windows can still read FAT32 partitions larger than 32GB, though some devices cannot.\n2. Run cmd.exe as administrator and enter the following commands followed by Enter\n\ndiskpart\nlist disk\nselect disk # (where # is your USB drive as determined from step 2)\nclean (This step will delete all data on your flash drive!)\ncreate partition primary\nactive\nformat fs=fat32 quick\nassign\nlist volume\nexit\nbootsect.exe /nt60 F: /mbr (where F: is the drive letter of your USB drive as reported by list volume)\n\n3. Copy the Windows files from the ISO or other source using robocopy\nrobocopy.exe E:\\ F:\\ /MIR\n\nwhere E:\\ is the source and F:\\ is the destination. Drag-and-drop or copy/paste can also be used, if you know what you're doing.\nConfigure your PC to boot from the USB drive\nIn some machines the USB thumbdrive will appear to the BIOS as any other hard drive. You need to muck with the boot sequence to place the thumbdrive higher in the boot order than the local hard drive.\n\nNote that after you do this you might\n  want to reset the boot order in order\n  to ensure that BitLocker doesn't\n  detect boot changes based on the fact\n  that the thumdrive is missing if it\n  was there when you encrypted your\n  drive.\n",
        "url": "https://serverfault.com/questions/2952/boot-and-install-windows-from-a-usb-thumb-drive"
    },
    {
        "title": "What sysadmin things should every programmer know?",
        "question": "As a programmer, we tend to take sysadmins for granted.  The few times I've been without a good sysadmin have really made me appreciate what you guys do.  When we're venturing into an environment without a sysadmin, what words of wisdom can you offer us?",
        "top_answer": "I'd start with:\n\nAlways have a backup system of some kind. Even better if it has a history.\nConsider single points of failure and how to deal with them should they fail.\nDepending on the amount of computers involved, looking into some way to make and create a standard image across computers will make everyone's life easier - no \"it works on mine\" because they have such and such a program not normally installed.\nDocument everything, if only because you will forget how you set something up.\nKeep abreast of security updates.\n",
        "url": "https://serverfault.com/questions/4176/what-sysadmin-things-should-every-programmer-know"
    },
    {
        "title": "Why drop caches in Linux?",
        "question": "In our servers we have a habit of dropping caches at midnight. \nsync; echo 3 > /proc/sys/vm/drop_caches\n\nWhen I run the code it seems to free up lots of RAM, but do I really need to do that. Isn't free RAM a waste?",
        "top_answer": "You are 100% correct. It is not a good practice to free up RAM. This is likely an example of cargo cult system administration.",
        "url": "https://serverfault.com/questions/597115/why-drop-caches-in-linux"
    },
    {
        "title": "How can I show mysql locks?",
        "question": "Is there anyway to show all the locks that are active in a mysql database?",
        "top_answer": "See Marko's link for InnoDB tables and the caveats.\nFor MyISAM, there isn't a dead easy \"this is the offending query\" solution. You should always start with a processlist. But be sure to include the full keyword so that the printed queries aren't truncated:\nSHOW FULL PROCESSLIST;\n\nThis will show you a list of all current processes, their SQL query and state. Now usually if a single query is causing many others to lock then it should be easy to identify. The affected queries will have a status of Locked and the offending query will be sitting out by itself, possibly waiting for something intensive, like a temporary table.\nIf it's not obvious then you will have to use your powers of SQL deduction to determine which piece of offending SQL may be the cause of your woes.",
        "url": "https://serverfault.com/questions/36260/how-can-i-show-mysql-locks"
    },
    {
        "title": "Multiple TXT fields for same subdomain",
        "question": "I would like to understand if multiple TXT records for the same subdomain are ok or could lead to issues. In particular, we have the requirement for one SPF record and one Google Domain Verification record on the root domain.\nIn AWS Route 53 they explicitly support this in the following way:\n\nEnter multiple values on separate lines. Enclose text in quotation\n  marks. Example:\n      \"Sample Text Entries\"\n      \"Enclose entries in quotation marks\"\n\nThis way a single TXT field can contain both the SPF and Google Domain Verification records.\nWhen I asked name.com on the other hand they suggested to add two separate TXT records as the Route 53 method is not supported.",
        "top_answer": "Yes, creating two separate records with the same name is OK\nThe way described is the way you create multiple records on Route 53.\nEntering two values in the textarea separated by a newline will result in two distinct records in the DNS. This is why Amazon call it a \"record set\" - it is a set of records.",
        "url": "https://serverfault.com/questions/815841/multiple-txt-fields-for-same-subdomain"
    },
    {
        "title": "Changing host permissions for MySQL users",
        "question": "I have the following grants for a user/database\nmysql> SHOW GRANTS FOR 'username'@'localhost';\n+---------------------------------------------------------------------------+\n| Grants for username@localhost                                             |\n+---------------------------------------------------------------------------+\n| GRANT USAGE ON *.* TO 'username'@'localhost' IDENTIFIED BY PASSWORD 'xxx' |\n| GRANT ALL PRIVILEGES ON `userdb`.* TO 'username'@'localhost'              |\n+---------------------------------------------------------------------------+\n\nTo enable external access to the database, I need to change localhost to %. One way to do this is REVOKE all permissions and set it again. The problem is, that there is a password set which I don't know, so if I revoke the permission, I can't set it back.\nIs there a way to change the hostname localhost to % (and back again) without revoking the permission itself?",
        "top_answer": "If you've got access to the mysql database, you can change the grant tables directly:\nUPDATE mysql.user SET Host='%' WHERE Host='localhost' AND User='username';\n\n...and an analogous UPDATE-statement to change it back.\nAlso you might need to make changes to the mysql.db table as well:\nUPDATE mysql.db SET Host='%' WHERE Host='localhost' AND User='username';\n\nand then flush to apply the privileges:\nFLUSH PRIVILEGES;\n",
        "url": "https://serverfault.com/questions/483339/changing-host-permissions-for-mysql-users"
    },
    {
        "title": "How to apply a filter to real time output of `tail -f `?",
        "question": "tail -f path\n\nThe above will output modifications to the file instantly, but I want to apply a filter to the output, only show when there is a keyword xxx in it.\nHow to approach this?",
        "top_answer": "With Unix you can pipe the output of one program into another.\nSo to filter tail, you can use grep:\ntail -f path | grep your-search-filter\n",
        "url": "https://serverfault.com/questions/157526/how-to-apply-a-filter-to-real-time-output-of-tail-f"
    },
    {
        "title": "Rsync creates a directory with the same name inside of destination directory",
        "question": "When I run this command\nrsync -avzp --del -e \"ssh -p myport\" user@hostname:/var/www/tests /var/www/tests\n\nfiles get synchronized but instead of saving files in /var/www/tests, Rsync creates one more directory \"tests\" inside of existing \"tests\":\n/var/www/tests/tests\n\nand puts files there. How to tell Rsync not to create a new directory?",
        "top_answer": "If you don't want another tests directory created, the correct command would be\nrsync -avzp --del -e \"ssh -p myport\" user@hostname:/var/www/tests/ /var/www/tests\n\nNote the / at the end of user@hostname:/var/www/tests/.",
        "url": "https://serverfault.com/questions/529287/rsync-creates-a-directory-with-the-same-name-inside-of-destination-directory"
    },
    {
        "title": "swap partition vs file for performance?",
        "question": "What is better for performance? A partition closer to the inside of the disk will have slower access times, and we must wait for the drive to switch between the OS and swap partitions.\nOn the other hand, a swap partition bypasses all of the filesystem allowing writes to the disk directly, which can be faster than a file.\nWhat is the performance trade off?\nHow much does having a fixed size swapfile make a difference?\nIs it a case that it will be longer to change to the swap partition, but performance will be better while it is on the swap partition that if it had been a swap file?",
        "top_answer": "\nOn hard disks, throughput and seeking is often faster towards the beginning of the disk, because that data is stored closer to the outer area of the disk, which has more sectors per cylinder. Thus, creating the swap at the beginning of the disk might improve performance.\nFor a 2.6 Linux kernel, there is no performance difference between a swap partition and an unfragmented swap file. When a swap partition/file is enabled by swapon, the 2.6 kernel finds which disk blocks the swapfile is stored on, so that when it comes time to swap, it doesn't have to deal with the filesystem at all.\n\nThus, if the swapfile isn't fragmented, it's exactly as if there were a swap partition at its same location. Or put another way, you'd get identical performance if you used a swap partition raw, or formatted it with a filesystem and then created a swapfile that filled all space, since either way on that disk there is a contiguous region used for swapping, which the kernel uses directly.\nSo if one creates the swapfile when the filesystem is fresh (thus ensuring it's not fragmented and at the beginning of the volume), performance should be identical to having a swap partition just before the volume. Further, if one created the swapfile say in the middle of the volume, with files on either side, one might get better performance, since there's less seeking to swap.\nOn Linux, if the swapfile is created unfragmented, and never expanded, it cannot become fragmented, at least with normal filesystems like ext3/4. It will always use the same disk blocks, which are contiguous.\nI conclude that about the only benefit of a dedicated swap partition is guaranteed unfragmentation when you need to expand it; if your swap will never be expanded, a file created on a fresh filesystem doesn't require an extra partition.",
        "url": "https://serverfault.com/questions/25653/swap-partition-vs-file-for-performance"
    },
    {
        "title": "What causes SSH error: kex_exchange_identification: Connection closed by remote host?",
        "question": "I setup a SSH server online that is publicly accessible by anyone. Therefore, I get a lot of connections from IPs all over the world. Weirdly, none actually try to authenticate to open a session.\nI can myself connect and authenticate without any problem.\nFrom time to time, I get the error: kex_exchange_identification: Connection closed by remote host in the server logs. What causes that?\nHere is 30 minutes of SSH logs (public IPs have been redacted):\n# journalctl SYSLOG_IDENTIFIER=sshd -S \"03:30:00\" -U \"04:00:00\"\n-- Logs begin at Fri 2020-01-31 09:26:25 UTC, end at Mon 2020-04-20 08:01:15 UTC. --\nApr 20 03:39:48 myhostname sshd[18438]: Connection from x.x.x.207 port 39332 on 10.0.0.11 port 22 rdomain \"\"\nApr 20 03:39:48 myhostname sshd[18439]: Connection from x.x.x.207 port 39334 on 10.0.0.11 port 22 rdomain \"\"\nApr 20 03:39:48 myhostname sshd[18438]: Connection closed by x.x.x.207 port 39332 [preauth]\nApr 20 03:39:48 myhostname sshd[18439]: Connection closed by x.x.x.207 port 39334 [preauth]\nApr 20 03:59:36 myhostname sshd[22186]: Connection from x.x.x.83 port 34876 on 10.0.0.11 port 22 rdomain \"\"\nApr 20 03:59:36 myhostname sshd[22186]: error: kex_exchange_identification: Connection closed by remote host\n\nAnd here is my SSH configuration:\n# ssh -V\nOpenSSH_8.2p1, OpenSSL 1.1.1d  10 Sep 2019\n# cat /etc/ssh/sshd_config \nUsePAM yes\nAddressFamily any\nPort 22\nX11Forwarding no\nPermitRootLogin prohibit-password\nGatewayPorts no\nPasswordAuthentication no\nChallengeResponseAuthentication no\nPrintMotd no # handled by pam_motd\nAuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2 /etc/ssh/authorized_keys.d/%u\nHostKey /etc/ssh/ssh_host_rsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\nKexAlgorithms [email\u00a0protected],diffie-hellman-group-exchange-sha256\nCiphers [email\u00a0protected],[email\u00a0protected],[email\u00a0protected],aes256-ctr,aes192-ctr,aes128-ctr\nMACs [email\u00a0protected],[email\u00a0protected],[email\u00a0protected],hmac-sha2-512,hmac-sha2-256,[email\u00a0protected]\nLogLevel VERBOSE\nUseDNS no\nAllowUsers root\nAuthenticationMethods publickey\nMaxStartups 3:100:60\n\n\nAfter searching the web, I have seen references to MaxStartups indicating that it could be the reason for this error but after changing the default value as shown in my sshd_config and attempting more than 3 connections, the server unambiguously indicates the probem\nApr 20 07:26:59 myhostname sshd[31468]: drop connection #3 from [x.x.x.226]:54986 on [10.0.0.11]:22 past MaxStartups\n\nSo, what causes error: kex_exchange_identification: Connection closed by remote host?",
        "top_answer": "\nWeirdly, none actually try to authenticate to open a session.\n\nSome spiders and services like Shodan scans public ipv4 addresses for open services, e.g. salt masters, ftp servers, RDPs, and also SSH services. These spiders usually only connect to the services without doing any valid authentication steps.\n\nI get the error: kex_exchange_identification: Connection closed by remote host in the server logs. What causes that?\n\nI haven't found conclusive answers about that, so... time to browse the source then. \nIn OpenSSH source code, kex_exchange_identification is a function to exchange server and client identification (duh), and the specified error happened if the socket connection between OpenSSH server and client is interrupted (see EPIPE), i.e. client already closed its connection.",
        "url": "https://serverfault.com/questions/1015547/what-causes-ssh-error-kex-exchange-identification-connection-closed-by-remote"
    },
    {
        "title": "GPG does not have enough entropy",
        "question": "I've got a ton of processes running in the background to try and get enough entropy, but I am still failing.\n**We need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\nNot enough random bytes available.  Please do some other work to give\nthe OS a chance to collect more entropy! (Need 210 more bytes)**\n\nI need a method to generate the key that works, cause what I'm trying to do is failing apparently.",
        "top_answer": "Have you had a look at RNG? \nFedora/Rh/Centos types: sudo yum install rng-tools\nOn deb types: sudo apt-get install rng-tools to set it up.\nThen run sudo rngd -r /dev/urandom before generating the keys.\nReference: http://it.toolbox.com/blogs/lim/how-to-generate-enough-entropy-for-gpg-key-generation-process-on-fedora-linux-38022",
        "url": "https://serverfault.com/questions/214605/gpg-does-not-have-enough-entropy"
    },
    {
        "title": "How do I create user accounts from the Terminal in Mac OS X 10.5?",
        "question": "I would like to be able to create new users in Mac OS X 10.5 remotely after ssh'ing into the machine.  How do I do this?",
        "top_answer": "Use the dscl command.  This example would create the user \"luser\", like so:\ndscl . -create /Users/luser\ndscl . -create /Users/luser UserShell /bin/bash\ndscl . -create /Users/luser RealName \"Lucius Q. User\"\ndscl . -create /Users/luser UniqueID \"1010\"\ndscl . -create /Users/luser PrimaryGroupID 80\ndscl . -create /Users/luser NFSHomeDirectory /Users/luser\n\nYou can then use passwd to change the user's password, or use:\ndscl . -passwd /Users/luser password\n\nYou'll have to create /Users/luser for the user's home directory and change ownership so the user can access it, and be sure that the UniqueID is in fact unique.\nThis line will add the user to the administrator's group:\ndscl . -append /Groups/admin GroupMembership luser\n",
        "url": "https://serverfault.com/questions/20702/how-do-i-create-user-accounts-from-the-terminal-in-mac-os-x-10-5"
    },
    {
        "title": "What is ADFS (Active Directory Federation Services)?",
        "question": "So I've been told that our PHP application may need to support authentication using ADFS.\n\n\nFor a non-Microsoft person, what is ADFS?\nHow does it differ to things like LDAP?\nHow does it work?  What kind of information would be included in a typical request to an ADFS server?  Is it designed for both authentication and authorization?\nAre ADFS servers typically accessible from the internet (whereas corporate AD domain controllers would not be)?\n\n\nI've tried reading some of the Technet docs, but it's full of Microsoft-speak that isn't hugely helpful.\nWikipedia is better (see below), but perhaps some of the ServerFault community can fill in some of the gaps.\n\nActive Directory Federation Services (ADFS) is a software component developed by Microsoft that can be installed on Windows Server operating systems to provide users with single sign-on access to systems and applications located across organizational boundaries. It uses a claims-based access control authorization model to maintain application security and implement federated identity.\nClaims-based authentication is the process of authenticating a user based on a set of claims about its identity contained in a trusted token. \nIn ADFS, identity federation is established between two organizations by establishing trust between two security realms. A federation server on one side (the Accounts side) authenticates the user through the standard means in Active Directory Domain Services and then issues a token containing a series of claims about the user, including its identity. On the other side, the Resources side, another federation server validates the token and issues another token for the local servers to accept the claimed identity. This allows a system to provide controlled access to its resources or services to a user that belongs to another security realm without requiring the user to authenticate directly to the system and without the two systems sharing a database of user identities or passwords.\nIn practice this approach is typically perceived by the user as follows:\n\nThe user logs into their local PC (as they typically would when commencing work in the morning)\nThe user needs to obtain information on a partner company's extranet website - for example to obtain pricing or product details\nThe user navigates to the partner company extranet site - for example: http://example.com\nThe partner website now does not require any password to be typed in - instead, the user credentials are passed to the partner extranet site using AD FS\nThe user is now logged into the partner website and can interact with the website 'logged in'\n\nFrom https://en.wikipedia.org/wiki/Active_Directory_Federation_Services\n",
        "top_answer": "\nFor a non-Microsoft person, what is ADFS?\n\nADFS is Microsoft's solution for Single Sign On and web based authentication.\nIt is used primarily to provide a single set of credentials that can access a variety of sites not necessarily hosted within the same domain.\n\nHow does it differ to things like LDAP?\n\nLDAP:\n\nCommunicates using TCP/UDP on port 389 (or port 636 for LDAPS)\nContains commands for searching/retrieving/adding/deleting/modifying users, profiles and other directory entries\nCan not be performed directly by a web browser, however HTTP authentication can be translated to LDAP using things like Apache's mod_authnz_ldap.\nWhen used for third-party website authentication, requires that username & password are provided to the third-party, which is not ideal for security.\nIs more of an open standard and has numerous Linux implementations.\n\nADFS:\n\nBetter designed for the web as it communicates over standard HTTPS\nFollows a safer process similar (but not exact) to OAuth where the original username/password are provided directly to the organisation's ADFS server (or a proxy, but not the third-party), which if valid, returns a unique token that can be used to access a third-party website.\nAlthough it does use make use of some open standards (HTTPS, SAML etc.) it is Microsoft-specific and requires Internet Information Services (IIS) which only runs on Windows Servers.\n\nSee also this answer on the subject.\n\nHow does it work? What kind of information would be included in a typical request to an ADFS server? Is it designed for both authentication and authorization?\n\nIt works by having a single site (site A) that hosts the ADFS / ADFS proxy servers, which has access to the credentials (usually by communicating with an Active Directory Domain Controller). It is then given a trust between other sites (sites B & C) that require authenticating through the ADFS.\nWhen a user attempts to access site B in their browser, the site redirects the user to the ADFS-proxy website (site A) which asks for their username & password, authenticates them, returns a set of cookies for remembering them, and redirects them back to the site B, along with an access token.\nIf the user then attempts to visit site C, they will also get redirected to site A for authentication from the ADFS-proxy website.  If the right cookies exist, the user will not be required to enter their password again, but get instantly redirected back to site C with a token.\nThe ADFS can be configured with specific claims (or permissions) for the user, for authorization purposes. So it can serve both roles. (Note the difference between authentication and authorization.)\nSome people prefer not to use it for authorization but instead keep the permissions management in the third-party website. The obvious downside is that both site A & B need to keep track of user accounts, while in the scenario where ADFS handles both, only the ADFS needs to be aware of the users.\n\nAre ADFS servers typically accessible from the internet (whereas corporate AD domain controllers would not be)?\n\nYes, nearly always. ADFS is based on the notion that it will be primarily used for website authentication. And is built around IIS.\nThe ADFS-proxy site is the one that is usually accessible from the internet. However the ADFS itself is not. The ADFS is generally a separate server from the ADFS-proxy.\n\nADFS Server\nServer that links to the credentials, and has the claims configuration as well as the trusts. Generally not publicly accessible.\nADFS Proxy Server\nServer that hosts the IIS instance that has the login pages for the websites requiring authentication. Communicates back to the ADFS when requiring authentication. Generally publicly accessible.\n",
        "url": "https://serverfault.com/questions/708669/what-is-adfs-active-directory-federation-services"
    },
    {
        "title": "What is the difference between Nginx variables $host, $http_host, and $server_name?",
        "question": "What is the difference between the three Nginx variables $host, $http_host, and $server_name?\nI have a rewrite rule where I'm not sure which one I should be using:\nlocation = /vb/showthread.php {\n    # /vb/showthread.php?50271-What-s-happening&p=846039\n    if ($arg_p) {\n        return 301 $scheme://$host/forum/index.php?posts/$arg_p/;\n        }\n\nI'm looking for an answer that doesn't just say 'use ___ variable in your rewrite rule' but also explains the theoretical differences between them.",
        "top_answer": "You should almost always use $host, as it's the only one guaranteed to have something sensible regardless of how the user-agent behaves, unless you specifically need the semantics of one of the other variables.\nThe difference is explained in the nginx documentation:\n\n$host contains \"in this order of precedence: host name from the request line, or host name from the 'Host' request header field, or the server name matching a request\"\n$http_host contains the content of the HTTP \"Host\" header field, if it was present in the request\n$server_name contains the server_name of the virtual host which processed the request, as it was defined in the nginx configuration. If a server contains multiple server_names, only the first one will be present in this variable.\n\nSince it is legal for user-agents to send the hostname in the request line rather than in a Host: header, though it is rarely done except when connecting to proxies, you have to account for this.\nYou also have to account for the case where the user-agent doesn't send a hostname at all, e.g. ancient HTTP/1.0 requests and modern badly-written software. You might do so by diverting them to a catch-all virtual host which doesn't serve anything, if you are serving multiple web sites, or if you only have a single web site on your server you might process everything through a single virtual host. In the latter case you have to account for this as well.\nOnly the $host variable accounts for all the possible things that a user-agent may do when forming an HTTP request.",
        "url": "https://serverfault.com/questions/706438/what-is-the-difference-between-nginx-variables-host-http-host-and-server-na"
    },
    {
        "title": "Explanation of nodev and nosuid in fstab",
        "question": "I see those two options constantly suggested on the web when someone describes how to mount a tmpfs or ramfs. Often also with noexec but I'm specifically interested in nodev and nosuid. I basically hate just blindly repeating what somebody suggested, without real understanding. And since I only see copy/paste instructions on the net regarding this, I ask here.\nThis is from documentation:\nnodev - Don't interpret block special devices on the filesystem.\nnosuid - Block the operation of suid, and sgid bits.\nBut I would like a practical explanation what could happen if I leave those two out. Let's say that I have configured tmpfs or ramfs(without these two mentioned options set) that is accessible(read+write) by a specific (non-root)user on the system. What can that user do to harm the system? Excluding the case of consuming all available system memory in case of ramfs",
        "top_answer": "You don't have to follow this blindly as a hard rule. But the reasoning for more security-focused situations is as follows.\n\nThe nodev mount option specifies that the filesystem cannot contain special devices: This is a security precaution. You don't want a user world-accessible filesystem like this to have the potential for the creation of character devices or access to random device hardware.\n\nThe nosuid mount option specifies that the filesystem cannot contain set userid files. Preventing setuid binaries on a world-writable filesystem makes sense because there's a risk of root escalation or other awfulness there.\n\n\nFor what it's worth, I don't use these parameters often... only on public facing systems where there are other compliance considerations.",
        "url": "https://serverfault.com/questions/547237/explanation-of-nodev-and-nosuid-in-fstab"
    },
    {
        "title": "Why do systems generally disable virtualization by default in BIOS settings?",
        "question": "I have yet to see a system whose default configuration enables MMU and directed I/O virtualization. Often this necessitates rebooting and going into the BIOS to enable it if you want, e.g., 64-bit support on your VMs.\nIs there some kind of substantial processor overhead that occurs if this is switched on and you're not using virtualization? If not, then what's the reason for it being off by default?",
        "top_answer": "There were some proof-of-concept rootkits like Blue Pill a while back that could own a system with VT on. After this discovery, most vendors began shipping their units with VT disabled as a general security precaution. ",
        "url": "https://serverfault.com/questions/390012/why-do-systems-generally-disable-virtualization-by-default-in-bios-settings"
    },
    {
        "title": "Does the \"bs\" option in \"dd\" really improve the speed?",
        "question": "Every now and then, I'm told that to increase the speed of a \"dd\" I should carefully choose a proper \"block size\".\nEven here, on ServerFault, someone else wrote that \"...the optimum block size is hardware dependent...\" (iain) or \"...the perfect size will depend on your system bus, hard drive controller, the particular drive itself, and the drivers for each of those...\" (chris-s)\nAs my feeling was a bit different (BTW: I tought that the time needed to deeply-tune the bs parameter was much higher than the gain received, in terms of time-saved, and that the default was reasonable), today I just went through some quick-and-dirty benchmarks.\nIn order to lower external influences, I decided to read:\n\nfrom an external MMC card\nfrom an internal partition \n\nand:\n\nwith related filesystems umounted \nsending the output to /dev/null to avoid issues related to \"writing speed\";\navoiding some basic issues of HDD-caching, at least when involving the HDD.\n\nIn the following table, I've reported my findings, reading 1GB of data with different values of \"bs\" (you can find the raw numbers at the end of this message):\n\nBasically it cames out that:\n\nMMC: with a bs=4 (yes! 4 bytes), I reached a throughput of 12MB/s. A not so distant values wrt to the maximum 14.2/14.3 that I got from bs=5 and above;\nHDD: with a bs=10 I reached 30 MB/s. Surely lower than the 95.3 MB got with the default bs=512 but... significant as well.\n\nAlso, it was very clear that the CPU sys-time was inversely proportional to the bs value (but this sounds reasonable, as the lower the bs, the higher the number of sys-calls generated by dd).\nHaving said all the above, now the question: can someone explain (a kernel hacker?) what are the major component/systems involved in such throughput, and if it really worth the effort in specifying a bs higher than the default?\n\nMMC case - raw numbers\nbs=1M\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=1M count=1000\n1000+0 record dentro\n1000+0 record fuori\n1048576000 byte (1,0 GB) copiati, 74,1239 s, 14,1 MB/s\n\nreal    1m14.126s\nuser    0m0.008s\nsys     0m1.588s\n\nbs=1k\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=1k count=1000000\n1000000+0 record dentro\n1000000+0 record fuori\n1024000000 byte (1,0 GB) copiati, 72,7795 s, 14,1 MB/s\n\nreal    1m12.782s\nuser    0m0.244s\nsys     0m2.092s\n\nbs=512\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=512 count=2000000\n2000000+0 record dentro\n2000000+0 record fuori\n1024000000 byte (1,0 GB) copiati, 72,867 s, 14,1 MB/s\n\nreal    1m12.869s\nuser    0m0.324s\nsys     0m2.620s\n\nbs=10\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=10 count=100000000\n100000000+0 record dentro\n100000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 70,1662 s, 14,3 MB/s\n\nreal    1m10.169s\nuser    0m6.272s\nsys     0m28.712s\n\nbs=5\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=5 count=200000000\n200000000+0 record dentro\n200000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 70,415 s, 14,2 MB/s\n\nreal    1m10.417s\nuser    0m11.604s\nsys     0m55.984s\n\nbs=4\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=4 count=250000000\n250000000+0 record dentro\n250000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 80,9114 s, 12,4 MB/s\n\nreal    1m20.914s\nuser    0m14.436s\nsys     1m6.236s\n\nbs=2\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=2 count=500000000\n500000000+0 record dentro\n500000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 161,974 s, 6,2 MB/s\n\nreal    2m41.976s\nuser    0m28.220s\nsys     2m13.292s\n\nbs=1\nroot@iMac-Chiara:/tmp# time dd if=/dev/sdc of=/dev/null bs=1 count=1000000000\n1000000000+0 record dentro\n1000000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 325,316 s, 3,1 MB/s\n\nreal    5m25.318s\nuser    0m56.212s\nsys     4m28.176s\n\n\nHDD case - raw numbers\nbs=1\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=1 count=1000000000\n1000000000+0 record dentro\n1000000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 341,461 s, 2,9 MB/s\n\nreal    5m41.463s\nuser    0m56.000s\nsys 4m44.340s\n\nbs=2\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=2 count=500000000\n500000000+0 record dentro\n500000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 164,072 s, 6,1 MB/s\n\nreal    2m44.074s\nuser    0m28.584s\nsys 2m14.628s\n\nbs=4\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=4 count=250000000\n250000000+0 record dentro\n250000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 81,471 s, 12,3 MB/s\n\nreal    1m21.473s\nuser    0m14.824s\nsys 1m6.416s\n\nbs=5\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=5 count=200000000\n200000000+0 record dentro\n200000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 66,0327 s, 15,1 MB/s\n\nreal    1m6.035s\nuser    0m11.176s\nsys 0m54.668s\n\nbs=10\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=10 count=100000000\n100000000+0 record dentro\n100000000+0 record fuori\n1000000000 byte (1,0 GB) copiati, 33,4151 s, 29,9 MB/s\n\nreal    0m33.417s\nuser    0m5.692s\nsys 0m27.624s\n\nbs=512 (offsetting the read, to avoid caching)\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=512 count=2000000 skip=6000000\n2000000+0 record dentro\n2000000+0 record fuori\n1024000000 byte (1,0 GB) copiati, 10,7437 s, 95,3 MB/s\n\nreal    0m10.746s\nuser    0m0.360s\nsys 0m2.428s\n\nbs=1k (offsetting the read, to avoid caching)\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=1k count=1000000 skip=6000000\n1000000+0 record dentro\n1000000+0 record fuori\n1024000000 byte (1,0 GB) copiati, 10,6561 s, 96,1 MB/s\n\nreal    0m10.658s\nuser    0m0.164s\nsys 0m1.772s\n\nbs=1k (offsetting the read, to avoid caching)\nroot@iMac-Chiara:/tmp# time dd if=/dev/sda3 of=/dev/null bs=1M count=1000 skip=7000\n1000+0 record dentro\n1000+0 record fuori\n1048576000 byte (1,0 GB) copiati, 10,7391 s, 97,6 MB/s\n\nreal    0m10.792s\nuser    0m0.008s\nsys 0m1.144s\n",
        "top_answer": "What you have done is only a read speed test.  if you are actually copying blocks to another device you have pauses in the reading while the other device is accepting the data you want to write,  when this happens you can hit rotational latency issues on the read device (if it's a hard disk) and so it's often significantly faster to read 1M chunks off the HDD  as you come up against rotational latency less often that way.\nI know when I'm copying hard disks I get a faster rate by specifying bs=1M than by using bs=4k or the default.  I'm talking speed improvements of 30 to 300 percent.  There's no need to tune it for absolute best unless it's all you do every day.  but picking something better than the default can cut hours off the execution time.\nWhen you're using it for real try a few different numbers and send the dd process a SIGUSR1 signal to get it to issue a status report so you can see how it's going.\n\u2717 killall -SIGUSR1 dd\n1811+1 records in\n1811+1 records out\n1899528192 bytes (1.9 GB, 1.8 GiB) copied, 468.633 s, 4.1 MB/s\n",
        "url": "https://serverfault.com/questions/650086/does-the-bs-option-in-dd-really-improve-the-speed"
    },
    {
        "title": "Linux - Is there a way to prevent/protect a file from being deleted even by root?",
        "question": "I have a very important file which an application in my workplace uses, i need to make sure it is not delete whatsoever, how can I do that?",
        "top_answer": "Yes, you can change the attributes of the file to read-only.\nThe command is:\nchattr +i filename\n\nAnd to disable it:\nchattr -i filename\n\nFrom man chattr:\n\nA file with the i attribute cannot be modified: it cannot be  deleted or  renamed,  no  link  can  be created to this file and no data can be written to the file.  Only the superuser or a  process  possessing  the CAP_LINUX_IMMUTABLE capability can set or clear this attribute.\n",
        "url": "https://serverfault.com/questions/648573/linux-is-there-a-way-to-prevent-protect-a-file-from-being-deleted-even-by-root"
    },
    {
        "title": "How can I set a short timeout with the ping command?",
        "question": "I am trying to write a script that lists all the hosts on my LAN (there a about 20 of them) and writes the ping status next to each host.  I have the DHCP leases file, so I have all the IPs (say, 10.0.0.1, 10.0.0.2, etc.), all I need is the ping status for each host.\nSo, my script launches a single ping for each host:\nping -c 1 10.0.0.1\n\nUnfortunately, when a host is offline, the ping takes a long time to timeout. I checked man ping, there seem to be two options to set the timeout delay: -w deadline and -W timeout. I think I'm interested in the latter.\nSo I tried this:\nping -c 1 -W 1 10.0.0.1\n\nBut waiting one second per offline host is still too long.  I tried to set it to below a second, but it does not seem to take the parameter into account at all:\nping -c 1 -W 0.1 10.0.0.1  # timeout option is ignored, apparently\n\nIs there a way to set the timeout to a lower value? If not, are there any alternatives?\nEdit\n\nThe O.S. is Debian Lenny.\nThe hosts I am trying to ping are actually access points.  They are on the same vlan and subnet as the users (for simplicity of deployment and replacement). This is why I do not want to scan all the subnet (with a ping -b for example).\n\nEdit #2\nI accepted the fping solution (thanks for all the other answers).  This command does exactly what I was looking for:\nfping -c1 -t500 10.0.0.1 10.0.0.2 10.0.0.3 10.0.0.4\n\nThis command takes at most 500ms to complete, and gives me the ping status of all the hosts at once:\n10.0.0.1 : [0], 84 bytes, 5.71 ms (5.71 avg, 0% loss)\n10.0.0.2 : [0], 84 bytes, 7.95 ms (7.95 avg, 0% loss)\n10.0.0.3 : [0], 84 bytes, 16.1 ms (16.1 avg, 0% loss)\n10.0.0.4 : [0], 84 bytes, 48.0 ms (48.0 avg, 0% loss)\n\n10.0.0.1 : xmt/rcv/%loss = 1/1/0%, min/avg/max = 5.71/5.71/5.71\n10.0.0.2 : xmt/rcv/%loss = 1/1/0%, min/avg/max = 7.95/7.95/7.95\n10.0.0.3 : xmt/rcv/%loss = 1/1/0%, min/avg/max = 16.1/16.1/16.1\n10.0.0.4 : xmt/rcv/%loss = 1/1/0%, min/avg/max = 48.0/48.0/48.0\n\nOn Debian Lenny, installation is trivial:\naptitude update\naptitude install fping\n",
        "top_answer": "fping might be a better tool than the stock ping you are using.  What OS are you on?\n\n\"fping differs from ping in that you can specify any number of targets on the command line, or specify a file containing the lists of targets to ping.\"\n\"Instead of sending to one target until it times out or replies, fping will send out a ping packet and move on to the next target in a round-robin fashion.\"\n\"Unlike ping, fping is meant to be used in scripts, so its output is designed to be easy to parse.\"\n\nExample:\nfping -c1 -t500 10.0.0.1 10.0.0.2 10.0.0.3 10.0.0.4\n\n\n-c Number of request packets to send to each target.\n-t Initial target timeout in milliseconds\n",
        "url": "https://serverfault.com/questions/200468/how-can-i-set-a-short-timeout-with-the-ping-command"
    },
    {
        "title": "Is there a proper way to clear logs?",
        "question": "I was wondering if there was a proper way to clear logs in general?\nI'm new to Ubuntu and I'm trying to set up Postfix. The log in question is /var/log/mail.log. I was wondering if there was a correct way to clear it, rather than me going in it and deleting all the lines and saving it. I find that sometimes errors don't get written to it immediately after I clear the log and save it.\nSide note: I'm having trouble setting up Postfix and am trying to make it easier for me to read the logs hoping it can help me out, instead of having to scroll all the way down.",
        "top_answer": "You can use:\n> /var/log/mail.log\n\nThat will truncate the log without you having to edit the file. It's also a reliable way of getting the space back.\nIn general it's a bad thing to use rm on the log then recreating the filename, if another process has the file open then you don't get the space back until that process closes it's handle on it and you can damage it's permissions in ways that are not immediately obvious but cause more problems later on.\nYasar has a nice answer using truncate\nAlso if you are watching the contents of the log you might like to use the tail command:\ntail -f /var/log/mail.log\n\nCtrl-C will break off the tailing.",
        "url": "https://serverfault.com/questions/285843/is-there-a-proper-way-to-clear-logs"
    },
    {
        "title": "dig show only answer",
        "question": "I want dig only to show the answer of my query.\nNormally, it prints out alot of additional info like this:\n;; <<>> DiG 9.7.3 <<>> google.de\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55839\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;google.de.                     IN      A\n\n;; ANSWER SECTION:\ngoogle.de.              208     IN      A       173.194.69.94\n\n;; Query time: 0 msec\n;; SERVER: 213.133.99.99#53(213.133.99.99)\n;; WHEN: Sun Sep 23 10:02:34 2012\n;; MSG SIZE  rcvd: 43\n\nI want this to be reduced to just the answer section.\ndig has alot of options, a good one i found was +noall +answer\n; <<>> DiG 9.7.3 <<>> google.de +noall +answer\n;; global options: +cmd\ngoogle.de.              145     IN      A       173.194.69.94\n\nIt leaves out most of the stuff, but still shows this options thing.\nAny ideas on how to remove it using dig options? I sure could cut it out using other tools, but a option with dig itself would be the cleanest and nicest.",
        "top_answer": "I am not sure why you are getting comments in the output.  That is the correct set of options for the behaviour you want.  Here are the same options with the same version of dig:\n$ dig -version\nDiG 9.7.3\n$ dig +noall +answer google.de\ngoogle.de.      55  IN  A   173.194.44.216\ngoogle.de.      55  IN  A   173.194.44.223\ngoogle.de.      55  IN  A   173.194.44.215\n$\n",
        "url": "https://serverfault.com/questions/431080/dig-show-only-answer"
    },
    {
        "title": "What is the literal escape character in Powershell?",
        "question": "When including a literal quote character inside a quoted string in Powershell, how do I escape the quote character to indicate it is a literal instead of a string delimeter?",
        "top_answer": "From help about_quoting_rules\n\nTo make double-quotation marks appear in a string,\nenclose the entire string in single quotation marks. For example:\n'As they say, \"live and learn.\"'\n\nThe output from this command is:\nAs they say, \"live and learn.\"\n\nYou can also enclose a single-quoted string in a double-quoted string.\nFor example:\n\"As they say, 'live and learn.'\"\n\nThe output from this command is:\nAs they say, 'live and learn.'\n\nTo force Windows PowerShell to interpret a double quotation mark literally,\nuse a backtick character. This prevents Windows PowerShell from interpreting\nthe quotation mark as a string delimiter. For example:\n\"Use a quotation mark (`\") to begin a string.\"\n\nThe output from this command is:\nUse a quotation mark (\") to begin a string.\n\nBecause the contents of single-quoted strings are interpreted literally,\nyou cannot use the backtick character to force a literal character\ninterpretation in a single-quoted string.\n\nThe use of the backtick character to escape other quotation marks in single quoted strings is not supported in recent versions of PowerShell. In earlier versions of PowerShell the backtick escape character could be used to escape a double quotation mark character within a single quoted string as detailed in the help about_quoting document that is available in those versions of PowerShell.",
        "url": "https://serverfault.com/questions/47811/what-is-the-literal-escape-character-in-powershell"
    },
    {
        "title": "How to sort ps output by process start time?",
        "question": "Is there a way to sort ps output by process start time, so newest are either at the top or bottom ?\nOn Linux ?\nOn SysV5 ?\nOn Mac ?",
        "top_answer": "This should work on Linux and SysV5\nps -ef --sort=start_time\n",
        "url": "https://serverfault.com/questions/27887/how-to-sort-ps-output-by-process-start-time"
    },
    {
        "title": "Why don't EC2 ubuntu images have swap?",
        "question": "I started a couple servers on EC2 and they don't have swap.\nAm I doing something wrong or is it that the machines just don't have any?",
        "top_answer": "You are right, the Ubuntu EC2 EBS images don't come with swap space configured (for 11.04 at least).  The \"regular\" instance-type images do have a swap partition, albeit only 896 MB on the one I tested.\nIf some process blows up and you don't have swap space, your server could come to a crawling halt for a good while before the OOM killer kicks in, whereas with swap, it merely gets slow.  For that reason, I always like to have swap space around, even with enough RAM. Here's your options:\n\nCreate an EBS volume (2-4 times the size of your RAM), attach it to your instance (I like calling it /dev/xvdm for \"memory\"), sudo mkswap /dev/xvdm, add it to fstab, sudo swapon -a, and you're good to go. I have done this before and it works fine, but it is probably a bit slower than instance store because it goes over the network.\nOr you might be able to repartition your disk to add a swap partition, though this might require creating a new AMI. I have not been able to do this in a running instance, because I cannot unmount the root file system, and I do not even have access to the disk device (/dev/xvda), only the partition (xvda1).\nOr you can create a swap file. This is my preferred solution right now.\nsudo dd if=/dev/zero of=/var/swapfile bs=1M count=2048 &&\nsudo chmod 600 /var/swapfile &&\nsudo mkswap /var/swapfile &&\necho /var/swapfile none swap defaults 0 0 | sudo tee -a /etc/fstab &&\nsudo swapon -a\n\nDone. :) I know a lot of people feel icky about using files instead of partitions, but it certainly works well enough as emergency swap space.\n",
        "url": "https://serverfault.com/questions/218750/why-dont-ec2-ubuntu-images-have-swap"
    },
    {
        "title": "How to change the name and description of an AWS EC2 security group?",
        "question": "How do you change the name and description of a security group in AWS EC2?\nMy security group is named quick-start-1 (the default) and I want to change it to \"HTTP, HTTPS and Limited SSH\".",
        "top_answer": "It's not possible to rename a security group, by GUI or by API.\nFor VPC EC2 instances\nYou can dynamically assign security groups assigned to VPC EC2 instances. Create a new SG with the desired name and the same rules.\nEC2 classic instances\nIt's not possible to change the security group that is assigned to EC2 classic instances.\nIf you must change the security group for an EC2 classic instance, then you need to:\n\nCreate an AMI from your instance, then\nLaunch a new copy of your instance from the AMI created in step #1, selecting the new security group at launch time.\n",
        "url": "https://serverfault.com/questions/463550/how-to-change-the-name-and-description-of-an-aws-ec2-security-group"
    },
    {
        "title": "zcat/gzcat works in linux, not on osx. general linux/osx compatibility",
        "question": "First the specific problem:\nIn linux, I use zcat to list a .zip file. In osx, zcat seems to automatically append .Z to the file name. Various people suggest replacing zcat with gzcat; however, gzcat complains that the file is not in gzip format! \n'file ' shows this:\n...Zip archive data, at least v2.0 to extract\nSo neither zcat or gzcat will work in osx, what can I do?\nI have a medium sized script in in bash which uses, zcat/gzcat, sed awk and other basic utilities to process a number of files. I'd like to duplicate that environment on my osx laptop so I can work offline. Any general suggestions how I can avoid such pain? I expect this is a fairly routine workflow so must have been sorted out by others.",
        "top_answer": "You are right. It's annoying behavior.\n$ zcat foo.txt.gz \nzcat: can't stat: foo.txt.gz (foo.txt.gz.Z): No such file or directory\n\nTry this:\n$ zcat < foo.txt.gz \nasdfadsf\n",
        "url": "https://serverfault.com/questions/570024/zcat-gzcat-works-in-linux-not-on-osx-general-linux-osx-compatibility"
    },
    {
        "title": "Command line program to test DHCP service",
        "question": "Is there a simple ping-like command to test whether a DHCP service is running on a network? ...on Linux",
        "top_answer": "Even if you don't use a Nagios monitoring server, you can grep the check_dhcp binary or compile it from source and use the check to test a DHCP-Server. E.g.\n./check_dhcp -v -s <dhcp_server_address> -r <expected_ip_address> -m <mac_address_touse>\n",
        "url": "https://serverfault.com/questions/171744/command-line-program-to-test-dhcp-service"
    },
    {
        "title": "How do I make a connection private on Windows Server 2012 R2",
        "question": "After a restart of one of our servers (a Windows Server 2012 R2), all private connections become public and vice versa (this user had the same problem). Stuff like pinging and iSCSI stopped working, and after some investigation it turned out this was the cause.\nThe problem is that I don't know how to make them private again. Left-clicking the network icon in the tray shows the \"modern\" sidebar, but it only shows a list of connections, and right-clicking them doesn't show any options.\nWhat could be the problem, and is there a way to change these settings? I have to make one of the connections public (Internet access), and two of them private (backbone).",
        "top_answer": "Powershell. Here is an example of changing the network profile of a network interface called Ethernet1 from whatever it is now to \"Private.\" I got this info from Get-Help Set-NetConnectionProfile -Full.\nPS C:\\>$Profile = Get-NetConnectionProfile -InterfaceAlias Ethernet1\n\nPS C:\\>$Profile.NetworkCategory = \"Private\"\n\nPS C:\\>Set-NetConnectionProfile -InputObject $Profile\n\nDocumentation: https://docs.microsoft.com/en-us/powershell/module/netconnection/set-netconnectionprofile?view=winserver2012r2-ps",
        "url": "https://serverfault.com/questions/639088/how-do-i-make-a-connection-private-on-windows-server-2012-r2"
    },
    {
        "title": "bad ownership or modes for chroot directory component",
        "question": "I created the user MY_USER. Set his home dir to /var/www/RESTRICTED_DIR, which is the path he should be restricted to.\nThen I edited sshd_config and set:\nMatch user MY_USER\n  ChrootDirectory /var/www/RESTRICTED_DIR\n\nThen I restarted ssh. Made MY_USER owner (and group owner) of RESTRICTED_DIR, and chmodded it to 755. I get\nAccepted password for MY_USER\nsession opened for user MY_USER by (uid=0)\nfatal: bad ownership or modes for chroot directory component \"/var/www/RESTRICTED_DIR\"\npam_unix(sshd:session): session closed for user MY_USER\n\nIf I removed the 2 lines from sshd_config the user can login successfully. Of course it can access all the server though. What's the problem? I even tried to chown RESTRICTED_DIR to root (as I read somewhere that someone solved this same problem doing it). No luck..",
        "top_answer": "From the man page:\n\nChrootDirectory\n  Specifies the pathname of a directory to chroot(2) to after\n  authentication.  All components of the pathname must be root-owned \n  directories that are not writable by any other user or\n  group.  After the chroot, sshd(8) changes the working directory\n  to the user's home directory.\n\nMy guess is one or more of the directories on the path do not meet those requirements (my suspicion is www is owned or writable by your web user, not root).\nGo back and follow the directions, ensuring that the requirements above in bold italics are met.",
        "url": "https://serverfault.com/questions/584986/bad-ownership-or-modes-for-chroot-directory-component"
    },
    {
        "title": "How can I find the path to an executable in OSX",
        "question": "Is there a command I can use to easily find the path to an executable? I'm looking for identify on my local machine - something like pwd?\npwd identify\n=> /usr/local/bin/identify\n",
        "top_answer": "which will search your path for the arguments you supply, it's found on just about any BSD or SysV UNIX\nmoriarty:~ dave$ which bash true false\n/bin/bash\n/usr/bin/true\n/usr/bin/false\n",
        "url": "https://serverfault.com/questions/24762/how-can-i-find-the-path-to-an-executable-in-osx"
    },
    {
        "title": "protocol version mismatch -- is your shell clean?",
        "question": "When following the instructions to do rsync backups given here: http://troy.jdmz.net/rsync/index.html\nI get the error \"protocol version mismatch -- is your shell clean?\"\nI read somewhere that I needed to silence the prompt (PS1=\"\") and motd (.hushlogin) displays to deal with this.  I have done this, the prompt and login banner (MOTD) not longer appear, but the error still appears when I run:\nrsync -avvvz -e \"ssh -i /home/thisuser/cron/thishost-rsync-key\" remoteuser@remotehost:/remote/dir /this/dir/\n\nBoth ssh client and sshd server are using version 2 of the protocol.\nWhat could be the problem?\n[EDIT]\nI have found http://www.eng.cam.ac.uk/help/jpmg/ssh/authorized_keys_howto.html\nwhich directs that it is sometimes necessary to \"Force v2 by using the -2 flag to ssh or slogin\n ssh -2 -i ~/.ssh/my_private_key remotemachine\"\n\nIt is not clear this solved the problem as I think I put this change in AFTER the error changed but the fact is the error has evolved to something else.  I'll update this when I learn more.  And I will certainly try the suggestion to run this in an emacs shell -",
        "top_answer": "One of your login scripts (.bashrc/.cshrc/etc.) is probably outputting data to the terminal (when it shouldn't be). This is causing ssh to error when it is connecting and getting ready to copy as it starts receiving extra data it doesn't expect. Remove output that is generated in the startup scripts.\nYou can check if your terminal is interactive and only output text by using the following code in a bashrc. Something equivalent exists for other shells as well:\nif shopt -q login_shell; then\n    [any code that outputs text here]\nfi\n\nor alternatively, like this, since the special parameter - contains i when the shell is interactive:\nif echo \"$-\" | grep i > /dev/null; then\n    [any code that outputs text here]\nfi\n\nFor more information see: rsync via ssh from linux to windows sbs 2003 protocol mismatch\nTo diagnose this, make sure that the following is the output you get when you ssh in to the host:\nUSER@HOSTNAME's password: \nLast login: Mon Nov  7 22:54:30 2011 from YOURIP\n[USER@HOSTNAME ~]$ \n\nIf you get any newlines or other data you know that extra output is being sent.  You could rename your .bashrc/.cshrc/.profile/etc. files to something else so that they won't output extra output.  Of course there is still system files that could cause this.  In that case, check with your sysadmin that the system files don't output data.",
        "url": "https://serverfault.com/questions/267154/protocol-version-mismatch-is-your-shell-clean"
    },
    {
        "title": "Is a CNAME to CNAME chain allowed?",
        "question": "Is it allowed in DNS to have a CNAME record that points to another CNAME record?\nThe reason we need this is that we have a hostname that we want to be looked up to the IP address of our web server computer. We also have another web server computer stand by that could be activated in case the first one would die. In such a case we would quickly need to point the hostname to the IP address of the stand by web server computer.\nUnfortunately the hostname resides in a DNS domain where any change would take long time due to manual operation dependent on other sysadmins. But we have another DNS domain where we can perform the changes ourselves quickly. Having CNAME to CNAME chain seems like a possible solution. But is it allowed? Will web browsers understand it?",
        "top_answer": "From RFC 1034 - Domain names - concepts and facilities:\n\nDomain names in RRs which point at another name should always point at\n  the primary name and not the alias.  This avoids extra indirections in\n  accessing information.  For example, the address to name RR for the\n  above host should be:\n52.0.0.10.IN-ADDR.ARPA  IN      PTR     C.ISI.EDU\n\nrather than pointing at USC-ISIC.ARPA.  Of course, by the robustness\n  principle, domain software should not fail when presented with CNAME\n  chains or loops; CNAME chains should be followed and CNAME loops\n  signalled as an error.\n\nSo yes, it is allowed and properly written software will handle it just OK. CNAME chains aren't however considered good practice and impose an overhead on the infrastructure.",
        "url": "https://serverfault.com/questions/309651/is-a-cname-to-cname-chain-allowed"
    },
    {
        "title": "How do you free up a port being held open by dead process?",
        "question": "A colleague of mine recently ran into a problem where a process that had supposedly died was still bound to a network port, preventing other processes from binding to that port.  Specifically, netstat -a -b was reporting that a process named System with PID 4476 had port 60001 open, except no process with PID 4476 existed, at least as far as I could tell.\nProcess Explorer and Task Manager did not list PID 4476 (though there was another process named System with PID 4, which had its own set of TCP connections that did not include 60001).  taskkill /PID 4476 also reported that PID 4476 could not be found.\nIs there a way to kill this mysterious System process to free up the port to which it's currently bound?  What can cause this to happen?  How can there be processes that none of Task Manager, Process Explorer, and taskkill don't know about?  Rebooting managed to fix the problem, but I'd like to know if there's a way to fix this without rebooting.",
        "top_answer": "I know this is an old thread, but in case anyone else is having the same issue, I had...\nWhat may be happening is that your process had a TCP port open when it crashed or otherwise exited without explicitly closing it. Normally the OS cleans up these sorts of things, but only when the process record goes away. While the process may not appear to be running any more, there is at least one thing that can keep a record of it around, in order to prevent reuse of its PID. This is the existence of a child process that is not detached from the parent.\nIf your program spawned any processes while it was running, try killing them. That should cause its process record to be freed and the TCP port to be cleaned up. Apparently windows does this when the record is released not when the process exits as I would have expected.",
        "url": "https://serverfault.com/questions/181015/how-do-you-free-up-a-port-being-held-open-by-dead-process"
    },
    {
        "title": "Why do I need nginx when I have uWSGI",
        "question": "There are many tutorials on how to configure nginx to cooperate with uWGSI when I want to deploy Django application. \nBut why do I need nginx in this kit? uWSGI itself can serve WSGI Python applications, it can serve static files, it can also do SSL. What can nginx do which uWSGI can not?",
        "top_answer": "You don't.\nThat's the simple answer, anyway -- you don't need it. uWSGI is itself a capable server.\nHowever, other servers like nginx have been around longer and are (probably, anyway) more secure, as well as having additional features not supported by uWSGI -- for example, improved handling of static resources (via any combination of Expires or E-Tag headers, gzip compression, pre-compressed gzip, etc.) that can significantly reduce server and network load; additionally, a server like nginx in front of your Django application can implement caching of your dynamic content as well, further helping to reduce server load, and even helping to facilitate the use of a CDN (which normally don't do well with dynamic content). You could even go further and have nginx on a completely separate server, reverse proxying requests for dynamic content to a load balanced cluster of application servers while handling the static content itself.\nFor example, my blog (while it is WordPress, it does have nginx in front of it) is tuned to cache posts for 24 hours, and to cache index pages for 5 minutes; while I don't see enough traffic for that to really matter most of the time, it helps my tiny little VPS weather the occasional surge that might otherwise knock it down -- such as the big surge of traffic when one of my articles got picked up by a Twitterer with many thousands of followers, many of whom re-tweeted it to their thousands of followers.\nIf I had been running a \"bare\" uWSGI server (and assuming it had been a Django site, rather than WordPress), it might have stood up to it just fine -- or it might have crashed and burned, costing me in missed visitors. Having nginx in front of it to handle that load can really help.\nAll that being said, if you're just running a little site that won't see a lot of traffic, there's no real need for nginx or anything else -- just use uWSGI on its own if that's what you want to do. On the other hand, if you'll see a lot of traffic... well, you still might want uWSGI, but you should at least consider something in front of it to help with the load. Actually, you should really be load-testing different configurations with your finished site to determine what does work best for you under your expected load, and use whatever that ends up being.",
        "url": "https://serverfault.com/questions/590819/why-do-i-need-nginx-when-i-have-uwsgi"
    },
    {
        "title": "Should network hardware be set to \"autonegotiate\" speeds or fixed speeds?",
        "question": "We recently had a little problem with networking where multiple servers would intermittently lose network connectivity in a fairly painful-to-resolve way (required hard reboot). This has been going on for about two weeks, seemingly at random, on different servers. No particular pattern that we could discern to it.\nAfter some digging into it, we saw that the switch was reporting 100 Mbps for the problem port:\n\nThis sounds remarkably like what happened in the Joel Spolsky article Five Whys\n\nMichael spent some time doing a post-mortem, and discovered that the problem was a simple configuration problem on the switch. There are several possible speeds that a switch can use to communicate (10, 100, or 1000 megabits/second). You can either set the speed manually, or you can let the switch automatically negotiate the highest speed that both sides can work with. The switch that failed had been set to autonegotiate. This usually works, but not always, and on the morning of January 10th, it didn\u2019t.\n\nWe have now disabled auto-negotiate on our network hardware and set it to a fixed rate of 1000 Mbps (gigabit).\nMy questions to those with more server hardware networking expertise:\n\nHow common are auto-negotiate problems with modern networking hardware?\nIs it considered good, standard networking practice to disable auto-negotiate and set fixed speeds when setting up networking?\n",
        "top_answer": "\nI have yet to see a problem with auto-negotiation of network speeds that isn't caused by either (a) a mismatch of manual on one end of the link and auto on the other or (b) a failing component of the link (cable, port, etc).\nThis depends on the admin, but my experience has shown me that if you manually specify the link speeds and duplex settings, than you are bound to run into speed mismatches.  Why?  Because it is nearly impossible to document the various connections between switches and servers and then follow that documentation when making changes.  Most failures I have seen are because of 1(a) and you only get in to that situation when you start manually setting speed/duplex settings.\n\nAs mention in the Cisco documentation: \n\nIf you disable autonegotiation, it hides link drops and other physical layer problems. Only disable autonegotiation to end-devices, such as older Gigabit NICs that do not support Gigabit autonegotiation. Do not disable autonegotiation between switches unless absolutely required, as physical layer problems can go undetected and result in spanning tree loops.\n\nUnless you are prepared to setup a change management system for network changes that requires the verification of speed/duplex (and don't forget flow control) or are willing to deal with occasional mismatches that come from manually specifying these settings on all network devices, then stick with the default configuration of auto/auto.\nIn the future, consider monitoring the errors on the switch ports with MRTG so you can spot these issues before you have a problem.\nEdit: I do see a lot of people referencing negotiation failures on old equipment.  Yes this was an issue a long time ago when the standards were being created and not all devices followed them.  Are your NICs and switches less than 10 years old?  If so, then this won't be an issue.",
        "url": "https://serverfault.com/questions/106160/should-network-hardware-be-set-to-autonegotiate-speeds-or-fixed-speeds"
    },
    {
        "title": "Making `wget` not save the page",
        "question": "I'm using the wget program, but I want it not to save the html file I'm downloading. I want it to be discarded after it is received. How do I do that?",
        "top_answer": "You can redirect the output of wget to /dev/null (or NUL on Windows):\nwget http://www.example.com -O /dev/null\n\nThe file won't be written to disk, but it will be downloaded.",
        "url": "https://serverfault.com/questions/73163/making-wget-not-save-the-page"
    },
    {
        "title": "How to determine the hostname from an IP address in a Windows network?",
        "question": "My LAN has 50 Windows hosts. At the Windows command line I try\nping   to get the IP address of a running Windows machine.\nThe question is how to get hostname of a specific IP address in the same Windows workgroup?\nAnother question is how to know the hostname of Windows machine from a Linux box if I have an IP address? Which command do you use? I have one host running Kubuntu 9.04.",
        "top_answer": "If you want to determine the name of a Windows machine without DNS, you should try Nbtstat. But that will only work on Windows:\nFor example,\nNBTSTAT -A 10.10.10.10\n\nOn Linux, you should try nmblookup that does nearly the same:\nnmblookup -A 10.10.10.10\n",
        "url": "https://serverfault.com/questions/88064/how-to-determine-the-hostname-from-an-ip-address-in-a-windows-network"
    },
    {
        "title": "Modify systemd unit file without altering upstream unit file",
        "question": "I have installed the pimd service by means of apt. This comes with an upstream systemd unit file (/lib/systemd/system/pimd.service).\nI want the service to be restarted when for some reason it gets killed, hence I wish to add the line Restart = always in the unit file. \nHowever, I don\u2019t want to modify the upstream unit file.\nIs there any workaround for this?",
        "top_answer": "Systemd provides two mechanisms to edit unit files:\n\nsystemctl edit <unit file>\nE.g. systemctl edit pimd (in the absence of a file extension, systemd assumes .service). This creates /etc/systemd/system/<unit file>.d/override.conf and opens a text editor for you to enter your overriding options in systemd unit syntax. The options you enter are merged with the package's defaults at runtime, with your overrides taking precedence.\nNote that some options require 'clearing' by setting them to a blank string before setting them to the desired value on a subsequent line. See this answer on Ask Ubuntu for more info and a thorough breakdown of editing systemd units.\n\nsystemctl edit --full <unit file>\nThis is equivalent to copying <unit file> (such as pimd.service) from /usr/lib/systemd/system/ to /etc/systemd/system/. An editor opens for you to make changes, and the resulting file completely supersedes the unit file supplied by the package maintainer.\n\n\nThe former command (without --full) is recommended unless you are largely overhauling the existing unit file, as it allows for future package updates to the unit file to take effect. With a --full edit, you assume responsibility for ensuring your unit file maintains compatibility with any updates.\nUsing the systemd-provided commands is preferable to creating/copying files yourself as it's less error-prone and systemd is immediately aware of the changes. Manual file modifications require you run systemctl daemon-reload for them to take effect.\nSee man systemctl for more information on systemctl edit and other commands for working with systemd unit files.",
        "url": "https://serverfault.com/questions/840996/modify-systemd-unit-file-without-altering-upstream-unit-file"
    },
    {
        "title": "What is \"anycast\" and how is it helpful?",
        "question": "I'd never heard of anycast until a few seconds ago when I read \"What are some cool or useful server/networking tricks?\".\nThe wikipedia \"Anycast\" article on it is quite formal and doesn't really evoke a mental picture of how it would be used.\nCan someone explain in a few informal sentences what \"anycast\" is, how you configure it (just in a general sense), and what its benefits are (what does it make easier)?",
        "top_answer": "Anycast is networking technique where the same IP prefix is advertised from multiple locations. The network then decides which location to route a user request to, based on routing protocol costs and possibly the 'health' of the advertising servers.\nThere are several benefits to anycast. First, in steady state, users of an anycast service (DNS is an excellent example) will always connect to the 'closest' (from a routing protocol perspective) DNS server. This reduces latency, as well as providing a level of load-balancing (assuming that your consumers are evenly distributed around your network).\nAnother advantage is ease of configuration management. Rather than having to configure different DNS servers depending on where a server/workstation is deployed (Asia, America, Europe), you have one IP address that is configured in every location.\nDepending on how anycast is implemented, it can also provide a level of high availability. If the advertisement of the anycast route is conditional on some sort of health check (e.g. a DNS query for a well known domain, in this example), then as soon as a server fails its route can be removed. Once the network reconverges, user requests will be seamlessly forwarded to the next closest instance of DNS, without the need for any manual intervention or reconfiguration.\nA final advantage is that of horizontal scaling; if you find that one server is being overly loaded, simply deploy another one in a location that would allow it to take some proportion of the overloaded server's requests. Again, as no client configuration is required, this can be done very quickly.",
        "url": "https://serverfault.com/questions/14985/what-is-anycast-and-how-is-it-helpful"
    },
    {
        "title": "Human readable format for http headers with tcpdump",
        "question": "I would like to view the HTTP headers sent from Apache (listening on port 80) to Tomcat (on port 4080) in a Linux machine.\nAccording to Wikipedia,\n\nHeader fields are colon-separated name-value pairs in clear-text string format.\n\nI've tried some variations of the following tcpdump command:\n$ sudo tcpdump -lnX dst port 4080 -c 10\n\n11:29:28.605894 IP SOME_IP.33273 > SOME_IP.4080: P 0:49(49) ack 1 win 23 <nop,nop,timestamp 1191760962 509391143>\n    0x0000:  4500 0065 3a9f 4000 3f06 0084 628a 9ec4  E..e:.@.?...b...\n    0x0010:  628a 9c97 81f9 0ff0 9e87 eee0 144b 90e1  b............K..\n    0x0020:  8018 0017 fb43 0000 0101 080a 4708 d442  .....C......G..B\n    0x0030:  1e5c b127 4845 4144 202f 6461 7070 6572  .\\.'HEAD./dapper\n    0x0040:  5f73 6572 7669 6e67 2f41 644d 6f6e 6b65  _serving/AdMonke\n    0x0050:  793f                                     y?\n\nThe result was always the same - a strange mix of gibberish and English words (e.g. HEAD). \nHow can I view the headers in a human-readable format? ",
        "top_answer": "Here's a one-liner I came up with for displaying request and response HTTP headers using tcpdump (which should work for your case too):\nsudo tcpdump -A -s 10240 'tcp port 4080 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)' | egrep --line-buffered \"^........(GET |HTTP\\/|POST |HEAD )|^[A-Za-z0-9-]+: \" | sed -r 's/^........(GET |HTTP\\/|POST |HEAD )/\\n\\1/g'\n\nIt limits cuts the packet off at 10Kb and only knows GET, POST and HEAD commands, but that should be enough in the majority of cases.\nEDIT: modified it to get rid of the buffers at every step to make it more responsive. Needs Perl and stdbuf now though, so use the original version if you don't have those:\nEDIT: Changed script port targets from 80 to 4080, to actually listen for traffic that went through apache already, instead of direct outside traffic arriving to port 80:\nsudo stdbuf -oL -eL /usr/sbin/tcpdump -A -s 10240 \"tcp port 4080 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)\" | egrep -a --line-buffered \".+(GET |HTTP\\/|POST )|^[A-Za-z0-9-]+: \" | perl -nle 'BEGIN{$|=1} { s/.*?(GET |HTTP\\/[0-9.]* |POST )/\\n$1/g; print }'\n\nSome explanations: \n\nsudo stdbuf -oL -eL  makes tcpdump run line-buffered \nthe tcpdump magic filter is explained in detail here: https://stackoverflow.com/questions/11757477/understanding-tcpdump-filter-bit-masking\ngrep is looking for any lines with GET, HTTP/ or POST; or any lines that look like a header (letters and numbers followed by colon)\nBEGIN{$|=1} causes perl to run line-buffered\ns/.*?(GET |HTTP/[0-9.]* |POST )/\\n$1/g adds a newline before the beginning of every new request or response\n",
        "url": "https://serverfault.com/questions/504431/human-readable-format-for-http-headers-with-tcpdump"
    },
    {
        "title": "SSH Allow Password For One User, Rest Only Allow Public Keys [duplicate]",
        "question": "Is it possible with ssh to allow passwords from a certain user, but deny using passwords for everybody else?\nBasically I want to allow password auth for user justin but everybody else must use public keys.\nPasswordAuthentication no\n\nSeems to be global though, no way to specify by user.",
        "top_answer": "Necromancing, but adding the following to your sshd_config should do the trick:\nMatch User <username>\nPasswordAuthentication yes\nMatch all\n\nNote that match is effective \"until either another Match line or the end of the file.\" (The indentation isn't significant.)",
        "url": "https://serverfault.com/questions/307407/ssh-allow-password-for-one-user-rest-only-allow-public-keys"
    },
    {
        "title": "How can I list my open ports on Debian?",
        "question": "What is the command to display a list of open ports on a Debian server?\nI tried netstat -a | egrep 'Proto|LISTEN' but I would like something more specific that actually lists the port number.",
        "top_answer": " netstat -pln\n\n-l will list listening ports, -p will also display the process, -n will show port numbers instead of names. Add -t to only show TCP ports.",
        "url": "https://serverfault.com/questions/357323/how-can-i-list-my-open-ports-on-debian"
    },
    {
        "title": "pg_dump and pg_restore: input file does not appear to be a valid archive",
        "question": "I have used pg_dump on one machine and copied result file to another, where I tried to restore it. I believe schema is the same. However, I get:\n\npg_restore: [archiver] input file does not appear to be a valid archive\n\nI have done following operations:\npg_dump -a -f db.txt dbname\n\nAnd:\npg_restore -a -d dbname db.txt\n\nWhat might be wrong?",
        "top_answer": "You are dumping in plain SQL format which was designed to feed to psql. This is not recognized by pg_restore.\ncat db.txt | psql dbname\n\nShould do the trick",
        "url": "https://serverfault.com/questions/260607/pg-dump-and-pg-restore-input-file-does-not-appear-to-be-a-valid-archive"
    },
    {
        "title": "Amazon Linux vs. Ubuntu for Amazon EC2 [closed]",
        "question": "I'm setting up my first website on Amazon EC2, and I'm trying to decide which distro to use. I've used Redhat and CentOS in the past, but I have no bias towards any system, I just want to use whatever is best (I also have had partially-managed servers in the past, so I haven't done too much server administration until recently). The website is just a web app written in PHP and MongoDB.\nI like the idea of having a lightweight OS that is described for Amazon Linux, but I worry that it could suffer in compatibility/updates compared to Ubuntu or other options that have teams focused exclusively on a server OS. Any advice?",
        "top_answer": "I was in a similar situation; fully managed dedicated server, LAMP, CentOS.  Then we decided to move to EC2.  Also, I had very little systems or linux administration experience.  I have almost zero experience with Ubuntu, so I really cannot speak to which is the so-called better OS.\nI tried a bunch of pre-built AMI's with minimal OS installs from Rightscale, Alestic, Scalr and Amazon.  I ended up building all my own AMI's on top of Amazon Linux, first using version 2010.11.01, now I've migrated all my custom AMI's to Amazon Linux version 2011.03.01.\nThe decision to go with an Amazon Linux AMI vs the other AMI providers was not an easy one.  I played around with and tested different setups for close to a month before I made my final decision.  In the end, since I wanted to use CentOS, it basically boiled down to one thing.  I figured who better to know what hardware related dependencies needed to be included in the OS than the people who designed, built and maintain EC2.  Nothing against Rightscale, Scalr or Alestic.\nSix months later, even though I hit a few bumps in the road, Amazon's Linux has been quite stable.  Though, I did decide to compile some of the software we use from the source (ie. php 5.3, MySQL 5.5, etc) because I ran into trouble with the pre-built packages Amazon maintained in their package repository.",
        "url": "https://serverfault.com/questions/275736/amazon-linux-vs-ubuntu-for-amazon-ec2"
    },
    {
        "title": "Nginx Redirect via Proxy, Rewrite and Preserve URL",
        "question": "In Nginx we have been trying to redirect a URL as follows:\nhttp://example.com/some/path -> http://192.168.1.24\n\nwhere the user still sees the original URL in their browser. Once the user is redirected, say they click on the link to /section/index.html, we would want this to make a request that leads to the redirect\nhttp://example.com/some/path/section/index.html -> http://192.168.1.24/section/index.html\n\nand again still preserve the original URL.\nOur attempts have involved various solutions using proxies and rewrite rules, and below shows the configuration that has brought us closest to a solution (note that this is the web server configuration for the example.com web server). However, there are still two problems with this:\n\nIt does not perform the rewrite properly, in that the request URL received by the web server http://192.168.1.24 includes /some/path and therefore fails to serve the required page.\nWhen you hover on a link once a page has been served, /some/path is missing from the URL\nserver {\n    listen          80;\n    server_name     www.example.com;\n\n    location /some/path/ {\n        proxy_pass http://192.168.1.24;\n        proxy_redirect http://www.example.com/some/path http://192.168.1.24;\n        proxy_set_header Host $host;\n    }\n\n    location / {\n        index index.html;\n        root  /var/www/example.com/htdocs;\n    }\n}\n\n\nWe are looking for a solution that only involves changing the web server configuration on example.com. We are able to change the config on 192.168.1.24 (also Nginx), however we want to try and avoid this because we will need to repeat this setup for hundreds of different servers whose access is proxied through example.com.",
        "top_answer": "First, you shouldn't use root directive inside the location block, it is a bad practice. In this case it doesn't matter though.\nTry adding a second location block:\nlocation ~ /some/path/(?<section>.+)/index.html {\n    proxy_pass http://192.168.1.24/$section/index.html;\n    proxy_set_header Host $host;\n}\n\nThis captures the part after /some/path/ and before index.html to a $section variable, which is then used to set the proxy_pass destination. You can make the regex more specific if you require.",
        "url": "https://serverfault.com/questions/586586/nginx-redirect-via-proxy-rewrite-and-preserve-url"
    },
    {
        "title": "What is the benefit of not allocating a terminal in ssh?",
        "question": "Every once in a while I will do something like\nssh user@host sudo thing\n\nand I am reminded that ssh doesn't allocate a pseudo-tty by default.  Why doesn't it?  What benefits would I be losing if I aliased ssh to ssh -t?",
        "top_answer": "The primary difference is the concept of interactivity. It's similar to running commands locally inside of a script, vs. typing them out yourself. It's different in that a remote command must choose a default, and non-interactive is safest. (and usually most honest)\nSTDIN\n\nIf a PTY is allocated, applications can detect this and know that it's safe to prompt the user for additional input without breaking things. There are many programs that will skip the step of prompting the user for input if there is no terminal present, and that's a good thing. It would cause scripts to hang unnecessarily otherwise.\nYour input will be sent to the remote server for the duration of the command. This includes control sequences. While a Ctrl-c break would normally cause a loop on the ssh command to break immediately, your control sequences will instead be sent to the remote server. This results in a need to \"hammer\" the keystroke to ensure that it arrives when control leaves the ssh command, but before the next ssh command begins.\n\nI would caution against using ssh -t in unattended scripts, such as crons. A non-interactive shell asking a remote command to behave interactively for input is asking for all kinds of trouble.\nYou can also test for the presence of a terminal in your own shell scripts. To test STDIN with newer versions of bash:\n# fd 0 is STDIN\n[ -t 0 ]; echo $?\n\nSTDOUT\n\nWhen aliasing ssh to ssh -t, you can expect to get an extra carriage return in your line ends. It may not be visible to you, but it's there; it will show up as ^M when piped to cat -e. You must then expend the additional effort of ensuring that this control code does not get assigned to your variables, particularly if you're going to insert that output into a database.\nThere is also the risk that programs will assume they can render output that is not friendly for file redirection. Normally if you were to redirect STDOUT to a file, the program would recognize that your STDOUT is not a terminal and omit any color codes. If the STDOUT redirection is from the output of the ssh client and the there is a PTY associated with the remote end of the client, the remote programs cannot make such a distinction and you will end up with terminal garbage in your output file. Redirecting output to a file on the remote end of the connection should still work as expected.\n\nHere is the same bash test as earlier, but for STDOUT:\n# fd 1 is STDOUT\n[ -t 1 ]; echo $?\n\n\nWhile it's possible to work around these issues, you're inevitably going to forget to design scripts around them. All of us do at some point. Your team members may also not realize/remember that this alias is in place, which will in turn create problems for you when they write scripts that use your alias.\nAliasing ssh to ssh -t is very much a case where you'll be violating the design principle of least surprise; people will be encountering problems they do not expect and may not understand what is causing them.",
        "url": "https://serverfault.com/questions/593399/what-is-the-benefit-of-not-allocating-a-terminal-in-ssh"
    },
    {
        "title": "How to get TX/RX bytes without ifconfig?",
        "question": "Since ifconfig is apparently being deprecated in major Linux distributions, I thought I'd learn something about the ip tool that's supposed to be used instead of ifconfig. \nAnd here I ran into a problem: when run on its own, ifconfig shows the number of bytes received/transmitted on each interface besides other info. I couldn't find a way to get this from ip. Is there no such function in this tool? What other built-in tools could I use for getting those stats?",
        "top_answer": "Another option is to use the /proc filesystem. The /proc/net/dev file contains statistics about the configured network interfaces. Each line is dedicated to one network  interface and it contains statistics for receive and transmit. The statistics include metrics such total number of received/transmittted bytes, packets, drops, errors and so on.\ncat /proc/net/dev\n\n    Inter-|   Receive                                                |  Transmit\n     face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed\n\n    lo:    29846937   129576     0    0    0     0          0       0 29846937   129576     0    0    0     0       0          0\n wlan0:    9467393340 8027251    0    0    0     0          0       0 2559312961 5896509    0    0    0     0       0          0\n\nOr you can try the netstat command which can display all network interfaces and related statistics:\nnetstat -i\n\nIface   MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg\nlo         65536   0   130435      0      0      0   130435      0      0      0 LRU\nwlan0       1492   0  8028018      0      0      0  5897361      0      0      0 BMRU\n",
        "url": "https://serverfault.com/questions/533513/how-to-get-tx-rx-bytes-without-ifconfig"
    },
    {
        "title": "create home directories after create users",
        "question": "I created some users with:\n\n$ useradd john\n\nand I forgot to specify the parameter -m to create the home directory and to have the skeleton files copied to each user. now I want to do that, and I don't want to recreate all users (there must be an easier way). so, is there any way to create the user directories and copy the skeleton files?\nI thought about creating the directories, chowning them to the corresponding user, copying all the skeleton files and chowning them to the corresponding user. but if there's a command like useradd -m that doesn't create the user again, but create the directories, it'd be better.",
        "top_answer": "This might sound like a silly idea, but if the users aren't actually doing anything, you could do:\ncat /etc/passwd | cut -f 1 -d : >/tmp/users.list\nThen edit /tmp/users.list to only contain the users you want. Then do:\n\nfor i in `cat /tmp/users.list`\ndo\n    userdel $i\n    useradd -m $i\ndone\n\nHowever, many Redhat based distributions will create you a new home directory when you first login, providing it is specified in /etc/passwd where the directory should be.\nTo test that, do an \"su - \" and see if it does \"the right thing\". If it doesn't, the above script will work quite nicely, I think.",
        "url": "https://serverfault.com/questions/63764/create-home-directories-after-create-users"
    },
    {
        "title": "Change default directory when I SSH to server",
        "question": "I was wondering if there is a way to change the default directory that I get put into after I SSH into my Ubuntu server.\n99% of the time when I'm logging into my server, it is to access files within a specific directory:\n/var/www/websites\n\nIs there a config file that I can edit that will make sure I am put straight into this directory when I login?",
        "top_answer": "There are four ways to achieve this:\n\nadd cd /var/www/websites to the end of your .bash_profile. This is executed only for interactive logins (e.g. ssh).\nadd cd /var/www/websites to the end of your .profile. This is more likely to be called by shells which are not bash (e.g. zsh). (Added from @Phil Hord's comment)\nadd cd /var/www/websites to the end of your .bashrc. I use this one on our puppetmasters as I always want to be in /etc/puppet/environments/dkaarsemaker there instead of my homedir :-)\nChange your homedirectory on the server to /var/www/websites (this is not really a good idea)\n",
        "url": "https://serverfault.com/questions/499565/change-default-directory-when-i-ssh-to-server"
    },
    {
        "title": "Vagrant / VirtualBox DNS 10.0.2.3 not working",
        "question": "I am running a fresh install of Linux Mint Nadia (14). I am following the instructions on Vagrant Getting Started but have gotten stuck on the Provisioning. It seems the Vagrant box cannot connect outside and so I can't install anything using either Chef or Puppet.\nIn the basic Vagrant resolve.conf contains nameserver 10.0.2.3. But with that set I can't ping us.archive.ubuntu.com.\nIf I change it to 8.8.8.8 then I can ping us.archive.ubuntu.com but it does not stay set, and after a reboot it changes back to 10.0.2.3 - so provisioning fails again.\nIdeally I would like for 10.0.2.3 to work on my setup. Failing that I would like a way to permanently change resolv.conf so that I can do provisioning.",
        "top_answer": "You can work around this issue in one of two ways, both of which are in the VirtualBox manual:\n\nEnabling DNS proxy in NAT mode\nThe NAT engine by default offers the same DNS servers to the guest\n  that are configured on the host. In some scenarios, it can be\n  desirable to hide the DNS server IPs from the guest, for example when\n  this information can change on the host due to expiring DHCP leases.\n  In this case, you can tell the NAT engine to act as DNS proxy using\n  the following command:\nVBoxManage modifyvm \"VM name\" --natdnsproxy1 on\n\nUsing the host's resolver as a DNS proxy in NAT mode\nFor resolving network names, the DHCP server of the NAT engine offers\n  a list of registered DNS servers of the host. If for some reason you\n  need to hide this DNS server list and use the host's resolver\n  settings, thereby forcing the VirtualBox NAT engine to intercept DNS\n  requests and forward them to host's resolver, use the following\n  command:\nVBoxManage modifyvm \"VM name\" --natdnshostresolver1 on\n\nNote that this setting is similar to the DNS proxy mode, however\n  whereas the proxy mode just forwards DNS requests to the appropriate\n  servers, the resolver mode will interpret the DNS requests and use the\n  host's DNS API to query the information and return it to the guest.\n",
        "url": "https://serverfault.com/questions/453185/vagrant-virtualbox-dns-10-0-2-3-not-working"
    },
    {
        "title": "How does vm.overcommit_memory work?",
        "question": "When I use the default settings:\nvm.overcommit_memory = 0\nvm.overcommit_ratio = 50\n\nI can read these values from /proc/meminfo file:\nCommitLimit:     2609604 kB\nCommitted_AS:    1579976 kB\n\nBut when I change vm.overcommit_memory from 0 to 2, I'm unable to start the same set of applications that I could start before the change, especially amarok. I had to change vm.overcommit_ratio to 300, so the limit could be increased. Now when I start amarok, /proc/meminfo shows the following:\nCommitLimit:     5171884 kB\nCommitted_AS:    3929668 kB\n\nThis machine has only 1GiB of RAM, but amarok works without problems when vm.overcommit_memory is set to 0. But in the case of setting it to 2, amarok needs to allocate over 2GiB of memory. Is it a normal behavior? If so, could anyone explain why, for instance, firefox (which consumes 4-6x more memory than amarok) works in the same way before and after the change?",
        "top_answer": "You can find the documentation in man 5 proc (or at kernel.org):\n\n/proc/sys/vm/overcommit_memory\n       This file contains the kernel virtual memory accounting mode.\n       Values are:\n\n              0: heuristic overcommit (this is the default)\n              1: always overcommit, never check\n              2: always check, never overcommit\n\n       In mode 0, calls of mmap(2) with MAP_NORESERVE are not\n       checked, and the default check is very weak, leading to the\n       risk of getting a process \"OOM-killed\".\n\n       In mode 2 (available since Linux 2.6), the total virtual\n       address space that can be allocated (CommitLimit in /proc/mem\u2010\n       info) is calculated as\n\n           CommitLimit = (total_RAM - total_huge_TLB) *\n                         overcommit_ratio / 100 + total_swap\n\n\nThe simple answer is that setting overcommit to 1, will set the stage so that when a program calls something like malloc() to allocate a chunk of memory (man 3 malloc), it will always succeed regardless if the system knows it will not have all the memory that is being asked for.\nThe underlying concept to understand is the idea of virtual memory. Programs see a virtual address space that may, or may not, be mapped to actual physical memory. By disabling overcommit checking, you tell the OS to just assume that there is always enough physical memory to backup the virtual space.\nExample\nTo highlight why this can sometimes matter, take a look at the Redis guidances on why vm.overcommit_memory should be set to 1 for it.",
        "url": "https://serverfault.com/questions/606185/how-does-vm-overcommit-memory-work"
    },
    {
        "title": "Best way to redirect all HTTP to HTTPS in IIS",
        "question": "We want ALL sites on our webserver (IIS 10) to enforce SSL (ie redirect HTTP to HTTPS).\nWe are currently 'Requiring SSL' on each site and setting up a 403 error handler to perform a 302 redirect to the https address for that specific site.\nThis works great.  But it's a pain to do for every single site, there's plenty of room for human error.\nIdeally I'd like to set up a permanent 301 redirect on all HTTP://* to HTTPS://*\nIs there a simple way to do this in IIS ?",
        "top_answer": "The IIS URL Rewrite Module 2.1 for IIS7+ may be your friend. The module can be downloaded from IIS URL Rewrite. Using the URL Rewrite Module and URL Rewrite Module 2.0 Configuration Reference explain how to use the module.\nOnce the module is installed, you can create a host wide redirect using IIS Manager. Select URL Rewrite, Add Rule(s)..., and Blank rule.\nName:\nRedirect to HTTPS\nMatch URL\nRequested URL: Matches the Pattern\nUsing: Wildcards\nPattern: *\nIgnore case: Checked\nConditions\nLogical grouping: Match Any\nCondition input: {HTTPS}\nCheck if input string: Matches the Pattern\nPattern: OFF\nIgnore case: Checked\nTrack capture groups across conditions: Not checked\nServer Variables\nLeave blank.\nAction\nAction type: Redirect\nRedirect URL: https://{HTTP_HOST}{REQUEST_URI}\nAppend query string: Not checked\nRedirect type: Permanent (301)\nApply the rule and run IISReset (or click Restart in the IIS Manager)\nAlternatively, after installing the module you could modify the applicationHost.config file as follows:  \n<system.webServer>\n  <rewrite>\n    <globalRules>\n      <rule name=\"Redirect to HTTPS\" enabled=\"true\" patternSyntax=\"Wildcard\" stopProcessing=\"true\">\n        <match url=\"*\" ignoreCase=\"true\" negate=\"false\" />\n        <conditions logicalGrouping=\"MatchAny\" trackAllCaptures=\"false\">\n          <add input=\"{HTTPS}\" ignoreCase=\"true\" matchType=\"Pattern\" negate=\"false\" pattern=\"OFF\" />\n        </conditions>\n        <action type=\"Redirect\" url=\"https://{HTTP_HOST}{REQUEST_URI}\" appendQueryString=\"false\" redirectType=\"Permanent\" />\n      </rule>\n    </globalRules>\n  </rewrite>\n</system.webServer>\n",
        "url": "https://serverfault.com/questions/893315/best-way-to-redirect-all-http-to-https-in-iis"
    },
    {
        "title": "How does \"restart: always\" policy work in docker-compose?",
        "question": "I have docker compose file with PostgreSQL and my application, like this:\nversion: '3'\n\nservices:\n  postgresql:\n    image: postgres:9.6.6\n    ports:\n      - 9932:5432\n    expose:\n      - \"5432\"\n    environment:\n      - POSTGRES_PASSWORD=pass\n    restart: always\n    volumes:\n      - /data:/var/lib/postgresql/data\n\n  myapp:\n    image: myapp\n    links:\n      - postgresql\n    depends_on:\n      - \"postgresql\"\n    restart: always\n    ports:\n      - \"5000:5000\"\n\nThe problem is that restart: always policy does not seem to work when I kill the container (simulating app crash using docker kill) and docker-compose does not restart my container, even though the Exit Code is 137. I observe the same behaviour when I use restart: on-failure policy. Versions 2 and 3 of docker-compose behave the same. My system is Ubuntu Server 16.04 x64.\nMy questions are:\n\nWhy docker-compose does not restart crashed (killed) container?\nHow to check if restart policy works?\n",
        "top_answer": "When you use docker kill, this is the expected behavior as Docker does not restart the container: \"If you manually stop a container, its restart policy is ignored until the Docker daemon restarts or the container is manually restarted. This is another attempt to prevent a restart loop\" (reference)\nIf you use docker stop or docker kill, you're manually stopping the container. You can do some tests about restart policies: restarting the docker daemon, rebooting your server, using a CMD inside a container and running an exit... \nFor example if I kill my container deployed with a restart policy, I see that it exited with code 137 but it is not restarted according to docker ps -a, it remains exited:\n[root@andromeda ~]# docker ps --all\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                        PORTS               NAMES\n819d1264c30a        redis:alpine        \"docker-entrypoint...\"   3 minutes ago       Exited (137) 34 seconds ago                       keepalive_redis_1\n\nBut if I restart the daemon... \n[root@andromeda ~]# docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\n819d1264c30a        redis:alpine        \"docker-entrypoint...\"   30 minutes ago      Up 2 seconds        6379/tcp            keepalive_redis_1\n\nThe container that was set with restart policy, starts again which is what documentation say, so docker kill is not the way you should test the restart policy as it's assumed that you have deliberately stopped the container and Docker wants to have a way to prevent restarting loops, if you kill it, you really want to kill it. \nI found the following links valuable that show the same behavior in different versions (so it's not a bug but the expected behavior):\n\nHow to ensure that containers restart\nThe same behavior expected when you use docker kill\nAnd another post about docker kill and restart\n",
        "url": "https://serverfault.com/questions/884759/how-does-restart-always-policy-work-in-docker-compose"
    },
    {
        "title": "Why is TCP accept() performance so bad under Xen?",
        "question": "The rate at which my server can accept() new incoming TCP connections is really bad under Xen. The same test on bare metal hardware shows 3-5x speed ups.\n\nHow come this is so bad under Xen?\nCan you tweak Xen to improve performance for new TCP connections?\nAre there other virtualization platforms better suited for this kind of use-case?\n\nBackground\nLately I've been researching some performance bottlenecks of an in-house developed Java server running under Xen. The server speaks HTTP and answers simple TCP connect/request/response/disconnect calls.\nBut even while sending boatloads of traffic to the server, it cannot accept more than ~7000 TCP connections per second (on an 8-core EC2 instance, c1.xlarge running Xen). During the test, the server also exhibit a strange behavior where one core (not necessarily cpu 0) gets very loaded >80%, while the other cores stay almost idle. This leads me to think the problem is related to the kernel/underlying virtualization.\nWhen testing the same scenario on a bare metal, non-virtualized platform I get test results showing TCP accept() rates beyond 35 000/second. This on a Core i5 4 core machine running Ubuntu with all cores almost fully saturated. To me that kind of figure seems about right.\nOn the Xen instance again, I've tried enable/tweak almost every settings there is in sysctl.conf. Including enabling Receive Packet Steering and Receive Flow Steering and pinning threads/processes to CPUs but with no apparent gains.\nI know degraded performance is to be expected when running virtualized. But to this degree? A slower, bare metal server outperforming virt. 8-core by a factor of 5?\n\nIs this really expected behavior of Xen?\nCan you tweak Xen to improve performance for new TCP connections?\nAre there other virtualization platforms better suited for this kind of use-case?\n\nReproducing this behavior\nWhen further investigating this and pinpointing the problem I found out that the netperf performance testing tool could simulate the similar scenario I am experiencing.\nUsing netperf's TCP_CRR test I have collected various reports from different servers (both virtualized and non-virt.). If you'd like to contribute with some findings or look up my current reports, please see https://gist.github.com/985475\nHow do I know this problem is not due to poorly written software?\n\nThe server has been tested on bare metal hardware and it almost saturates all cores available to it.\nWhen using keep-alive TCP connections, the problem goes away.\n\nWhy is this important?\nAt ESN (my employer) I am the project lead of Beaconpush, a Comet/Web Socket server written in Java. Even though it's very performant and can saturate almost any bandwidth given to it under optimal conditions, it's still limited to how fast new TCP connections can be made. That is, if you have a big user churn where users come and go very often, many TCP connections will have to be set up/teared down. We try to mitigate this keeping connections alive as long as possible.\nBut in the end, the accept() performance is what keeps our cores from spinning and we don't like that.\n\nUpdate 1\nSomeone posted this question to Hacker News, there's some questions/answers there as well. But I'll try keeping this question up-to-date with information I find as I go along.\nHardware/platforms I've tested this on:\n\nEC2 with instance types c1.xlarge (8 cores, 7 GB RAM) and cc1.4xlarge (2x Intel Xeon X5570, 23 GB RAM). AMIs used was ami-08f40561 and ami-1cad5275 respectively. Someone also pointed out that the \"Security Groups\" (i.e EC2s firewall) might affect as well. But for this test scenario, I've tried only on localhost to eliminate external factors such as this. Another rumour I've heard is that EC2 instances can't push more than 100k PPS.\nTwo private virtualized server running Xen. One had zero load prior to the test but didn't make a difference.\nPrivate dedicated, Xen-server at Rackspace. About the same results there.\n\nI'm in the process of re-running these tests and filling out the reports at https://gist.github.com/985475 If you'd like to help, contribute your numbers. It's easy!\n(The action plan has been moved to a separate, consolidated answer)",
        "top_answer": "Right now: Small packet performance sucks under Xen\n(moved from the question itself to a separate answer instead)\nAccording to a user on HN (a KVM developer?) this is due to small packet performance in Xen and also KVM. It's a known problem with virtualization and according to him, VMWare's ESX handles this much better. He also noted that KVM are bringing some new features designed alleviate this (original post).\nThis info is a bit discouraging if it's correct. Either way, I'll try the steps below until some Xen guru comes along with a definitive answer :)\nIain Kay from the xen-users mailing list compiled this graph:\n\nNotice the TCP_CRR bars, compare \"2.6.18-239.9.1.el5\" vs \"2.6.39 (with Xen 4.1.0)\".\nCurrent action plan based on responses/answers here and from HN:\n\nSubmit this issue to a Xen-specific mailing list and the xensource's bugzilla as suggested by syneticon-dj\nA message was posted to the xen-user list, awaiting reply. \nCreate a simple pathological, application-level test case and publish it.A test server with instructions have been created and published to GitHub. With this you should be able to see a more real-world use-case compared to netperf.\nTry a 32-bit PV Xen guest instance, as 64-bit might be causing more overhead in Xen. Someone mentioned this on HN. Did not make a difference.\nTry enabling net.ipv4.tcp_syncookies in sysctl.conf as suggested by abofh on HN. This apparently might improve performance since the handshake would occur in the kernel. I had no luck with this.\nIncrease the backlog from 1024 to something much higher, also suggested by abofh on HN. This could also help since guest could potentially accept() more connections during it's execution slice given by dom0 (the host).\nDouble-check that conntrack is disabled on all machines as it can halve the accept rate (suggested by deubeulyou). Yes, it was disabled in all tests.\nCheck for \"listen queue overflow and syncache buckets overflow in netstat -s\" (suggested by mike_esspe on HN).\nSplit the interrupt handling among multiple cores (RPS/RFS I tried enabling earlier are supposed to do this, but could be worth trying again). Suggested by adamt at HN. \nTurning off TCP segmentation offload and scatter/gather acceleration as suggested by Matt Bailey. (Not possible on EC2 or similar VPS hosts)\n",
        "url": "https://serverfault.com/questions/272483/why-is-tcp-accept-performance-so-bad-under-xen"
    },
    {
        "title": "Make a Docker application write to stdout",
        "question": "I'm deploying a 3rd-party application in compliance with the 12 factor advisory, and one of the points tell that application logs should be printed to stdout/stderr: then clustering software can collect it.\nHowever, the application can only write to files or syslog. How do I print these logs instead?",
        "top_answer": "An amazing recipe is given in the nginx Dockerfile:\n# forward request and error logs to docker log collector\nRUN ln -sf /dev/stdout /var/log/nginx/access.log \\\n    && ln -sf /dev/stderr /var/log/nginx/error.log\n\nSimply, the app can continue writing to it as a file, but as a result the lines will go to stdout & stderr!",
        "url": "https://serverfault.com/questions/599103/make-a-docker-application-write-to-stdout"
    },
    {
        "title": "Dump a linux process's memory to file",
        "question": "Is it possible to dump the current memory allocated for a process (by PID) to a file? Or read it somehow?",
        "top_answer": "I'm not sure how you dump all the memory to a file without doing this repeatedly (if anyone knows an automated way to get gdb to do this please let me know), but the following works for any one batch of memory assuming you know the pid:  \n$ cat /proc/[pid]/maps\n\nThis will be in the format (example):  \n00400000-00421000 r-xp 00000000 08:01 592398                             /usr/libexec/dovecot/pop3-login\n00621000-00622000 rw-p 00021000 08:01 592398                             /usr/libexec/dovecot/pop3-login\n00622000-0066a000 rw-p 00622000 00:00 0                                  [heap]\n3e73200000-3e7321c000 r-xp 00000000 08:01 229378                         /lib64/ld-2.5.so\n3e7341b000-3e7341c000 r--p 0001b000 08:01 229378                         /lib64/ld-2.5.so\n\nPick one batch of memory (so for example 00621000-00622000) then use gdb as root to attach to the process and dump that memory:  \n$ gdb --pid [pid]\n(gdb) dump memory /root/output 0x00621000 0x00622000\n\nThen analyse /root/output with the strings command, less you want the PuTTY all over your screen.",
        "url": "https://serverfault.com/questions/173999/dump-a-linux-processs-memory-to-file"
    },
    {
        "title": "How to remove invalid characters from filenames?",
        "question": "I have files with invalid characters like these\n009_-_\ufffd%86ndringsh\u00e5ndtering.html\n\nIt is a \u00c6 where something have gone wrong in the filename.\nIs there a way to just remove all invalid characters?\nor could tr be used somehow?\necho \"009_-_\ufffd%86ndringsh\u00e5ndtering.html\" | tr ???\n",
        "top_answer": "One way would be with sed:\nmv 'file' $(echo 'file' | sed -e 's/[^A-Za-z0-9._-]/_/g')\n\nReplace file with your filename, of course. This will replace anything that isn't a letter, number, period, underscore, or dash with an underscore. You can add or remove characters to keep as you like, and/or change the replacement character to anything else, or nothing at all.",
        "url": "https://serverfault.com/questions/348482/how-to-remove-invalid-characters-from-filenames"
    },
    {
        "title": "How to find the physical volume(s) that hold a logical volume in LVM",
        "question": "I have a volume group (VG) that contains two physical volumes (PV).\nSeveral logical volumes (LV) in the VG are likely to use extents on both PVs.\nIs there a way to tell which LVs occupy space on which PVs?",
        "top_answer": "The pvdisplay command has a -m option to show the mapping of physical extents to logical volumes and logical extents.\nI have set up the following situation on a test machine:\n\n3 disks of 1GB each added to the system and used as physical volumes for vg_test\n6 logical volumes made with various sizes (ranging from 300M to 1.1G) so that they are spread over the physical volumes\n\nRunning pvdisplay -m on this machine results in the following output:\n[root@centos6 ~]# pvdisplay -m\n  --- Physical volume ---\n  PV Name               /dev/sdb\n  VG Name               vg_test\n  PV Size               1.00 GiB / not usable 4.00 MiB\n  Allocatable           yes \n  PE Size               4.00 MiB\n  Total PE              255\n  Free PE               5\n  Allocated PE          250\n  PV UUID               eR2ko2-aKRf-uCfq-O2L0-z6em-ZYT5-23YhKb\n\n  --- Physical Segments ---\n  Physical extent 0 to 74:\n    Logical volume  /dev/vg_test/one\n    Logical extents 0 to 74\n  Physical extent 75 to 149:\n    Logical volume  /dev/vg_test/two\n    Logical extents 0 to 74\n  Physical extent 150 to 249:\n    Logical volume  /dev/vg_test/four\n    Logical extents 0 to 99\n  Physical extent 250 to 254:\n    FREE\n\n  --- Physical volume ---\n  PV Name               /dev/sdc\n  VG Name               vg_test\n  PV Size               1.00 GiB / not usable 4.00 MiB\n  Allocatable           yes \n  PE Size               4.00 MiB\n  Total PE              255\n  Free PE               10\n  Allocated PE          245\n  PV UUID               rByjXK-NA6D-ifnY-lKdF-eFWg-Ndou-psGJUq\n\n  --- Physical Segments ---\n  Physical extent 0 to 124:\n    Logical volume  /dev/vg_test/three\n    Logical extents 0 to 124\n  Physical extent 125 to 224:\n    Logical volume  /dev/vg_test/five\n    Logical extents 0 to 99\n  Physical extent 225 to 244:\n    Logical volume  /dev/vg_test/six\n    Logical extents 255 to 274\n  Physical extent 245 to 254:\n    FREE\n\n  --- Physical volume ---\n  PV Name               /dev/sdd\n  VG Name               vg_test\n  PV Size               1.00 GiB / not usable 4.00 MiB\n  Allocatable           yes (but full)\n  PE Size               4.00 MiB\n  Total PE              255\n  Free PE               0\n  Allocated PE          255\n  PV UUID               TCJnZM-0ss9-o5gY-lgD3-7Kq6-18IH-sN04To\n\n  --- Physical Segments ---\n  Physical extent 0 to 254:\n    Logical volume  /dev/vg_test/six\n    Logical extents 0 to 254\n\nAs you can see, You get a nice overview of where the extents for each of the 6 logical volumes are.",
        "url": "https://serverfault.com/questions/461385/how-to-find-the-physical-volumes-that-hold-a-logical-volume-in-lvm"
    },
    {
        "title": "How to search Powershell command history from previous sessions",
        "question": "I use current Windows 10 with Powershell 5.1. Often, I want to look up commands I have used in the past to modify and/or re-run them. Inevitably, the commands I'm looking for were run in a previous or different PowerShell window/session.\nWhen I hammer the \u2191 key, I can browse through many, many commands from many, many sessions, but when I try to search through them using Get-History | Where-Object {$_.CommandLine -Like \"*docker cp*\"}, I get no results. Basic troubleshooting reveals that Get-History doesn't show anything from previous sessions, as shown by:\nC:\\Users\\Me> Get-History\n\n  Id CommandLine\n  -- -----------\n   1 Get-History | Where-Object {$_.CommandLine -Like \"*docker cp*\"}\n\nHow can I search through the previous commands that the \u2191 key provides using Get-History or another Cmdlet?",
        "top_answer": "The persistent history you mention is provided by PSReadLine. It is separate from the session-bound Get-History.\nThe history is stored in a file defined by the property (Get-PSReadlineOption).HistorySavePath. View this file with Get-Content (Get-PSReadlineOption).HistorySavePath, or a text editor, etc. Inspect related options with Get-PSReadlineOption. PSReadLine also performs history searches via ctrl+r.\nUsing your provided example:\nGet-Content (Get-PSReadlineOption).HistorySavePath | ? { $_ -like '*docker cp*' }",
        "url": "https://serverfault.com/questions/891265/how-to-search-powershell-command-history-from-previous-sessions"
    },
    {
        "title": "Software vs hardware RAID performance and cache usage",
        "question": "I've been reading a lot on RAID controllers/setups and one thing that comes up a lot is how hardware controllers without cache offer the same performance as software RAID. Is this really the case?\nI always thought that hardware RAID cards would offer better performance even without cache. I mean, you have dedicated hardware to perform the tasks. If that is the case what is the benefit of getting a RAID card that has no cache, something like a LSI 9341-4i that isn't exactly cheap.\nAlso if a performance gain is only possible with cache, is there a cache configuration that writes to disk right away but keeps data in cache for reading operations making a BBU not a priority?",
        "top_answer": "In short: if using a low-end RAID card (without cache), do yourself a favor and switch to software RAID. If using a mid-to-high-end card (with BBU or NVRAM), then hardware is often (but not always! see below) a good choice.\nLong answer: when computing power was limited, hardware RAID cards had the significant advantage to offload parity/syndrome calculation for RAID schemes involving them (RAID 3/4/5, RAID6, ecc).\nHowever, with the ever increasing CPU performance, this advantage basically disappeared: even my laptop's ancient CPU (Core i5 M 520, Westmere generation) has XOR performance of over 4 GB/s and RAID-6 syndrome performance over 3 GB/s per single execution core.\nThe advantage that hardware RAID maintains today is the presence of a power-loss protected DRAM cache, in the form of BBU or NVRAM. This protected cache gives very low latency for random write access (and reads that hit) and basically transforms random writes into sequential writes. A RAID controller without such a cache is near useless. Moreover, some low-end RAID controllers do not only come without a cache, but forcibly disable the disk's private DRAM cache, leading to slower performance than without RAID card at all. An example are DELL's PERC H200 and H300 cards: they totally disable the disk's private cache and (if newer firmware has not changed that) actively forbid to re-activate it. Do yourself a favor and do not, ever, never buy such controllers. While even higher-end controllers often disable disk's private cache, they at least have their own protected cache - making HDD's (but not SSD's!) private cache somewhat redundant.\nThis is not the end, though. Even capable controllers (the one with BBU or NVRAM cache) can give inconsistent results when used with SSD, basically because SSDs really need a fast private cache for efficient FLASH page programming/erasing. And while some (most?) controllers let you re-enable disk's private cache (eg: PERC H700/710/710P), if that private cache is volatile you risk to lose data in case of power loss. The exact behavior really is controller and firmware dependent (eg: on a DELL S6/i with 256 MB WB cache and enabled disk's cache, I had no losses during multiple, planned power loss testing), giving uncertainty and much concern.\nOpen source software RAIDs, on the other hand, are much more controllable beasts - their software is not enclosed inside a proprietary firmware, and have well-defined metadata patterns and behaviors. Software RAID make the (right) assumption that disk's private DRAM cache is not protected, but at the same time it is critical for acceptable performance - so rather than disabling it, they use ATA FLUSH / FUA commands to write critical data on stable storage. As they often run from the SATA ports attached to the chipset SB, their bandwidth is very good and driver support is excellent.\nHowever, if used with mechanical HDDs, synchronized, random write access patterns (eg: databases, virtual machines) will greatly suffer compared to an hardware RAID controller with WB cache. On the other hand, when used with enterprise SSDs (ie: with a powerloss protected write cache), software RAID often excels and give results even higher than hardware RAID cards. Unfortunately consumer SSDs only have volatile write cache, delivering very low IOPS in synchronized write workloads (albeit very fast at reads and async writes).\nAlso consider that software RAIDs are not all created equal. Windows software RAID has a bad reputation, performance wise, and even Storage Space seems not too different. Linux MD Raid is exceptionally fast and versatile, but Linux I/O stack is composed of multiple independent pieces that you need to carefully understand to extract maximum performance. ZFS parity RAID (ZRAID) is extremely advanced but, if not correctly configured, can give you very poor IOPs; mirroring+striping, on the other side, performs quite well. Anyway, it need a fast SLOG device for synchronous write handling (ZIL).\nBottom line:\n\nif your workloads are not synchronized random write sensitive, you don't need a RAID card\nif you need a RAID card, do not buy a RAID controller without WB cache\nif you plan to use SSD, software RAID is preferred but keep in mind that for high synchronized random writes you need a powerloss-protected SSD (ie: Intel S/P/DC, Samsung PM/SM, etc). For pure performance the best choice probably is Linux MD Raid, but nowadays I generally use striped ZFS mirrors. If you can not afford losing half the space due to mirrors and you needs ZFS advanced features, go with ZRAID but carefully think about your VDEVs setup.\nif you, even using SSD, really need an hardware RAID card, use SSDs with write-protected caches.\nif you need RAID6 when using normal, mechanical HDDs, consider to buy a fast RAID card with 512 MB (or more) WB cache. RAID6 has a high write performance penalty, and a properly-sized WB cache can at least provide a fast intermediate storage for small synchronous writes (eg: filesystem journal).\nif you need RAID6 with HDDs but you can't / don't want to buy a hardware RAID card, carefully think about your software RAID setup. For example, a possible solution with Linux MD Raid is to use two arrays: a small RAID10 array for journal writes / DB logs, and a RAID6 array for raw storage (as fileserver). On the other hand, software RAID5/6 with SSDs is very fast, so you probably don't need a RAID card for an all-SSDs setup.\n",
        "url": "https://serverfault.com/questions/685289/software-vs-hardware-raid-performance-and-cache-usage"
    },
    {
        "title": "Best location to keep SSL certificates and private keys on Ubuntu servers?",
        "question": "On Ubuntu, it looks like the best place for a private key used to sign a certificate (for use by nginx) is in /etc/ssl/private/\nThis answer adds that the certificate should go in /etc/ssl/certs/ but that seems like an unsafe place. Do .crt files need to be kept safe or are they considered public?",
        "top_answer": "The .crt file is sent to everything that connects; it is public. (chown root:root and chmod 644)\nTo add to the private key location; make sure you secure it properly as well as having it in there. (chown root:ssl-cert and chmod 640)",
        "url": "https://serverfault.com/questions/259302/best-location-to-keep-ssl-certificates-and-private-keys-on-ubuntu-servers"
    },
    {
        "title": "How to keep rsync from chown'ing transferred files?",
        "question": "I have an sshfs connection setup with a remote filesystem on a Linux server. I'm doing an rsync from my local server to the ftpfs-filesystem. Because of the nature of this setup, I can't chown anything on the sshfs filesystem.\nWhen I do the rsync, it tries to chown all the files after it transfers them. This results in chown errors, even though it transfers the files just fine.\nWith rsync, is there a way to tell it to not try and chown the files? If I rsync like 1000 files I end up with a log of 1000 chown: permission denied (error 13) errors. I know it doesn't hurt anything to get these errors since the ownership of the files is determined by the sshfs configuration itself, but it would be nice to not get them.",
        "top_answer": "You are probably running rsync like this:\nrsync -a dir/ remote:/dir/\n\nThe -a option according to the documentation is equivalent to: -rlptgoD\n      -a, --archive    archive mode; equals -rlptgoD (no -H,-A,-X)\n\nYou probably want to remove the -o and -g options:\n      -o, --owner                 preserve owner (super-user only)\n      -g, --group                 preserve group\n\nSo instead your rsync command should look something like this:\nrsync -rlptD dir/ remote:/dir/\n\nOr as @glglgl points out:\nrsync -a --no-o --no-g dir/ remote:/dir/\n\nThe remaining options in use are:\n      -r, --recursive             recurse into directories\n      -l, --links                 copy symlinks as symlinks\n      -p, --perms                 preserve permissions\n      -t, --times                 preserve modification times\n      -D                          same as --devices --specials\n          --devices               preserve device files (super-user only)\n          --specials              preserve special files\n",
        "url": "https://serverfault.com/questions/364709/how-to-keep-rsync-from-chowning-transferred-files"
    },
    {
        "title": "How to make scp copy hidden files?",
        "question": "I often use SCP to copy files around - particularly web-related files. The problem is that whenever I do this, I can't get my command to copy hidden files (eg, .htaccess).\nI typically invoke this:\nscp -rp src/ user@server:dest/\n\nThis doesn't copy hidden files. I don't want to have to invoke this again (by doing something like scp -rp src/.* ... - and that has strange . and .. implications anyway.\nI didn't see anything in the scp man page about an \"include hidden files\".\nHow can I accomplish this?",
        "top_answer": "That should absolutely match hidden files. The / at the end of the source says \"every file under this directory\". Nevertheless, testing and research bear you out. This is stupid behavior. \nThe \"answer\" is to append a dot to the end of the source:\nscp -rp src/. user@server:dest/\nThe real answer is to use rsync. ",
        "url": "https://serverfault.com/questions/21580/how-to-make-scp-copy-hidden-files"
    },
    {
        "title": "Where is my mysql log on OS X?",
        "question": "I checked /var/log and /usr/local/mysql and i can't seem to find the log.  I am trying to troubleshoot an error establishing a database connection with a php function.",
        "top_answer": "As Chealion mentioned, there are several ways that your mysql could have been installed. Each of which will place your data dir and/or logs in different locations. The following command will give you (and us) a good indication of where to look.\nps auxww|grep [m]ysqld\n\n# Putting brackets around the first char is a `grep`+`ps` trick\n# to keep it from matching its own process.\n# Note: For zsh compatibility put quotes around the grep regex\n\nCan you post the result of that command here please? Mine looks like this:\n_mysql     101   0.0  0.3   112104  13268   ??  S    12:30AM   0:13.20 /opt/local/libexec/mysqld --basedir=/opt/local --datadir=/opt/local/var/db/mysql --user=mysql --pid-file=/opt/local/var/db/mysql/rbronosky-mbp.pid\nroot        76   0.0  0.0   600172    688   ??  S    12:30AM   0:00.02 /bin/sh /opt/local/lib/mysql/bin/mysqld_safe --datadir=/opt/local/var/db/mysql --pid-file=/opt/local/var/db/mysql/rbronosky-mbp.pid\n\nFrom that you can see that my datadir is /opt/local/var/db/mysql (because I installed via MacPorts). Let's take this lesson a bit further... \nFrom the first line you can see the my daemon is /opt/local/libexec/mysqld. The mysqld can be called with --verbose --help to get a list of all command line options (and here is the important/valuable part!) followed by the values that would be used if you were launching mysqld instead of just checking the help output. The values are the result of your compile time configuration, my.cnf file, and any command line options. I can exploit this feature to find out EXACTLY where my log files are, like so:\n/opt/local/libexec/mysqld --verbose --help|grep '^log'\n\nMine looks like this:\nlog                               /tmp/mysql.log\nlog-bin                           /tmp/mysql-bin\nlog-bin-index                     (No default value)\nlog-bin-trust-function-creators   FALSE\nlog-bin-trust-routine-creators    FALSE\nlog-error                         /tmp/mysql.error.log\nlog-isam                          myisam.log\nlog-queries-not-using-indexes     FALSE\nlog-short-format                  FALSE\nlog-slave-updates                 FALSE\nlog-slow-admin-statements         FALSE\nlog-slow-queries                  (No default value)\nlog-tc                            tc.log\nlog-tc-size                       24576\nlog-update                        (No default value)\nlog-warnings                      1\n\nLO AND BEHOLD! all of the advice in the world was not going to help me because my log file is kept in a completely non-standard location! I keep mine in /tmp/ because on my laptop, I don't care (actually I prefer) to loose all of my logs on reboot.\nLet's put it all together and make you a oneliner:\n$(ps auxww|sed -n '/sed -n/d;/mysqld /{s/.* \\([^ ]*mysqld\\) .*/\\1/;p;}') --verbose --help|grep '^log'\n\nExecute that one command and you will get a list of all of the logs for your running instance of mysql.\nEnjoy!\nThis Bash-Fu brought to you for free by my commitment to all things Open Source.",
        "url": "https://serverfault.com/questions/42531/where-is-my-mysql-log-on-os-x"
    },
    {
        "title": "Why is rsync skipping the main directory?",
        "question": "I'm trying to use rync locally (on a windows machine) to a remote server (my osx box) in order to test a remote deploy build script.  I've done rsync before just fine between 2 linux servers, but I'm having problems now.  Here is the output:\n$ rsync -v -e ssh [email\u00a0protected]:/Library/WebServer/sites/staging/app1/ ./export\n\nskipping directory /Library/WebServer/sites/staging/app1/.\n\nsent 8 bytes  received 13 bytes  3.82 bytes/sec\ntotal size is 0  speedup is 0.00\n\n$ \n\nor\n$ rsync -avz -e ssh [email\u00a0protected]:/Library/WebServer/sites/staging/app1/ ./export\n\nreceiving file list ... done\n./\n\nsent 26 bytes  received 68 bytes  17.09 bytes/sec\ntotal size is 0  speedup is 0.00\n\n\n$ \n\nremote app1 directory is empty while local export directory has 4 sub directories and then a bunch of files in each of those",
        "top_answer": "rsync -v -e ssh [email\u00a0protected]:/Library/WebServer/sites/staging/app1/ ./export\n\nYou didn't give it any options to put it into recursive mode like -r or -a.\n\nremote app1 directory is empty while local export directory has 4 sub\n  directories and then a bunch of files in each of those\n\nDo you have the options backwards here?  The command should be rsync [source] [DESTINATION].  If the app1 directory is empty and you are trying to copy an empty directory then you aren't going to do anything useful.\nPerhaps you need something like this instead?\nrsync -avz ./export/  [email\u00a0protected]:/Library/WebServer/sites/staging/app1/ \n\nAlso:\n\nYou should almost always include a trailing slash on directories with rsync.\nAlmost every version of rsync released in the last 5-10 years defaults to using ssh has the remote transport.  You probably don't have to specify the -e ssh.  \n",
        "url": "https://serverfault.com/questions/363555/why-is-rsync-skipping-the-main-directory"
    },
    {
        "title": "What\u2019s the difference between the \u201cmainline\u201d and \u201cstable\u201d branches of nginx?",
        "question": "There seem to be two active branches of the nginx web server: a \u201cmainline\u201d branch (currently 1.9.x) and a \u201cstable\u201d branch (currently 1.8.x). Can anyone provide an official source that describes the difference between these two branches and how one would choose between them?",
        "top_answer": "The difference between mainline and stable is explained here:\nhttps://www.nginx.com/blog/nginx-1-6-1-7-released/\nA graphical summary:\n\nAmong other differences:\n\nNote that stable does not mean more reliable or more bug-free. In\n  fact, the mainline is generally regarded as more reliable because we\n  port all bug fixes to it, and not just critical fixes as for the\n  stable branch. On the other hand, changes in the stable branch are\n  very unlikely to affect third-party modules. We don\u2019t make the same\n  commitment concerning the mainline, where new features can affect the\n  operation of third-party modules.\n\nAlso:\n\nWe recommend that in general you deploy the NGINX mainline branch at\n  all times.\n",
        "url": "https://serverfault.com/questions/715049/what-s-the-difference-between-the-mainline-and-stable-branches-of-nginx"
    },
    {
        "title": "Heartbleed: how to reliably and portably check the OpenSSL version?",
        "question": "I was looking at a reliable and portable way to check the OpenSSL version on GNU/Linux and other systems, so users can easily discover if they should upgrade their SSL because of the Heartbleed bug.\nI thought it would be easy, but I quickly ran into a problem on Ubuntu 12.04 LTS with the latest OpenSSL 1.0.1g:\nopenssl version -a\nI was expecting to see a full version, but instead I got this:\nOpenSSL 1.0.1 14 Mar 2012\nbuilt on: Tue Jun  4 07:26:06 UTC 2013\nplatform: [...]\nTo my unpleasant surprise, the version letter doesn't show. No f, no g there, just \"1.0.1\" and that's it. The listed dates do not assist in discovering a (non-)vulnerable version either.\nThe difference between 1.0.1 (a-f) and 1.0.1g is crucial.\nQuestions:\n\nWhat is a reliable way to check the version, preferably cross distro?\nWhy isn't the version letter showing in the first place? I was unable to test this on anything else but Ubuntu 12.04 LTS.\n\nOthers are reporting this behaviour as well. A few examples:\n\nhttps://twitter.com/orblivion/status/453323034955223040\nhttps://twitter.com/axiomsofchoice/status/453309436816535554\n\nSome (distro-specific) suggestions rolling in:\n\nUbuntu and Debian: apt-cache policy openssl and apt-cache policy libssl1.0.0. Compare the version numbers to the packages here: http://www.ubuntu.com/usn/usn-2165-1/\nFedora 20: yum info openssl (thanks @znmeb on twitter) and yum info openssl-libs\n\nChecking if a older version of OpenSSL is still resident:\n\nIt's not completely reliable, but you can try lsof -n | grep ssl | grep DEL. See Heartbleed: how to reliably and portably check the OpenSSL version? on why this may not work for you.\n\nIt turns out that updating the OpenSSL package on Ubuntu and Debian isn't always enough. You should also update the libssl1.0.0 package, and -then- check if openssl version -a indicates built on: Mon Apr  7 20:33:29 UTC 2014.",
        "top_answer": "Based on the date displayed by your version of OpenSSL, it seems you are seeing the full version displayed there.\nOpen SSL 1.0.1 was released on March 14th, 2012.  1.0.1a was released on April 19th of 2012.\nSo, I'm going to go ahead and assert that openssl version -a is the proper, cross-distro way to display the full version of OpenSSL that's installed on the system. It seems to work for all the Linux distros I have access to, and is the method suggested in the help.ubuntu.com OpenSSL documentation, as well.  Ubuntu LTS 12.04 shipped with vanilla OpenSSL v1.0.1, which is the  version that looks like an abbreviated version, on account of not having a letter following it.\nHaving said that, it appears that there is a major bug in Ubuntu (or how they package OpenSSL), in that openssl version -a continues to return the original 1.0.1 version from March 14, 2012, regardless of whether or not OpenSSL has been upgraded to any of the newer versions.  And, as with most things when it rains, it pours.  \nUbuntu is not the only major distro in the habit of backporting updates into OpenSSL (or other packages), rater than relying on the upstream updates and version numbering that everyone recognizes.  In the case of OpenSSL, where the letter version numbers represent only bug fix and security updates, this seems nearly incomprehensible, but I have been informed that this may be because of the FIPS-validated plugin major Linux distros ship packaged with OpenSSL.  Because of requirements around revalidation that trigger due to any change, even changes that plug security holes, it is version-locked.\nFor example, on Debian, the fixed version displays a version number of 1.0.1e-2+deb7u5 instead of the upstream version of 1.0.1g.\nAs a result, at this time, there is no reliable, portable way to check SSL versions across Linux distributions, because they all use their own backported patches and updates with different version numbering schemes.  You will have to look up the fixed version number for each different distribution of Linux you run, and check the installed OpenSSL version against that distribution's specific version numbering to determine if your servers are running a vulnerable version or not.",
        "url": "https://serverfault.com/questions/587324/heartbleed-how-to-reliably-and-portably-check-the-openssl-version"
    },
    {
        "title": "What happens when you plug two sides of a cable to a single networking device?",
        "question": "What is likely to happen when you plug two ends of a network cable to a single switch/router? Will this create problems on the network, or just be ignored?",
        "top_answer": "Depends on the router/switch. \nIf it's \"Managed\" - Like decent Netgear, Cisco or HP Procurve, or has STP (Spanning Tree Protocol) or one of its variants enabled, there's a few seconds of absolute insanity, then the switch realises that there's a loop in the network topology, and blocks one of the ports. \n(I've only described the STP re-convergence as \"absolute insanity\" because if you're using old-style, slow, STP then re-convergence can take 30s or more, depending on network complexity.  Vendor specific STP extensions such as BackboneFast and so on will decrease this, but you might still end up with a short period of a slightly unstable network. Rapid STP is a lot quicker to converge, due to a different algorithm)\nIf it's \"Unmanaged\"- Like pretty much all SOHO grade gear, and a fair proportion of small 4-8 port switches, then all hell breaks loose, as you've just created a loop in a network, and all the traffic tends to just bounce about inside the loop.  \nThe reason this happens is because switches rely on a process of MAC address learning to map MAC addresses to physical ports.  In a non-looped network,  one MAC address will only be visible to the switch on a given physical port.  If you have a loop, then the switch will see multiple paths to the same MAC address, and possibly multiple MAC addresses on multiple ports, so instead of the traffic being switched efficiently, it will be broadcast to wherever it sees the MACs.  This is known as a \"Broadcast Storm\".\nThis can quickly use up all of a switch's CPU power, fill the transmit and receive buffers, as well as polluting the MAC address table.\nBasically, if you create a loop in the network, you'll know about it, either through monitoring (detecting a change in the STP topology [you do have monitoring, right?]), or in everything falling over dramatically.   \nIf you look at a switch that has a broadcast storm on it, you tend to find that all of the port activity lights are blinking all at the same time.",
        "url": "https://serverfault.com/questions/366072/what-happens-when-you-plug-two-sides-of-a-cable-to-a-single-networking-device"
    },
    {
        "title": "Best system administrator accident [closed]",
        "question": "I'm looking for amusing stories of system administrator accidents you have had.  Deleting the CEO's email, formatting the wrong hard drive, etc.\nI'll add my own story as an answer.",
        "top_answer": "I had fun discovering the difference between the linux \"killall\" command (kills all processes matching the specified name, useful for stopping zombies) and the solaris \"killall\" command (kills all processes and halts the system, useful for stopping the production server in the middle of peak hours and getting all your co-workers to laugh at you for a week).",
        "url": "https://serverfault.com/questions/7902/best-system-administrator-accident"
    },
    {
        "title": "How to open port for a specific IP address with firewall-cmd on CentOS? [duplicate]",
        "question": "I would like to open port 4567 for the IP address 1.2.3.4 with the firewall-cmd command on a CentOS 7.1 server.\nHow can I achieve this, as the documentation I could find was too specific on this?",
        "top_answer": "Try this command\nfirewall-cmd --permanent --zone=public --add-rich-rule='\n  rule family=\"ipv4\"\n  source address=\"1.2.3.4/32\"\n  port protocol=\"tcp\" port=\"4567\" accept'\n\nCheck the zone file later to inspect the XML configuration\ncat /etc/firewalld/zones/public.xml\n\nReload the firewall\nfirewall-cmd --reload\n",
        "url": "https://serverfault.com/questions/684602/how-to-open-port-for-a-specific-ip-address-with-firewall-cmd-on-centos"
    },
    {
        "title": "Temporarily ignore my `~/.ssh/known_hosts` file?",
        "question": "Is there a way to temporarily ignore my ~/.ssh/known_hostsfile?\nmbp:~ alexus$ ssh 10.52.11.171\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nxx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx.\nPlease contact your system administrator.\nAdd correct host key in /Users/alexus/.ssh/known_hosts to get rid of this message.\nOffending RSA key in /Users/alexus/.ssh/known_hosts:155\nRSA host key for 10.52.11.171 has changed and you have requested strict checking.\nHost key verification failed.\nmbp:~ alexus$ \n\nNOTE:\n.. by a few answer(s)/comment(s) i realize that my question is a bit misleading, so short it is expected behavior), so it's normal (in my case) there is a valid reason behind it on why I want to see \"ignore it\")",
        "top_answer": "You can use ssh -o StrictHostKeyChecking=no to turn off checking known_hosts momentarily. But I'd advise against this. You should really check why the host key has changed.\nAnother option is to add a specific entry to your ~/.ssh/config for the host in question. This might be valid approach if you have a certain host which generates new host keys every time it reboots and it gets rebooted for a valid reason several times a day.\nHost <your problematic host>\n  StrictHostKeyChecking no\n",
        "url": "https://serverfault.com/questions/559885/temporarily-ignore-my-ssh-known-hosts-file"
    },
    {
        "title": "ssh connection takes forever to initiate, stuck at \"pledge: network\"",
        "question": "Connection to one of my servers using ssh takes more than 20 seconds to initiate.\nThis is not related to LAN or WAN conditions, since connection to itself takes the same (ssh localhost). After connection is finally establised, it is super fast to interract with the server.\nUsing -vvv shows that the connection is stuck after saying \"pledge: network\". At this point, authentication (here using key) is already done, as visible here :\n...\ndebug1: Authentication succeeded (publickey).\nAuthenticated to myserver.mydomain.com ([xx.xx.xx.xx]:22).\ndebug1: channel 0: new [client-session]\ndebug2: channel 0: send open\ndebug1: Requesting [email\u00a0protected]\ndebug1: Entering interactive session.\ndebug1: pledge: network\n\n(...stuck here for 15 to 25 seconds...)\ndebug1: client_input_global_request: rtype [email\u00a0protected] want_reply 0\ndebug2: callback start\ndebug2: fd 3 setting TCP_NODELAY\ndebug2: client_session2_setup: id 0\n...\n\nServer is Ubuntu 16.04. It already happened to me in the past with another server (was Ubuntu 12.04) , nerver found the solution and the problem disapeared  after a while...\nsshd_config is the default one provided by Ubuntu.\nSo far I have tried :\n\nusing -o GSSAPIAuthentication=no in the ssh command \nusing password instead of a key \nusing UsePrivilegeSeparation no instead of yes, in sshd_config\n",
        "top_answer": "This is probably an issue with D-Bus and systemd. If the dbus service is restarted for some reason, you will also need to restart systemd-logind.\nYou can check if this is the issue by opening the ssh daemon log (on Ubuntu it should be /var/log/auth.log) and check if it has these lines:\nsshd[2721]: pam_systemd(sshd:session): Failed to create session: Connection timed out\n\nIf yes, just restart systemd-logind service:\nsystemctl restart systemd-logind\n\nI had this same issue on CentOS 7, because the messagebus was restarted (which is how the D-Bus service is called on CentOS).",
        "url": "https://serverfault.com/questions/792486/ssh-connection-takes-forever-to-initiate-stuck-at-pledge-network"
    },
    {
        "title": "How do I list all connected Salt Stack minions?",
        "question": "Sometimes my saltmaster hangs for a while on salt '*' test.ping waiting for downed minions to reply. Is there a way so see a list of connected minions, regardless of whether they respond to test.ping?",
        "top_answer": "The official answer:\nsalt-run manage.up\n\nAlso useful are:\nsalt-run manage.status\n\nsalt-run manage.down\n",
        "url": "https://serverfault.com/questions/529049/how-do-i-list-all-connected-salt-stack-minions"
    },
    {
        "title": "How to accurately check if package is installed in yum?",
        "question": "I keep getting answers like:\nyum list installed | grep bind\n\nor \nrpm -qa | grep bind\n\nBut that is not accurate as I'm getting a list of few other bind packages like these:\nbind-utils-9.8.2-0.17.rc1.el6_4.5.x86_64\nrpcbind-0.2.0-11.el6.x86_64\nbind-libs-9.8.2-0.17.rc1.el6_4.5.x86_64\nsamba-winbind-3.6.9-151.el6.x86_64\nsamba-winbind-clients-3.6.9-151.el6.x86_64\nypbind-1.20.4-30.el6.x86_64\n\nThat is not I wanted. Instead I would want to accurately check if bind core package has been installed. Eg. bind.x86_64 32:9.8.2-0.17.rc1.el6_4.6\nI was hoping for something like:\nyum check installed bind\n\nBut hopefully someone could shed the light.",
        "top_answer": "Have you tried this?\n$ yum list installed bind\n",
        "url": "https://serverfault.com/questions/558936/how-to-accurately-check-if-package-is-installed-in-yum"
    },
    {
        "title": "What is the Windows equivalent of Unix \"whoami\" command?",
        "question": "Is there a Windows equivalent of Unix \"whoami\" command?  If so, what is it?",
        "top_answer": "Since Windows 2000, the whoami command has been part of the standard command line (thanks to pk for clearing that up in comments!).\nYou can do this: Open a command prompt and type \"set\" then hit enter.  This shows active environment variables.  Current logged on username is stored in the USERNAME env variable and your domain is stored in the USERDOMAIN variable.\nTo piggy-back off the other answers, from a cmd line:\necho %USERDOMAIN%\\%USERNAME%\n\nwill get you the complete logged on user in domain\\username format.\nYou can do the same thing with Powershell with this:\nwrite-host $env:userdomain\\$env:username\n",
        "url": "https://serverfault.com/questions/9977/what-is-the-windows-equivalent-of-unix-whoami-command"
    },
    {
        "title": "Get list of AD groups a user is a member of",
        "question": "Suppose I have the user id of a user in Active Directory.  I'd like to get a list of all AD groups in which that user is currently a member of.  How can I do this from the Windows command line?\nI've tried the following:\ndsget user \"DC=jxd123\" -memberof\n\nError:\ndsquery failed:'-memberof' is an unknown parameter.\ntype dsquery /? for help.\n",
        "top_answer": "You can do this in PowerShell pretty easily. I'm sure you can do it with the ds tools too, but they're old and crusty and PowerShell should be used for everything possible nowadays.\nImport-Module ActiveDirectory\n(Get-ADUser userName \u2013Properties MemberOf | Select-Object MemberOf).MemberOf\n\nShorter version\n(Get-ADUser userName \u2013Properties MemberOf).MemberOf\n",
        "url": "https://serverfault.com/questions/532106/get-list-of-ad-groups-a-user-is-a-member-of"
    },
    {
        "title": "10 servers to administer and I'm a history major [closed]",
        "question": "Through reasons that don't warrant exhaustive discussion, I find myself in charge of 10 servers:\n\nA domain controller-~500 hosts/~350 users\nIIS web server-This is where we make our money\nSQL server-The crown jewels\nExchange server\nLinux box for data entry\nAV server\nBackup server\nA few others tossed around\n\nThe company where I work believes everybody is replaceable and therefore believes they can pay a minimum wage for any position. The IT manager and Sysadmin quit recently and I think I was the only person who did not take a big step backward when the call went out for volunteers. This also explains why someone with my background is in this position. That is the reality, as much as I wish it otherwise.\nWhat are the things I should be doing to keep those systems running? There is no written procedure left behind and I crammed the A+ and Network+ certs in the last two months but that leaves me with some theory and no practical experience.\nI am in the process of teaching myself powershell but from here to there is a long way. I have no scripting or programming experience.\nWhat tasks should I be performing? What practices should I implement?\nI understand I am probably hosed but a lifeline to get me through would be helpful.",
        "top_answer": "Honestly, I would find another job, unless your current task is just to keep everything running until they hire a new SysAdmin. You are being setup for failure. You are doing the job of at least two people if this is all hosted locally and nothing is documented.\nDon't worry about the scripting or programming anything just yet. Get a handle on keeping everything running.\nAre you in charge of the corporate firewall too? \nThe quick and dirty daily tasks I see you needing to do are (in no particular order):\n\ncheck the nightly backups\ncheck the exchange queues to make sure they are processing\ncheck the SQL backups\ncheck the AV server for alerts if anything failed to scan or update\n",
        "url": "https://serverfault.com/questions/339426/10-servers-to-administer-and-im-a-history-major"
    },
    {
        "title": "Nginx redirect one path to another",
        "question": "I'm sure this has been asked before, but I can't find a solution that works.\nA website has switched CMS services, but has the same domain, how do I set up an nginx rewrite for a single page?\nE.g. \nOld Page\nhttp://sitedomain.co.uk/content/unique-page-name\nNew page\nhttp://sitedomain.co.uk/new-name/unique-page-name\nPlease note, I don't want everything within the content page to be redirected, but literally just the url mentioned above. I have about 9 redirects to set up, non of which fit in a pattern.\nThanks!\nEdit: I found this solution, which seems to be working, except for the fact that it redirects without a slash:\nif ( $request_filename ~ content/unique-page-name/ ) {\n   rewrite ^ http://sitedomain.co.uk/new-name/unique-page-name/? permanent;\n}\n\nBut this redirects to:\nhttp://sitedomain.co.uknew-name/unique-page-name/",
        "top_answer": "Direct quote from Pitfalls and Common Mistakes: Taxing Rewrites:\n\nBy using the return directive we can completely avoid evaluation of regular expression.\n\nPlease use return instead of rewrite for permanent redirects. Here's my approach to this use-case...\nlocation = /content/unique-page-name {\n  return 301 /new-name/unique-page-name;\n}\n",
        "url": "https://serverfault.com/questions/548591/nginx-redirect-one-path-to-another"
    },
    {
        "title": "How does Ubuntu keep track of the \"System restart required\" flag in motd?",
        "question": "I have a few Ubuntu servers (8.10, 9.10) that is set to automatically install security updates. Some times these updates requires a restart of the system, and this string is shown in motd:\n*** System restart required ***\n\nTo get a notice about these, I plan to write a Nagios test to monitor if the server is in need of a reboot. So, my question:\nIs there a better way than parsing /etc/motd to find out if a reboot is needed?",
        "top_answer": "Check for the presence of /var/run/reboot-required.",
        "url": "https://serverfault.com/questions/92932/how-does-ubuntu-keep-track-of-the-system-restart-required-flag-in-motd"
    },
    {
        "title": "How to view connected users to open vpn server?",
        "question": "I'm developing a website for managing OpenVPN users with Django framework. But I need to know is there any way to extract active users from OpenVPN? My server is running Ubuntu 12.04.",
        "top_answer": "There should be a status log you can look at to show you, mine is, for examle:\ncat /etc/openvpn/openvpn-status.log\nEDIT:\nAs an alternative, adding the flag --management IP port [pw-file] or adding that same directive to your server.conf, for example: \nmanagement localhost 7505\nThis would allow  you to telnet to that port and offer you a list of commands to run:\ntelnet localhost 7505\nhelp",
        "url": "https://serverfault.com/questions/571592/how-to-view-connected-users-to-open-vpn-server"
    },
    {
        "title": "nginx real_ip_header and X-Forwarded-For seems wrong",
        "question": "The wikipedia description of the HTTP header X-Forwarded-For is:\n\nX-Forwarded-For: client1, proxy1, proxy2, ...\n\nThe nginx documentation for the directive real_ip_header reads, in part:\n\nThis directive sets the name of the header used for transferring the replacement IP address.\n  In case of X-Forwarded-For, this module uses the last ip in the X-Forwarded-For header for replacement.  [Emphasis mine]\n\nThese two descriptions seem at odds with one another.  In our scenario, the X-Forwarded-For header is exactly as described -- the client's \"real\" IP address is the left-most entry.  Likewise, the behavior of nginx is to use the right-most value -- which, obviously, is just one of our proxy servers.  \nMy understanding of X-Real-IP is that it is supposed to be used to determine the actual client IP address -- not the proxy.  Am I missing something, or is this a bug in nginx?\nAnd, beyond that, does anyone have any suggestions for how to make the X-Real-IP header display the left-most value, as indicated by the definition of X-Forwarded-For?",
        "top_answer": "I believe the key to solving X-Forwarded-For woes when multiple IPs are chained is the recently introduced configuration option, real_ip_recursive (added in nginx 1.2.1 and 1.3.0). From the nginx realip docs:\n\nIf recursive search is enabled, an original client address that matches one of the trusted addresses is replaced by the last non-trusted address sent in the request header field.\n\nnginx was grabbing the last IP address in the chain by default because that was the only one that was assumed to be trusted. But with the new real_ip_recursive enabled and with multiple set_real_ip_from options, you can define multiple trusted proxies and it will fetch the last non-trusted IP.\nFor example, with this config:\nset_real_ip_from 127.0.0.1;\nset_real_ip_from 192.168.2.1;\nreal_ip_header X-Forwarded-For;\nreal_ip_recursive on;\n\nAnd an X-Forwarded-For header resulting in:\nX-Forwarded-For: 123.123.123.123, 192.168.2.1, 127.0.0.1\n\nnginx will now pick out 123.123.123.123 as the client's IP address.\nAs for why nginx doesn't just pick the left-most IP address and requires you to explicitly define trusted proxies, it's to prevent easy IP spoofing.\nLet's say a client's real IP address is 123.123.123.123. Let's also say the client is up to no good, and they're trying to spoof their IP address to be 11.11.11.11. They send a request to the server with this header already in place:\nX-Forwarded-For: 11.11.11.11\n\nSince reverse proxies simply add IPs to this X-Forwarded-For chain, let's say it ends up looking like this when nginx gets to it:\nX-Forwarded-For: 11.11.11.11, 123.123.123.123, 192.168.2.1, 127.0.0.1\n\nIf you simply grabbed the left-most address, that would allow the client to easily spoof their IP address. But with the above example nginx config, nginx will only trust the last two addresses as proxies. This means nginx will correctly pick 123.123.123.123 as the IP address, despite that spoofed IP actually being the left-most.",
        "url": "https://serverfault.com/questions/314574/nginx-real-ip-header-and-x-forwarded-for-seems-wrong"
    },
    {
        "title": "How long does negative DNS caching typically last?",
        "question": "If a DNS server looks up a record and it's missing, it will often \"negatively cache\" the fact that this record is missing, and not try to look it up again for a while.  I don't see anything in the RFC about the TTL on negative caching should be, so I'm guessing it's somewhat arbitrary.  In the real world, how long do these negative records stick around for?",
        "top_answer": "The TTL for negative caching is not arbitrary. It is taken from the SOA record at the top of the zone to which the requested record would have belonged, had it existed. For example:\nexample.org.    IN      SOA     master-ns1.example.org. Hostmaster.example.org. (\n            2012091201 43200 1800 1209600 86400 )\n\nThe last value in the SOA record (\"86400\") is the amount of time clients are asked to cache negative results under example.org..\nIf a client requests doesnotexist.example.org., it will cache the result for 86400 seconds.",
        "url": "https://serverfault.com/questions/426807/how-long-does-negative-dns-caching-typically-last"
    },
    {
        "title": "What does * * * * * (five asterisks) in a cron file mean? [duplicate]",
        "question": "The first noncomment line in a legacy crontab file begins with five asterisks:\n* * * * * ([a_command]) >/dev/null 2>&1\n\nThe authors are gone, so I do not know their intent.  What does all-wildcards mean to (Solaris 8) cron?  The betting here is either run once, run continuously, or run never, which is unfortunately broad.\nIf you are wondering about the comment line preceding this, it is \"Do not delete.\"\nNote: This cron file is working. This question is not a duplicate of a question about broken cron files or cron files which require troubleshooting.",
        "top_answer": "Every minute of every day of every week of every month, that command runs.\nman 5 crontab has the documentation of this. If you just type man crontab, you get the documentation for the crontab command. What you want is section 5 of the manual pages which covers system configuration files including the /etc/crontab file. For future reference, the sections are described in man man:\n   1   Executable programs or shell commands\n   2   System calls (functions provided by the kernel)\n   3   Library calls (functions within program libraries)\n   4   Special files (usually found in /dev)\n   5   File formats and conventions eg /etc/passwd\n   6   Games\n   7   Miscellaneous  (including  macro  packages and conven\u2010\n       tions), e.g. man(7), groff(7)\n   8   System administration commands (usually only for root)\n   9   Kernel routines [Non standard]\n",
        "url": "https://serverfault.com/questions/162388/what-does-five-asterisks-in-a-cron-file-mean"
    },
    {
        "title": "Do SPF Records For Primary Domain apply to subdomains?",
        "question": "I have a quick question regarding SPF records: Do they need to be present for all subdomains? \nLets say that I have a TXT record with SPF info for domain.com \nLet's also say that I have a seperate email domain for subdomain.domain.com \nWill the SPF policy/info for domain.com also apply to the subdomain? Or do I need to add a separate TXT record for that too? ",
        "top_answer": "You need to have separate SPF records for each subdomain you wish to send mail from.\nThe following was originally posted on openspf.org, which used to be a great resource for this kind of thing.\nLatest link http://www.open-spf.org/FAQ/The_demon_question/\n\nThe Demon Question: What about subdomains?\nIf I get mail from\npielovers.demon.co.uk, and there's no SPF data for pielovers, should I\ngo back one level and test SPF for demon.co.uk? No. Each subdomain at\nDemon is a different customer, and each customer might have their own\npolicy. It wouldn't make sense for Demon's policy to apply to all its\ncustomers by default; if Demon wants to do that, it can set up SPF\nrecords for each subdomain.\nSo the advice to SPF publishers is this: you should add an SPF record\nfor each subdomain or hostname that has an A or MX record.\nSites with wildcard A or MX records should also have a wildcard SPF\nrecord, of the form: * IN TXT \"v=spf1 -all\"\n\nThis makes sense - a subdomain may very well be in a different geographical location and have a very different SPF definition.\nThe 'include:' directive for SPF may be used to provide all subdomains with the same entries.  For example, on the SPF record for subdomain mailfrom.example.com enter 'include:example.com'. In this fashion whenever you update the definition for example.com your subdomains will automatically pick up the updated values.",
        "url": "https://serverfault.com/questions/322949/do-spf-records-for-primary-domain-apply-to-subdomains"
    },
    {
        "title": "How to reload default Mac OSX routing table without rebooting",
        "question": "Greetings, \nI'm using vpnc for a VPN client. I'm also doing some tricky things with route to make sure I can still access my local network, etc. etc. (the particulars here are not very important).\nSometimes I get the routing table so jacked up I get ping: sendto: Network is unreachable for urls that should otherwise resolve. \nCurrently, if I restart Mac OS X then everything is back to normal. What I'd like to do is reset the routing tables to the \"default\" (e.g. what it is set to at boot) without a whole system reboot. \nI think that step 1 is route flush (to remove all routes). And step 2 needs to reload all of the default routes.\nAny thoughts on how to do this? (e.g. what is step 2?)\nEDIT\nAlso, I'm noticing another symptom is traceroute also fails on the address in question. For instance:\ntraceroute the.good.dns.name\ntraceroute: bind: Can't assign requested address",
        "top_answer": "You need to flush the routes . Use route -n flush several times . Afterwards add your routes with route add. ",
        "url": "https://serverfault.com/questions/78365/how-to-reload-default-mac-osx-routing-table-without-rebooting"
    },
    {
        "title": "How do I redirect subdomains to a different port on the same server?",
        "question": "I have some subdomains I want to redirect to specific ports on the same server.\nSay I have\ndev.mydomain.com \n\nI want dev.mydomain.com to transparently redirect to mydomain.com:8080 and I want to preserve the original sub-domain name the url of the browser.\nHow do I do this with Apache 2.2? I have Apache 2.2 running on default port 80. I can't figure out the write configuration to get this to happen.\nI have already set up dev.mydomain.com to resolve in DNS to mydomain.com.\n\nThis is for an intranet development server that has a non-routable ip\n  address so I am not so concerned about exploits and security that\n  would compromise a publicly facing server.\n",
        "top_answer": "Solution\nHere is what I finally came up with after being set in the right direction by Miles Erickson. I wanted the address bar to reflect the original subdomain/domain of the request and not the redirected server and port, but he put me on the right path to Google up a solution using VirtualHost and I finally found a solution that included the use of mod_proxy.\nFirst, make sure mod_proxy is enabled:\nsudo a2enmod proxy\nsudo a2enmod proxy_http\nsudo a2enmod proxy_balancer\nsudo a2enmod lbmethod_byrequests\nsudo systemctl restart apache2\n\nNext, add the following to your site config (e.g., /etc/apache2/sites-available/000-default.conf):\n<VirtualHost *:80>\n    ServerAdmin [email\u00a0protected]\n    ServerName dev.mydomain.com\n    ProxyPreserveHost On\n\n    # setup the proxy\n    <Proxy *>\n        Order allow,deny\n        Allow from all\n    </Proxy>\n    ProxyPass / http://localhost:8888/\n    ProxyPassReverse / http://localhost:8888/\n</VirtualHost>\n",
        "url": "https://serverfault.com/questions/195611/how-do-i-redirect-subdomains-to-a-different-port-on-the-same-server"
    },
    {
        "title": "Practical maximum open file descriptors (ulimit -n) for a high volume system",
        "question": "We recently began load testing our application and noticed that it ran out of file descriptors after about 24 hours.\nWe are running RHEL 5 on a Dell 1955:\nCPU:  2 x Dual Core 2.66GHz 4MB 5150 / 1333FSB\nRAM:  8GB RAM \nHDD:  2 x 160GB 2.5\" SATA Hard Drives\nI checked the file descriptor limit and it was set at 1024.  Considering that our application could potentially have about 1000 incoming connections as well as a 1000 outgoing connections, this seems quite low.  Not to mention any actual files that need to be opened.\nMy first thought was to just increase the ulimit -n parameter by a few orders of magnitude and then re-run the test but I wanted to know any potential ramifications of setting this variable too high.\nAre there any best practices towards setting this other than figuring out how many file descriptors our software can theoretically open?",
        "top_answer": "These limits came from a time where multiple \"normal\" users (not apps) would share the server, and we needed ways to protect them from using too many resources.\nThey are very low for high performance servers and we generally set them to a very high number. (24k or so) If you need higher numbers, you also need to change the sysctl file-max option (generally limited to 40k on ubuntu and 70k on rhel) .\nSetting ulimit:\n# ulimit -n 99999\n\nSysctl max files:\n#sysctl -w fs.file-max=100000\n\nAlso, and very important, you may need to check if your application has a memory/file descriptor leak. Use lsof to see all it has open to see if they are valid or not. Don't try to change your system to work around applications bugs.",
        "url": "https://serverfault.com/questions/48717/practical-maximum-open-file-descriptors-ulimit-n-for-a-high-volume-system"
    },
    {
        "title": "What does passing the -xe parameters to /bin/bash do",
        "question": "Exactly what the title says. I'm not having much luck finding the proper documentation to see what -xe does in the following use case:\n#!/bin/bash -xe\nwhat do those parameters do and where it is documented?",
        "top_answer": "If you read the man page for bash you'll find the following at the top of the OPTIONS section:\n\n All of the  single-character shell options documented in the\n description of the set builtin command can be used as options when the\n shell is invoked. In addition, bash interprets the following options\n when it is invoked...\n\nAnd if you read the documentation for the set command later on in the man page, you'll find:\n\n -e      Exit  immediately  if a pipeline (which may consist of a\n single simple command),  a subshell command enclosed in parentheses,\n or one of the commands executed as part of a command list enclosed by\n braces (see SHELL GRAMMAR above) exits with a non-zero  status. \n\n -x      After expanding each simple command, for command, case\n command, select command, or arithmetic  for  command,  display\n the  expanded value of PS4, followed by the command and its\n expanded arguments or associated word list.\n\nIn other words, -e makes the shell exit immediately whenever\nsomething returns an error (this is often used in shell scripts as a\nfailsafe mechanism), and -x enables verbose execution of scripts so\nthat you can see what's happening.",
        "url": "https://serverfault.com/questions/391255/what-does-passing-the-xe-parameters-to-bin-bash-do"
    },
    {
        "title": "Recommended LogParser queries for IIS monitoring?",
        "question": "As Stack Overflow grows, we're starting to look closely at our IIS logs to identify problem HTTP clients -- things like rogue web spiders, users who have a large page set to refresh every second, poorly written one-off web scrapers, tricksy users who try to increment page count a zillion times, and so forth.  \nI've come up with a few LogParser queries that help us identify most of the oddities and abnormalities when pointed at an IIS log file. \nTop bandwidth usage by URL\nSELECT top 50 DISTINCT \nSUBSTR(TO_LOWERCASE(cs-uri-stem), 0, 55) AS Url, \nCount(*) AS Hits, \nAVG(sc-bytes) AS AvgBytes, \nSUM(sc-bytes) as ServedBytes \nFROM {filename} \nGROUP BY Url \nHAVING Hits >= 20 \nORDER BY ServedBytes DESC\n\n\nurl                                                   hits  avgbyte  served\n-------------------------------------------------     ----- -------  -------\n/favicon.ico                                          16774 522      8756028\n/content/img/search.png                               15342 446      6842532\n\nTop hits by URL\nSELECT TOP 100 \ncs-uri-stem as Url, \nCOUNT(cs-uri-stem) AS Hits \nFROM {filename} \nGROUP BY cs-uri-stem \nORDER BY COUNT(cs-uri-stem) DESC\n\n\nurl                                                                    hits\n-------------------------------------------------                      -----\n/content/img/sf/vote-arrow-down.png                                    14076\n/content/img/sf/vote-arrow-up.png                                      14018\n\nTop bandwidth and hits by IP / User-Agent\nSELECT TOP 30\nc-ip as Client, \nSUBSTR(cs(User-Agent), 0, 70) as Agent, \nSum(sc-bytes) AS TotalBytes, \nCount(*) as Hits \nFROM {filename} \ngroup by c-ip, cs(User-Agent) \nORDER BY TotalBytes desc\n\n\nclient         user-agent                                      totbytes   hits\n-------------  ---------------------------------------------   ---------  -----\n66.249.68.47   Mozilla/5.0+(compatible;+Googlebot/2.1;         135131089  16640\n194.90.190.41  omgilibot/0.3++omgili.com                       133805857  6447\n\nTop bandwidth by hour by IP / User-Agent\nSELECT TOP 30\nTO_STRING(time, 'h') as Hour, \nc-ip as Client, \nSUBSTR(cs(User-Agent), 0, 70) as Agent, \nSum(sc-bytes) AS TotalBytes, \ncount(*) as Hits \nFROM {filename} \ngroup by c-ip, cs(User-Agent), hour \nORDER BY sum(sc-bytes) desc\n\n\nhr   client        user-agent                                  totbytes   hits\n--   ------------- -----------------------------------------   --------   ----\n9    194.90.190.41 omgilibot/0.3++omgili.com                   30634860   1549\n10   194.90.190.41 omgilibot/0.3++omgili.com                   29070370   1503\n\nTop hits by hour by IP / User-Agent\nSELECT TOP 30\nTO_STRING(time, 'h') as Hour, \nc-ip as Client, \nSUBSTR(cs(User-Agent), 0, 70) as Agent, \ncount(*) as Hits, \nSum(sc-bytes) AS TotalBytes \nFROM {filename} \ngroup by c-ip, cs(User-Agent), hour \nORDER BY Hits desc\n\n\nhr   client         user-agent                                  hits  totbytes\n--   -------------  -----------------------------------------   ----  --------\n10   194.90.190.41  omgilibot/0.3++omgili.com                   1503  29070370\n12   66.249.68.47   Mozilla/5.0+(compatible;+Googlebot/2.1      1363  13186302\n\nThe {filename} of course would be a path to an IIS logfile, such as\nc:\\working\\sologs\\u_ex090708.log\nI did a lot of web searches for good IIS LogParser queries and found precious little. These 5, above, have helped us tremendously in identifying serious problem clients. But I'm wondering -- what are we missing?\nWhat other ways are there to slice and dice the IIS logs (preferably with LogParser queries) to mine them for statistical anomalies? Do you have any good IIS LogParser queries you run on your servers? ",
        "top_answer": "A good indicator for hacking activies or other attacks is the number of errors per hour. The following script returns the dates and hours that had more than 25 error codes returned. Adjust the value depending on the amount of traffic on the site (and the quality of your web application ;-) ).\nSELECT date as Date, QUANTIZE(time, 3600) AS Hour, \n       sc-status as Status, count(*) AS ErrorCount\nFROM   {filename} \nWHERE  sc-status >= 400 \nGROUP BY date, hour, sc-status \nHAVING ErrorCount > 25\nORDER BY ErrorCount DESC\n\nThe result could something like this:\n\nDate       Hour     Status ErrorCount\n---------- -------- ------ ------\n2009-07-24 18:00:00 404    187\n2009-07-17 13:00:00 500    99\n2009-07-21 21:00:00 404    80\n2009-07-03 04:00:00 404    45\n...\n\nThe next query detects an unusually high number of hits on a single URL from one IP address. In this example I chose 500, but you may have to change the query for edge cases (excluding the IP address of Google London for example ;-) .)\nSELECT DISTINCT date AS Date, cs-uri-stem AS URL,\n      c-ip AS IPAddress, Count(*) AS Hits\nFROM  {filename}\nGROUP BY date, c-ip, cs-uri-stem\nHAVING Hits > 500\nORDER BY Hits Desc\n\n\nDate       URL                                 IPAddress       Hits\n---------- ----------------------------------- --------------- ----\n2009-07-24 /Login.aspx                         111.222.111.222 1889\n2009-07-12 /AccountUpdate.aspx                 11.22.33.44     973\n2009-07-19 /Login.aspx                         123.231.132.123 821\n2009-07-21 /Admin.aspx                         44.55.66.77     571\n...\n",
        "url": "https://serverfault.com/questions/45516/recommended-logparser-queries-for-iis-monitoring"
    },
    {
        "title": "How to check the physical status of an ethernet port in Linux?",
        "question": "I want to check if a specified ethX is physically up or down. How do I do that with the command line?",
        "top_answer": "$ ethtool <eth?>\nFor example:\n$ ethtool eth0\n\nprovides:\nSettings for eth0:\n        Supported ports: [ TP ]\n        Supported link modes:   10baseT/Half 10baseT/Full\n                                100baseT/Half 100baseT/Full\n                                1000baseT/Full\n        Supports auto-negotiation: Yes\n        Advertised link modes:  10baseT/Half 10baseT/Full\n                                100baseT/Half 100baseT/Full\n                                1000baseT/Full\n        Advertised pause frame use: No\n        Advertised auto-negotiation: Yes\n        Speed: 1000Mb/s\n        Duplex: Full\n        Port: Twisted Pair\n        PHYAD: 1\n        Transceiver: internal\n        Auto-negotiation: on\n        MDI-X: on\n        Supports Wake-on: pumbg\n        Wake-on: g\n        Current message level: 0x00000001 (1)\n        Link detected: yes\n",
        "url": "https://serverfault.com/questions/15776/how-to-check-the-physical-status-of-an-ethernet-port-in-linux"
    },
    {
        "title": "IIS 7.5 (Windows 7) - HTTP Error 401.3 - Unauthorized",
        "question": "I'm trying to test my ASP.Net website on localhost and I'm getting this error:\nHTTP Error 401.3 - Unauthorized\nYou do not have permission to view this directory or page because of\nthe access control list (ACL) configuration or encryption settings for\nthis resource on the Web server.\n\nI have the following users on the website application folder, with full read/write permissions:\n\nNETWORK SERVICE\nIIS_IUSRS\nSYSTEM\nAdministrators\nNathan (me)\n\nWhat can I try to fix this?",
        "top_answer": "IIS 7 also creates \"IUSR\" as default user to access files via IIS. So make user IUSR has read access to files/folders.\nHow to check if IUSR has read Access?\nRight Click -> Folder -> Properties -> Security Tab\nSee if IUSR is in Group or user names list, If No.\nClick Edit -> Add -> Advanced -> Find Now -> Select IUSR and click OK four times",
        "url": "https://serverfault.com/questions/38222/iis-7-5-windows-7-http-error-401-3-unauthorized"
    },
    {
        "title": "Copy directory structure intact to AWS S3 bucket",
        "question": "I want to use the AWS S3 cli to copy a full directory structure to an S3 bucket.\nSo far, everything I've tried copies the files to the bucket, but the directory structure is collapsed. (to say it another way, each file is copied into the root directory of the bucket)\nThe command I use is:\naws s3 cp --recursive ./logdata/ s3://bucketname/\n\nI've also tried leaving off the trailing slash on my source designation (ie, the copy from argument). I've also used a wildcard to designate all files ... each thing I try simply copies the log files into the root directory of the bucket.",
        "top_answer": "(Improving the solution of Shishir)  \n\nSave the following script in a file (I named the file s3Copy.sh)\n\npath=$1 # the path of the directory where the files and directories that need to be copied are located\ns3Dir=$2 # the s3 bucket path\n\nfor entry in \"$path\"/*; do\n    name=`echo $entry | sed 's/.*\\///'`  # getting the name of the file or directory\n    if [[ -d  $entry ]]; then  # if it is a directory\n        aws s3 cp  --recursive \"$name\" \"$s3Dir/$name/\"\n    else  # if it is a file\n        aws s3 cp \"$name\" \"$s3Dir/\"\n    fi\ndone\n\n\nRun it as follows:\n/PATH/TO/s3Copy.sh  /PATH/TO/ROOT/DIR/OF/SOURCE/FILESandDIRS  PATH/OF/S3/BUCKET\nFor example if s3Copy.sh is stored in the home directory and I want to copy all the files and directories located in the current directory, then I run this:\n~/s3Copy.sh . s3://XXX/myBucket\n\nYou can easily modify the script to allow for other arguments of s3 cp such as --include, --exclude, ...",
        "url": "https://serverfault.com/questions/682708/copy-directory-structure-intact-to-aws-s3-bucket"
    },
    {
        "title": "Automate the installation of postfix on Ubuntu",
        "question": "My system configuration script does an apt-get install -y postfix.  Unfortunately the script is halted when the postfix installer displays a configuration screen. Is there a method to force postfix to use the defaults during installation so that an automated script can continue to the end?\nDoes the postfix installer maybe check for existing configuration in /etc/postfix, and if it exists, not bother the user with the configuration screen?",
        "top_answer": "You can use pre-seeding for this, using the debconf-set-selections command to pre-answer the questions asked by debconf before installing the package.\nFor example:\ndebconf-set-selections <<< \"postfix postfix/mailname string your.hostname.com\"\ndebconf-set-selections <<< \"postfix postfix/main_mailer_type string 'Internet Site'\"\napt-get install --assume-yes postfix\n",
        "url": "https://serverfault.com/questions/143968/automate-the-installation-of-postfix-on-ubuntu"
    },
    {
        "title": "How to get Apache2 to redirect to a subdirectory",
        "question": "I am running apache2 on Debian etch, with multiple virtual hosts.\nI want to redirect so that http://git.example.com goes to http://git.example.com/git/\nShould be really simple, but Google isn't quite cutting it. I've tried the Redirect and Rewrite stuff and they don't quite seem to do what I want ...",
        "top_answer": "Feel a bit silly - a bit more googling turned up the answer I was after:\nRedirectMatch ^/$ /git/\n\nBasically redirecting the root, and only the root.\nThis code could do in a .htaccess file (there is a tag for this, so I assume that is the original use case).  But if you can edit ,the main server apache config then put it in the section for your website probably inside a <VirtualHost> section.  \nThe docs for RedirectMatch say that the context can be \"server config, virtual host, directory, .htaccess\".",
        "url": "https://serverfault.com/questions/9992/how-to-get-apache2-to-redirect-to-a-subdirectory"
    },
    {
        "title": "Is it possible to use rsync over sftp (without an ssh shell)?",
        "question": "Rsync over ssh, works great every time. \nHowever, trying to rsync to a host which allows only sftp logins, but not ssh logins, provides the following error:\n\nrsync -av /source ssh user@remotehost:/target/\nprotocol version mismatch -- is your\n  shell clean? (see the rsync man page\n  for an explanation) rsync error:\n  protocol incompatibility (code 2) at\n  compat.c(171) [sender=3.0.6]\n\nHere's the relevant section from the rsync man page:\n\nThis  message  is usually caused by your startup scripts or\n  remote shell facility producing\n  unwanted garbage on the stream that\n  rsync is using for\n  its transport. The way to diagnose this problem is to run your\n  remote shell like this:\n          ssh remotehost /bin/true > out.dat\n\nthen look at out.dat. If everything is working correctly then\n  out.dat should be a zero length file.\n  If you are getting the above error\n  from  rsync\n         then  you will probably find that out.dat contains some text or\n  data. Look at the contents and try to\n  work out what is producing it. The\n  most com\u2010\n         mon cause is incorrectly configured shell startup scripts (such\n  as .cshrc or .profile) that contain\n  output statements for non-interactive\n  logins.\n\nTrying this on my system produced the following in out.dat:\n\nssh-dummy-shell: Command not allowed.\n\nAs I thought, the host is not allowing ssh logins.\nThe following link shows that it is possible to accomplish this task using fuse with sshfs - however it is extremely slow, and not fit for production use.\nIs there any chance of getting rsync sftp to work?",
        "top_answer": "Unfortunately not directly. rsync requires a clean link with a shell that will allow it to start the remote copy of rsync, when run this way.\nIf you have some way of running long-lived listening processes on the host you could try starting rsync manually listening for connections on a non-privileged port, but most techniques for doing that would require proper shell access via SSH too, and it relies on the hosts firewall arrangements letting connections in on the port you chose (and the host having rsync installed in the first place). Running rsync as a publicly addressable service (rather than indirectly via SSH or similar) is not generally recommended for non-public data though.\nIf you host allows scripting in PHP or similar and does not have it locked down so extra processes can not be execed by user scripts, then you could try starting rsync in listening mode that way. If your end is connectible (you are running SSH accessible to the outside world) you could try this in reverse - have a script run rsync on the server but instead of listening for incoming connections have it contact your local service and sync that way. This still relies on rsync actually being installed on the host which is not a given, or that you can upload a working copy, but does not have the security implications of running an rsync daemon in a publicly addressable fashion and talking to it over an unencrypted channel.\nMessing around as described above may be against the hosts policies though, even if it works at all, and could get you kicked off. You are better off asking if a full shell can be enabled for that account and either abandoning rsync for that host or abandoning that host and moving elsewhere if they will not do that.",
        "url": "https://serverfault.com/questions/135618/is-it-possible-to-use-rsync-over-sftp-without-an-ssh-shell"
    },
    {
        "title": "logrotating files in a directories and its subdirectories",
        "question": "Is it possible to get logrotate to consider logfiles in a directory and all its subdirectories?  (i.e. without explicitly listing the subdirectories.)",
        "top_answer": "How deep do your subdirectories go?\n/var/log/basedir/*.log /var/log/basedir/*/*.log {\n    daily\n    rotate 5\n}\n\nWill rotate all .log files in basedir/ as well as all .log files in any direct child of basedir. If you also need to go 1 level deeper just add another /var/log/basedir/*/*/*.log until you have each level covered.\nThis can be tested by using a separate logrotate config file which contains a constraint that will not be met (a high minsize) and then running log rotate yourself in verbose mode\nlogrotate -d testconfig.conf\n\nthe -d flag will list each log file it is considering to rotate.",
        "url": "https://serverfault.com/questions/208006/logrotating-files-in-a-directories-and-its-subdirectories"
    },
    {
        "title": "What's wrong with always being root?",
        "question": "I have a feeling this is a stupid question, but this is something I've wondered for awhile.\nI have a VPS and this is my first big linux venture. I am the only person who has access to it. My question is, what is wrong with just logging in as root as opposed to making an account and giving them sudo access? If a sudoer can do everything root can, then what's the difference? If a hacker could crack my password to my standard, non-root account, then he could also execute sudo commands, so how does a hacker cracking my root account matter any more or less?",
        "top_answer": "If you're logged in as root, you can easily wipe directories or do something that in retrospect is really dumb on the system with the flip of a finger, while as a user you normally have to put a few extra mental cycles into what you're typing before doing something that is dangerous.\nAlso any program you run as root as root privileges, meaning if someone or something gets you to run/compile/browse a website that is dangerous and wants to damage your system, such as a trojan or other malware, it has full access to your system and can do what it wants, including access to TCP ports below 1024 (so it can turn your system into a remailer without your knowledge, for example).\nBasically you're kind of asking for trouble that logging in as yourself may prevent. I've known many people that ended up being glad they had that safety net in a moment of carelessness.\nEDIT: There is also the issue of root being the most well known, thus an easy target, for scripts and hacks. Systems that disable the account and instead force users to use sudo means that any attempt to crack root from ssh or a local exploit to the account are banging their heads against a wall. They'd have to guess/crack a password and username. It's security through obscurity to a degree but it's hard to argue that it doesn't foil most script kiddie attacks.",
        "url": "https://serverfault.com/questions/57962/whats-wrong-with-always-being-root"
    },
    {
        "title": "How to backup 20+TB of data?",
        "question": "We have a NAS server at the company I work for that is being used for storing photography sessions. Each session is approximately 100gb. Over the last couple of years this server has accumulated 10+ TB of data, and we are increasing the amount of photoshoots exponentially. I estimate that by the end of next year we will have 20+ TB stored on this NAS. We are currently backing this server up to tape using LTO-5 tapes with Symantec BackupExec. Since the size of this server has grown, full backups of this server are not completing overnight. Does anyone have any suggestion on how to backup this amount of data? Should we be backing it up to tape? Are there any other options which may be better?",
        "top_answer": "You need to take a step back and stop thinking \"I've got 20TB on my NAS I need to back up!\" and develop a storage strategy that takes into account the nature of your data:\n\nWhere is it coming from and how much new data are you getting? (you've got this in your question)\nHow is the data used once you have it?  Are people editing the pictures?  Do you keep the originals and generate edited versions?  \nHow long do you need to keep all the data?  Are people still making changes to pictures from 2 years ago?  \n\nDepending on the answers to the last two questions, you probably need more of a Archiving System than a radically different backup system.  \nData that is static (e.g. 2 year old pictures that you retain \"just in case\") doesn't need to be backed up every night, or even every week, it needs to be archived.  What you actually do might be more complex, but conceptually, all the old pictures can be written off to tape (multiple copies!) and not backed up any more.\nBased on your comments, some additional thoughts:\n\nSince you keep the originals of each shoot untouched and work on a copy, and assuming that at least some of the original pictures are duds, you might be able to cut the amount of data that needs to be backed up in half.  \nIf you still can't finish a full backup within whatever window of time you have, a common way to speed things up is to do a disk-to-disk backup first and then later copy the backup set off to tape.\n",
        "url": "https://serverfault.com/questions/457301/how-to-backup-20tb-of-data"
    },
    {
        "title": "What solutions exist to allow the use of revision control for server configuration files? [closed]",
        "question": "In an environment with multiple system administrators, I see a few advantages to adding the server config files into a revision control system.  Most notable are the ability to track changes, who made them, and of course being able to roll back to known working configs.\nI'm mainly interested in Unix/Linux solutions, but would be curious to Windows implementations as well.",
        "top_answer": "I have tested this at home (~ 3 hosts) for some time now, trying different\nscms (RCS, Subversion, git). The setup that works perfectly for me right now is git with\nthe setgitperms hook.\nThings you need to consider:\nHandling of file permissions and ownership\n\nRCS: does this natively\nSubversion: last I tried, you needed a wrapper around svn to do this\ngit: the setgitperms hook handles this transparently (needs a fairly\nrecent version of git with support for post-checkout hooks, though)\n\nAlso, if you don't want to all of your /etc under version control, but only\nthe files that you actually modified (like me), you'll need an scm that\nsupports this kind of use.\n\nRCS: works only on single files anyway.\nSubversion: I found this to be tricky.\ngit: no probem, put \"*\" in the top-level .gitignore file and add only those\nfiles you want using git add --force\n\nFinally, there are some problematic directories under /etc where packages can drop\nconfig snippets that are then read by some program or daemon (/etc/cron.d,\n/etc/modprobe.d, etc.). Some of these programs are smart enough to ignore\nRCS files (e.g. cron), some are not (e.g. modprobe). Same thing with .svn\ndirectories. Again a big plus for git (only creates one top-level .git\ndirectory).",
        "url": "https://serverfault.com/questions/5410/what-solutions-exist-to-allow-the-use-of-revision-control-for-server-configurati"
    },
    {
        "title": "yum search - package version",
        "question": "How can I tell the version of a package after doing a yum search?\ne.g.\nyum search rabbitmq\n\nreturns\nrabbitmq-server.noarch : The RabbitMQ server\n\nI need to know the version of this server.",
        "top_answer": "You can find the version number of a package in your repositories with the yum info command.\n# yum info rabbitmq-server\nAvailable Packages\nName        : rabbitmq-server\nArch        : noarch\nVersion     : 2.6.1\nRelease     : 1.fc16\nSize        : 1.1 M\nRepo        : updates\nCommitter   : Peter Lemenkov <[email\u00a0protected]>\nCommittime  : Tue Nov  8 13:00:00 2011\nBuildtime   : Tue Nov  8 10:31:03 2011\nSummary     : The RabbitMQ server\nURL         : http://www.rabbitmq.com/\nLicense     : MPLv1.1\nDescription : RabbitMQ is an implementation of AMQP, the emerging standard for high\n            : performance enterprise messaging. The RabbitMQ server is a robust and\n            : scalable implementation of an AMQP broker.\n\nTo find the version numbers of installed packages, you can use rpm with the -q option.\n# rpm -q kernel\nkernel-3.3.1-5.fc16.x86_64\nkernel-3.3.2-1.fc16.x86_64\nkernel-3.3.2-6.fc16.x86_64\n",
        "url": "https://serverfault.com/questions/385226/yum-search-package-version"
    },
    {
        "title": "php5-fpm: server reached pm.max_children",
        "question": "I have NGINX and php5-fpm. Several times per hour my website stucks and in logfile I see the following:\n\nWARNING: [pool www] server reached pm.max_children setting (5),\nconsider raising it.\n\nThe /etc/php5/fpm/pool.d/www.conf file contains the following configuration:\npm = dynamic\npm.max_children = 5\npm.start_servers = 2\npm.min_spare_servers = 1\npm.max_spare_servers = 3\n\nServer:\nAMD Opteron\u2122 3280, Octo-Core, 8x 2.4 GHz, 16 GB DIMM (DDR3).\nWhat numbers should I put in www.conf file for this server?",
        "top_answer": "There are many possible reasons why your PHP-FPM would reach the max_children. Most common ones are:\n\nA lot of parallel requests from your clients\nSlow execution of the PHP scripts\nVery low setting of the max_children\n\nLooking at the specs of your machine, assuming there is nothing else than PHP+Nginx running, I think you could set it much higher than 5. You say you have 8 Cores, usually Nginx needs much less CPU than PHP, so with 5 children you will probably never be able to use all of them. I'm usually setting it to something like the number of cores x 2 or number of cores x 4, depending on the memory consumption of your PHP scripts.",
        "url": "https://serverfault.com/questions/479443/php5-fpm-server-reached-pm-max-children"
    },
    {
        "title": "How can I zip/compress a symlink?",
        "question": "Is it possible and how can I zip a symlink from a linux shell?",
        "top_answer": "You can store symlinks as symlinks (as opposed to a copy of the file/directory they point to) using the --symlinks or -y parameters of the zip program.\nAssuming foo is a directory containing symlinks:\nzip --symlinks -r foo.zip foo/\n# or\nzip -y -r foo.zip foo/\n\nWhich of the flags to use depends on your version of zip; more current versions may support both. Check zip --help or man zip to find the correct one for your situation.\nRar equivalent:\nrar a -ol foo.rar foo/\n\ntar stores them as is by default.\ntar czpvf foo.tgz foo/\n\nNote that the symlink occupies almost no disk space by itself (just an inode). It's just a kind of pointer in the filesystem, as you probably know.",
        "url": "https://serverfault.com/questions/265675/how-can-i-zip-compress-a-symlink"
    },
    {
        "title": "Can IIS be configure to forward request to another web server?",
        "question": "I have several web site set up on one IIS 6 server distinguished by Host Header.\nHowever, I wish to have one of the sites served by a Linux / Apache server on my network. Do I need to use a reverse proxy add-in for IIS, or is there a simple way to tell IIS to pass on all requests to another server?",
        "top_answer": "For IIS 7.5, Microsoft provides official modules for this!\n\nURL Rewrite: http://www.iis.net/download/URLRewrite\nReverse proxy: http://www.iis.net/download/ApplicationRequestRouting\n\nIn the site settings, you'll get an \"URL Rewrite\" icon.\n\nOpen it\nright click on the \"inbound rules list\"\nSelect \"Add Rule(s)\"\nChoose \"Reverse proxy\"\n\nIn this dialog you can enter the hostname + port to forward to. After adding the rule, opening the edit dialog offers more customizations.",
        "url": "https://serverfault.com/questions/47537/can-iis-be-configure-to-forward-request-to-another-web-server"
    },
    {
        "title": "Nginx - Meaning of the ~ (tilde) in the location block of the nginx.conf?",
        "question": "What is the meaning of the tilde after the location block in the nginx configuration?\nfor example \nlocation ~ ^/download/(.*)$ {\n  alias /home/website/files/$1;\n}\n\nWhat is the difference between with and without the  \"~\" ?",
        "top_answer": "The tilde instructs nginx to perform a case-sensitive regular expression match, instead of a straight string comparison.\nMore details in the docs.",
        "url": "https://serverfault.com/questions/581474/nginx-meaning-of-the-tilde-in-the-location-block-of-the-nginx-conf"
    },
    {
        "title": "What does that mean: packages excluded due to repository priority protections",
        "question": "When updating with yum i recieve the following message:\nyum update\nLoaded plugins: fastestmirror, priorities\nLoading mirror speeds from cached hostfile\n * atomic: www7.atomicorp.com\n * base: mirror.de.leaseweb.net\n * extras: mirror.de.leaseweb.net\n * updates: mirror.de.leaseweb.net\n118 packages excluded due to repository priority protections\nSetting up Update Process\nNo Packages marked for Update\n\nWhat does that mean ? How to install these packages ?",
        "top_answer": "Some packages are held by more than one repository. The priorities plugin choose packages from the highest-priority repository, excluding duplicate entries from other repos.",
        "url": "https://serverfault.com/questions/312472/what-does-that-mean-packages-excluded-due-to-repository-priority-protections"
    },
    {
        "title": "Dump nginx config from running process?",
        "question": "Apparently, I shouldn't have spent sleepless night trying to debug an application. I wanted to restart my nginx and discovered that its config file is empty. I don't remember truncating it, but fat fingers and reduced attention probably played their part.\nI don't have backup of that config file. I know I should have made it.\nGood for me, current nginx daemon is still running. Is there a way to dump its configuration to a config file that it'll understand later?",
        "top_answer": "You need a gdb installed to dump memory regions of running process.\n# Set pid of nginx master process here\npid=8192\n\n# generate gdb commands from the process's memory mappings using awk\ncat /proc/$pid/maps | awk '$6 !~ \"^/\" {split ($1,addrs,\"-\"); print \"dump memory mem_\" addrs[1] \" 0x\" addrs[1] \" 0x\" addrs[2] ;}END{print \"quit\"}' > gdb-commands\n\n# use gdb with the -x option to dump these memory regions to mem_* files\ngdb -p $pid -x gdb-commands\n\n# look for some (any) nginx.conf text\ngrep worker_connections mem_*\ngrep server_name mem_*\n\nYou should get something like \"Binary file mem_086cb000 matches\". Open this file in editor, search for config (e.g. \"worker_connections\" directive), copy&paste. Profit!\nUpdate:\nThis method isn't entirely reliable. It's based on assumption that nginx process will read configuration and don't overwrite/reuse this memory area later. Master nginx process gives us best chances for that I guess.",
        "url": "https://serverfault.com/questions/361421/dump-nginx-config-from-running-process"
    },
    {
        "title": "Using variables in Apache config files to reduce duplication?",
        "question": "Is it possible to use variables in Apache config files?\nFor example, when I'm setting up a site with Django+WSGI, the config file might look like:\n<Directory /path/to/foo/>\n    Order allow,deny\n    Allow from all\n</Directory>\nAlias /foo/static /path/to/foo/static\nWSGIScriptAlias /foo /path/to/foo/run_wsgi\n\nAnd I'd like to turn the '/path/to/foo' into a variable so it only needs to be defined in one place. Something like:\nVariable FOO /path/to/foo\n\u2026\n\nThanks!",
        "top_answer": "You could use mod_macro, which has been included in Apache httpd since version 2.4 \nBefore that it had to be installed separately, see mod_macro. For example on Debian: apt-get install libapache2-mod-macro; a2enmod macro.\nExample configuration\n/etc/apache2/conf.d/vhost.macro\n<Macro VHost $host $port>\n  <VirtualHost $host:$port>\n\n    ServerName $host\n    DocumentRoot /var/vhosts/$host\n\n    <Directory /var/vhosts/$host>\n      # do something here...\n    </Directory>\n  </VirtualHost>\n</Macro>\n\n/etc/apache2/sites-available/vhost.mysite.com\nUse VHost vhost.mysite.com 80\n",
        "url": "https://serverfault.com/questions/64656/using-variables-in-apache-config-files-to-reduce-duplication"
    },
    {
        "title": "what does \"***\" mean when traceroute",
        "question": "this is the result of my traceroute\ntraceroute 211.140.5.120\n 1  141.1.31.2 (111.1.31.2)  0.397 ms  0.380 ms  0.366 ms\n 2  141.1.28.38 (111.1.28.38)  3.999 ms  3.971 ms  3.982 ms\n 3  142.11.124.193 (112.11.124.133)  1.315 ms  1.533 ms  1.455 ms\n 4   (201.141.0.261)  2.615 ms  2.749 ms  2.572 ms\n 5   (201.141.0.82)  2.705 ms  2.564 ms  2.680 ms\n 6   (201.118.231.14)  5.375 ms  5.126 ms  5.252 ms\n 7  * * *\n 8  * * *\n 9  * * *\n10  * * *\n11  * * *\n12  * * *\n13  * * *\n14  * * *\n15  * * *\n16  * * *\n17  * * *\n18  * * *\n19  * * *\n20  * * *\n21  * * *\n22  * * *\n23  * * *\n24  * * *\n25  * * *\n26  * * *\n27  * * *\n28  * * *\n29  * * *\n30  * * *\n\nI want to know what does the *** mean and does the result mean there are really more than 30 hops between my host and the target server ? ",
        "top_answer": "All implementations of traceroute rely on ICMP (type 11) packets being sent to the originator. \nThis  program  attempts trace route by launching UDP  probe packets with a small ttl (time to live) then listening for an ICMP \"time exceeded\" reply from a  gateway.   It  starts\nprobes with a ttl of one and increase by one until we get an ICMP \"port unreachable\"  (which  means  we  got  to \"host\") or hit a max (which defaults to 30 hops & can be changed with the -m flag).  Three probes (change  with  -q flag)  are  sent at each ttl setting and a line is printed showing the ttl, address of the gateway  and  round  trip time  of  each probe( so three * ). If there is no response within a 5 sec. timeout interval (changed with the  -w  flag),  a  \"*\"  is printed for that probe. \nSo in your case we can conclude that we got response only upto 201.118.231.14. Afterwards the nodes are not responding to the ICMP packets (type 11) upto hop 30 which is the max time-to-live (max number of hops). You can increase max-time-to-live using -m flag.",
        "url": "https://serverfault.com/questions/334029/what-does-mean-when-traceroute"
    },
    {
        "title": "How to restart Nginx on Mac OS X?",
        "question": "I just installed Nginx on Mac OS X (thanks http://www.kevinworthington.com/nginx-mac-os-snow-leopard-2-minutes/), but how do I restart Nginx on Mac OS X?\nThanks!",
        "top_answer": "sudo nginx -s stop && sudo nginx\n",
        "url": "https://serverfault.com/questions/225948/how-to-restart-nginx-on-mac-os-x"
    },
    {
        "title": "How can I run arbitrarily complex command using sudo over ssh?",
        "question": "I have a system that I can only log in to under my username (myuser), but I need to run commands as other user (scriptuser). So far, I have come up with the following to run the commands I need:\nssh -tq myuser@hostname \"sudo -u scriptuser bash -c \\\"ls -al\\\"\"\n\nIf however, when I try to run a more complex command, such as [[ -d \"/tmp/Some directory\" ]] && rm -rf \"/tmp/Some directory\" I quickly get into trouble with quoting. I'm not sure how I could pass this example complex command to bash -c, when \\\" already delimites the boundaries of the command I'm passing (and so I don't know how to quote /tmp/Some directory, which includes a spaces.\nIs there a general solution allowing me to pass any command no matter how complex/crazy the quoting is, or is this some sort of limitation I have reached? Are there other possible and perhaps more readable solutions?",
        "top_answer": "A trick I use sometimes is to use base64 to encode the commands, and pipe it to bash on the other site:\nMYCOMMAND=$(base64 -w0 script.sh)\nssh user@remotehost \"echo $MYCOMMAND | base64 -d | sudo bash\"\n\nThis will encode the script, with any commas, backslashes, quotes and variables inside a safe string, and send it to the other server. (-w0 is required to disable line wrapping, which happens at column 76 by default).  On the other side, $(base64 -d) will decode the script and feed it to bash to be executed.\nI never got any problem with it, no matter how complex the script was. Solves the problem with escaping, because you don't need to escape anything. It does not creates a file on the remote host, and you can run vastly complicated scripts with ease.",
        "url": "https://serverfault.com/questions/625641/how-can-i-run-arbitrarily-complex-command-using-sudo-over-ssh"
    },
    {
        "title": "Apache2 config variable is not defined",
        "question": "I installed apache2 on ubuntu 13.10.\nIf I try to restart it using \nsudo /etc/init.d/apache2 restart\n\nI get this message:\n\nAH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message\n\nSo I read that I should edit my httpd.conf file. But, since I can't find it in /etc/apache2/ folder, I tried to locate it using this command:\n/usr/sbin/apache2 -V\n\nBut the output I get is this:\n[Fri Nov 29 17:35:43.942472 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_LOCK_DIR} is not defined\n[Fri Nov 29 17:35:43.942560 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_PID_FILE} is not defined\n[Fri Nov 29 17:35:43.942602 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_RUN_USER} is not defined\n[Fri Nov 29 17:35:43.942613 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_RUN_GROUP} is not defined\n[Fri Nov 29 17:35:43.942627 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_LOG_DIR} is not defined\n[Fri Nov 29 17:35:43.947913 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_LOG_DIR} is not defined\n[Fri Nov 29 17:35:43.948051 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_LOG_DIR} is not defined\n[Fri Nov 29 17:35:43.948075 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_LOG_DIR} is not defined\n\nAH00526: Syntax error on line 74 of /etc/apache2/apache2.conf:\nInvalid Mutex directory in argument file:${APACHE_LOCK_DIR}\n\nLine 74 of /etc/apache2/apache2.conf is this:\nMutex file:${APACHE_LOCK_DIR} default\n\nI gave a look at my /etc/apache2/envvar file, but I don't know what to do with it.\nWhat should I do?",
        "top_answer": "[Fri Nov 29 17:35:43.942472 2013] [core:warn] [pid 14655] AH00111: Config variable ${APACHE_LOCK_DIR} is not defined\n\nThis message is displayed because you directly executed the apache2 binary.\nIn Ubuntu/Debian the apache config relies on the envvar file which is only activated.\nIf you start apache with the init script or apachectl.\nYour original problem is that you have no a proper hostname (fqdn) for your machine.\nIf you can't change it, change the ServerName variable in /etc/apache2/apache2.conf to localhost or your prefered FQDN. ",
        "url": "https://serverfault.com/questions/558283/apache2-config-variable-is-not-defined"
    },
    {
        "title": "Running Cron every 2 hours [duplicate]",
        "question": "I have the cron job as shown below, and wanted it to run every 2 hours, but it keeps running every 2 minutes.\nCan someone tell me where I'm going wrong?\n  \n  \n* */2 * * * /path-to-script\n",
        "top_answer": "An asterisk in the minute (first) field tells it to run every minute, regardless of the other fields.\nYou need to specify an exact minute to run within the hour. Be that on the hour (0), half past (30), etc..\n0 */2 * * * /path-to-script\n",
        "url": "https://serverfault.com/questions/43510/running-cron-every-2-hours"
    },
    {
        "title": "Hundreds of failed ssh logins",
        "question": "Every night I get hundreds, sometimes thousands, of failed ssh logins on my RedHat 4 server. For firewall reasons from remote sites, I need to run on the standard port. Is there anything I should be doing to block this. I notice that many come from the same IP address. Shouldn't it be stopping those after a while?",
        "top_answer": "You can use iptables to rate-limit new incoming connections to the SSH port. I'd have to see your entire iptables configuration in order to give you a turnkey solution, but you're basically talking about adding rules like:\niptables -A INPUT -p tcp --dport 22 -m recent --update --seconds 60 --hitcount 5 --name SSH --rsource -j DROP \niptables -A INPUT -p tcp --dport 22 -m recent --set --name SSH --rsource -j ACCEPT \n\nThese rules assume that you're accepting ESTABLISHED connections earlier in the table (so that only new connections will hit these rules). New SSH connections will hit these rules and be marked. In 60 seconds, 5 attempts from a single IP address will result in new incoming connections from that IP being dropped.\nThis has worked well for me.\nEdit: I prefer this method to \"fail2ban\" because no additional software to be installed, and happens totally in kernel-mode. It doesn't handle parsing log files like \"fail2ban\" will, but if your problem is only with SSH I wouldn't use something user-mode that requires software installation and is more complex.",
        "url": "https://serverfault.com/questions/17870/hundreds-of-failed-ssh-logins"
    },
    {
        "title": "How to check what port mysql is running on",
        "question": "On my windows dev box mysql is running on port 3306\nHow can I check what port it is running on the unix server that I have to upload the app to.",
        "top_answer": "I did\n\nmysql> SHOW GLOBAL VARIABLES LIKE 'PORT';\n\nAnd that indicated that I was using port 3306 and that my search for the error continues.",
        "url": "https://serverfault.com/questions/116100/how-to-check-what-port-mysql-is-running-on"
    },
    {
        "title": "Downloading docker image for transfer to non-internet-connected machine",
        "question": "I'm working in an office where my laptop is internet-connected, but tightly controlled. I am not allowed to install unauthorized software onto it.\nMy development workstation is mine to do with as I please, but it does not have an internet connection.\nIs there any way for me to download Docker images from the hub as a file that I could then sneaker-net to my dev workstation? Similar to how I can download RPMs or Ruby Gems and burn them to CD? Or is the only way of downloading the images using the 'docker pull' command?",
        "top_answer": "Short: use the save CLI command.\nhttps://docs.docker.com/engine/reference/commandline/save/\n\nYou can pull the image on a computer that have access to the internet.\nsudo docker pull ubuntu\n\nThen you can save this image to a file\nsudo docker save -o ubuntu_image.docker ubuntu\n\nTransfer the file on the offline computer (USB/CD/whatever) and load the image from the file:\nsudo docker load -i ubuntu_image.docker\n\n(On older versions this was just docker load image.docker, see comments for more info.)",
        "url": "https://serverfault.com/questions/701248/downloading-docker-image-for-transfer-to-non-internet-connected-machine"
    },
    {
        "title": "nginx url rewriting: difference between break and last",
        "question": "I don't understand the difference between break and last (flags of rewrite). The documentation is rather abstruse. I've tried to switch between the two in some of my configs, but I couldn't spot any difference in behavior. Can someone please explain these flags in more detail? Preferably with an example that shows different behavior when flipping one flag to another.",
        "top_answer": "You may have different sets of rewrite rules for different locations. When rewrite module meets last, it stops processing the current set and the rewritten request is passed once again to find the appropriate location (and the new set of rewriting rules). If the rule ends with break, the rewriting also stops, but the rewritten request is not passed to another location.\nThat is, if there are two locations: loc1 and loc2, and there's a rewriting rule in loc1 that changes loc1 to loc2 AND ends with last, the request will be rewritten and passed to location loc2. If the rule ends with break, it will belong to location loc1.",
        "url": "https://serverfault.com/questions/131474/nginx-url-rewriting-difference-between-break-and-last"
    },
    {
        "title": "Private IP address in public DNS",
        "question": "We have an SMTP only mail server behind a firewall which will have a public A record of mail..  The only way to access this mail server is from another server behind the same firewall.  We do not run our own private DNS server.\nIs it a good idea to use the private IP address as an A record in a public DNS server - or is it best to keep these server records in each servers local hosts file?",
        "top_answer": "Some people will say no public DNS records should ever disclose private IP addresses....with the thinking being that you are giving potential attackers a leg up on some information that might be required to exploit private systems.\nPersonally, I think that obfuscation is a poor form of security, especially when we are talking about IP addresses because in general they are easy to guess anyway, so I don't see this as a realistic security compromise.\nThe bigger consideration here is making sure your public users don't pickup this DNS record as part of the normal public services of your hosted application. ie: External DNS lookups somehow start resolving to an address they can't get to.\nAside from that, I see no fundamental reason why putting private address A records into the public space is a problem....especially when you have no alternate DNS server to host them on.\nIf you do decide to put this record into the public DNS space, you might consider creating a separate zone on the same server to hold all the \"private\" records. This will make it clearer that they are intended to be private....however for just one A record, I probably wouldn't bother.",
        "url": "https://serverfault.com/questions/4458/private-ip-address-in-public-dns"
    },
    {
        "title": "Why use Chef/Puppet over shell scripts?",
        "question": "New to Puppet and Chef tools. Seems like the job that they are doing can be done with shell scripting. Maybe it was done in shell scripts until these came along.\nI would agree they are more readable. But, are there any other advantages over shell scripts besides just being readable?",
        "top_answer": "A domain-specific language makes a big difference in the amount of code you write.  For example, you could argue that there's not much difference between:\nchmod 640 /my/file\n\nand\nfile { \"/my/file\":\n    mode => 640,\n}\n\nbut there's a great deal of difference between these:\nFILE=/my/file\nchmod 640 $FILE\nchown foo $FILE\nchgrp bar $FILE\nwget -O $FILE \"http://my.puppet.server/dist/$FILE\"\n     # where the URL contains \"Hello world\"\n\nand\nfile { \"/my/file\":\n    mode => 640,\n    owner => foo,\n    group => bar,\n    content => \"Hello world\",\n}\n\nWhat happens if the wget fails?  How will your script handle that?  And what happens if there's something after that in your script that requires $FILE to be there with the correct contents?\nYou might argue that one could just put echo \"Hello world\" > $FILE in the script, except that in the first example the script must be run on the client, whereas puppet compiles all of this on the server.  So if you change the content, you only have to change it on the server and it changes it for as many systems as you want to put it on.  And puppet handles dependencies and transfer problems for you automatically.\nThere's just no comparison - proper configuration management tools save you time and complexity.  The more you try to do, the more shell scripts seem inadequate, and the more effort you will save by doing it with puppet.",
        "url": "https://serverfault.com/questions/504070/why-use-chef-puppet-over-shell-scripts"
    },
    {
        "title": "OpenVPN vs. IPsec - Pros and cons, what to use?",
        "question": "Interestingly I have not found any good search results when searching for \"OpenVPN vs IPsec\". So here's my question:\nI need to set up a private LAN over an untrusted network. And as far as I know, both approaches seem to be valid. But I do not know which one is better.\nI would be very thankful if you can list the pros and cons of both approaches and maybe your suggestions and experiences regarding what to use.\nUpdate (Regarding the comment/question):\nIn my concrete case, the goal is to have any number of servers (with static IPs) connected transparently to each other. But a small portion of dynamic clients like \"road warriors\" (with dynamic IPs) should also be able to connect. The main goal is however having a \"transparent secure network\" run on top of the untrusted network. I am quite a newbie so I do not know how to correctly interpret \"1:1 Point to Point Connections\" => The solution should support broadcasts and all that stuff so it is a fully functional network.",
        "top_answer": "I have all of the scenarios setup in my environment. (openvpn site-site, road warriors; cisco ipsec site-site, remote users)\nBy far the openvpn is faster. The openvpn software is less overhead on the remote users.  The openvpn is/can be setup on port 80 with tcp so that it passes at places that have limited free internet.  The openvpn is more stable. \nOpenvpn in my environment does not force policy to the end user.  Openvpn key distribution is a little harder to do securely.  Openvpn key passwords are up to the end users (they can have blank passwords).  Openvpn is not approved by certain auditors (the ones that only read bad trade rags).  Openvpn takes a little bit of brains to setup (unlike cisco). \nThis is my experience with openvpn:  I know that most of my negatives can be alleviated through either configuration changes or process changes.  So take all my negatives with a bit of skepticism.",
        "url": "https://serverfault.com/questions/202917/openvpn-vs-ipsec-pros-and-cons-what-to-use"
    },
    {
        "title": "How does a team of Systems Administrators share passwords securely?",
        "question": "What are best practices for sharing hundreds of passwords among a few people?  These passwords protect mission critical data, and cannot ever be visible beyond a small team.",
        "top_answer": "I would probably write a custom web-based solution hosted on a corporate intranet. (take a look at http://lastpass.com for inspiration, or to use it. Sharing passwords is one of its features, though it may not work for your volume.)\nEDIT: Sure, best solution, don't share them. Storing cleartext passwords in any medium is dangerous, particularly when the purpose of storing them is to share them. There is a nearly infinite number of solutions, each bringing an associated peril. Why not put them on an encrypted disk image, burn that image to a single CD, put the CD into a safe that only one armed guard can open, and have authorized people present photo ID to have it unlocked?\nThe point is we don't really know your scenario. Why are you sharing hundreds of mission-critical passwords? Are they for your backoffice intranet, VPN, or are they customer passwords that you keep around in plaintext for some reason? Are all the people you need to share it with in the same installation? Would a physical transference like an encrypted CD or a printed table stored in a safe actually work? Or are your sysadmins spread over the globe, making electronic means of sharing them the only solution?",
        "url": "https://serverfault.com/questions/147663/how-does-a-team-of-systems-administrators-share-passwords-securely"
    },
    {
        "title": "What does \"debconf: delaying package configuration, since apt-utils is not installed\" mean?",
        "question": "I have just installed Debian and I was just installing some packages using apt-get instal when I saw this message:\ndebconf: delaying package configuration, since apt-utils is not installed\n\nWhat does this mean? And once I have installed apt-utils how can configure the packages?",
        "top_answer": "apt-utils contains the /usr/bin/apt-extracttemplates program which is used when packages need to ask you questions about how to be configured.  This package being Priority: important, means it should really be installed except in rare circumstances.\nIf there is configuration pending, dpkg-reconfigure [package] will perform it. If you missed configuring a number of packages and don't know which, run dpkg-reconfigure -au to go through (a)ll (u)nseen configuration questions.",
        "url": "https://serverfault.com/questions/358943/what-does-debconf-delaying-package-configuration-since-apt-utils-is-not-insta"
    },
    {
        "title": "How to scan local network for SSH-able computers?",
        "question": "I am often on one computer in my house and I would like to SSH to another one, but often don't know the IP address of the one I want to connect to. Is there a way, from the command line, to scan the local network so I can find the computer I want to connect to?",
        "top_answer": "Use \"nmap\" - this will tell you which hosts are up on a network, and indeed which have port 22 open. You could combine it with a few other tools (like grep) to produce more targeted output if need be.\nNote: do this only on YOUR network. Running up nmap or its equivalents on someone else's network is considered bad form.\nsudo nmap -p 22 192.168.0.0/24\n",
        "url": "https://serverfault.com/questions/376894/how-to-scan-local-network-for-ssh-able-computers"
    },
    {
        "title": "Install openssl-dev on Ubuntu server",
        "question": "In order to compile NGinx in need to install openssl and openssl-dev (I'am following a book guide).\nSo i'am doing this :\nsudo apt-get install openssl openssl-dev\n\nBut i get an error telling me that it's impossible to find openssl-dev.\nAlso after some googling, it seems that libssl-dev is equal to openssl-dev, is that true ? (apt-get found libssl-dev on my server)\nHere is my server version : 2.6.32-22-server\nAny help welcome !",
        "top_answer": "If the likelihood that the dependencies for the version of a package that is in the release of Ubuntu (or other Debian derived arrangements) is the same as the deps for the version you are trying to build, you could run apt-get build-dep nginx or aptitude build-dep nginx - this will not install the nginx package but will instead install all those listed as dependencies (and their dependencies, as usual) which includes libssl-dev (the package that you are currently looking for).\nIn most cases this will allow the build of the other (presumably newer) version to be completed successfully, and it saves you installing each library and its header files one by one yourself. Even if there are new dependencies in the other version you are trying to build, build-dep <package> is a good place to start as it means that you only have to manually install the extra new dependencies.\nAs an example, the result on one of my servers is:\nuser@host:~$ sudo aptitude build-dep nginx\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nReading extended state information\nInitialising package states... Done\nThe following NEW packages will be installed:\n  autotools-dev cvs{a} debhelper gettext{a} html2text{a} intltool-debian{a}\n  libcroco3{a} libmail-sendmail-perl{a} libpcre3-dev libpcrecpp0{a}\n  libssl-dev libsys-hostname-long-perl{a} po-debconf{a} zlib1g-dev\n0 packages upgraded, 14 newly installed, 0 to remove and 19 not upgraded.\nNeed to get 7,217kB of archives. After unpacking 22.9MB will be used.\nDo you want to continue? [Y/n/?]\n\nIt is intending to install some libraries and headers, to enable an nginx build, but not nginx itself. \nOne thing to note is that if you are compiling your own copy because you want different build options rather than needing a different version for some reason, you may be better of compiling from the repository's source for the package rather than using the upstream sources directly. This SO question is the first useful page that came out of a quick search, though you are likely to find more detailed tutorials easily if you need that.\nAn other small thing to note: the packages installed as a result of apt-get build-dep will be marked as manually installed as if you have done this by hand as you are currently doing. That means you can't remove them all in one go (there is no apt-get unintall-dep or similar) - though that is no different from the situation you'll get from manual library/header installs anyway (I only mention the fact as some people expect there to be a one-step way to undo a build-dep operation, and there is not).",
        "url": "https://serverfault.com/questions/249340/install-openssl-dev-on-ubuntu-server"
    },
    {
        "title": "Choosing between meaningful and meaningless hostnames [closed]",
        "question": "Assume an environment with a puppet-managed cluster of different servers - various hardware, software, operating systems, virtual/dedicated, etc.\nWould you choose meaningful hostnames (mysqlmaster01..99, mysqlslave001..999, vpnprimary, vpnbackup, etc.) or would you prefer meaningless hostnames such as characters from a book or movie?\nThe problem I see with meaningful hostnames is that names usually represent a single service and if a server has more than one purpose it gets really messy (especially if server roles change often).\nIsn't mapping a service name to an IP address and maintaining that mapping what DNS is supposed to do?\nWhat are the advantages and drawbacks of both approaches and what actual problems have you had to tackle with the approach you chose?",
        "top_answer": "Once upon a time I had an opportunity to decide on a naming scheme.  So I went round and asked my developers, who after all were the people who had to work with these names on a day-to-day basis, whether they preferred functional names (that is, names which represent, in some encoded form, the purpose of the machine) or mnemonic names (that is, names drawn from some pre-existing human naming scheme, which contained no implicit content about the machine's purpose).\nOut of 38 developers, 37 preferred mnemonic names; only one preferred functional names.  So I named them all after rivers (there's a very large pool of possible names, and many of them are short, easy to remember, and quick to type).\nThe human brain is pretty well-designed for attaching meaning to names.  If you provide names that are memorable, people will pretty quickly remember what those names are used for, and use them.  If you use names drawn from some common background (eg rivers, elements, stars, counties, drinks, you get the idea) it helps people to immediately recognise a company hostname when they come across it; otherwise statements like \"all the email ended up on betelgeuse\" can be a bit confusing).\nConversely, my developers felt that they had in previous jobs had a really hard time remembering exactly what pr1ms001 was.\nBut I should add that we used CNAMEs in the internal DNS to provide a functional name to mnemonic name mapping, so if you really found it easier to remember that the main mail server at the first cluster at the PR site was pr1ms001, then the DNS would let you know that that was currently orwell.  Also, that let us have many functional names per machine, so as long as you always used the functional name relevant to the function you were working on, you could be sure that pr1imap001 would always point to the IMAP server, even if we moved that functionality from orwell to rhine.  And when hudson died, we could change the name of the replacement without affecting operational functions, so that we never had the \"do you mean new hudson or old hudson?\" confusion.",
        "url": "https://serverfault.com/questions/479945/choosing-between-meaningful-and-meaningless-hostnames"
    },
    {
        "title": "How to avoid lftp Certificate verification error?",
        "question": "I'm trying to get my Pelican blog working. It uses lftp to transfer the actual blog to ones server, but I always get an error:\nmirror: Fatal error: Certificate verification: subjectAltName does not match \u2018blogname.com\u2019\n\nI think lftp is checking the SSL and the quick setup of Pelican just forgot to include that I don't have SSL on my FTP.\n\nThis is the code in Pelican's Makefile:\nftp_upload: $(OUTPUTDIR)/index.html\nlftp ftp://$(FTP_USER)@$(FTP_HOST) -e \"mirror -R $(OUTPUTDIR) $(FTP_TARGET_DIR) ; quit\"\n\nwhich renders in terminal as:\n    lftp ftp://[email\u00a0protected] -e \"mirror -R /Volumes/HD/Users/me/Test/output /myblog_directory ; quit\"\n\n\nWhat I managed so far is, denying the SSL check by changing the Makefile to:\nlftp ftp://$(FTP_USER)@$(FTP_HOST) -e \"set ftp:ssl-allow no\" \"mirror -R $(OUTPUTDIR) $(FTP_TARGET_DIR) ; quit\"\n\nDue to my incorrect implementation I get logged in correctly (lftp [email\u00a0protected]:~>) but the one line feature doesn't work anymore and I have to enter the mirror command by hand:\nmirror -R /Volumes/HD/Users/me/Test/output/ /myblog_directory\n\nThis works without an error and timeout. The question is how to do this with a one liner.\n\nIn addition I tried:\n\nset ssl:verify-certificate/ftp.myblog.com no\nThis trick to disable certificate verification in lftp:\n$ cat ~/.lftp/rc \nset ssl:verify-certificate no\n\nHowever, it seems there is no \"rc\" folder in my lftp directory - so this prompt has no chance to work.",
        "top_answer": "From the manpage:\n\n-c commands\n  Execute the given commands and exit. Commands can be separated with a semicolon (;), AND (&&) or OR (||). Remember to quote the commands argument properly in the shell.  This option must be used alone without other arguments.\n\nSo you want to specify the commands as a single argument, separated by semicolons:\nlftp ftp://$(FTP_USER)@$(FTP_HOST) -e \"set ftp:ssl-allow no; mirror -R $(OUTPUTDIR) $(FTP_TARGET_DIR) ; quit\"\n\nYou can actually omit the quit command and use -c instead of -e.",
        "url": "https://serverfault.com/questions/411970/how-to-avoid-lftp-certificate-verification-error"
    },
    {
        "title": "Why is ssh agent forwarding not working?",
        "question": "In my own computer, running MacOSX, I have this in ~/.ssh/config\nHost *\nForwardAgent yes\nHost b1\nForwardAgent yes\n\nb1 is a virtual machine running Ubuntu 12.04. I ssh to it like this:\nssh pupeno@b1\n\nand I get logged in without being asked for a password because I already copied my public key. Due to forwarding, I should be able to ssh to pupeno@b1 from b1 and it should work, without asking me for a password, but it doesn't. It asks me for a password.\nWhat am I missing?\nThis is the verbose output of the second ssh:\npupeno@b1:~$ ssh -v pupeno@b1\nOpenSSH_5.9p1 Debian-5ubuntu1, OpenSSL 1.0.1 14 Mar 2012\ndebug1: Reading configuration data /etc/ssh/ssh_config\ndebug1: /etc/ssh/ssh_config line 19: Applying options for *\ndebug1: Connecting to b1 [127.0.1.1] port 22.\ndebug1: Connection established.\ndebug1: identity file /home/pupeno/.ssh/id_rsa type -1\ndebug1: identity file /home/pupeno/.ssh/id_rsa-cert type -1\ndebug1: identity file /home/pupeno/.ssh/id_dsa type -1\ndebug1: identity file /home/pupeno/.ssh/id_dsa-cert type -1\ndebug1: identity file /home/pupeno/.ssh/id_ecdsa type -1\ndebug1: identity file /home/pupeno/.ssh/id_ecdsa-cert type -1\ndebug1: Remote protocol version 2.0, remote software version OpenSSH_5.9p1 Debian-5ubuntu1\ndebug1: match: OpenSSH_5.9p1 Debian-5ubuntu1 pat OpenSSH*\ndebug1: Enabling compatibility mode for protocol 2.0\ndebug1: Local version string SSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1\ndebug1: SSH2_MSG_KEXINIT sent\ndebug1: SSH2_MSG_KEXINIT received\ndebug1: kex: server->client aes128-ctr hmac-md5 none\ndebug1: kex: client->server aes128-ctr hmac-md5 none\ndebug1: sending SSH2_MSG_KEX_ECDH_INIT\ndebug1: expecting SSH2_MSG_KEX_ECDH_REPLY\ndebug1: Server host key: ECDSA 35:c0:7f:24:43:06:df:a0:bc:a7:34:4b:da:ff:66:eb\ndebug1: Host 'b1' is known and matches the ECDSA host key.\ndebug1: Found key in /home/pupeno/.ssh/known_hosts:1\ndebug1: ssh_ecdsa_verify: signature correct\ndebug1: SSH2_MSG_NEWKEYS sent\ndebug1: expecting SSH2_MSG_NEWKEYS\ndebug1: SSH2_MSG_NEWKEYS received\ndebug1: Roaming not allowed by server\ndebug1: SSH2_MSG_SERVICE_REQUEST sent\ndebug1: SSH2_MSG_SERVICE_ACCEPT received\ndebug1: Authentications that can continue: publickey,password\ndebug1: Next authentication method: publickey\ndebug1: Trying private key: /home/pupeno/.ssh/id_rsa\ndebug1: Trying private key: /home/pupeno/.ssh/id_dsa\ndebug1: Trying private key: /home/pupeno/.ssh/id_ecdsa\ndebug1: Next authentication method: password\npupeno@b1's password:\n",
        "top_answer": "It turns out my key was not in the agent, and this fixed it:\nOS X:\nssh-add -K\n\nLinux/Unix:\nssh-add -k\n\nYou can list loaded keys using:\nssh-add -l\n\nssh-add -L # for more detail\n",
        "url": "https://serverfault.com/questions/404447/why-is-ssh-agent-forwarding-not-working"
    },
    {
        "title": "How can I create a location in nginx that works with AND without a trailing slash?",
        "question": "Right now I have this config:\nlocation ~ ^/phpmyadmin/(.*)$\n{\n        alias /home/phpmyadmin/$1;\n}\n\nHowever, if I visit www.mysite.com/phpmyadmin (note the lack of trailing slash), it won't find what I'm looking for a 404. I assume because I don't include the trailing slash. How can I fix this?",
        "top_answer": "It might be in the regular expression that you're using --\nlocation ~ ^/phpmyadmin/(.*)$\n\nThe above will match /phpmyadmin/, /phpmyadmin/anything/else/here, but it won't match /phpmyadmin because the regular expression includes the trailing slash.\nYou probably want something like this:\nlocation ~ /phpmyadmin/?(.*)$ {\n    alias /home/phpmyadmin/$1;\n}\n\nThe question mark is a regular expression quantifier and should tell nginx to match zero or one of the previous character (the slash).\n\nWarning: The community seen this solution, as is, as a possible\n  security risk\n",
        "url": "https://serverfault.com/questions/376162/how-can-i-create-a-location-in-nginx-that-works-with-and-without-a-trailing-slas"
    },
    {
        "title": "Log all commands run by admins on production servers",
        "question": "It is company policy for admins to login to the servers via a personal username, and then run sudo -i to become root.  Upon running sudo -i, sudo will create an environmental variable called SUDO_USER, which contains the original user's username.\nIs there a way to log ALL commands within syslog with something akin to the following syntax:\n${TIME/DATE STAMP}: [${REAL_USER}|${SUDO_USER}]: ${CMD}\n\nAn example entry would be:\nSat Jan 19 22:28:46 CST 2013: [root|ksoviero]: yum install random-pkg\n\nObviously it doesn't have to be exactly the above syntax, it just has to include a minimum of the real user (eg. root), the sudo user (eg. ksoviero), and the full command that was run (eg. yum install random-pkg).\nI've already tried snoopy, but it did not include the SUDO_USER variable.",
        "top_answer": "Update:\n2 more things that have popped up in the comments and in follow-up questions:\n\nUsing auditd this way will dramatically increase your log volume, especially if the system is heavily in use via commandline. Adjust your log retention policy.\nAuditd logs on the host where they are created are just as secure as other files on the same box. Forward your logs to a remote log collection server like ELK or Graylog to preserve your logs' integrity. Plus, adding to the point above, it allows to more aggressively delete old logs.\n\nAs was suggested by Michael Hampton, auditd is the correct tool for the job here.\nI tested this on an Ubuntu 12.10 installation, so your mileage may vary on other systems.\n\nInstall auditd:\napt-get install auditd\nAdd these 2 lines to /etc/audit/audit.rules:\n-a exit,always -F arch=b64 -F euid=0 -S execve\n-a exit,always -F arch=b32 -F euid=0 -S execve\n\nThese will track all commands run by root (euid=0). Why two rules? The execve syscall must be tracked in both 32 and 64 bit code.\n\nTo get rid of auid=4294967295 messages in logs, add audit=1 to the kernel's cmdline (by editing /etc/default/grub)\nPlace the line\nsession  required                pam_loginuid.so\n\nin all PAM config files that are relevant to login (/etc/pam.d/{login,kdm,sshd}), but not in the files that are relevant to su or sudo.\nThis will allow auditd to get the calling user's uid correctly when calling sudo or su.\n\nRestart your system now.\nLet's login and run some commands:\n\n\n    $ id -u\n    1000\n    $ sudo ls /\n    bin  boot  data  dev  etc  home  initrd.img  initrd.img.old  lib  lib32  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  scratch  selinux  srv  sys  tmp  usr  var  vmlinuz  vmlinuz.old\n    $ sudo su -\n    # ls /etc\n    [...]\n\nThis will yield something like this in /var/log/audit/auditd.log:\n----\ntime->Mon Feb  4 09:57:06 2013\ntype=PATH msg=audit(1359968226.239:576): item=1 name=(null) inode=668682 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=PATH msg=audit(1359968226.239:576): item=0 name=\"/bin/ls\" inode=2117 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=CWD msg=audit(1359968226.239:576):  cwd=\"/home/user\"\ntype=EXECVE msg=audit(1359968226.239:576): argc=2 a0=\"ls\" a1=\"/\"\ntype=SYSCALL msg=audit(1359968226.239:576): arch=c000003e syscall=59 success=yes exit=0 a0=10cfc48 a1=10d07c8 a2=10d5750 a3=7fff2eb2d1f0 items=2 ppid=26569 pid=26570 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=1 comm=\"ls\" exe=\"/bin/ls\" key=(null)\n----\ntime->Mon Feb  4 09:57:06 2013\ntype=PATH msg=audit(1359968226.231:575): item=1 name=(null) inode=668682 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=PATH msg=audit(1359968226.231:575): item=0 name=\"/usr/bin/sudo\" inode=530900 dev=08:01 mode=0104755 ouid=0 ogid=0 rdev=00:00\ntype=CWD msg=audit(1359968226.231:575):  cwd=\"/home/user\"\ntype=BPRM_FCAPS msg=audit(1359968226.231:575): fver=0 fp=0000000000000000 fi=0000000000000000 fe=0 old_pp=0000000000000000 old_pi=0000000000000000 old_pe=0000000000000000 new_pp=ffffffffffffffff new_pi=0000000000000000 new_pe=ffffffffffffffff\ntype=EXECVE msg=audit(1359968226.231:575): argc=3 a0=\"sudo\" a1=\"ls\" a2=\"/\"\ntype=SYSCALL msg=audit(1359968226.231:575): arch=c000003e syscall=59 success=yes exit=0 a0=7fff327ecab0 a1=7fd330e1b958 a2=17cc8d0 a3=7fff327ec670 items=2 ppid=3933 pid=26569 auid=1000 uid=1000 gid=1000 euid=0 suid=0 fsuid=0 egid=1000 sgid=1000 fsgid=1000 tty=pts0 ses=1 comm=\"sudo\" exe=\"/usr/bin/sudo\" key=(null)\n----\ntime->Mon Feb  4 09:57:09 2013\ntype=PATH msg=audit(1359968229.523:578): item=1 name=(null) inode=668682 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=PATH msg=audit(1359968229.523:578): item=0 name=\"/bin/su\" inode=44 dev=08:01 mode=0104755 ouid=0 ogid=0 rdev=00:00\ntype=CWD msg=audit(1359968229.523:578):  cwd=\"/home/user\"\ntype=EXECVE msg=audit(1359968229.523:578): argc=2 a0=\"su\" a1=\"-\"\ntype=SYSCALL msg=audit(1359968229.523:578): arch=c000003e syscall=59 success=yes exit=0 a0=1ceec48 a1=1cef7c8 a2=1cf4750 a3=7fff083bd920 items=2 ppid=26611 pid=26612 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=1 comm=\"su\" exe=\"/bin/su\" key=(null)\n----\ntime->Mon Feb  4 09:57:09 2013\ntype=PATH msg=audit(1359968229.519:577): item=1 name=(null) inode=668682 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=PATH msg=audit(1359968229.519:577): item=0 name=\"/usr/bin/sudo\" inode=530900 dev=08:01 mode=0104755 ouid=0 ogid=0 rdev=00:00\ntype=CWD msg=audit(1359968229.519:577):  cwd=\"/home/user\"\ntype=BPRM_FCAPS msg=audit(1359968229.519:577): fver=0 fp=0000000000000000 fi=0000000000000000 fe=0 old_pp=0000000000000000 old_pi=0000000000000000 old_pe=0000000000000000 new_pp=ffffffffffffffff new_pi=0000000000000000 new_pe=ffffffffffffffff\ntype=EXECVE msg=audit(1359968229.519:577): argc=3 a0=\"sudo\" a1=\"su\" a2=\"-\"\ntype=SYSCALL msg=audit(1359968229.519:577): arch=c000003e syscall=59 success=yes exit=0 a0=7fff327ecab0 a1=7fd330e1b958 a2=17cc8d0 a3=7fff327ec670 items=2 ppid=3933 pid=26611 auid=1000 uid=1000 gid=1000 euid=0 suid=0 fsuid=0 egid=1000 sgid=1000 fsgid=1000 tty=pts0 ses=1 comm=\"sudo\" exe=\"/usr/bin/sudo\" key=(null)\n----\ntime->Mon Feb  4 09:57:09 2013\ntype=PATH msg=audit(1359968229.543:585): item=1 name=(null) inode=668682 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=PATH msg=audit(1359968229.543:585): item=0 name=\"/bin/bash\" inode=6941 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=CWD msg=audit(1359968229.543:585):  cwd=\"/root\"\ntype=EXECVE msg=audit(1359968229.543:585): argc=1 a0=\"-su\"\ntype=SYSCALL msg=audit(1359968229.543:585): arch=c000003e syscall=59 success=yes exit=0 a0=13695a0 a1=7fffce08a3e0 a2=135a030 a3=7fffce08c200 items=2 ppid=26612 pid=26622 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=1 comm=\"bash\" exe=\"/bin/bash\" key=(null)\n----\ntime->Mon Feb  4 09:57:11 2013\ntype=PATH msg=audit(1359968231.663:594): item=1 name=(null) inode=668682 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=PATH msg=audit(1359968231.663:594): item=0 name=\"/bin/ls\" inode=2117 dev=08:01 mode=0100755 ouid=0 ogid=0 rdev=00:00\ntype=CWD msg=audit(1359968231.663:594):  cwd=\"/root\"\ntype=EXECVE msg=audit(1359968231.663:594): argc=3 a0=\"ls\" a1=\"--color=auto\" a2=\"/etc\"\ntype=SYSCALL msg=audit(1359968231.663:594): arch=c000003e syscall=59 success=yes exit=0 a0=7fff8c709950 a1=7f91a12149d8 a2=1194c50 a3=7fff8c709510 items=2 ppid=26622 pid=26661 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=1 comm=\"ls\" exe=\"/bin/ls\" key=(null)\n\nThe auid column contains the calling user's uid, which allows you filter for commands run by this user with\n ausearch -ua 1000\n\nThis will even list commands the user ran as root.\nSources:\n\nhttp://www.woitasen.com.ar/2011/11/auditing-user-actions-after-sudo/\nhttp://linux.die.net/man/8/pam_loginuid\nhttp://linux.die.net/man/8/auditd\n",
        "url": "https://serverfault.com/questions/470755/log-all-commands-run-by-admins-on-production-servers"
    },
    {
        "title": "Transfer 15TB of tiny files",
        "question": "I'm archiving data from one server to another. Initially I started a rsync job. It took 2 weeks for it to build the file list just for 5 TB of data and another week to transfer 1 TB of data.\nThen I had to kill the job as we need some down time on the new server.\nIt's been agreed that we will tar it up since we probably won't need to access it again. I was thinking of breaking it into 500 GB chunks. After I tar it then I was going to copy it across through ssh. I was using tar and pigz but it is still too slow.\nIs there a better way to do it? I think both servers are on Redhat. Old server is Ext4 and the new one is XFS.\nFile sizes range from few kb to few mb and there are 24 million jpegs in 5TB. So I'm guessing around 60-80 million for 15TB.\nedit: After playing with rsync, nc, tar, mbuffer and pigz for a couple of days. The bottleneck is going to be the disk IO. As the data is striped across 500 SAS disks and around 250 million jpegs. However, now I learnt about all these nice tools that I can use in future.",
        "top_answer": "I have had very good results using tar, pigz (parallel gzip) and nc.\nSource machine:\ntar -cf - -C /path/of/small/files . | pigz | nc -l 9876\n\nDestination machine:\nTo extract:\nnc source_machine_ip 9876 | pigz -d | tar -xf - -C /put/stuff/here\n\nTo keep archive:\nnc source_machine_ip 9876 > smallstuff.tar.gz\n\nIf you want to see the transfer rate just pipe through pv after pigz -d!",
        "url": "https://serverfault.com/questions/721223/transfer-15tb-of-tiny-files"
    },
    {
        "title": "In Bash, are wildcard expansions guaranteed to be in order?",
        "question": "Is the expansion of a wildcard in Bash guaranteed to be in alphabetical order? I am forced to split a large file into 10 Mb pieces so that they can be be accepted by my Mercurial repository.\nSo I was thinking I could use:\nsplit -b 10485760 Big.file BigFilePiece.\n\nand then in place of:\ncat BigFile | bigFileProcessor\n\nI could do:\ncat BigFilePiece.* | bigFileProcessor\n\nin its place.\nHowever, I could not find anywhere that guaranteed that the expansion of the asterisk (aka wildcard, aka *) would always be in alphabetical order so that .aa came before .ab (as opposed to be timestamp ordering or something like that).\nAlso, are there any flaws in my plan? How great is the performance cost of cating the file together?",
        "top_answer": "Yes, globbing expansion is alphabetical.\nFrom the Bash man page:\n\nPathname Expansion\nAfter  word  splitting,  unless  the -f option has been set, bash scans\neach word for the characters *, ?, and [.  If one of  these  characters\nappears,  then  the word is regarded as a pattern, and replaced with an\nalphabetically sorted list of file names matching the pattern.\n",
        "url": "https://serverfault.com/questions/122737/in-bash-are-wildcard-expansions-guaranteed-to-be-in-order"
    },
    {
        "title": "Why does htop have three load averages?",
        "question": "On htop, it shows three load averages, one bold, one normal, one red. What do they signify?",
        "top_answer": "The first one is a 1 minute load average, second is 5 minutes, third is 15 minutes, just like the uptime command. They do not have any special correlation in htop other than to stand out from each other.\nThis is a good read on understanding what load is: http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages",
        "url": "https://serverfault.com/questions/204753/why-does-htop-have-three-load-averages"
    },
    {
        "title": "sudoers: how to disable requiretty per user",
        "question": "I don't want to comment out the line in /etc/sudoers:\nDefaults    requiretty\n\nInstead, I only want a certain user not to require a tty.\nHow can this be done?",
        "top_answer": "You said that you want one particular user to not require a tty. That's the default behavior. Nevertheless, you can explicitly set that like this:\nDefaults:username !requiretty\n\nIf you want everyone else to require a tty, then you'll have to uncommment the line.",
        "url": "https://serverfault.com/questions/111064/sudoers-how-to-disable-requiretty-per-user"
    },
    {
        "title": "How to specify hostname for the running container?",
        "question": "I'm having trouble setting the hostname on a running docker container. I'm also having trouble understanding how to specify hostname after the image is started.\nI started a container from an image I downloaded:\nsudo docker run -p 8080:80 -p 2222:22 oskarhane/docker-wordpress-nginx-ss\n\nBut I forgot to specify hostname through -h; how can I specify the hostname now that the container is running?",
        "top_answer": "Edit /etc/hostname is one thing for which you need ssh access inside the container. Otherwise, you can spin up the container with -h option. \nTo set the host and domain names:\n$ docker run -h foo.bar.baz -i -t ubuntu bash\nroot@foo:/# hostname\nfoo\nroot@foo:/# hostname -d\nbar.baz\nroot@foo:/# hostname -f\nfoo.bar.baz\n",
        "url": "https://serverfault.com/questions/590191/how-to-specify-hostname-for-the-running-container"
    },
    {
        "title": "Dealing with HTTP w00tw00t attacks",
        "question": "I have a server with apache and I recently installed mod_security2 because I get attacked a lot by this:\nMy apache version is apache v2.2.3 and I use mod_security2.c\nThis were the entries from the error log:\n[Wed Mar 24 02:35:41 2010] [error] \n[client 88.191.109.38] client sent HTTP/1.1 request without hostname \n(see RFC2616 section 14.23): /w00tw00t.at.ISC.SANS.DFind:)\n\n[Wed Mar 24 02:47:31 2010] [error] \n[client 202.75.211.90] client sent HTTP/1.1 request without hostname \n(see RFC2616 section 14.23): /w00tw00t.at.ISC.SANS.DFind:)\n\n[Wed Mar 24 02:47:49 2010] [error]\n[client 95.228.153.177] client sent HTTP/1.1 request without hostname\n(see RFC2616 section 14.23): /w00tw00t.at.ISC.SANS.DFind:)\n\n[Wed Mar 24 02:48:03 2010] [error] \n[client 88.191.109.38] client sent HTTP/1.1 request without hostname\n(see RFC2616 section 14.23): /w00tw00t.at.ISC.SANS.DFind:)\n\nHere are the errors from the access_log:\n202.75.211.90 - - \n[29/Mar/2010:10:43:15 +0200] \n\"GET /w00tw00t.at.ISC.SANS.DFind:) HTTP/1.1\" 400 392 \"-\" \"-\"\n211.155.228.169 - - \n[29/Mar/2010:11:40:41 +0200] \n\"GET /w00tw00t.at.ISC.SANS.DFind:) HTTP/1.1\" 400 392 \"-\" \"-\"\n211.155.228.169 - - \n[29/Mar/2010:12:37:19 +0200] \n\"GET /w00tw00t.at.ISC.SANS.DFind:) HTTP/1.1\" 400 392 \"-\" \"-\" \n\nI tried configuring mod_security2 like this:\nSecFilterSelective REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\\.DFind\"\nSecFilterSelective REQUEST_URI \"\\w00tw00t\\.at\\.ISC\\.SANS\"\nSecFilterSelective REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\"\nSecFilterSelective REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\\.DFind:\"\nSecFilterSelective REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\\.DFind:\\)\"\n\nThe thing in mod_security2 is that SecFilterSelective can not be used, it gives me errors. Instead I use a rule like this:\nSecRule REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\\.DFind\"\nSecRule REQUEST_URI \"\\w00tw00t\\.at\\.ISC\\.SANS\"\nSecRule REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\"\nSecRule REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\\.DFind:\"\nSecRule REQUEST_URI \"w00tw00t\\.at\\.ISC\\.SANS\\.DFind:\\)\"\n\nEven this does not work. I don't know what to do anymore. Anyone have any advice?\n\nUpdate 1\n\nI see that nobody can solve this problem using mod_security. So far using ip-tables seems like the best option to do this but I think the file will become extremely large because the ip changes serveral times a day.\nI came up with 2 other solutions, can someone comment on them on being good or not.\n\nThe first solution that comes to my mind is excluding these attacks from my apache error logs. This will make is easier for me to spot other urgent errors as they occur and don't have to spit trough a long log.\nThe second option is better i think, and that is blocking hosts that are not sent in the correct way. In this example the w00tw00t attack is send without hostname, so i think i can block the hosts that are not in the correct form. \n\n\nUpdate 2\n\nAfter going trough the answers I came to the following conclusions.\n\nTo have custom logging for apache will consume some unnecessary recourses, and if there really is a problem you probably will want to look at the full log without anything missing.\nIt is better to just ignore the hits and concentrate on a better way of analyzing your error logs. Using filters for your logs a good approach for this.\n\n\nFinal thoughts on the subject\n\nThe attack mentioned above will not reach your machine if you at least have an up to date system so there are basically no worries.\nIt can be hard to filter out all the bogus attacks from the real ones after a while, because both the error logs and access logs get extremely large.\nPreventing this from happening in any way will cost you resources and it is a good practice not to waste your resources on unimportant stuff.\nThe solution i use now is Linux logwatch. It sends me summaries of the logs and they are filtered and grouped. This way you can easily separate the important from the unimportant.\nThank you all for the help, and I hope this post can be helpful to someone else too.",
        "top_answer": "From your error log they are sending a HTTP/1.1 request without the Host: portion of the request.  From what I read, Apache replies with a 400 (bad request) error to this request, before handing over to mod_security. So, it doesn't look like your rules will be processed.  (Apache dealing with it before requiring to hand over to mod_security)\nTry yourself:\n\ntelnet hostname 80\nGET /blahblahblah.html HTTP/1.1  (enter)\n(enter)\n\nYou should get the 400 error and see the same error in your logs.  This is a bad request and apache is giving the correct answer.\nProper request should look like:\n\nGET /blahblahblah.html HTTP/1.1\nHost: blah.com\n\nA work around for this issue could be to patch mod_uniqueid, to generate a unique ID even for a failed request, in order that apache passes the request on to its request handlers.\nThe following URL is a discussion about this work around, and includes a patch for mod_uniqueid you could use:\n http://marc.info/?l=mod-security-users&m=123300133603876&w=2\nCouldn't find any other solutions for it and wonder if a solution is actually required.",
        "url": "https://serverfault.com/questions/125607/dealing-with-http-w00tw00t-attacks"
    },
    {
        "title": "Linux: using find to locate files older than <date>",
        "question": "find has good support for finding files the more modified less than X days ago, but how can I use find to locate all files modified before a certain date?\nI can't find anything in the find man page to do this, only to compare against another files time or to check for differences between created time and now. Is making a file with the desired time and comparing against that the only way to do this?",
        "top_answer": "If you have only '-newer file' then you can use this workaround:\n# create 'some_file' having a creation date of 16 Mar 2010:\ntouch -t 201003160120 some_file\n\n# find all files created after this date\nfind . -newer some_file\n\nman touch:\n  -t STAMP\n          use [[CC]YY]MMDDhhmm[.ss] instead of current time\n\nAssuming that your touch has this option (mine is touch 5.97).",
        "url": "https://serverfault.com/questions/122824/linux-using-find-to-locate-files-older-than-date"
    },
    {
        "title": "Centos 7 save iptables settings",
        "question": "Problem: iptables resets to default settings after server reboot.\nI'm trying to set rule like this:\niptables -I INPUT -p tcp --dport 3000 -j ACCEPT\nafter that I do:\nservice iptables save\nand it writes back something like this \niptables: Saving firewall rules to /etc/sysconfig/iptables:[  OK  ]\nand after this I just ran (this was done once):\nchkconfig iptables on (I have read that this has to be done in order to restore settings after reboot)\nAfter that I reboot and run this command:\nsystemctl list-unit-files | grep iptables\nand I see that iptables.service is enabled, however, the rule (to open port 3000) does not work anymore.\nHow do I persist these settings?",
        "top_answer": "CentOS 7 is using FirewallD now!  Use the --permanent flag to save settings.\nExample:\nfirewall-cmd --zone=public --add-port=3000/tcp --permanent\n\nThen reload rules:\nfirewall-cmd --reload\n",
        "url": "https://serverfault.com/questions/626521/centos-7-save-iptables-settings"
    },
    {
        "title": "AWS RDS connection limits",
        "question": "RDS server come up with 40 connection max, as in the following documentation\nI am using Magento 1.9, and at some points, i reach the max number then website is out of service.\nDo you have any recommended way to solve this issue?   \nFrom my understanding, if i have 2 web servers connection to an RDS server.. then I should have 2 RDS connections, not more.",
        "top_answer": "AWS RDS max_connections limit variable is based on Instance type, so you can upgrade your RDS or make more replica.\nThe RDS types with max_connections limit:\n\nt2.micro 66\nt2.small 150\nm3.medium 296\nt2.medium 312\nm3.large 609\nt2.large 648\nm4.large 648\nm3.xlarge 1237\nr3.large 1258\nm4.xlarge 1320\nm2.xlarge 1412\nm3.2xlarge 2492\nr3.xlarge 2540\n\n\nReferring by max_connections at AWS RDS MySQL Instance Sizes in 2015\n\nUpdate 2017-07\nThe current RDS MySQL max_connections setting is default by {DBInstanceClassMemory/12582880}, if you use t2.micro with 512MB RAM, the max_connections could be (512*1024*1024)/12582880 ~= 40, and so on.\n\nEach Web server could have many connections to RDS, which depends on your SQL requests from Web server.",
        "url": "https://serverfault.com/questions/862387/aws-rds-connection-limits"
    },
    {
        "title": "Storing a million images in the filesystem",
        "question": "I have a project that will generate a huge number of images.  Around 1,000,000 for start. They are not large images so I will store them all on one machine at start.  \nHow do you recommended on storing these images efficiently?  (NTFS file system currently)\nI am considering a naming scheme... for start all the images will have an incremental name from 1 up\nI hope this will help me sort them later if needed, and throw them in different folders.\n\n\nwhat would be a better naming scheme:\n\na/b/c/0 ... z/z/z/999\n\nor\n\na/b/c/000 ... z/z/z/999\n\nany idea on this ?",
        "top_answer": "I'd recommend using a regular file system instead of databases. Using file system is easier than a database, you can use normal tools to access files, file systems are designed for this kind of usage etc. NTFS should work just fine as a storage system.\nDo not store the actual path to database. Better to store the image's sequence number to database and have function that can generate path from the sequence number. e.g:\n File path = generatePathFromSequenceNumber(sequenceNumber);\n\nIt is easier to handle if you need to change directory structure some how. Maybe you need to move the images to different location, maybe you run out of space and you start storing some of the images on the disk A and some on the disk B etc. It is easier to change one function than to change paths in database.\nI would use this kind of algorithm for generating the directory structure:\n\nFirst pad you sequence number with leading zeroes until you have at least 12 digit string. This is the name for your file. You may want to add a suffix:\n\n\n12345 -> 000000012345.jpg\n\nThen split the string to 2 or 3 character blocks where each block denotes a directory level. Have a fixed number of directory levels (for example 3):\n\n\n000000012345 -> 000/000/012\n\nStore the file to under generated directory:\n\n\nThus the full path and file filename for file with sequence id 123 is  000/000/012/00000000012345.jpg\nFor file with sequence id 12345678901234 the path would be 123/456/789/12345678901234.jpg\n\n\nSome things to consider about directory structures and file storage:\n\nAbove algorithm gives you a system where every leaf directory has maximum of 1000 files (if you have less that total of 1 000 000 000 000 files)\nThere may be limits how many files and subdirectories a directory can contain, for example ext3 files system on Linux has a limit of 31998 sub-directories per one directory.\nNormal tools (WinZip, Windows Explorer, command line, bash shell, etc.) may not work very well if you have large number of files per directory (> 1000)\nDirectory structure itself will take some disk space, so you'll do not want too many directories.\nWith above structure you can always find the correct path for the image file by just looking at the filename, if you happen to mess up your directory structures.\nIf you need to access files from several machines, consider sharing the files via a network file system.\nThe above directory structure will not work if you delete a lot of files. It leaves \"holes\" in directory structure. But since you are not deleting any files it should be ok.\n",
        "url": "https://serverfault.com/questions/95444/storing-a-million-images-in-the-filesystem"
    },
    {
        "title": "mirrorlist.centos.org no longer resolve?",
        "question": "mirrorlist.centos.org no longer online?\nI can resolve to centos.org but not to mirrorlist.centos.org\nHere is the output of my resolv.conf, dig, and nslookup if anyone interested.\nrosdi@H-TQ0nmTkQIbRhN:~/test_ssl$ nslookup mirrorlist.centos.org\nServer:         1.1.1.1\nAddress:        1.1.1.1#53\n\n** server can't find mirrorlist.centos.org: NXDOMAIN\n\nrosdi@H-TQ0nmTkQIbRhN:~/test_ssl$ nslookup centos.org\nServer:         1.1.1.1\nAddress:        1.1.1.1#53\n\nNon-authoritative answer:\nName:   centos.org\nAddress: 52.56.83.118\nName:   centos.org\nAddress: 81.171.33.201\nName:   centos.org\nAddress: 81.171.33.202\nName:   centos.org\nAddress: 2001:4de0:aaae::202\nName:   centos.org\nAddress: 2a05:d01c:c6a:cc02:225e:ab54:d58c:8b14\nName:   centos.org\nAddress: 2001:4de0:aaae::201\n\nrosdi@H-TQ0nmTkQIbRhN:~/test_ssl$ cat /etc/resolv.conf\nnameserver 1.1.1.1\nnameserver 8.8.8.8\n\nrosdi@H-TQ0nmTkQIbRhN:~/test_ssl$ dig mirrorlist.centos.org\n\n; <<>> DiG 9.18.24-0ubuntu5-Ubuntu <<>> mirrorlist.centos.org\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 61636\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;mirrorlist.centos.org.         IN      A\n\n;; AUTHORITY SECTION:\ncentos.org.             2795    IN      SOA     ns1.centos.org. hostmaster.centos.org. 2024070102 28800 7200 2400000 3600\n\n;; Query time: 70 msec\n;; SERVER: 1.1.1.1#53(1.1.1.1) (UDP)\n;; WHEN: Mon Jul 01 16:31:41 +08 2024\n;; MSG SIZE  rcvd: 101\n\nrosdi@H-TQ0nmTkQIbRhN:~/test_ssl$ nslookup mirrorlist.centos.org\nServer:         1.1.1.1\nAddress:        1.1.1.1#53\n\n** server can't find mirrorlist.centos.org: NXDOMAIN\n\nrosdi@H-TQ0nmTkQIbRhN:~/test_ssl$\n\nmxtoolbox also showing the same thing.\n",
        "top_answer": "mirrorlist.centos.org doesn't exists anymore.\nFrom the .repo file:\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n\nTo resolve the issue you can mass update all .repo files:\nsed -i s/mirror.centos.org/vault.centos.org/g /etc/yum.repos.d/CentOS-*.repo\nsed -i s/^#.*baseurl=http/baseurl=http/g /etc/yum.repos.d/CentOS-*.repo\nsed -i s/^mirrorlist=http/#mirrorlist=http/g /etc/yum.repos.d/CentOS-*.repo\n",
        "url": "https://serverfault.com/questions/1161816/mirrorlist-centos-org-no-longer-resolve"
    },
    {
        "title": "Remove \"www\" and redirect to \"https\" with nginx",
        "question": "I want to create a rule in nginx that does two things:\n\nRemoves the \"www.\" from the request URI\nRedirects to \"https\" if the request URI is \"http\"\n\nThere are plenty of examples of how to do each of those things individually, but I can't figure out a solution that does both correctly (i.e. doesn't create a redirect loop and handles all cases properly).\nIt needs to handle all of these cases:\n1. http://www.example.com/path\n2. https://www.example.com/path\n3. http://example.com/path\n4. https://example.com/path\n\nThese should all end up at https://example.com/path (#4) without looping. Any ideas?",
        "top_answer": "The best way to accomplish this is using three server blocks: one to redirect http to https, one to redirect the https www-name to no-www, and one to actually handle requests.  The reason for using extra server blocks instead of ifs is that server selection is performed using a hash table, and is very fast.  Using a server-level if means the if is run for every request, which is wasteful.  Also, capturing the requested uri in the rewrite is wasteful, as nginx already has this information in the $uri and $request_uri variables (without and with query string, respectively).\nserver {\n    server_name www.example.com example.com;\n    return 301 https://example.com$request_uri;\n}\n\nserver {\n    listen 443 ssl;\n    ssl_certificate /path/to/server.cert;\n    ssl_certificate_key /path/to/server.key;\n    server_name www.example.com;\n    return 301 https://example.com$request_uri;\n}\n\nserver {\n    listen 443 ssl;\n    ssl_certificate /path/to/server.cert;\n    ssl_certificate_key /path/to/server.key;\n    server_name example.com;\n\n    <locations for processing requests>\n}\n",
        "url": "https://serverfault.com/questions/258378/remove-www-and-redirect-to-https-with-nginx"
    },
    {
        "title": "Allow SCP but not actual login using SSH",
        "question": "Is there any way to configure a user on a Linux box (Centos 5.2 in this case) so that they can use scp to retrieve files, but can't actually login to the server using SSH?",
        "top_answer": "DEPRECATED: Please note the following answer is out of date. rssh is no longer maintained and is no longer a secure method.\nrssh shell (http://pizzashack.org/rssh/) is designed for precisely this purpose.\nSince RHEL/CentOS 5.2 doesn't include a package for rssh, you might look here to obtain an RPM: http://dag.wieers.com/rpm/packages/rssh/\nTo use it just set it as a shell for a new user like this:\nuseradd -m -d /home/scpuser1 -s /usr/bin/rssh scpuser1\npasswd scpuser1\n\n..or change the shell for an existing one like this:\nchsh -s /usr/bin/rssh scpuser1\n\n..and edit /etc/rssh.conf to configure rssh shell - especially uncomment allowscp line to enable SCP access for all rssh users.\n(You may also want to use chroot to keep the users contained in their homes but that's another story.)",
        "url": "https://serverfault.com/questions/83856/allow-scp-but-not-actual-login-using-ssh"
    },
    {
        "title": "How does Windows decide which DNS Server to use when resolving names?",
        "question": "What algorithm does Windows use to decide which DNS Server it will query in order to resolve names?\nLet's say I have several interfaces, all active, some with no dns server specified, some told to determine it automatically, and some with it specified manually (in interface ipv4 AND interface ipv6).\nI'm asking for an answer to this general question hoping that I know how to solve a more specific problem in Windows Vista - I have two interfaces, one a lower metric and a DNS server specified manually. nslookup uses THIS DNS server and resolves the names correctly. However, all other applications fail to resolve the name unless I manually specify a DNS server for the other interface, which the applications then use. nslookup also uses the DNS server specified for this other interface once it is specified.\nThanks",
        "top_answer": "If I'm not mistaken, it's determined by the NIC binding order in the Advanced Settings in the network connections folder. You can verify it by changing the binding order of the various NIC's and running nslookup as a test.\nTo expand on my answer, citing the article that Evan linked, here is an excerpt from said article:\n\nThe DNS Client service queries the DNS servers in the following order:\n\nThe DNS Client service sends the name query to the first DNS server on the preferred adapter\u2019s list of DNS servers and waits one second for a response.\nIf the DNS Client service does not receive a response from the first DNS server within one second, it sends the name query to the first DNS servers on all adapters that are still under consideration and waits two seconds for a response.\nIf the DNS Client service does not receive a response from any DNS server within two seconds, the DNS Client service sends the query to all DNS servers on all adapters that are still under consideration and waits another two seconds for a response.\nIf the DNS Client service still does not receive a response from any DNS server, it sends the name query to all DNS servers on all adapters that are still under consideration and waits four seconds for a response.\nIf it the DNS Client service does not receive a response from any DNS server, the DNS client sends the query to all DNS servers on all adapters that are still under consideration and waits eight seconds for a response.\n\n\nThe preferred adapter in step 1 being the adapter that's listed first in the binding order.",
        "url": "https://serverfault.com/questions/84291/how-does-windows-decide-which-dns-server-to-use-when-resolving-names"
    },
    {
        "title": "Does each server behind a load balancer need their own SSL certificate?",
        "question": "If you have 5 web servers behind a load balancer (such as haproxy) and they are serving up content for the same domain, do you need SSL certificates for all the servers, or can you use the same certificate on each server?\nI know you can put all SSL requests on a specific server, but that requires distributed session info and hoping it doesn't come to that.",
        "top_answer": "\nIf you have 5 web servers behind a load balancer (...)\n  do you need SSL certificates for all the servers, \n\nIt depends.\nIf you do your load balancing on the TCP or IP layer (OSI layer 4/3, a.k.a L4, L3), then yes, all HTTP servers will need to have the SSL certificate installed.\nIf you load balance on the HTTPS layer (L7), then you'd commonly install the certificate on the load balancer alone, and use plain un-encrypted HTTP over the local network between the load balancer and the webservers (for best performance on the web servers).\nIf you have a large installation, then you may be doing Internet -> L3 load balancing -> layer of L7 SSL concentrators -> load balancers -> layer of L7 HTTP application servers...\nWilly Tarreau, the author of HAProxy, has a really nice overview of the canonical ways of load balancing HTTP/HTTPS.\nIf you install a certificate on each server, then be sure to get a certificate that supports this. Normally certificates can be installed on multiple servers, as long as the servers all serve traffic for one Fully Qualified Domain Name only. But verify what you're buying, certificate issuers can have a confusing product portfolio...",
        "url": "https://serverfault.com/questions/68753/does-each-server-behind-a-load-balancer-need-their-own-ssl-certificate"
    },
    {
        "title": "How to unify package installation tasks in ansible?",
        "question": "I am starting with ansible and will use it, among others, to install packages on several Linux distros.\nI see in the docs that the yum and apt commands are separated - what would be the easiest way to unify them and use something like this:\n- name: install the latest version of Apache\n  unified_install: name=httpd state=latest\n\ninstead of\n- name: install the latest version of Apache on CentOS\n  yum: name=httpd state=latest\n  when: ansible_os_family == \"RedHat\"\n\n- name: install the latest version of Apache on Debian\n  apt: pkg=httpd state=latest \n  when: ansible_os_family == \"Debian\"\n\nI understand that the two package managers are different, but they still have a set of common basic usages. Other orchestators (salt for instance) have a single install command.",
        "top_answer": "Update: As of Ansible 2.0, there is now a generic & abstracted package module\nUsage Examples:\nNow when the package name is the same across different OS families, it's as simple as:\n---\n- name: Install foo\n  package: name=foo state=latest\n\nWhen the package name differs across OS families, you can handle it with distribution or OS family specific vars files:\n---\n# roles/apache/apache.yml: Tasks entry point for 'apache' role. Called by main.yml\n# Load a variable file based on the OS type, or a default if not found.\n- include_vars: \"{{ item }}\"\n  with_first_found:\n    - \"../vars/{{ ansible_distribution }}-{{ ansible_distribution_major_version | int}}.yml\"\n    - \"../vars/{{ ansible_distribution }}.yml\"\n    - \"../vars/{{ ansible_os_family }}.yml\"\n    - \"../vars/default.yml\"\n  when: apache_package_name is not defined or apache_service_name is not defined\n\n- name: Install Apache\n  package: >\n    name={{ apache_package_name }}\n    state=latest\n\n- name: Enable apache service\n  service: >\n    name={{ apache_service_name }}\n    state=started\n    enabled=yes\n  tags: packages\n\nThen, for each OS that you must handle differently... create a vars file:\n---\n# roles/apache/vars/default.yml\napache_package_name: apache2\napache_service_name: apache2\n\n---\n# roles/apache/vars/RedHat.yml\napache_package_name: httpd\napache_service_name: httpd\n\n---\n# roles/apache/vars/SLES.yml\napache_package_name: apache2\napache_service_name: apache2\n\n---\n# roles/apache/vars/Debian.yml\napache_package_name: apache2\napache_service_name: apache2\n\n---\n# roles/apache/vars/Archlinux.yml\napache_package_name: apache\napache_service_name: httpd\n\n\nEDIT: Since Michael DeHaan (creator of Ansible) has chosen not to abstract out the package manager modules like Chef does,\nIf you are still using an older version of Ansible (Ansible < 2.0), unfortunately you'll need to handle doing this in all of your playbooks and roles.  IMHO this pushes a lot of unnecessary repetitive work onto playbook & role authors... but it's the way it currently is.  Note that I'm not saying we should try to abstract package managers away while still trying to support all of their specific options and commands, but just have an easy way to install a package that is package manager agnostic.  I'm also not saying that we should all jump on the Smart Package Manager bandwagon, but that some sort of package installation abstraction layer in your configuration management tool is very useful to simplify cross-platform playbooks/cookbooks.  The Smart project looks interesting, but it is quite ambitious to unify package management across distros and platforms without much adoption yet... it'll be interesting to see whether it is successful.  The real issue is just that package names sometimes tend to be different across distros, so we still have to do case statements or when: statements to handle the differences.\nThe way I've been dealing with it is to follow this tasks directory structure in a playbook or role:\nroles/foo\n\u2514\u2500\u2500 tasks\n \u00a0\u00a0 \u251c\u2500\u2500 apt_package.yml\n \u00a0\u00a0 \u251c\u2500\u2500 foo.yml\n \u00a0\u00a0 \u251c\u2500\u2500 homebrew_package.yml\n \u00a0\u00a0 \u251c\u2500\u2500 main.yml\n \u00a0\u00a0 \u2514\u2500\u2500 yum_package.yml\n\nAnd then have this in my main.yml:\n---\n# foo: entry point for tasks\n#                 Generally only include other file(s) and add tags here.\n\n- include: foo.yml tags=foo\n\nThis in foo.yml (for package 'foo'):\n---\n# foo: Tasks entry point. Called by main.yml\n- include: apt_package.yml\n  when: ansible_pkg_mgr == 'apt'\n- include: yum_package.yml\n  when: ansible_pkg_mgr == 'yum'\n- include: homebrew_package.yml\n  when: ansible_os_family == 'Darwin'\n\n- name: Enable foo service\n  service: >\n    name=foo\n    state=started\n    enabled=yes\n  tags: packages\n  when: ansible_os_family != 'Darwin'\n\nThen for the different package managers:\nApt:\n---\n# tasks file for installing foo on apt based distros\n\n- name: Install foo package via apt\n  apt: >\n    name=foo{% if foo_version is defined %}={{ foo_version }}{% endif %}\n    state={% if foo_install_latest is defined and foo_version is not defined %}latest{% else %}present{% endif %}\n  tags: packages\n\nYum:\n---\n# tasks file for installing foo on yum based distros\n- name: Install EPEL 6.8 repos (...because it's RedHat and foo is in EPEL for example purposes...)\n  yum: >\n    name={{ docker_yum_repo_url }}\n    state=present\n  tags: packages\n  when: ansible_os_family == \"RedHat\" and ansible_distribution_major_version|int == 6\n\n- name: Install foo package via yum\n  yum: >\n    name=foo{% if foo_version is defined %}-{{ foo_version }}{% endif %}\n    state={% if foo_install_latest is defined and foo_version is not defined %}latest{% else %}present{% endif %}\n  tags: packages\n\n- name: Install RedHat/yum-based distro specific stuff...\n  yum: >\n    name=some-other-custom-dependency-on-redhat\n    state=latest\n  when: ansible_os_family == \"RedHat\"\n  tags: packages\n\nHomebrew:\n---\n- name: Tap homebrew foobar/foo\n  homebrew_tap: >\n    name=foobar/foo\n    state=present\n\n- homebrew: >\n    name=foo\n    state=latest\n\nNote that this is awfully repetitive and not D.R.Y., and although some things might be different on the different platforms and will have to be handled, generally I think this is verbose and unwieldy when compared to Chef's:\npackage 'foo' do\n  version node['foo']['version']\nend\n\ncase node[\"platform\"]\nwhen \"debian\", \"ubuntu\"\n  # do debian/ubuntu things\nwhen \"redhat\", \"centos\", \"fedora\"\n  # do redhat/centos/fedora things\nend\n\nAnd yes, there is the argument that some package names are different across distros.  And although there is currently a lack of easily accessible data, I'd venture to guess that most popular package names are common across distros and could be installed via an abstracted package manager module.  Special cases would need to be handled anyway, and would already require extra work making things less D.R.Y.  If in doubt, check pkgs.org.",
        "url": "https://serverfault.com/questions/587727/how-to-unify-package-installation-tasks-in-ansible"
    },
    {
        "title": "How to ensure OpenVPN connection uses specific DNS?",
        "question": "I'm using OpenVPN through Tunnelblick on MacOS X Lion.\nI need to set specific DNS (with local IP, which works only when VPN is up) for the duration of this VPN session only.\nI do not have access to the OpenVPN server configuration. Only client config. Also, DNS from the server doesn't work.\nSo it works like this: I connect to VPN, go the Network preferences and manually set DNS.\nAfter VPN is disconnected, I switch back to default. It works, but it needs to be automatic.\nAfter some exploration I found that OpenVPN up- and down- scripts might help me with that. Unfortunately, I haven't found any specific documentation about how exactly it can be done.\nHow it can or can't be done? Any advice would be appreciated!",
        "top_answer": "try adding:\n# put actual dns name here\ndhcp-option DNS 10.11.12.13\n\nto your client's config",
        "url": "https://serverfault.com/questions/416708/how-to-ensure-openvpn-connection-uses-specific-dns"
    },
    {
        "title": "How to configure a Windows machine to allow file sharing with a DNS alias",
        "question": "What process is necessary to configure a Windows environment to allow me to use DNS CNAME to reference servers?\nI want to do this so that I can name my servers something like SRV001, but still have \\\\file point to that server, so when SRV002 replaces it I don't have to update any of the links people have, just update the DNS CNAME and everyone will get pointed to the new server.",
        "top_answer": "To facilitate failover schemes, a common technique is to use DNS CNAME records (DNS Aliases) for different machine roles. Then instead of changing the Windows computername of the actual machine name, one can switch a DNS record to point to a new host.\nThis can work on Microsoft Windows machines, but to make it work with file sharing the following configuration steps need to be taken.\nOutline\n\nThe Problem\nThe Solution \n\n\nAllowing other machines to use filesharing via the DNS Alias (DisableStrictNameChecking)\nAllowing server machine to use filesharing with itself via the DNS Alias (BackConnectionHostNames)\nProviding browse capabilities for multiple NetBIOS names (OptionalNames)\nRegister the Kerberos service principal names (SPNs) for other Windows functions like Printing (setspn)\n\nReferences\n\n1. The Problem\nOn Windows machines, file sharing can work via the computer name, with or without full qualification, or by the IP Address. By default, however, filesharing will not work with arbitrary DNS aliases. To enable filesharing and other Windows services to work with DNS aliases, you must make registry changes as detailed below and reboot the machine.\n2. The Solution\nAllowing other machines to use filesharing via the DNS Alias (DisableStrictNameChecking)\nThis change alone will allow other machines on the network to connect to the machine using any arbitrary hostname. (However this change will not allow a machine to connect to itself via a hostname, see BackConnectionHostNames below).\n\nEdit the registry key HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\lanmanserver\\parameters and add a value DisableStrictNameChecking of type DWORD set to 1.\nEdit the registry key (on 2008 R2) HKLM\\SYSTEM\\CurrentControlSet\\Control\\Print and add a value DnsOnWire of type DWORD set to 1\n\nAllowing server machine to use filesharing with itself via the DNS Alias (BackConnectionHostNames)\nThis change is necessary for a DNS alias to work with filesharing from a machine to find itself. This creates the Local Security Authority host names that can be referenced in an NTLM authentication request.\nTo do this, follow these steps for all the nodes on the client computer:\n\nTo the registry subkey HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Lsa\\MSV1_0, add new Multi-String Value BackConnectionHostNames\nIn the Value data box, type the CNAME or the DNS alias, that is used for the local shares on the computer, and then click OK. \n\n\nNote: Type each host name on a separate line. \n\n\nProviding browse capabilities for multiple NetBIOS names (OptionalNames)\nAllows ability to see the network alias in the network browse list.\n\nEdit the registry key HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\lanmanserver\\parameters and add a value OptionalNames of type Multi-String\nAdd in a newline delimited list of names that should be registered under the NetBIOS browse entries\n\n\nNames should match NetBIOS conventions (i.e. not FQDN, just hostname)\n\n\nRegister the Kerberos service principal names (SPNs) for other Windows functions like Printing (setspn)\nNOTE: Should not need to do this for basic functions to work, documented here for completeness. We had one situation in which the DNS alias was not working because there was an old SPN record interfering, so if other steps aren't working check if there are any stray SPN records.\nYou must register the Kerberos service principal names (SPNs), the host name, and the fully-qualified domain name (FQDN) for all the new DNS alias (CNAME) records. If you do not do this, a Kerberos ticket request for a DNS alias (CNAME) record may fail and return the error code KDC_ERR_S_SPRINCIPAL_UNKNOWN.\nTo view the Kerberos SPNs for the new DNS alias records, use the Setspn command-line tool (setspn.exe). The Setspn tool is included in Windows Server 2003 Support Tools. You can install Windows Server 2003 Support Tools from the Support\\Tools folder of the Windows Server 2003 startup disk.\nHow to use the tool to list all records for a computername:\nsetspn -L computername\n\nTo register the SPN for the DNS alias (CNAME) records, use the Setspn tool with the following syntax:\nsetspn -A host/your_ALIAS_name computername\nsetspn -A host/your_ALIAS_name.company.com computername\n\n3. References\nAll the Microsoft references work via: http://support.microsoft.com/kb/\n\nConnecting to SMB share on a Windows 2000-based computer or a Windows Server 2003-based computer may not work with an alias name\n\n\nCovers the basics of making file sharing work properly with DNS alias records from other computers to the server computer.\nKB281308\n\nError message when you try to access a server locally by using its FQDN or its CNAME alias after you install Windows Server 2003 Service Pack 1: \"Access denied\" or \"No network provider accepted the given network path\"\n\n\nCovers how to make the DNS alias work with file sharing from the file server itself. \nKB926642\n\nHow to consolidate print servers by using DNS alias (CNAME) records in Windows Server 2003 and in Windows 2000 Server \n\n\nCovers more complex scenarios in which records in Active Directory may need to be updated for certain services to work properly and for browsing for such services to work properly, how to register the Kerberos service principal names (SPNs). \nKB870911\n\nDistributed File System update to support consolidation roots in Windows Server 2003\n\n\nCovers even more complex scenarios with DFS (discusses OptionalNames).\nKB829885\n\n",
        "url": "https://serverfault.com/questions/23823/how-to-configure-a-windows-machine-to-allow-file-sharing-with-a-dns-alias"
    },
    {
        "title": "Command to prepend string to each line?",
        "question": "Looking for something like this? Any ideas?\ncmd | prepend \"[ERRORS] \"\n\n[ERROR] line1 text\n[ERROR] line2 text\n[ERROR] line3 text\n... etc\n",
        "top_answer": "cmd | while read line; do echo \"[ERROR] $line\"; done\n\nhas the advantage of only using bash builtins so fewer processes will be created/destroyed so it should be a touch faster than awk or sed.\n@tzrik points out that it might also make a nice bash function.  Defining it like:\nfunction prepend() { while read line; do echo \"${1}${line}\"; done; }\n\nwould allow it to be used like:\ncmd | prepend \"[ERROR] \"\n",
        "url": "https://serverfault.com/questions/72744/command-to-prepend-string-to-each-line"
    },
    {
        "title": "Cannot SSH: debug1: expecting SSH2_MSG_KEX_DH_GEX_REPLY [closed]",
        "question": "We have a server on Amazon EC2 running SSH is on a standard (22) port.\nI placed my public key at the <username>/.ssh/authorized_keys file.\nThe fun thing is that yesterday it was working great! But today, I don't know what happened! I just can't log in.\nssh -vvvv servername\n\nis stuck on\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_REPLY\n\nI got someone to confirm that my public key is there.\nI added a new public key from another computer (windows 7 + putty) and I was able to log in. This other computer with Win7 is on the same LAN which means that the external IP is the same.\nMy private key works for other servers but not with this.",
        "top_answer": "Change the network interface MTU to solve it. This is a bug for ubuntu 14.04.\nThis worked for me:\nsudo ip li set mtu 1200 dev wlan0\n\nOR\nsudo ifconfig wlan0 mtu 1200\n\nssh fails to connect to VPN host - hangs at 'expecting SSH2_MSG_KEX_ECDH_REPLY'",
        "url": "https://serverfault.com/questions/210408/cannot-ssh-debug1-expecting-ssh2-msg-kex-dh-gex-reply"
    },
    {
        "title": "How Often Do Windows Servers Need to be Restarted?",
        "question": "A little background: We have several Windows servers (2003, 2008) for our department. We're a division of IT so we manage our own servers. Of the four of us here I'm the only one with a slight amount of IT knowledge. (Note the \"slight amount\".) My boss says the servers need to be restarted at least weekly. I disagree. Our IT Department says that because she restarts them constantly that's the reason why our hard drives fail and power supplies go out on them. (That's happened to a few of our servers a couple times over the last four years, and very recently.)\nSo the question is: How often does everyone restart their Windows servers? Is there an industry standard or recommendation? Is our IT department correct in saying that because we re-start that's why we're having hardware issues? (I need a reason if I'm going to change her mind!)",
        "top_answer": "\nMy boss says the servers need to be restarted at least weekly\n\nI strongly disagree.  Microsoft has made great strides since the good-ole [NT, anyone?] days with regard to stability and uptime.  It's a shame the consensus within IT support has not changed along with this.\n\nHow often does everyone restart their Windows servers? \n\nOnly when required -- Either because of an OS/software update, a critical software failure which cannot be recovered via other methods, hardware upgrade/replacement or other activity that cannot happen without a restart.1\n\nIs there an industry standard or recommendation? \n\nI have never seen a standard recommendation, per se, but I could not agree with any recommendation [except from MS themselves] which would indicate a required reboot at a specific time interval \"just-because\".\n\nIs our IT department correct in saying that because we re-start that's why we're having hardware issues? \n\nRestarting [and, more so, power cycling] is the most stressful period of hardware activity for a computer.  You have most everything spinning up to 100% -- disk and fans... ...as well as significant fluctuations in component temperatures.  Modern hardware is incredibly resilient, but that shouldn't be a reason for just bouncing servers, on a whim, a few times a week. \n1   Aside, I loathe when techs \"just\" reboot a Windows server in the case of a failed service, or the like.  I understand the need to get the service running again, but a reboot should be the last step in trouble shooting a server. Identifying, and fixing[!], the root cause of failure should almost never result in \"Meh, just reboot it....\"",
        "url": "https://serverfault.com/questions/274089/how-often-do-windows-servers-need-to-be-restarted"
    },
    {
        "title": "Enable HTTP Strict Transport Security (HSTS) in IIS 7",
        "question": "What is the best way to turn on HTTP Strict Transport Security on an IIS 7 web server?\nCan I just through the GUI and add the proper HTTP response header or should I be using appcmd and if so what switches?",
        "top_answer": "IIS has the ability to add custom headers to responses.  This would seem to be the easiest way to go about it.\nAccording to the documentation on IIS.net you can add these headers through IIS Manager:\n\n\nIn the Connections pane, go to the site, application, or directory for which you want to set a custom HTTP header.\nIn the Home pane, double-click HTTP Response Headers.\nIn the HTTP Response Headers pane, click Add... in the Actions pane.\nIn the Add Custom HTTP Response Header dialog box, set the name and value for your custom header, and then click OK.\n\n",
        "url": "https://serverfault.com/questions/417173/enable-http-strict-transport-security-hsts-in-iis-7"
    },
    {
        "title": "Tried to create 2 record set type=TXT in Route53",
        "question": "I'm trying to add a second TXT record to a domain, but I get the following error: \n\nTried to create resource record set type='TXT but it already exists\n\nCan I add two records at the same domain?",
        "top_answer": "You would enter all the TXT values at the same time... even the one that already exists.\nExample CLI:\nroute53 --zone example.com -c --type TXT --name example.com --values \"text1\",\"text2\",\"text3\" \n\nExample WebUI:\n\"txt=ABC123\"\n\"txt=CDE456\"\n\nSee here as well: https://superuser.com/questions/573305/unable-to-create-txt-record-using-amazon-route-53",
        "url": "https://serverfault.com/questions/616407/tried-to-create-2-record-set-type-txt-in-route53"
    },
    {
        "title": "How to make wireshark filter POST-requests only?",
        "question": "How to make wireshark filter POST-requests only?",
        "top_answer": "You can use the following filter:\nhttp.request.method == \"POST\"\n",
        "url": "https://serverfault.com/questions/309515/how-to-make-wireshark-filter-post-requests-only"
    },
    {
        "title": "What should I do when I got the KEYEXPIRED error message after an apt-get update?",
        "question": "While updating my packages on a debian based system by a \nsudo apt-get update\n\nI've got that error message :\nReading package lists... Done\nW: GPG error: ftp://ftp.fr.debian.org stable/non-US Release: \nThe following signatures were invalid: KEYEXPIRED 1138684904\n\nWhat should I do to fix this ?",
        "top_answer": "To find any expired repository keys and their IDs, use apt-key as follows:\nLANG=C apt-key list | grep expired\n\nYou will get a result similar to the following:\npub   4096R/BE1DB1F1 2011-03-29 [expired: 2014-03-28]\n\nThe key ID is the bit after the / i.e. BE1DB1F1 in this case.\nTo update the key, run\nsudo apt-key adv --recv-keys --keyserver YOUR_GPGKEY_HOST_DOMAIN BE1DB1F1\n\nNote:\n\nUpdating the key will obviously not work if the package maintainer has not (yet) uploaded a new key. In that case there is little you can do other than contacting the maintainer, filing a bug against your distribution etc.\nYOUR_GPGKEY_HOST_DOMAIN  indicates domain name of any available GPG key server, such as\n\nkeyserver.ubuntu.com\nkeys.openpgp.org\npgp.mit.edu\n\n\n(update 2023.2.22) The SKS key server keys.gnupg.net is deprecated and gone.\n\nOne liner to update all expired keys: (thanks to @ryanpcmcquen)\nfor K in $(apt-key list | grep expired | cut -d'/' -f2 | cut -d' ' -f1); do sudo apt-key adv --recv-keys --keyserver keys.gnupg.net $K; done\n",
        "url": "https://serverfault.com/questions/7145/what-should-i-do-when-i-got-the-keyexpired-error-message-after-an-apt-get-update"
    },
    {
        "title": "How do I delete a route from Linux routing table",
        "question": "This turns out to be harder than I thought.  The routes I want to delete are the \"!\" rejected routes, but I can't seem to formulate the right \"route del\" command to pull it off.\nHere is the routing table...\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n67.40.227.206   *               255.255.255.255 UH    0      0        0 ppp0\n192.168.46.79   *               255.255.255.255 UH    0      0        0 ipsec0\n192.168.46.79   -               255.255.255.255 !H    2      -        0 -\n192.168.1.0     *               255.255.255.0   U     0      0        0 eth0\n10.1.0.0        *               255.255.0.0     U     0      0        0 ipsec0\n10.1.0.0        -               255.255.0.0     !     2      -        0 -\ndefault         *               0.0.0.0         U     3      0        0 ppp0\ndefault         *               0.0.0.0         U     4      0        0 ppp0\n\nI have two entries for 192.168.46.79 and 10.1.0.0.  These are auto-generated by the little Linux based router I'm using.  I can ping the IPSEC tunnels from the shell itself, but traffic from the LAN takes the second route (the rejected \"!\" or \"!H\" route) for reasons I simply don't understand.",
        "top_answer": "with the route -n command you'll obtain\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         192.168.178.1   0.0.0.0         UG    0      0        0 eth0\n0.0.0.0         160.98.123.1    0.0.0.0         UG    600    0        0 wlan0\n\nsudo route del -net 0.0.0.0 gw 192.168.178.1 netmask 0.0.0.0 dev eth0\nyou'll get all parameters respectively from above",
        "url": "https://serverfault.com/questions/181094/how-do-i-delete-a-route-from-linux-routing-table"
    },
    {
        "title": "How to keep the full path with rsync?",
        "question": "When backing up with rsync, How do I keep the full directory structure?\nFor example, the remote server is saturn, and I want to backup saturn's /home/udi/files/pictures to a local directory named backup.\nI want to have (locally) backup/home/udi/files/pictures rather than backup/pictures.",
        "top_answer": "Use the -R or --relative option to preserve the full path.",
        "url": "https://serverfault.com/questions/39522/how-to-keep-the-full-path-with-rsync"
    },
    {
        "title": "Straight forward way to run ssh-agent and ssh-add on login via SSH?",
        "question": "I'm trying to have the following commands be auto-executed when I login to my server via ssh:\nssh-agent /bin/bash\nssh-add ~/.ssh/id_rsa\n\nMy ssh key has a passphrase and I'm fine with entering it once per login.\nI tried putting this in my .bashrc file, however I believe that ssh-agent starts a new bash session. When I try to login after having this in my .bashrc, it gets stuck, and I have to type 'exit' to then see the 'enter passphrace to unlock key' prompt\nAny other suggestions?\nServer is running Ubuntu LTS",
        "top_answer": "You can try adding this:\neval $(ssh-agent -s)\nssh-add ~/.ssh/id_rsa\n\nThis way the ssh-agent does not start a new shell, it just launches itself in the background and spits out the shell commands to set the appropriate environment variables.\nAs said in the comment, maybe you do not want to run the agent at all on the remote host, but rather on the box you are working from, and use\nssh -A remote-host\n\nto forward the services of your local ssh agent to the remote-host.\nFor security reasons you should only use agent forwarding with hosts run by trustworthy people, but it is better than running a complete agent remotely any time. ",
        "url": "https://serverfault.com/questions/672346/straight-forward-way-to-run-ssh-agent-and-ssh-add-on-login-via-ssh"
    },
    {
        "title": "How to test if my server is vulnerable to the ShellShock bug?",
        "question": "How can I ensure my Bash installation is not vulnerable to the ShellShock bug anymore after the updates?",
        "top_answer": "To check for the CVE-2014-6271 vulnerability\nenv x='() { :;}; echo vulnerable' bash -c \"echo this is a test\"\n\nit should NOT echo back the word vulnerable.\n\n\n\n/tmp/echo\ncd /tmp; env X='() { (a)=>\\' bash -c \"echo date\"; cat echo\n\nit should say the word date then complain with a message like cat: echo: No such file or directory. If instead it tells you what the current datetime is then your system is vulnerable.\n\n\nbash -c 'true <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF <<EOF' || echo \"CVE-2014-7186 vulnerable, redir_stack\"\n\nit should NOT echo back the text CVE-2014-7186 vulnerable, redir_stack.\n\n\n(for x in {1..200} ; do echo \"for x$x in ; do :\"; done; for x in {1..200} ; do echo done ; done) | bash || echo \"CVE-2014-7187 vulnerable, word_lineno\"\n\nit should NOT echo back the text CVE-2014-7187 vulnerable, word_lineno.\n\n\nenv HTTP_COOKIE=\"() { x() { _; }; x() { _; } <<`perl -e '{print \"A\"x1000}'`; }\" bash -c \"echo testing CVE-2014-6277\"\n\nA pass result on this one is it ONLY echoing back the text testing CVE-2014-6277. If it runs perl or if it complains that perl is not installed that is definitely a fail. I'm not sure on any other failure characteristics as I no longer have any unpatched systems.\n\n\nenv HTTP_COOKIE='() { _; } >_[$($())] { echo hi mom; id; }' bash -c \"echo testing CVE-2014-6278\"\n\nA pass for this test is that it should ONLY echo back the text testing CVE-2014-6278. If yours echoes back hi mom anywhere that is definitely a fail.",
        "url": "https://serverfault.com/questions/631257/how-to-test-if-my-server-is-vulnerable-to-the-shellshock-bug"
    },
    {
        "title": "How to redirect domain A to domain B using A-Records and CNAME records only",
        "question": "I have 2 domains hosted with different hosts.  I need to redirect Domain A to Domain B.  Unfortunately I can't do a 301 redirect from Host A, but can only modify/add DNS entries (A-Records and CNAMEs) at Host A.\nSurely it is possible to redirect www.DomainA.com to www.DomainB.com using only A-records and CNAMEs?\nAt present, the DNS entries are:\nDomainA.com.    3600    IN    SOA       ns1.HostA.net.\nwww             3600    IN    CNAME     www.DomainB.com.    \nDomainA.com.    3600    IN    NS        ns1.HostA.net.  \nDomainA.com.    3600    IN    NS        ns2.HostA.net.  \nDomainA.com.    3600    IN    NS        ns3.HostA.net.\n\nI want to redirect \nDomainA.com -> DomainB.com\n*.DomainA.com -> *.DomainB.com\n\nI've tried the suggestion from this other post but it didn't work.\nHow can I achieve this only with A-Records and CNAMEs please?  Thank you for your advice.\nPrembo.",
        "top_answer": "So you are not looking at redirection as such (as that happens at the app level, i.e. on Apache/Nginx/wherever) but rather on the DNS resolution. The host on which DomainA is hosted will or should never be hit, based on your description as you want the DNS requests to be resolved to the IPs of DomainB. Unless I'm missing something in your request?\nAs Shane pointed out DNS is not capable of HTTP redirection - that's an application/webserver duty. You could make DomainA and DomainB resolve to the same IP on DNS and all would work. But if you're looking to do this on per URL/per-path way then this is not possible - DNS is not capable of that - it's a simple DNS->IP service, what's happening with the actual URL is the webserver's task.\nAfter the comment below, what I'd do is to refer all DNS records for DomainA to the same IP(s) as DomainB is pointed to - this way you will get HTTP request hitting hostB and then it's just a simple matter of:\n\ncreating a particular Apache Name Baseed Virtual host - which will be serving files from its own DocumentRoot\ncreating permanent redirect on Apache like this:\n\nThis will rewrite anything coming to DomainB to DomainA which can be hosted on the same server or somewhere else. I appreciate that the second option is probably an overhead and not necessary if you can/are allowed to create Name Based Virtual hosts on apache.\n<VirtualHost *:80>\n  ServerName DomainB\n  Redirect permanent / http://DomainA/\n</VirtualHost>\n\nI'd go with 1. - point all DNS records of DomainA to the same IP(s) as DomainB is pointing and create particular Name Based VirtualHosts on Apache.",
        "url": "https://serverfault.com/questions/385893/how-to-redirect-domain-a-to-domain-b-using-a-records-and-cname-records-only"
    },
    {
        "title": "Multiple data centers and HTTP traffic: DNS Round Robin is the ONLY way to assure instant fail-over?",
        "question": "Multiple A records pointing to the same domain seem to be used almost exclusively to implement DNS Round Robin as a cheap load balancing technique.\nThe usual warning against DNS RR is that it is not good for high availability. When 1 IP goes down clients will continue to use it for minutes. \nA load balancer is often suggested as a better choice.\nBoth claims are not completely true:\n\nWhen the traffic is HTTP then, most of the HTML browsers are able to automatically try the next A record if the previous is down, without a new DNS look-up. Read here chapter 3.1 and here.\nWhen multiple data centers are involved then, DNS RR is the only option to distribute traffic across them.\n\nSo, is it true that, with multiple data centers and HTTP traffic, the use of DNS RR is the ONLY way to assure instant fail-over when one data center goes down?\nThanks,\nValentino\nEdit:\n\nOff course each data center has a local Load Balancer with hot spare.\nIt's OK to sacrifice session affinity for an instant fail-over. \nAFAIK the only way for a DNS to suggest a data center instead of another is to reply with just the IP (or IPs) associated to that data center. If the data center becomes unreachable then all those IP are also unreachables. This means that, even if smart HTML browsers are able to instantly try another A record , all the attempts will fail until the local cache entry expires and a new DNS lookup is done, fetching the new working IPs (I assume DNS automatically suggests to a new data center when one fail). So, \"smart DNS\" cannot assure instant fail-over.\nConversely a DNS round-robin permits it. When one data center fail, the smart HTML browsers (most of them) instantly try the other cached A records jumping to another (working) data center. So, DNS round-robin doesn't assure session affinity or the lowest RTT but seems to be the only way to assure instant fail-over when the clients are \"smart\" HTML browsers.\n\nEdit 2:\n\nSome people suggest TCP Anycast as a definitive solution. In this paper (chapter 6) is explained that Anycast fail-over is related to BGP convergence. For this reason Anycast can employ from 15 minutes to 20 seconds to complete.\n20 seconds are possible on networks where the topology was optimized for this.\nProbably just CDN operators can grant such fast fail-overs.\n\nEdit 3:*\n\nI did some DNS look-ups and traceroutes (maybe some expert can double check) and:\n\n\nThe only CDN using TCP Anycast seems to be CacheFly, other operators like CDN networks and BitGravity use CacheFly. Seems that their edges cannot be used as reverse proxies. Therefore, they cannot be used to grant instant failover.\nAkamai and LimeLight seems to use geo-aware DNS. But! They return multiple A records.\nFrom traceroutes seems that the returned IPs are on the same data center. So, I'm puzzled on how they can offer a 100% SLA when one data center goes down.\n\n",
        "top_answer": "When I use the term \"DNS Round Robin\" I generally mean in in the sense of the \"cheap load balancing technique\" as OP describes it.\nBut that's not the only way DNS can be used for global high availability. Most of the time, it's just hard for people with different (technology) backgrounds to communicate well.\nThe best load balancing technique (if money is not a problem) is generally considered to be:\n\nA Anycast'ed global network of 'intelligent' DNS servers,\nand a set of globally spread out datacenters,\nwhere each DNS node implements Split Horizon DNS,\nand monitoring of availability and traffic flows are available to the 'intelligent' DNS nodes in some fashion,\nso that the user DNS request flows to the nearest DNS server via IP Anycast,\nand this DNS server hands out a low-TTL A Record / set of A Records for the nearest / best datacenter for this end user via 'intelligent' split horizon DNS.\n\nUsing anycast for DNS is generally fine, because DNS responses are stateless and almost extremely short. So if the BGP routes change it's highly unlikely to interrupt a DNS query.\nAnycast is less suited for the longer and stateful HTTP conversations, thus this system uses split horizon DNS. A HTTP session between a client and server is kept to one datacenter; it generally cannot fail over to another datacenter without breaking the session.\nAs I indicated with \"set of A Records\" what I would call 'DNS Round Robin' can be used together with the setup above. It is typically used to spread the traffic load over multiple highly available load balancers in each datacenter (so that you can get better redundancy, use smaller/cheaper load balancers, not overwhelm the Unix network buffers of a single host server, etc).\n\nSo, is it true that, with multiple data centers\n  and HTTP traffic, the use of DNS RR is the ONLY\n  way to assure high availability?\n\nNo it's not true, not if by 'DNS Round Robin' we simply mean handing out multiple A records for a domain. But it's true that clever use of DNS is a critical component in any global high availability system. The above illustrates one common (often best) way to go.\nEdit: The Google paper \"Moving Beyond End-to-End Path Information to Optimize CDN Performance\" seems to me to be state-of-the-art in global load distribution for best end-user performance.\nEdit 2: I read the article \"Why DNS Based .. GSLB ..  Doesn't Work\" that OP linked to, and it is a good overview -- I recommend looking at it. Read it from the top.\nIn the section \"The solution to the browser caching issue\" it advocates DNS responses with multiple A Records pointing to multiple datacenters as the only possible solution for instantaneous fail over.\nIn the section \"Watering it down\" near the bottom, it expands on the obvious, that sending multiple A Records is uncool if they point to datacenters on multiple continents, because the client will connect at random and thus quite often get a 'slow' DC on another continent. Thus for this to work really well, multiple datacenters on each continent are needed.\nThis is a different solution than my steps 1 - 6. I can't provide a perfect answer on this, I think a DNS specialist from the likes of Akamai or Google is needed, because much of this boils down to practical know-how on the limitations of deployed DNS caches and browsers today. AFAIK, my steps 1-6 are what Akamai does with their DNS (can anyone confirm this?).\nMy feeling -- coming from having worked as a PM on mobile browser portals (cell phones) -- is that the diversity and level of total brokeness of the browsers out there is incredible. I personally would not trust a HA solution that requires the end user terminal to 'do the right thing'; thus I believe that global instantaneous fail over without breaking a session isn't feasible today.\nI think my steps 1-6 above are the best that are available with commodity technology. This solution does not have instantaneous fail over.\nI'd love for one of those DNS specialists from Akamai, Google etc to come around and prove me wrong. :-)",
        "url": "https://serverfault.com/questions/69870/multiple-data-centers-and-http-traffic-dns-round-robin-is-the-only-way-to-assur"
    },
    {
        "title": "In Linux, what is the difference between \"buffers\" and \"cache\" reported by the free command?",
        "question": "This is an old question that I've seen from time to time.  My understanding of it is rather limited (having read about the differences a long time ago, but the factoid(s) involved never really stuck).\nAs I understand it,\n\nBuffers\nAre used by programs with active I/O operations, i.e. data waiting to be written to disk\nCache\nIs the result of completed I/O operations, i.e. buffers that have been flushed or data read from disk to satisfy a request.\n\nCan I get a clear explanation for posterity?",
        "top_answer": "The \"cached\" total will also include some other memory allocations, such as any tmpfs filesytems. To see this in effect try:\nmkdir t\nmount -t tmpfs none t\ndd if=/dev/zero of=t/zero.file bs=10240 count=10240\nsync; echo 3 > /proc/sys/vm/drop_caches; free -m\numount t\nsync; echo 3 > /proc/sys/vm/drop_caches; free -m\n\nand you will see the \"cache\" value drop by the 100Mb that you copied to the ram-based filesystem (assuming there was enough free RAM, you might find some of it ended up in swap if the machine is already over-committed in terms of memory use). The \"sync; echo 3 > /proc/sys/vm/drop_caches\" before each call to free should write anything pending in all write buffers (the sync) and clear all cached/buffered disk blocks from memory so free will only be reading other allocations in the \"cached\" value.\nThe RAM used by virtual machines (such as those running under VMWare) may also be counted in free's \"cached\" value, as will RAM used by currently open memory-mapped files (this will vary depending on the hypervisor/version you are using and possibly between kernel versions too).\nSo it isn't as simple as \"buffers counts pending file/network writes and cached counts recently read/written blocks held in RAM to save future physical reads\", though for most purposes this simpler description will do.",
        "url": "https://serverfault.com/questions/23433/in-linux-what-is-the-difference-between-buffers-and-cache-reported-by-the-f"
    },
    {
        "title": "What is the correct way to open a range of ports in iptables",
        "question": "I have come across articles advising for the following:\niptables -A INPUT -p tcp 1000:2000 -j ACCEPT\n\nAnd others stating that the above will not work and iptables only supports multiple port declarations with the --multiport option.\nIs there a correct way to open many ports with iptables?",
        "top_answer": "This is the correct way:\niptables -A INPUT -p tcp --match multiport --dports 1024:3000 -j ACCEPT\nAs an example. Source here.",
        "url": "https://serverfault.com/questions/594835/what-is-the-correct-way-to-open-a-range-of-ports-in-iptables"
    },
    {
        "title": "How to check for modified config files on a Debian system?",
        "question": "How to find all Debian managed configuration files which have been changed from the default?",
        "top_answer": "To find all Debian managed configuration files which have been changed from the default you can use a command like this.\ndpkg-query -W -f='${Conffiles}\\n' '*' | awk 'OFS=\"  \"{print $2,$1}' | md5sum -c 2>/dev/null | awk -F': ' '$2 !~ /OK/{print $1}'\n\nEdit (works with localized systems):\ndpkg-query -W -f='${Conffiles}\\n' '*' | awk 'OFS=\"  \"{print $2,$1}' | LANG=C md5sum -c 2>/dev/null | awk -F': ' '$2 !~ /OK/{print $1}' | sort | less\n\nEdit (works with packages with OK in the filename):\ndpkg-query -W -f='${Conffiles}\\n' '*' | awk 'OFS=\"  \"{print $2,$1}' | LANG=C md5sum -c 2>/dev/null | awk -F': ' '$2 !~ /OK$/{print $1}' | sort | less\n\nEdit (works independently of localization or filename)\ndpkg-query -W -f='${Conffiles}\\n' '*' | awk 'OFS=\"  \"{print $2,$1}' | md5sum --quiet -c 2>/dev/null | cut -d : -f 1\n",
        "url": "https://serverfault.com/questions/90400/how-to-check-for-modified-config-files-on-a-debian-system"
    },
    {
        "title": "Docker containers can't resolve DNS on Ubuntu 14.04 Desktop Host",
        "question": "I'm running into a problem with my Docker containers on Ubuntu 14.04 LTS.\nDocker worked fine for two days, and then suddenly I lost all network connectivity inside my containers. The error output below initially lead me to believe it was because apt-get is trying to resolve the DNS via IPv6.  \nI disabled IPv6 on my host machine and still, removed all images, pulled base ubuntu, and still ran into the problem.\nI changed my /etc/resolve.conf nameservers from my local DNS server to Google's public DNS servers (8.8.8.8 and 8.8.4.4) and still have no luck. I also set the DNS to Google in the DOCKER_OPTS of /etc/default/docker and restarted docker. \nI also tried pulling coreos, and yum could not resolve DNS either.\nIt's weird because while DNS does not work, I still get a response when I ping the same update servers that apt-get can't resolve.  \nI'm not behind a proxy, I'm on a very standard local network, and this version of Ubuntu is up to date and fresh (I installed two days ago to be closer to docker).   \nI've thoroughly researched this through other posts on stackoverflow and github issues, but haven't found any resolution. I'm out of ideas as to how to solve this problem, can anyone help?\nError Message\n\u279c  arthouse git:(docker) \u2717 docker build --no-cache .\nSending build context to Docker daemon 51.03 MB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu:14.04\n ---> 5506de2b643b\nStep 1 : RUN apt-get update\n ---> Running in 845ae6abd1e0\nErr http://archive.ubuntu.com trusty InRelease\nErr http://archive.ubuntu.com trusty-updates InRelease\nErr http://archive.ubuntu.com trusty-security InRelease   \nErr http://archive.ubuntu.com trusty-proposed InRelease  \nErr http://archive.ubuntu.com trusty Release.gpg\n  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nErr http://archive.ubuntu.com trusty-updates Release.gpg\n  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nErr http://archive.ubuntu.com trusty-security Release.gpg\n  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nErr http://archive.ubuntu.com trusty-proposed Release.gpg\n  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nReading package lists...\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty/InRelease  \nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty-updates/InRelease  \nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty-security/InRelease  \nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty-proposed/InRelease  \nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty/Release.gpg  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty-updates/Release.gpg  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty-security/Release.gpg  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty-proposed/Release.gpg  Cannot initiate the connection to archive.ubuntu.com:80 (2001:67c:1360:8c01::19). - connect (101: Network is unreachable) [IP: 2001:67c:1360:8c01::19 80]\nW: Some index files failed to download. They have been ignored, or old ones used instead.\n\nContainer IFCONFIG/PING\n\u279c  code  docker run -it ubuntu /bin/bash\nroot@7bc182bf87bb:/# ifconfig\neth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:04  \n          inet addr:172.17.0.4  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe11:4/64 Scope:Link\n          UP BROADCAST RUNNING  MTU:1500  Metric:1\n          RX packets:7 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:738 (738.0 B)  TX bytes:648 (648.0 B)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n\nroot@7bc182bf87bb:/# ping google.com\nPING google.com (74.125.226.0) 56(84) bytes of data.\n64 bytes from lga15s42-in-f0.1e100.net (74.125.226.0): icmp_seq=1 ttl=56 time=12.3 ms\n--- google.com ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 12.367/12.367/12.367/0.000 ms\nroot@7bc182bf87bb:/# ping 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=44 time=21.8 ms\n64 bytes from 8.8.8.8: icmp_seq=2 ttl=44 time=21.7 ms\n64 bytes from 8.8.8.8: icmp_seq=3 ttl=44 time=21.7 ms\n\nAlso, apt-get update fails when I force IPv4:\nroot@6d925cdf84ad:/# sudo apt-get update -o Acquire::ForceIPv4=true\nErr http://archive.ubuntu.com trusty InRelease\n\nErr http://archive.ubuntu.com trusty-updates InRelease\n\nErr http://archive.ubuntu.com trusty-security InRelease\n\nErr http://archive.ubuntu.com trusty-proposed InRelease\n\nErr http://archive.ubuntu.com trusty Release.gpg\n  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.153 80]\nErr http://archive.ubuntu.com trusty-updates Release.gpg\n  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.153 80]\nErr http://archive.ubuntu.com trusty-security Release.gpg\n  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.153 80]\nErr http://archive.ubuntu.com trusty-proposed Release.gpg\n  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.153 80]\nReading package lists... Done\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty/InRelease  \n",
        "top_answer": "Woo, I found a post on github that solved my problem.\nAfter Steve K. pointed out that it wasn't actually a DNS issue and was a connectivity issue, I was able to find a post on github that described how to fix this problem.\nApparently the docker0 network bridge was hung up. Installing bridge-utils and running the following got my Docker in working order:\napt-get install bridge-utils\npkill docker\niptables -t nat -F\nifconfig docker0 down\nbrctl delbr docker0\nservice docker restart\n",
        "url": "https://serverfault.com/questions/642981/docker-containers-cant-resolve-dns-on-ubuntu-14-04-desktop-host"
    },
    {
        "title": "What is the difference between SAN, NAS and DAS? ",
        "question": "What is the difference between SAN, NAS and DAS? ",
        "top_answer": "First it is best to define the difference between a block device and filesystem. This is easier grasped if you are familiar with UNIX because it makes an objective distinction between the two things. Still the same applies to Windows.\n\nA block device is a handle to the raw disk.\n\n\nSuch as /dev/sda for a disk or /dev/sda1 for a partition on that disk.\n\nA filesystem is layered on top of the block device in order to store data. You can then mount this.\n\n\nSuch as mount /dev/sda1 /mnt/somepath.\n\n\nWith those terms in mind it's then easier to see the distinction between the following.\n\nDAS is a block device from a disk which is physically [directly] attached to the host machine.\n\n\nYou must place a filesystem upon it before it can be used.  \nTechnologies to do this include IDE, SCSI, SATA, etc.\n\nSAN is a block device which is delivered over the network.\n\n\nLike DAS you must still place a filesystem upon it before it can used.\nTechnologies to do this include FibreChannel, iSCSI, FoE, etc.\n\nNAS is a filesystem delivered over the network.  \n\n\nIt is ready to mount and use.\nTechnologies to do this include NFS, CIFS, AFS, etc.\n\n\n",
        "url": "https://serverfault.com/questions/81723/what-is-the-difference-between-san-nas-and-das"
    },
    {
        "title": "How can I check in bash if a shell is running in interactive mode?",
        "question": "How can I tell (in ~/.bashrc) if I'm running in interactive mode, or, say, executing a command over ssh. I want to avoid printing of ANSI escape sequences in .bashrc if it's the latter.",
        "top_answer": "According to man bash:\n\nPS1 is set and $- includes i if bash is interactive,  allowing\n         a shell script or a startup file to test this state.\n\nSo you can use:\nif [[ $- == *i* ]]\nthen\n    do_interactive_stuff\nfi\n\nAlso:\n\nWhen  an  interactive  shell that is not a login shell is started, bash\n         reads and executes commands from  /etc/bash.bashrc  and  ~/.bashrc,  if\n         these  files  exist.\n\nSo ~/.bashrc is only sourced for interactive shells. Sometimes, people source it from ~/.bash_profile or ~/.profile which is incorrect since it interferes with the expected behavior. If you want to simplify maintenance of code that is common, you should use a separate file to contain the common code and source it independently from both rc files.\nIt's best if there's no output to stdout from login rc files such as ~/.bash_profile or ~/.profile since it can interfere with the proper operation of rsync for example.\nIn any case, it's still a good idea to test for interactivity since incorrect configuration may exist.",
        "url": "https://serverfault.com/questions/146745/how-can-i-check-in-bash-if-a-shell-is-running-in-interactive-mode"
    },
    {
        "title": "How to properly set permissions for NFS folder? Permission denied on mounting end.",
        "question": "I'm trying to connect to an NFS folder on my dev server. The owner of the folder on the dev server is darren and group darren.\nWhen I export and mount it to my Mac using the Disk Utility it mounts, but then when I try to open the folder is says I do not have permissions. I have set rw, sync, and no_subtree_check. The user on the Mac is darren with a bunch of groups.\nDo I need to have the same group and user set to access the folder?",
        "top_answer": "NFS is built on top of RPC authentication. With NFS version 3, the most common authentication mechanism is AUTH_UNIX. The user id and group id of the client system are sent in each RPC call, and the permissions these IDs have on the file being accessed are checked on the server. For this to work, the UID and GIDs must be the same on the server and the clients. However, you can force all access to occur as a single user and group by combining the all_squash, anonuid, and anongid export options. all_squash will map all UIDs and GIDs to the anonymous user, and anonuid and anongid  set the UID and GID of the anonymous user. For example, if your UID and GID on your dev server are both 1001, you could export your home directory with a line like\n/home/darren       192.168.1.1/24(rw,all_squash,anonuid=1001,anongid=1001)\nI'm less familiar with NFS version 4, but I think you can set up rpc.idmapd on the clients to alter the uid and gid they send to the server.",
        "url": "https://serverfault.com/questions/240897/how-to-properly-set-permissions-for-nfs-folder-permission-denied-on-mounting-en"
    },
    {
        "title": "What does -qq argument for apt-get mean?",
        "question": "I just received a Vagrantfile and post install bash script. The vagrantfile downloads standard Ubuntu from Ubuntu Cloud but I found something in the bash script.\nFew lines of script reads as:\napt-get update -qq > /dev/null\napt-get -qq -y install apache2 > /dev/null\n\nI tried to search around the internet what -qq in shell script stands for, didn't get any mention of it, so, am asking here if anyone knows what it stands for.\nAFAIK > /dev/null means the ongoing process is not printed in the screen, for that it doesn't require the -qq flag. So, I am really curious to know.",
        "top_answer": "The -qq is a flag to apt-get to make it less noisy.\n-qq No output except for errors\n\nYou are correct about the >/dev/null. By redirecting all the STDOUT, the -qq becomes redundant.",
        "url": "https://serverfault.com/questions/644180/what-does-qq-argument-for-apt-get-mean"
    },
    {
        "title": "Which OS is running in my Docker container?",
        "question": "Most of the time, using one of these two, I can tell which OS is running in my Docker container (alpine, centOS, etc)\nBut this time, I can't tell:\nbash-4.2$ uname -a       \nLinux 6fe5c6d1451c 2.6.32-504.23.4.el6.x86_64 #1 SMP Tue Jun 9 20:57:37 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n\nbash-4.2$ more /etc/issue\n\\S\nKernel \\r on an \\m\n\nAny way to get a text version of the OS it is running ?",
        "top_answer": "I like to use Screenfetch. You might want to try that.\nIf you look into the code you can see how it determines the distribution:\n\nlsb_release -sirc\ncat /etc/os-release\n\nAnd to cover CentOS too:\n\ncat /etc/issue\n",
        "url": "https://serverfault.com/questions/805389/which-os-is-running-in-my-docker-container"
    },
    {
        "title": "Is Postfix the same thing as Sendmail?",
        "question": "I have Postfix setup on my server so that I can send outgoing mail using the command-line:\nmail -s \"Subject\" [email\u00a0protected]\n\n\nIs this using Sendmail or Postfix ? \nIs \"Sendmail\" just a software category or a distinct program ?\nIf something is \"Sendmail-ready\" does that mean it will work with Postfix ?\n\nEverything I've read online seems to use these two terms interchangeably.",
        "top_answer": "Sendmail is a different (and much older) program from Postfix. However for every mail server to succeed in the Unix environment, a sendmail binary (with some of the expected command line options) must be provided.\nEDIT: See for example the manual page for the sendmail program provided by Postfix ",
        "url": "https://serverfault.com/questions/244263/is-postfix-the-same-thing-as-sendmail"
    },
    {
        "title": "What is the difference between a Source NAT, Destination NAT and Masquerading?",
        "question": "What is the difference between a Source NAT, Destination NAT and Masquerading?\nFor example, I thought IP Masqurading was what they used to call it in Linux?\nBut what confuses me is that in our Astaro firewall there is IP Masquarading as well as NAT options.   What's the difference between all these?",
        "top_answer": "Source NAT changes the source address in IP header of a packet.  It may also change the source port in the TCP/UDP headers.  The typical usage is to change the a private (rfc1918) address/port into a public address/port for packets leaving your network.\nDestination NAT changes the destination address in IP header of a packet.  It may also change the destination port in the TCP/UDP headers.The typical usage of this is to redirect incoming packets with a destination of a public address/port to a private IP address/port inside your network.\nMasquerading is a special form of Source NAT where the source address is unknown at the time the rule is added to the tables in the kernel.  If you want to allow hosts with private address behind your firewall to access the Internet and the external address is variable (DHCP) this is what you need to use.  Masquerading will modify the source IP address and port of the packet to be the primary IP address assigned to the outgoing interface.  If your outgoing interface has a address that is static, then you don't need to use MASQ and can use SNAT which will be a little faster since it doesn't need to figure out what the external IP is every time.",
        "url": "https://serverfault.com/questions/119365/what-is-the-difference-between-a-source-nat-destination-nat-and-masquerading"
    },
    {
        "title": "What is the difference between a hostname and a fully qualified domain name?",
        "question": "I am new to the world of setting up servers and am baffled by the term hostname and fully qualified domain name (FQDN).\nFor example, if I want to set up a server that hosts files on the local network i.e. a file server, what would I use a hostname such as myfileserver or something else?\nWhat if I wanted to set up a web server, mail server, etc. that external users could access? ",
        "top_answer": "Your hostname is the name of your computer.\nYour fully qualified domain name is your hostname plus the domain your company uses often ending in .local.\nSo if the name of your computer is bob, and your company's domain is contoso.local, your computer's fully qualified domain name (FQDN) is bob.contoso.local.:\n\nHostname: bob\nDomain: contoso.com\nFQDN: bob.contoso.com.\n\nIn the case of a domain like contoso.local I did not use an \"external\" internet domain name.  This name doesn't have to be the only way that you address the server.\nIf you make it available by its IP address you can use DNS or that IP address to allow external users to access it.\nThe dot at the end of the FQDN is used to indicate the empty top-level domain.\nSome more information on DNS:\n\nhttp://www.howstuffworks.com/dns.htm\nhttp://en.wikipedia.org/wiki/.local\nhttps://en.wikipedia.org/wiki/Fully_qualified_domain_name\n\nEdit: Thanks for the comment on .local domains RobM",
        "url": "https://serverfault.com/questions/269838/what-is-the-difference-between-a-hostname-and-a-fully-qualified-domain-name"
    },
    {
        "title": "What is the difference between PV and HVM virtualization types in ec2?",
        "question": "AWS EC2 offers two types of virtualization of Ubuntu Linux EC2 machines - PV and HVM.\nPV:\n\nHVM:\n\nWhat is the difference between these types?",
        "top_answer": "Amazon run on Xen, which provides Para-virtualization (PV) or Hardware-assisted virtualization (HVM).\nPara-virtualization used to be the recommended choice, as it gave you better performance (with a much closer integration to the virtualization host, through patched specialized kernels/drivers on both the host and the guest).\nHardware-assisted virtualization uses the benefits provided in modern hardware, and it doesn't require any kind of custom kernel or patches. Recent benchmarks has proven that HVM is actually faster on certain workloads.",
        "url": "https://serverfault.com/questions/637102/what-is-the-difference-between-pv-and-hvm-virtualization-types-in-ec2"
    },
    {
        "title": "Can't find .so in the same directory as the executable?",
        "question": "I have an executable which needs to link with libtest.so dynamically,so I put them in the same directory,then :\ncd path_to_dir\n./binary\n\nBut got this:\nerror while loading shared libraries: libtest.so: cannot open shared object file: No such file or directory\n\nHow can it be unable to find libtest.so which is already in the same directory as the executable itself?",
        "top_answer": "The loader never checks the current directory for shared objects unless it is explicitly directed to via $LD_LIBRARY_PATH. See the ld.so(8) man page for more details.",
        "url": "https://serverfault.com/questions/279068/cant-find-so-in-the-same-directory-as-the-executable"
    },
    {
        "title": "Why Block Port 22 Outbound?",
        "question": "I'm a programmer, and I have worked for a few clients whose networks block outgoing connections on port 22.  Considering that programmers often need to use port 22 for ssh, this seems like a counterproductive procedure.  At best, it forces the programmers to bill the company for 3G Internet.  At worst, it means they can't do their jobs effectively.\nGiven the difficulties this creates, could an experienced sysadmin please explain the desired benefit to what seems like a lose-lose action?",
        "top_answer": "I don't see that anyone has spelled out the specific risk with SSH port forwarding in detail.\nIf you are inside a firewall and have outbound SSH access to a machine on the public internet, you can SSH to that public system and in the process create a tunnel so that people on the public internet can ssh to a system inside your network, completely bypassing the firewall.\nIf fred is your desktop and barney is an important server at your company and wilma is public, running (on fred):\nssh -R*:9000:barney:22 wilma\nand logging in will let an attacker ssh to port 9000 on wilma and talk to barney's SSH daemon.\nYour firewall never sees it as an incoming connection because the data is being passed through a connection that was originally established in the outgoing direction.\nIt's annoying, but a completely legitimate network security policy.",
        "url": "https://serverfault.com/questions/25545/why-block-port-22-outbound"
    },
    {
        "title": "How to remove strict RSA key checking in SSH and what's the problem here?",
        "question": "I have a Linux server that whenever I connect it shows me the message that changed the SSH host key:\n\n$ ssh root@host1\n  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n  @    WARNING: REMOTE HOST\n  IDENTIFICATION HAS CHANGED!     @\n  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n  IT IS POSSIBLE THAT SOMEONE IS DOING\n  SOMETHING NASTY! Someone could be\n  eavesdropping on you right now\n  (man-in-the-middle attack)! It is also\n  possible that the RSA host key has\n  just been changed. The fingerprint for\n  the RSA key sent by the remote host is\n  93:a2:1b:1c:5f:3e:68:47:bf:79:56:52:f0:ec:03:6b.\n  Please contact your system\n  administrator. Add correct host key in\n  /home/emerson/.ssh/known_hosts to get\n  rid of this message. Offending key in\n  /home/emerson/.ssh/known_hosts:377\nRSA host key for host1 has changed and\n  you have requested strict checking.\n  Host key verification failed.\n\nIt keeps me for a very few seconds logged in and then it closes the connection.\n\nhost1:~/.ssh # Read from remote host host1: Connection reset by peer\n  Connection to host1 closed.\n\nDoes anyone know what's happening and what I could do to solve this problem?",
        "top_answer": "Please don't delete the entire known_hosts file as recommended by some people, this totally voids the point of the warning.  It's a security feature to warn you that a man in the middle attack may have happened.\nI suggest you identify why it thinks something has changed, most likely an SSH upgrade altered the encryption keys due to a possible security hole.  You can then purge that specific line from your known_hosts file:\nsed -i 377d ~/.ssh/known_hosts\n\nThis deletes line 377 as shown after the colon in the warning:\n\n/home/emerson/.ssh/known_hosts:377\n\nAlternatively you can remove the relevant key by doing the following\nssh-keygen -R 127.0.0.1 (obviously replace with the server's IP)\n\nPlease DO NOT purge the entire file and ensure this is actually the machine you want to be connecting to prior to purging the specific key.",
        "url": "https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here"
    },
    {
        "title": "How do I join two named pipes into single input stream in linux",
        "question": "Using the pipes (|) feature in Linux I can forward chain the standard input to one or several output streams.  \nI can use tee to split the output to separate sub processes.\nIs there a command to join two input streams?\nHow would I go about this?  How does diff work?",
        "top_answer": "Personally, my favorite (requires bash and other things that are standard on most Linux distributions)\nThe details can depend a lot on what the two things output and how you want to merge them ...\nContents of command1 and command2 after each other in the output:\ncat <(command1) <(command2) > outputfile\n\nOr if both commands output alternate versions of the same data that you want to see side-by side (I've used this with snmpwalk; numbers on one side and MIB names on the other):\npaste <(command1) <(command2) > outputfile\n\nOr if you want to compare the output of two similar commands (say a find on two different directories)\ndiff <(command1) <(command2) > outputfile\n\nOr if they're ordered outputs of some sort, merge them:\nsort -m <(command1) <(command2) > outputfile\n\nOr run both commands at once (could scramble things a bit, though):\ncat <(command1 & command2) > outputfile\n\nThe <() operator sets up a named pipe (or /dev/fd) for each command, piping the output of that command into the named pipe (or /dev/fd filehandle reference) and passes the name on the commandline.  There's an equivalent with >().  You could do: command0 | tee >(command1) >(command2) >(command3) | command4 to simultaneously send the output of one command to 4 other commands, for instance.",
        "url": "https://serverfault.com/questions/171095/how-do-i-join-two-named-pipes-into-single-input-stream-in-linux"
    },
    {
        "title": "Generating a self-signed cert with openssl that works in Chrome 58",
        "question": "As of Chrome 58 it no longer accepts self-signed certs that rely on Common Name: https://productforums.google.com/forum/#!topic/chrome/zVo3M8CgKzQ;context-place=topicsearchin/chrome/category$3ACanary%7Csort:relevance%7Cspell:false\nInstead it requires using Subject Alt Name. I have been previously following this guide on how to generate a self-signed cert: https://devcenter.heroku.com/articles/ssl-certificate-self which worked great because I required the server.crt and server.key files for what I'm doing. I now need to generate new certs that include the SAN however all of my attempts to do so have not worked with Chrome 58.\nHere is what I've done:\nI followed the steps on the above mentioned Heroku article to generate the key. I then wrote a new OpenSSL config file:\n[ req ]\ndefault_bits        = 2048\ndistinguished_name  = req_distinguished_name\nreq_extensions      = san\nextensions          = san\n[ req_distinguished_name ]\ncountryName         = US\nstateOrProvinceName = Massachusetts\nlocalityName        = Boston\norganizationName    = MyCompany\n[ san ]\nsubjectAltName      = DNS:dev.mycompany.com\n\nThen generated the server.crt with the following command:\nopenssl req \\\n-new \\\n-key server.key \\\n-out server.csr \\\n-config config.cnf \\\n-sha256 \\\n-days 3650\n\nI'm on a Mac, so I opened the server.crt file with Keychain, added it to my System Certificates. I then set it to Always Trust.\nWith the exception of the config file to set the SAN value these were similar steps I used in prior versions of Chrome to generate and trust the self-signed cert.\nHowever, after this I still get the ERR_CERT_COMMON_NAME_INVALID in Chrome 58.",
        "top_answer": "My solution:\nopenssl req \\\n    -newkey rsa:2048 \\\n    -x509 \\\n    -nodes \\\n    -keyout server.key \\\n    -new \\\n    -out server.crt \\\n    -subj /CN=dev.mycompany.com \\\n    -reqexts SAN \\\n    -extensions SAN \\\n    -config <(cat /System/Library/OpenSSL/openssl.cnf \\\n        <(printf '[SAN]\\nsubjectAltName=DNS:dev.mycompany.com')) \\\n    -sha256 \\\n    -days 3650\n\nStatus: Works for me",
        "url": "https://serverfault.com/questions/845766/generating-a-self-signed-cert-with-openssl-that-works-in-chrome-58"
    },
    {
        "title": "What is the debian-sys-maint MySQL user (and more)?",
        "question": "I have been bitten several times by the 'debian-sys-maint' user that is installed by default on the mysql-server packages installed from the Ubuntu repositories.\nGenerally what happens is I pull a fresh copy of our production database (which is not running on Debian/Ubuntu) for troubleshooting or new development and forget to exclude the mysql.user table hence losing the debian-sys-maint user.  \nIf we add new mysql users for whatever reason, I have to 'merge' these into my development environment as opposed to just overlaying the table.\nWithout the user my system still seems functional, but plagued with errors such as:\nsudo /etc/init.d/mysql restart\nStopping MySQL database server: mysqld...failed.\nerror: 'Access denied for user 'debian-sys-maint'@'localhost' (using password: YES)'\n\n\nWhat is debian-sys-maint used for?\n\n\nIs there a better way for the package maintainers to do what they're trying to do?\n\nWhat is the easiest way to restore it after I've lost it?\nWhat is the correct/minimal set of privileges for this user?  \n\n\nSeems like poor idea to 'grant all privileges on *.* ...'\n\n\nEdit\nAdditional question -  Is the password in /etc/mysql/debian.cnf already hashed or is this the plaintext password?  It matters when you go to recreate the user and I never seem to get it right on the first try.\nThanks",
        "top_answer": "\nWhat is debian-sys-maint used for?\n\nOne major thing it is used for is telling the server to roll the logs.  It needs at least the reload and shutdown privilege.\nSee the file /etc/logrotate.d/mysql-server\nIt is used by the /etc/init.d/mysql script to get the status of the server.  It is used to gracefully shutdown/reload the server.\nHere is the quote from the README.Debian\n* MYSQL WON'T START OR STOP?:\n=============================\nYou may never ever delete the special mysql user \"debian-sys-maint\". This user\ntogether with the credentials in /etc/mysql/debian.cnf are used by the init\nscripts to stop the server as they would require knowledge of the mysql root\nusers password else.\n\n\n\nWhat is the easiest way to restore it after I've lost it?\n\nThe best plan is to simply not lose it.  If you really lose the password, reset it, using another account. If you have lost all admin privileges on the mysql server follow the guides to reset the root password, then repair the debian-sys-maint.\nYou could use a command like this to build a SQL file that you can use later to recreate the account.\nmysqldump --complete-insert --extended-insert=0 -u root -p mysql | grep 'debian-sys-maint' > debian_user.sql\n\n\n\nIs the password in\n  /etc/mysql/debian.cnf already hashed\n\nThe password is not hashed/encrypted when installed, but new versions of mysql now have a way to encrypt the credentials (see: https://serverfault.com/a/750363).",
        "url": "https://serverfault.com/questions/9948/what-is-the-debian-sys-maint-mysql-user-and-more"
    },
    {
        "title": "Create a directory under /var/run at boot",
        "question": "I had a daemon that needed its own dir in /var/run for its PID file with write permission granted to the daemon's user.\nI found I could create this dir with these commands:\n# mkdir /var/run/mydaemon\nThen I could change its ownership to the user/group under which I wished to run the process:\n# chown myuser:myuser /var/run/mydaemon\nBut this dir would be GONE whenever I issue a reboot!  How do I get this dir to create every time the machine boots?",
        "top_answer": "There are two alternatives to have systemd create directories under /var/run / /run.\nTypically the easiest is to declare a RuntimeDirectory in the unit file of your service. Example:\nRuntimeDirectory=foo\n\nThis will create /var/run/foo for a system unit. (Note: DO NOT provide a full path,  just the path under /var/run) For full docs please see the appropriate entry in systemd.exec docs.\n\nFor runtime directories that require more complex or different configuration or lifetime guarantees, use tmpfiles.d and\nhave your package drop a file /usr/lib/tmpfiles.d/mydaemon.conf :\n\n#Type Path            Mode UID      GID    Age Argument\nd     /run/mydaemon   0755 myuser myuser   -   -\n\n\nSee the full tmpfiles.d docs here.",
        "url": "https://serverfault.com/questions/779634/create-a-directory-under-var-run-at-boot"
    },
    {
        "title": "Is STARTTLS less safe than TLS/SSL?",
        "question": "In Thunderbird (and I assume in many other clients, too) I have the option to choose between \"SSL/TLS\" and \"STARTTLS\".\nAs far as I understand it, \"STARTTLS\" means in simple words \"encrypt if both ends support TLS, otherwise don't encrypt the transfer\". And \"SSL/TLS\" means in simple words \"always encrypt or don't connect at all\". Is this correct?\nOr in other words:\nIs STARTTLS less secure than SSL/TLS, because it can fallback to plaintext without notifying me?",
        "top_answer": "The answer, based on the STARTTLS RFC for SMTP (RFC 3207) is:\nSTARTTLS is less secure than TLS.\nInstead of doing the talking myself, I will allow the RFC to speak for itself, with the four relevant bits highlighted in BOLD:\n\nA man-in-the-middle attack can be launched by deleting the \"250\nSTARTTLS\" response from the server.  This would cause the client not\nto try to start a TLS session.  Another man-in-the-middle attack is\nto allow the server to announce its STARTTLS capability, but to alter\nthe client's request to start TLS and the server's response.  In\norder to defend against such attacks both clients and servers MUST be\nable to be configured to require successful TLS negotiation of an\nappropriate cipher suite for selected hosts before messages can be\nsuccessfully transferred.  The additional option of using TLS when\npossible SHOULD also be provided.  An implementation MAY provide the\nability to record that TLS was used in communicating with a given\npeer and generating a warning if it is not used in a later session.\n\n\nIf the TLS negotiation fails or if the client receives a 454\nresponse, the client has to decide what to do next.  There are three\nmain choices: go ahead with the rest of the SMTP session, [...]\n\nAs you can see, the RFC itself states (not very clearly, but clearly enough) that there is NOTHING requiring clients to establish a secure connection and inform users if a secure connection failed. It explicitly gives clients the option to silently establish plain-text connections.",
        "url": "https://serverfault.com/questions/523804/is-starttls-less-safe-than-tls-ssl"
    },
    {
        "title": "How do I edit git's history to correct an incorrect email address/name [closed]",
        "question": "When I started using git I just did a git init and started calling add and commit.  Now I am starting to pay attention and I can see that my commits are showing up as cowens@localmachine, rather than the address I want.  It appears as if setting GIT_AUTHOR_EMAIL and GIT_COMMITTER_EMAIL will do what I want, but I still have those old commits with the wrong email address/name.  How can I correct the old commits?",
        "top_answer": "You can go back and fix all your commits with a single call to git filter-branch. This has the same effect as rebase, but you only need to do one command to fix all your history, instead of fixing each commit individually.\nYou can fix all the wrong emails with this command:\ngit filter-branch --env-filter '\n    oldname=\"(old name)\"\n    oldemail=\"(old email)\"\n    newname=\"(new name)\"\n    newemail=\"(new email)\"\n    [ \"$GIT_AUTHOR_EMAIL\"=\"$oldemail\" ] && GIT_AUTHOR_EMAIL=\"$newemail\"\n    [ \"$GIT_COMMITTER_EMAIL\"=\"$oldemail\" ] && GIT_COMMITTER_EMAIL=\"$newemail\"\n    [ \"$GIT_AUTHOR_NAME\"=\"$oldname\" ] && GIT_AUTHOR_NAME=\"$newname\"\n    [ \"$GIT_COMMITTER_NAME\"=\"$oldname\" ] && GIT_COMMITTER_NAME=\"$newname\"\n    ' HEAD\n\nMore information is available from the git docs",
        "url": "https://serverfault.com/questions/12373/how-do-i-edit-gits-history-to-correct-an-incorrect-email-address-name"
    },
    {
        "title": "SSH key authentication using LDAP",
        "question": "In short:\nWould like a way to do SSH key authentication via LDAP.\nProblem:\nWe use LDAP (slapd) for directory services and we've recently moved to using our own AMI for building instances. The reason the AMI bit is important is that, ideally, we would like to be able to login with SSH via key authentication as soon as the instance is running and not have to wait for our somewhat slow configuration management tool to kickoff a script to add the correct keys to the instance.\nThe ideal scenario is that, when adding a user to LDAP we add their key as well and they'd be immediately be able to login.\nKey authentication is a must because password-based login is both less secure and bothersome.\nI've read this question which suggests there's a patch for OpenSSH called OpenSSH-lpk to do this but this is no longer needed with OpenSSH server >= 6.2\n\nAdded a sshd_config(5) option AuthorizedKeysCommand to\n     support fetching authorized_keys from a command in addition to (or\n     instead of) from the filesystem. The command is run under an account\n     specified by an AuthorizedKeysCommandUser sshd_config(5) option\n\nHow can I configure OpenSSH and LDAP to implement this?",
        "top_answer": "Update LDAP to include the OpenSSH-LPK schema\nWe first need to update LDAP with a schema to add the sshPublicKey attribute for users:\ndn: cn=openssh-lpk,cn=schema,cn=config\nobjectClass: olcSchemaConfig\ncn: openssh-lpk\nolcAttributeTypes: ( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey'\n    DESC 'MANDATORY: OpenSSH Public key'\n    EQUALITY octetStringMatch\n    SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 )\nolcObjectClasses: ( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' SUP top AUXILIARY\n    DESC 'MANDATORY: OpenSSH LPK objectclass'\n    MAY ( sshPublicKey $ uid )\n    )\n\nCreate a script that queries LDAP for a user's public key:\nThe script should output the public keys for that user, example:\nldapsearch '(&(objectClass=posixAccount)(uid='\"$1\"'))' 'sshPublicKey' | sed -n '/^ /{H;d};/sshPublicKey:/x;$g;s/\\n *//g;s/sshPublicKey: //gp'\nUpdate sshd_config to point to the script from the previous step\n\nAuthorizedKeysCommand /path/to/script\nAuthorizedKeysCommandUser nobody\n\nBonus: Update sshd_config to allow password authentication from internal RFC1918 networks as seen in this question:\nOnly allow password authentication to SSH server from internal network\nUseful links:\n\nhttps://github.com/AndriiGrytsenko/openssh-ldap-publickey\nPrivate key authentication with pam_ldap\n\nEDIT: Added user nobody as suggested TRS-80 ",
        "url": "https://serverfault.com/questions/653792/ssh-key-authentication-using-ldap"
    },
    {
        "title": "nginx: Log complete request / response with all headers?",
        "question": "We have an application server that sometimes hangs. We suspect it is due to a bad request from a client.\nCan nginx log the complete request/response (like fiddler captures) to files, so we can see the requests that were sent before the hang?\n(We probably need to avoid pcap and that approach and do it all in nginx)\nIf nginx is not the right tool for this, what (other than a network analyzer) might be?",
        "top_answer": "To get the request body sent by visitors, use client_body_in_file_only on; and log the \"temporary\" file it's written to in the logs by appending var $request_body_file to the log format. \"Temporary\" files will be located in client_temp directory by default.\nYou can log request headers $http_<header> too and sent headers with $sent_http_<header>. \nIf you have request body and headers you should be able to replay it and get the response your visitor had.\nAlso something like gor should highly be considered so you could replay the traffic on an other environment where you could let nginx write these temporary files without causing IO issues in production (nginx won't purge them with on value that's why It's not that \"temporary\" in this case).",
        "url": "https://serverfault.com/questions/636790/nginx-log-complete-request-response-with-all-headers"
    },
    {
        "title": "Temporarily disable ssh public key authentication from client",
        "question": "Is there a way to temporarily disable public key authentication when ssh'ing, and use password authentication instead?\nI currently want to access remote server, but I'm using another laptop, not mine.\nBrowsing that link, I found that the command ssh -o PreferredAuthentications=keyboard-interactive -o PubkeyAuthentication=no host1.example.org doesn't work everywhere. And yes, it doesn't work for me. I'm using: OpenSSH_5.9p1 Debian-5ubuntu1, OpenSSL 1.0.1 14 Mar 2012\nEdit: I also tried to type ssh -o PreferredAuthentications=password -o PubkeyAuthentication=no\n but still have \"Permission denied (publickey)\".\nSo, is there a specific configuration to do in the remote server, for that command to work? \nOr, when that command will work as expected?\nThanks a lot for advices.",
        "top_answer": "This sounds like a configuration issue on the server side.\nIf the server allows both public key and password authentication then even if you try to connect without a private key file present on the client, it should prompt you for a password.\nIf you are getting the error message  \"Permission denied (publickey)\"  then it sounds like password authentication is not supported on your server.\nWithout seeing the /etc/sshd_config file, it is difficult to know but my guess would be that you need to make sure the following line exists:\nPasswordAuthentication yes\n\nRestart the ssh server, and when you connect from the client you should be prompted for a password if there is no private key present, or if the private key doesn't match the public key on the server.\nA more secure alternative to this of course would be to copy your private key to the laptop which you are using, or in-fact generate a new private key to be used on that laptop and add the public key to .ssh/authorized_keys",
        "url": "https://serverfault.com/questions/493213/temporarily-disable-ssh-public-key-authentication-from-client"
    },
    {
        "title": "Where's the conventional place to store git repositories in a linux file system tree?",
        "question": "If I make an analogy with the hosting of a web server, I would say that git's data should be in /var/git, so my git repository would be in /var/git/myrepo\nQ: Is that the right guess ?",
        "top_answer": "There is no right or wrong answer here, except the one dictated by your own personal religion and the contents of the hier(7) manpage on your system.  \ntypical Linux hier manpage ; typical BSD hier manpage)\n/var/git/* seems reasonable to me personally. That's where I keep mine.",
        "url": "https://serverfault.com/questions/432959/wheres-the-conventional-place-to-store-git-repositories-in-a-linux-file-system"
    },
    {
        "title": "How to grant network access to LocalSystem account?",
        "question": "How do you grant access to network resources to the LocalSystem (NT AUTHORITY\\SYSTEM) account?\n\nBackground\nWhen accessing the network, the LocalSystem account acts as the computer on the network:\n\nLocalSystem Account\nThe LocalSystem account is a predefined local account used by the service control manager.  \n...and acts as the computer on the\n  network.\n\nOr to say the same thing again: The LocalSystem account acts as the computer on the network:\n\nWhen a service runs under the\n  LocalSystem account on a computer that\n  is a domain member, the service has\n  whatever network access is granted to\n  the computer account, or to any groups\n  of which the computer account is a\n  member.\n\nHow does one grant a \"computer\" access to a shared folder and files?\n\nNote: \n\nComputer accounts typically have few\n  privileges and do not belong to\n  groups.\n\nSo how would i grant a computer access to one of my shares; considering that \"Everyone\" already has access?\nNote: workgroup\n| Account        | Presents credentials |\n|----------------|----------------------|\n| LocalSystem    | Machine$             |\n| LocalService   | Anonymous            |\n| NetworkService | Machine$             |\n",
        "top_answer": "In a domain environment, you can grant access rights to computer accounts; this applies to processes running on those computers as LocalSystem or NetworkService (but not LocalService, which presents anonymous credentials on the network) when they connect to remote systems.\nSo, if you have a computer called MANGO, that machine will have a computer account in Active Directory called MANGO$, which you can grant permissions to.\n\nNote: You can't do any of this in a workgroup environment; this applies only to domains.",
        "url": "https://serverfault.com/questions/135867/how-to-grant-network-access-to-localsystem-account"
    },
    {
        "title": "How dangerous is a wet server room floor",
        "question": "We have a couple of tower servers in a small server room. The carpet is wet as a result of the cooler and no-one else really seems concerned about this but I'm not too happy. I'm only a lowly developer, but I seem to be more concerned than the hardware guys!\nIs this dangerous? What's the worst that could happen? My instinct says water + (electric * allOfOurData) = dangerous.",
        "top_answer": "Carpet is a big 'NO! NO!!' for a room hosting equipments that are of high value, because of the fire risk. Water is too, for obvious reasons. You should straight call maintenance immediately and have them repair the drainage system. The water could really cause problems, inform your superiors right away and draw their attention on the matter.",
        "url": "https://serverfault.com/questions/327738/how-dangerous-is-a-wet-server-room-floor"
    },
    {
        "title": "How to read in N random characters from /dev/urandom?",
        "question": "read /dev/urandom 3\n\nThe above is not working..How can I read random bytes from /dev/urandom in bash?",
        "top_answer": "random=\"$(dd if=/dev/urandom bs=3 count=1)\"\n\nif specifies the input file, bs the block size (3 bytes), and count the number of blocks (1 * 3 = 3 total bytes)",
        "url": "https://serverfault.com/questions/283294/how-to-read-in-n-random-characters-from-dev-urandom"
    },
    {
        "title": "ZFS vs XFS",
        "question": "We're considering building a ~16TB storage server. At the moment, we're considering both ZFS and XFS as filesystem. What are the advantages, disadvantages? What do we have to look for? Is there a third, better option?",
        "top_answer": "I've found XFS more well suited to extremely large filesystems with possibly many large files. I've had a functioning 3.6TB XFS filesystem for over 2 years now with no problems. Definitely works better than ext3, etc at that size (especially when dealing with many large files and lots of I/O).\nWhat you get with ZFS is device pooling, striping and other advanced features built into the filesystem itself. I can't speak to specifics (I'll let others comment), but from what I can tell, you'd want to use Solaris to get the most benefit here. It's also unclear to me how much ZFS helps if you're already using hardware RAID (as I am).",
        "url": "https://serverfault.com/questions/190/zfs-vs-xfs"
    },
    {
        "title": "When building from Dockerfile, Debian/Ubuntu package install debconf Noninteractive install not allowed",
        "question": "I've set the following environment so that no question/dialog is asked during apt-get install:\nENV DEBIAN_FRONTEND noninteractive    # export DEBIAN_FRONTEND=\"noninteractive\"\n\nWhich is equivalent to:\nexport DEBIAN_FRONTEND=\"noninteractive\"\n\nYet, when building an image from a Dockerfile, at the end of one specific Debian/Ubuntu package install (using apt-get install), package configuration debconf says:\ndebconf: unable to initialize frontend: Noninteractive    # export DEBIAN_FRONTEND=\"noninteractive\"\ndebconf: (Bareword \"Debconf::FrontEnd::Noninteractive\" not allowed while \"strict subs\" in use at (eval 35) line 3, <> line 1.)\ndebconf: falling back to frontend: Noninteractive\nSubroutine BEGIN redefined at (eval 36) line 2, <> line 1.\n",
        "top_answer": "It should be actively discouraged to set the DEBIAN_FRONTEND to noninteractive via ENV. The reason is that the environment variable persists after the build, e.g. when you run docker exec -it ... bash. The setting would not make sense here.\nThere are two other possible ways:\n\nSet it via ARG as this only is available during build:\n ARG DEBIAN_FRONTEND=noninteractive\n RUN apt-get -qq -y install {your-package}\n\n\nSet it on-the-fly when required.\n RUN apt-get update && \\\n     DEBIAN_FRONTEND=noninteractive apt-get -qq -y install {your-package}\n\n\n",
        "url": "https://serverfault.com/questions/618994/when-building-from-dockerfile-debian-ubuntu-package-install-debconf-noninteract"
    },
    {
        "title": "Your system administrator does not allow the use of saved credentials to log on to the remote computer",
        "question": "At our office, all of our Windows 7 Clients get this error message when we try and RDP to a remote Windows 2008 Server outside of the office:\n\nYour system administrator does not allow the user of saved credentials to \n      log on to the remote computer XXX because its identity is not fully verified. \n      Please enter new credentials\n\n\nA quick google search leads to some posts they all suggest I edit group policy, etc.\nI'm under the impression, that the common fix for this, is to follow those instructions on every Windows 7 machine.\nIs there any way that I can do something via the Active Directory which could update all Windows 7 clients in the office LAN?",
        "top_answer": "If you don't want to change local or server side GPOs:\nGo to Control Panel -> Credential Manager on the local computer you are trying to connect from.\nYou will see three sections:\n\nWindows Credentials\nCertificate-Based Credentials\nGeneric Credentials\n\nRemove the credentials from Windows Credentials and add it to Generic Credentials.",
        "url": "https://serverfault.com/questions/396722/your-system-administrator-does-not-allow-the-use-of-saved-credentials-to-log-on"
    },
    {
        "title": "Sending cron output to a file with a timestamp in its name",
        "question": "I have a crontab like this on a LAMP setup:\n0 0 * * * /some/path/to/a/file.php > $HOME/cron.log 2>&1\n\nThis writes the output of the file to cron.log. However, when it runs again, it overwrites whatever was previously in the file.\nHow can I get cron to output to a file with a timestamp in its filename?\nAn example filename would be something like this: 2010-02-26-000000-cron.log\nI don't really care about the format, as long as it has a timestamp of some kind.",
        "top_answer": "Try:\n0 0 * * * /some/path/to/a/file.php > $HOME/`date +\\%Y\\%m\\%d\\%H\\%M\\%S`-cron.log 2>&1\n\nPlay around with the date format, if you like; just be sure to escape any % like \\%, as above.",
        "url": "https://serverfault.com/questions/117360/sending-cron-output-to-a-file-with-a-timestamp-in-its-name"
    },
    {
        "title": "How to handle relative urls correctly with a reverse proxy",
        "question": "I have a reverse proxy setup as follows in Apache:\nServer A with address www.example.com/folder is the reverse proxy server. \nIt maps to: Server B with address test.madeupurl.com\nThis kind of works. But the problem I have is, on www.example.com/folder, all of the relative links are of the forms www.example.com/css/examplefilename.css rather than www.example.com/folder/css/examplefilename.css\nHow do I fix this?\nSo far my reverse proxy has this on Server A (www.example.com):\n<Location /folder>\n    ProxyPass  http://test.madeupurl.com\n    ProxyPassReverse http://test.madeupurl.com\n</Location>\n",
        "top_answer": "The Apache ProxyPassRewrite does not rewrite the response bodies received from http://test.example.com, only headers (like redirects to a 404 page and such).\nA number of alternatives:\nOne) Rewrite the internal app to use relative paths instead of absolute. i.e. ../css/style.css instead of /css/style.css\nTwo) Redeploy the internal app in a the same subdirectory /folder rather than in the root of test.example.com.\nThree)  One and two are often unlikely to happen... If you're lucky the internal app only uses two or three subdirectories and those are unused on your main site, simply write a bunch of ProxyPass lines:\n# Expose Internal App to the internet.\nProxyPass /externalpath/  http://test.example.com/\nProxyPassReverse /externalpath/  http://test.example.com/\n# Internal app uses a bunch of absolute paths. \nProxyPass /css/  http://test.example.com/css/\nProxyPassReverse /css/  http://test.example.com/css/\nProxyPass /icons/  http://test.example.com/icons/\nProxyPassReverse /icons/  http://test.example.com/icons/\n\nFour) Create a separate subdomain for the internal app and simply reverse proxy everything:\n<VirtualHost *:80>\n   ServerName app.example.com/\n   # Expose Internal App to the internet.\n   ProxyPass /  http://test.internal.example.com/\n   ProxyPassReverse /  http://test.internal.example.com/\n</VirtualHost>\n\nFive) Sometimes developers are completely clueless and have their applications not only generate absolute URL's but even include the hostname part in their URL's and the resulting HTML code looks like this: <img src=http://test.example.com/icons/logo.png>.\nA) You can use combo solution of a split horizon DNS and scenario 4. Both internal and external users use the test.example.com, but your internal DNS points directly to the ip-address of test.example.com's server. For external users the public record for test.example.com points to the ip-address of your public webserver www.example.com and you can then use solution 4.\nB) You can actually get apache to to not only proxy requests to test.example.com, but also rewrite the response body before it will be transmitted to your users. (Normally a proxy only rewrites HTTP headers/responses). mod_substitute in apache 2.2. I haven't tested if it stacks well with mod_proxy, but maybe the following works:\n<Location /folder/>\n  ProxyPass http://test.example.com/\n  ProxyPassReverse http://test.example.com/ \n  AddOutputFilterByType SUBSTITUTE text/html\n  Substitute \"s|test.example.com/|www.example.com/folder/|i\" \n</Location>\n\nLate edit\nSix) Sometimes developers provide special application settings specifically for deployments where the application is behind a reverse proxy. The application can then be configured to generate self referential URL's, with the protocol, the site name and URI path that users are/should be using, rather then the hostname, the protocol and/or the URI the application detects. Expect terms like external URL,  site URL. Then you don't need to solve anything in Apache.",
        "url": "https://serverfault.com/questions/561892/how-to-handle-relative-urls-correctly-with-a-reverse-proxy"
    },
    {
        "title": "Can't seem to disable Java Automatic Update",
        "question": "I'm just tweaking out my new Windows 7 laptop and wanted to disable the automatic Java updating (and thus kill the silly jusched.exe background process), but I can't seem to get it to actually turn it off.\nI found the Java Control Panel applet and found the settings on the Update tab that should control it. I can turn them off, apply them, and close the dialog successfully. But if I just open the dialog backup again right away, I see that the changes weren't actually made. I've tried it numerous times and it just doesn't take. What's up with that?\nI also tried to disable the icon in the system tray and got the same effect. Changing the size of the Temporary Internet Files cache work however.\nAny ideas? Thanks!",
        "top_answer": "Actually this problem is due to the control panel requiring administrator privileges to allow the Java control panel to save your settings (it hasn't been fixed for ages, thanks to Sun Microsystems).\nFirst, you need to find the Java Control Panel executable, in one of the following locations:\nC:\\Program Files\\Java\\jre[version]\\bin\\javacpl.exe\n\nor\nC:\\Program Files (x86)\\Java\\jre[version]\\bin\\javacpl.exe\n\nThe path will differ depending on your system's architecture and which version of Java you have installed. For example, a 32-bit version of Java 7 installed on a 64-bit version of Windows will have it in:\nC:\\Program Files (x86)\\Java\\jre7\\bin\\javacpl.exe\n\nOnce you've found the file, right-click it and select \"Run as administrator\".\nFrom there, un-check \"Check for Updates Automatically\" on the Update tab and click OK. You can verify that the setting has been applied by navigating to the same screen as you normally would through the Control Panel.\nYou can also check your running processes to see that jusched.exe is no longer running - it was automatically terminated when you clicked OK.",
        "url": "https://serverfault.com/questions/14303/cant-seem-to-disable-java-automatic-update"
    },
    {
        "title": "Securely add a host (e.g. GitHub) to the SSH known_hosts file",
        "question": "How can I add a host key to the SSH known_hosts file securely?\nI'm setting up a development machine, and I want to (e.g.) prevent git from prompting when I clone a repository from github.com using SSH.\nI know that I can use StrictHostKeyChecking=no (e.g. this answer), but that's not secure.\nSo far, I've found...\n\nGitHub publishes their SSH key fingerprints at GitHub's SSH key fingerprints\n\nI can use ssh-keyscan to get the host key for github.com.\n\n\nHow do I combine these facts? Given a prepopulated list of fingerprints, how do I verify that the output of ssh-keyscan can be added to the known_hosts file?\n\nI guess I'm asking the following:\nHow do I get the fingerprint for a key returned by ssh-keyscan?\nLet's assume that I've already been MITM-ed for SSH, but that I can trust the GitHub HTTPS page (because it has a valid certificate chain).\nThat means that I've got some (suspect) SSH host keys (from ssh-keyscan) and some (trusted) key fingerprints. How do I verify one against the other?\n\nRelated: how do I hash the host portion of the output from ssh-keyscan? Or can I mix hashed/unhashed hosts in known_hosts?",
        "top_answer": "Now that GitHub provides their SSH keys and fingerprints via their metadata API endpoint (as of January 2022), you can leverage the trust you have in GitHub's TLS certificate used on api.github.com (due to it being signed by a certificate authority (CA) which is in your system's trusted root certificate store) to securely fetch their SSH host keys.\nIf you have jq installed you can do it with this one-liner\ncurl --silent https://api.github.com/meta \\\n  | jq --raw-output '\"github.com \"+.ssh_keys[]' >> ~/.ssh/known_hosts\n\nOr if you want to use Python\ncurl --silent https://api.github.com/meta | \\\n  python3 -c 'import json,sys;print(*[\"github.com \" + x for x in json.load(sys.stdin)[\"ssh_keys\"]], sep=\"\\n\")' \\\n  >> ~/.ssh/known_hosts\n",
        "url": "https://serverfault.com/questions/856194/securely-add-a-host-e-g-github-to-the-ssh-known-hosts-file"
    },
    {
        "title": "Can I specify a port in an entry in my /etc/hosts on OS X? [duplicate]",
        "question": "\nPossible Duplicate:\nHow to use DNS to redirect domain to specific port on my server \n\nI want to trick my browser into going to localhost:3000 instead of xyz.com.  I went into /etc/hosts on OS X 10.5 and added the following entry:\n127.0.0.1:3000 xyz.com\n\nThat does not work but without specifying the port the trick works.  Is there a way to do this specifying the port?",
        "top_answer": "No, the hosts file is simply a way to statically resolve names when no DNS server is present.",
        "url": "https://serverfault.com/questions/54357/can-i-specify-a-port-in-an-entry-in-my-etc-hosts-on-os-x"
    },
    {
        "title": "IPTables only allow localhost access",
        "question": "I have struggled throughout the years to get a solid understanding on iptables.  Any time I try and read through the man pages my eyes start to glaze over.\nI have a service that I only want to allow the localhost to have access to.\nWhat terms (or configuration, if someone is feeling generous) should I Google for to allow only localhost host to have access to a given port?",
        "top_answer": "If by service you mean a specific port, then the following two lines should work. Change the \"25\" to whatever port you're trying to restrict.\niptables -A INPUT -p tcp -s localhost --dport 25 -j ACCEPT\niptables -A INPUT -p tcp --dport 25 -j DROP\n",
        "url": "https://serverfault.com/questions/247176/iptables-only-allow-localhost-access"
    },
    {
        "title": "How do I edit the crontab of another user on my linux server?",
        "question": "I have a user on my linux server who has sudo.  I want to edit the crontab of another user.  I'm new to this though and don't understand what the man pages are telling me yet.\nMan crontab tells me I can use this format for the crontab command:\ncrontab [ -u user ] { -l | -r [ -i ] | -e }\n\nBut what does that mean?  I want to edit the crontab of a user named jake\nso I tried \ncrontab jake -e\ncrontab [jake] -e\ncrontab [-u jake] -e\n\nAnd I get the same error every time: \"usage error: no arguments permitted after this option\"\nSo what do I type in to edit jake's crontab?",
        "top_answer": "Try \ncrontab -e -u jake\n\nYou will need to be root for this to work.",
        "url": "https://serverfault.com/questions/185703/how-do-i-edit-the-crontab-of-another-user-on-my-linux-server"
    },
    {
        "title": "How to combine various certificates into single .pem",
        "question": "I've just finished reading over this great thread explaining the different SSL formats.\nNow I'm essentially looking for the opposite of How to split a PEM file\nThere's 4 files I want to consolidate, originally created for Apache, I'm looking at files specified by\n\nSSLCertificateFile\nSSLCertificateKeyFile\nSSLCertificateChainFile\nSSLCACertificateFile\n\nWhat I'm mostly curious about is the order of the files in the consolidated dereivative, is that important?  EG. if I were to just cat them together in the order they appear above, into a .pem, would it be valid, or should they be ordered a specific way?\nFYI, I'm doing this for sake of using these certs as a combined single .pem in SimpleSAMLphp.",
        "top_answer": "The order does matter, according to RFC 4346.\nHere is a quote directly taken from the RFC:\n  certificate_list\n    This is a sequence (chain) of X.509v3 certificates.  The sender's\n    certificate must come first in the list.  Each following\n    certificate must directly certify the one preceding it.  Because\n    certificate validation requires that root keys be distributed\n    independently, the self-signed certificate that specifies the root\n    certificate authority may optionally be omitted from the chain,\n    under the assumption that the remote end must already possess it\n    in order to validate it in any case.\n\nBased on this information, the server certificate should come first, followed by any intermediate certs, and finally the root trusted authority certificate (if self-signed). I could not find any information on the private key, but I think that should not matter because a private key in pem is easy to identify as it starts and ends with the text below, which has the keyword PRIVATE in it.\n -----BEGIN RSA PRIVATE KEY-----\n -----END RSA PRIVATE KEY-----\n",
        "url": "https://serverfault.com/questions/476576/how-to-combine-various-certificates-into-single-pem"
    },
    {
        "title": "How do I connect to ssh with a different public key?",
        "question": "I have two public keys, one for some servers and one for others. How do I specify which key to use when connecting to a server?",
        "top_answer": "Assuming you're on a Unix/Linux environment, you can create or edit the file ~/.ssh/config.\nThat config file allows you to establish the parameters to use for each host; so, for example:\nHost host1\n  HostName <hostname_or_ip>\n  IdentityFile ~/.ssh/identity_file1\n\nHost Host2\n  HostName <hostname_or_ip2>\n  User differentusername\n  IdentityFile ~/.ssh/identity_file2\n\nNote that host1 and host2 can also be not hostnames, but rather labels to identify a server.\nNow you can log onto the to hosts with:\nssh host1\nssh host2\n",
        "url": "https://serverfault.com/questions/295768/how-do-i-connect-to-ssh-with-a-different-public-key"
    },
    {
        "title": "How to setup linux permissions for the WWW folder?",
        "question": "Updated Summary\nThe /var/www directory is owned by root:root which means that no one can use it and it's entirely useless. Since we all want a web server that actually works (and no-one should be logging in as \"root\"), then we need to fix this. \nOnly two entities need access.\n\nPHP/Perl/Ruby/Python all need access to the folders and files since they create many of them (i.e. /uploads/). These scripting languages should be running under nginx or apache (or even some other thing like FastCGI for PHP).\nThe developers\n\nHow do they get access? I know that someone, somewhere has done this before. With however-many billions of websites out there you would think that there would be more information on this topic.\n\nI know that 777 is full read/write/execute permission for owner/group/other. So this doesn't seem to be needed correct as it gives random users full permissions.\nWhat permissions are need to be used on /var/www so that:\n\nSource control like git or svn\nUsers in a group like \"websites\" (or even added to \"www-data\")\nServers like apache or lighthttpd\nAnd PHP/Perl/Ruby\n\ncan all read, create, and run files (and directories) there? \nIf I'm correct, Ruby and PHP scripts are not \"executed\" directly - but passed to an interpreter. So there is no need for execute permission on files in /var/www...? Therefore, it seems like the correct permission would be chmod -R 1660 which would make\n\nall files shareable by these four entities\nall files non-executable by mistake\nblock everyone else from the directory entirely\nset the permission mode to \"sticky\" for all future files\n\nIs this correct?\nUpdate 1: I just realized that files and directories might need different permissions - I was talking about files above so i'm not sure what the directory permissions would need to be.\nUpdate 2: The folder structure of /var/www changes drastically as one of the four entities above are always adding (and sometimes removing) folders and sub folders many levels deep. They also create and remove files that the other 3 entities might need read/write access to. Therefore, the permissions need to do the four things above for both files and directories. Since none of them should need execute permission (see question about ruby/php above) I would assume that rw-rw-r-- permission would be all that is needed and completely safe since these four entities are run by trusted personnel (see #2) and all other users on the system only have read access.\nUpdate 3: This is for personal development machines and private company servers. No random \"web customers\" like a shared host.\nUpdate 4: This article by slicehost seems to be the best at explaining what is needed to setup permissions for your www folder. However, I'm not sure what user or group apache/nginx with PHP OR svn/git run as and how to change them.\nUpdate 5: I have (I think) finally found a way to get this all to work (answer below). However, I don't know if this is the correct and SECURE way to do this. Therefore I have started a bounty. The person who has the best method of securing and managing the www directory wins.",
        "top_answer": "After more research it seems like another (possibly better way) to answer this would be to setup the www folder like so.\n\nsudo usermod -a -G developer user1 (add each user to developer group)\nsudo chgrp -R developer /var/www/site.com/ so that developers can work in there\nsudo chmod -R 2774 /var/www/site.com/ so that only developers can create/edit files (other/world can read)\nsudo chgrp -R www-data /var/www/site.com/uploads so that www-data (apache/nginx) can create uploads.\n\nSince git runs as whatever user is calling it, then as long as the user is in the \"developer\" group they should be able to create folders, edit PHP files, and manage the git repository.\nNote: In step (3): '2' in 2774 means to 'set Group ID' for the directory. This causes new files and sub directories created within it to inherit the group ID of the parent directory (instead of the primary group of the user) Reference: http://en.wikipedia.org/wiki/Setuid#setuid_and_setgid_on_directories ",
        "url": "https://serverfault.com/questions/124800/how-to-setup-linux-permissions-for-the-www-folder"
    },
    {
        "title": "failed to get D-Bus connection: Operation not permitted",
        "question": "I'm trying to list services on my CentOS image running in Docker using\nsystemctl list-units  \n\nbut I get this error message:\nFailed to get D-Bus connection: Operation not permitted\n\nAny suggestions what the problem might be?",
        "top_answer": "My guess is that you're running a non-privileged container. systemd requires CAP_SYS_ADMIN capability but Docker drops that capability in the non privileged containers, in order to add more security.\nsystemd also requires RO access to the cgroup file system within a container. You can add it with -v /sys/fs/cgroup:/sys/fs/cgroup:ro\nSo, here a few steps on how to run CentOS with systemd inside a Docker container:\n\nPull centos image.\nSet up a docker file like the one below:\n\nFROM centos\nMAINTAINER \"Yourname\" <[email\u00a0protected]>\nENV container docker\nRUN yum -y update; yum clean all\nRUN yum -y install systemd; yum clean all; \\\n(cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \\\nrm -f /lib/systemd/system/multi-user.target.wants/*;\\\nrm -f /etc/systemd/system/*.wants/*;\\\nrm -f /lib/systemd/system/local-fs.target.wants/*; \\\nrm -f /lib/systemd/system/sockets.target.wants/*udev*; \\\nrm -f /lib/systemd/system/sockets.target.wants/*initctl*; \\\nrm -f /lib/systemd/system/basic.target.wants/*;\\\nrm -f /lib/systemd/system/anaconda.target.wants/*;\nVOLUME [ \"/sys/fs/cgroup\" ]\nCMD [\"/usr/sbin/init\"]\n\n\nBuild it - docker build -t centos7-systemd - < mydockerfile\n\nRun a container with docker run --rm --privileged -ti -e container=docker -v /sys/fs/cgroup:/sys/fs/cgroup:ro centos7-systemd /usr/sbin/init\n\nYou should have systemd in your container.\n\n",
        "url": "https://serverfault.com/questions/824975/failed-to-get-d-bus-connection-operation-not-permitted"
    },
    {
        "title": "What is the correct syntax to run cron every 4 hours? [duplicate]",
        "question": "I have the following syntax (which I think is correcT?) but it runs the command every minute!\n* */4 * * * /cmd.sh\n",
        "top_answer": "0 0,4,8,12,16,20 * * * /cmd.sh\n\nThat's probably how I would do it.  This will run the job every 4 hours, on the hours of 00:00, 04:00, 08:00 12:00, 16:00, 20:00.\nThis is just a little more verbose way of writing */4, but it should work the same.",
        "url": "https://serverfault.com/questions/39004/what-is-the-correct-syntax-to-run-cron-every-4-hours"
    },
    {
        "title": "How to change a SSH host key?",
        "question": "I cloned a server and so they've the same RSA key fingerprint.\nIt seems to be defined in /etc/ssh/ssh_host_rsa_key.pub.\nWhat is the correct way to change that?\nThanks.",
        "top_answer": "Or, remove keys and\nssh-keygen -A\n\nExplanation:\n-A: For each of the key types (rsa1, rsa, dsa, ecdsa and ed25519) for which host keys do not exist, generate the host keys with the default key file path, an empty passphrase, default bits for the key type, and default comment. This is used by /etc/rc to generate new host keys.",
        "url": "https://serverfault.com/questions/471327/how-to-change-a-ssh-host-key"
    },
    {
        "title": "Is there a way to avoid SSH typing delay?",
        "question": "Can I tell SSH to send the data only after pressing enter or tab, and not after each individual keypress?",
        "top_answer": "No, because SSH has no way of knowing whether what you're typing would require an enter or tab to action -- if you're trying to go through your command history, for instance, the ^R or up arrows wouldn't be sent by themselves, and that would be... unpleasant.\nYou don't have to wait between each character for it to appear on the screen, though; if you know what you have to type, bash away at it as quick as you like, and the terminal will catch up in about one round-trip time from when you stopped typing, which is about as good as you'll get out of a line-buffered setup anyway (packet loss is different, but it introduces it's own interesting quirks).",
        "url": "https://serverfault.com/questions/302505/is-there-a-way-to-avoid-ssh-typing-delay"
    },
    {
        "title": "memcache vs memcached?",
        "question": "I want to use memcached\nhttp://www.danga.com/memcached/\nI have installed it through yum install memcached\nBut now I need to connect to PHP, and there is an extension named memcache and one named memcached? ARGH\nhttps://www.php.net/manual/en/book.memcache.php\nhttps://www.php.net/manual/en/book.memcached.php\nCould someone point me in the right direction here.. which one is going to work?\nAlso, do I need to open any ports for it to work even though it's local?\nAfter running it, I try telnet 127.0.0.1 11211 and I get connection refused.",
        "top_answer": "The short answer: Either one is what you are looking for, but my first choice would be memcache (the first one you listed), purely based on its correct use of nomenclature.\nNow here's how I came to that conclusion:\nHere is a quick backgrounder in naming conventions (for those unfamiliar), which explains the frustration by the question asker: For many *nix applications, the piece that does the backend work is called a \"daemon\" (think \"service\" in Windows-land), while the interface or client application is what you use to control or access the daemon. The daemon is most often named the same as the client, with the letter \"d\" appended to it. For example \"imap\" would be a client that connects to the \"imapd\" daemon.\nThis naming convention is clearly being adhered to by memcache when you read the introduction to the memcache module (notice the distinction between memcache and memcached in this excerpt):\n\nMemcache module provides handy\n  procedural and object oriented\n  interface to memcached, highly\n  effective caching daemon, which was\n  especially designed to decrease\n  database load in dynamic web\n  applications. \nThe Memcache module also provides a\n  session handler (memcache). \nMore information about memcached can\n  be found at \u00bb\n  http://www.danga.com/memcached/.\n\nThe frustration here is caused by the author of the PHP extension which was badly named memcached, since it shares the same name as the actual daemon called memcached. Notice also that in the introduction to memcached (the php module), it makes mention of libmemcached, which is the shared library (or API) that is used by the module to access the memcached daemon:\n\nmemcached is a high-performance,\n  distributed memory object caching\n  system, generic in nature, but\n  intended for use in speeding up\n  dynamic web applications by\n  alleviating database load. \nThis extension uses libmemcached\n  library to provide API for\n  communicating with memcached servers.\n  It also provides a session handler\n  (memcached). \nInformation about libmemcached can be\n  found at \u00bb\n  http://tangent.org/552/libmemcached.html.\n\nIn summary, both are functionally the same, but they simply have different authors, and the one is simply named more appropriately than the other.",
        "url": "https://serverfault.com/questions/63383/memcache-vs-memcached"
    },
    {
        "title": "Listen to UDP data on local port with netcat",
        "question": "netcat -ul -p2115 fails with a usage statement.\nWhat am I doing wrong?",
        "top_answer": "To quote the nc man page:\n\n-l Used to specify that nc should listen for an incoming connection rather than initiate a connection to a remote host.  It is an error to use this option in conjunction with the -p, -s, or -z options.  Additionally, any timeouts specified with the -w option are ignored.\n\nThe key here is that -p cannot be combined with the -l flag.  When using the -l flag, any ports specified in the positional arguments are used.  So instead, you could use the following:\nnetcat -ul 2115\n",
        "url": "https://serverfault.com/questions/207683/listen-to-udp-data-on-local-port-with-netcat"
    },
    {
        "title": "How do I grep through binary files that look like text?",
        "question": "I have binary files that should be text (they're exported logs), but I can't open it with less (it looks ugly - it looks like a binary file). I found that I could open it with vi and I can cat it (you'll see the actual logs), but what I'd really like to do is grep through them (without having to open up each one with vi and then perform a search). Is there a way for me to do that?",
        "top_answer": "You can use grep anyway to search through the file - it does not really care if the input file is really text or not. From 'man grep':\n    -a, --text\n          Process a binary file as if it were text; this is equivalent to the --binary-files=text option.\n\n   --binary-files=TYPE\n          If  the  first few bytes of a file indicate that the file contains binary data, assume that the file is\n          of type TYPE.  By default, TYPE is binary, and grep normally outputs either a one-line  message  saying\n          that a binary file matches, or no message if there is no match.  If TYPE is without-match, grep assumes\n          that a binary file does not match; this is equivalent  to  the  -I  option.   If  TYPE  is  text,  grep\n          processes  a  binary  file  as  if  it  were  text; this is equivalent to the -a option.  Warning: grep\n          --binary-files=text might output binary garbage, which can have nasty side effects if the output  is  a\n          terminal and if the terminal driver interprets some of it as commands.\n\nPlease mark the words of caution at the end of the second paragraph. You might want to redirect the results from grep into a new file and examine this with vi / less. ",
        "url": "https://serverfault.com/questions/328101/how-do-i-grep-through-binary-files-that-look-like-text"
    },
    {
        "title": "How can I allow one user to su to another without allowing root access?",
        "question": "I'd like to allow certain users to su to another user account without having to know that account's password, but not allow access to any other user account (i.e. root).\nFor instance, I'd like to allow Tom the DBA to su to the oracle user, but not to the tomcat user or root.\nI imagine this could be done with the /etc/sudoers file - is it possible?  If so, how?",
        "top_answer": "Yes, this is possible.\nIn /etc/sudoers the item immediately following the equals is the user that the command will be allowed to execute as.\ntom  ALL=(oracle) /bin/chown tom *\n\nThe user (tom) can type sudo -u oracle /bin/chown tom /home/oracle/oraclefile",
        "url": "https://serverfault.com/questions/17814/how-can-i-allow-one-user-to-su-to-another-without-allowing-root-access"
    },
    {
        "title": "How to PREPEND rules rather than APPEND using iptables?",
        "question": "Pretty basic question: how to PREPEND rules on IPTABLES rather than to APPEND?\nI have DROP statements at the bottom of my rules.  I have a software to add new rules but adding rules after DROP statements isn't good.  Every time I want to add a new rule, I have to flush the table (which is inefficient).  \nIs there a way to prepend a rule i.e., add a rule to the top of the table rather than the bottom?  \nMany thanks.",
        "top_answer": "Use the -I switch:\nsudo iptables -I INPUT 1 -i lo -j ACCEPT\n\nThis would insert a rule at position #1 in the INPUT chain.",
        "url": "https://serverfault.com/questions/374993/how-to-prepend-rules-rather-than-append-using-iptables"
    },
    {
        "title": "How to run VBoxManage.exe? [closed]",
        "question": "The file is located in Program Files/Oracle/VirtualBox/VBoxManage.exe and is used as a command-line interface with VirtualBox.\nI'm using it to convert the .vdi image to a .vdmk (for VMware).\nhttp://scottlinux.com/2011/06/24/convert-vdi-to-vmdk-virtualbox-to-vmware/ \nHere's an example script:\n$ VBoxManage list hdds\n\nBut where do I run this command? In Windows cmd? I tried both in cmd and in Linux but I can't figure it out.",
        "top_answer": "It's a pretty sure bet that running an exe file in Linux won't work. In Windows you do run it from the command prompt. If you get a message about the command not being found then either add the path to the command to your PATH environment variable or specify the full path to the command.\nIf the command runs but it's not doing what you think it should be doing then read the documentation.",
        "url": "https://serverfault.com/questions/365423/how-to-run-vboxmanage-exe"
    },
    {
        "title": "How to use docker secrets without a swarm cluster?",
        "question": "Currently we im a running application on a single docker container, the application needs all sorts of sensitive data to be passed as environments variables,\nIm putting those on the run command so they don't end up in the image and then on a repository, however i end up with a very non-secure run command,\nNow, i understand that docker secrets exist, however, how can i use them without deploying a cluster? or is there any other way to secure this data?\nBest Regards,",
        "top_answer": "You can't... It does not support secrets without Swarm.\nUnless ''may be'' you ''Swarm'' using only one node.\nThe other solution would be, I think to use a third party vault software like this one:\nhttps://www.vaultproject.io/\nBut then, to use the secrets in your containers from Vault, you would need to read the doc.\nHope this bring you to the right path to start.",
        "url": "https://serverfault.com/questions/871090/how-to-use-docker-secrets-without-a-swarm-cluster"
    },
    {
        "title": "How passively monitor for tcp packet loss? (Linux)",
        "question": "How can I passively monitor the packet loss on TCP connections to/from my machine?\nBasically, I'd like a tool that sits in the background and watches TCP ack/nak/re-transmits to generate a report on which peer IP addresses \"seem\" to be experiencing heavy loss.\nMost questions like this that I find of SF suggest using tools like iperf.  But, I need to monitor connections to/from a real application on my machine.\nIs this data just sitting there in the Linux TCP stack?",
        "top_answer": "For a general sense of the scale of your problem netstat -s will track your total number of retransmissions.       \n# netstat -s | grep retransmitted\n     368644 segments retransmitted\n\nYou can aso grep for segments to get a more detailed view:\n# netstat -s | grep segments\n         149840 segments received\n         150373 segments sent out\n         161 segments retransmitted\n         13 bad segments received\n\nFor a deeper dive, you'll probably want to fire up Wireshark.\nIn Wireshark set your filter to tcp.analysis.retransmission to see retransmissions by flow.\nThat's the best option I can come up with.\nOther dead ends explored:\n\nnetfilter/conntrack tools don't seem to keep retransmits \nstracing netstat -s showed that it is just printing /proc/net/netstat \ncolumn 9 in /proc/net/tcp looked promising, but it unfortunately appears to be unused. \n",
        "url": "https://serverfault.com/questions/318909/how-passively-monitor-for-tcp-packet-loss-linux"
    },
    {
        "title": "What is the difference between /sbin/nologin and /bin/false?",
        "question": "I have often heard it recommended that a user account should be disabled by setting its shell to /bin/false. But, on my existing Linux systems, I see that a great number of existing accounts (all of them service accounts) have a shell of /sbin/nologin instead.\nI see from the man page that /sbin/nologin prints a message to the user saying the account is disabled, and then exits. Presumably /bin/false would not print anything.\nI also see that /sbin/nologin is listed in /etc/shells, while /bin/false is not.\nThe man page says that FTP will disable access for users with a shell not listed in /etc/shells and implies that other programs may do the same. Does that mean that somebody could FTP in with an account that has /sbin/nologin as its shell?\nWhat is the difference here? Which one of these should I use to disable a user account, and in what circumstances? What other effects does a listing in /etc/shells have?",
        "top_answer": "/bin/false is a utility program, companion to /bin/true, which is useful in some abstract sense to ensure that unix is feature-complete.  However, emergent purposes for these programs have been found; consider the BASH statement /some/program || /bin/true, which will always boolean-evaluate to true ($? = 0) no matter the return of /some/program.\nAn emergent use of /bin/false, as you identified, is as a null shell for users not allowed to log in.  The system in this case will behave exactly as though the shell failed to run.\nPOSIX (though I may be wrong and it may the the SUS) constrains both these commands to do exactly nothing other than return the appropriate boolean value.\n/sbin/nologin is a BSD utility which has similar behaviour to /bin/false (returns boolean false), but prints output as well, as /bin/false is prohibited from doing.  This is supposed to help the user understand what happened, though in practice many terminal emulators will simply close when the shell terminates, rendering the message all but unreadable anyway in some cases.\nThere is little purpose to listing /sbin/nologin in /etc/shells.  The standard effect of /etc/shells is to list the programs permissible for use with chsh when users are changing their own shell (and there is no credible reason to change your own shell to /sbin/nologin).  The superuser can change anyone's shell to anything.  However, you may want to list both /sbin/nologin and /bin/false in /etc/rsh, which will prohibit users with these shells from changing their shell using chsh in the unfortunate event that they get a shell.\nFTP daemons may disallow access to users with a shell not in /etc/shells, or they may use any other logic they wish.  Running FTP is to be avoided in any case because sftp (which provides similar functionality) is similar but secure.  Some sites use /sbin/nologin to disable shell access while allowing sftp access by putting it in /etc/shells.  This may open a backdoor if the user is allowed to create cronjobs.\nIn either case, scp will not operate with an invalid shell.  scponly can be used as a shell in this instance.\nAdditionally, the choice of shell affects the operation of su - (AKA su -l).  Particularly, the output of /sbin/nologin will be printed to stdout if it is the shell; this cannot be the case with /bin/false.  In either case commands run with su -cl will fail.\nFinally, the answer:\nTo disable an account, you should do three things.\n\nSet the shell to /sbin/nologin\nSet the password field in /etc/passwd to the appropriate locked value for your UNIX (! on Linux, but *LOCKED* on FreeBSD).  This prevents SSH login with keys unless UsePAM yes is set in the sshd_config.\nSet the account expiration date to the distant past (e.g., usermod --expiredate 1).  This step will prevent SSH login with any method if PAM is used to process the login.\n\nIf it's a service account, it's enough to make sure that it has no SSH authorized keys in its home directory and the first two steps above.  If you're worried someone might get an SSH certificate for it or something, you could always list your service accounts and groups in DenyUsers and DenyGroups in sshd_config.",
        "url": "https://serverfault.com/questions/519215/what-is-the-difference-between-sbin-nologin-and-bin-false"
    },
    {
        "title": "Should I respond to an \"ethical hacker\" who's requesting a bounty?",
        "question": "I run a small internet based business from home and make a living at it to feed my family, but I'm still a one man show and internet security is far from my area of expertise.\nYesterday I received two emails from a guy who calls himself an \"ethical hacker\" and has identified two vulnerabilities in my system which he says could be exploited by hackers.  I believe him.\nThe problem is, at the bottom of each email he says he \"expects a bounty to be paid\".  Is this black mail?  Is this his way of saying you'd better pay me or I'm going to wreak havoc?  Or is this a typical and legitimate method for people to make a living without any nefarious intentions?\nEDIT:  For more clarification:  He gave me two examples of vulnerabilities with screenshots and clear instructions on how to fix those vulnerabilities. One was to change the \"?all\" part of my SPF record to \"-all\" to block all other domains from sending emails for my domain. In the other email he explained how my site was able to be shown inside an iframe (enabling a technique called \"clickjacking\") and he also included an example of the code and instructions on how to prevent it.\nUPDATE Jan 2025\nI ignored the email and received a follow up email, which I also ignored, and then I never heard from him again.",
        "top_answer": "A true \"ethical hacker\" would tell you what issue (s)he found in your system, not ask money for that; (s)he could offer to fix it as a contractor, but that would be after telling you what the actual problem is; and in any case, it's a completely different thing from just trying to scare you into paying.\nThis is plain and simple blackmail.\n(Also, it's a very real possibility that there is no real vulnerability and someone is just trying to scam you into paying money for nothing).",
        "url": "https://serverfault.com/questions/1033101/should-i-respond-to-an-ethical-hacker-whos-requesting-a-bounty"
    },
    {
        "title": "Does changing default port number actually increase security? [closed]",
        "question": "I have seen advice saying you should use different port numbers for private applications (e.g. intranet, private database, anything that no outsider will use).\nI am not entirely convinced that can improve security because\n\nPort scanners exist\nIf an application is vulnerable, it remains so regardless of its port number.\n\nDid I miss something or have I answered my own question?",
        "top_answer": "It doesn't provide any serious defense against a targetted attack. If your server is being targetted then, as you say, they will port scan you and find out where your doors are.\nHowever, moving SSH off the default port of 22 will deter some of the non-targetted and amateur script kiddie type attacks. These are relatively unsophisticated users who are using scripts to port scan large blocks of IP addresses at a time specifically to see if port 22 is open and when they find one, they will launch some sort of attack on it (brute force, dictionary attack, etc). If your machine is in that block of IPs being scanned and it is not running SSH on port 22 then it will not respond and therefore will not show up in the list of machines for this script kiddie to attack. Ergo, there is some low-level security provided but only for this type of opportunistic attack.\nBy way of example, if you have the time - log dive on your server (assuming SSH is on port 22) and pull out all the unique failed SSH attempts that you can. Then move SSH off that port, wait some time, and go log diving again. You will undoubtedly find less attacks.\nI used to run Fail2Ban on a public webserver and it was really, really obvious when I moved SSH off port 22. It cut the opportunistic attacks by orders of magnitude.",
        "url": "https://serverfault.com/questions/316516/does-changing-default-port-number-actually-increase-security"
    },
    {
        "title": "Why is SSH password authentication a security risk?",
        "question": "Most guides for OpenSSH configuration advise to disable password authentication in favor of key-based authentication. But in my opinion password authentication has a significant advantage: an ability to connect from absolutely anywhere without a key. If used always with a strong password, this should not be a security risk. Or should it?",
        "top_answer": "There are pro's and con's for either pw or key-based authentication.\nIn some cases, for example, key-based authentication is less secure than password authentication. In other cases, its pw-based that's less secure. In some cases, one is more convenient, in others, less.\nIt all boils down to this: When you do key-based authentication, you must secure your key with a passphrase. Unless you have ssh-agent running (ssh-agent frees you from entering your passphrase every time), you've gained nothing in terms of convenience. Security is disputable: the attack vector now shifted from the server to YOU, or your account, or your personal machine, (...) - those may or may not be  easier to break.\nThink outside of the box when deciding this. Whether you gain or loose in terms of security depends on the rest of your environment and other measures.\nedit: Oh, just saw that you're talking about a home server. I was in the same situation, \"password\" or \"USB stick with key on it\" always with me? I went for the former but changed the SSH listening port to something different than 22. That stops all those lame script kiddies brute forcing whole network ranges.",
        "url": "https://serverfault.com/questions/334448/why-is-ssh-password-authentication-a-security-risk"
    },
    {
        "title": "How do I protect my company from my IT guy? [closed]",
        "question": "I'm going to hire an IT guy to help manage my office's computers and network. We're a small shop, so he'll be the only one doing IT.\nOf course, I'll interview carefully, check references, and run a background check. But you never know how things will work out.\nHow do I limit my company's exposure if the guy I hire turns out to be evil? How do I avoid making him the single most powerful person in the organization?",
        "top_answer": "You do it the same way you protect the company from head of Sales running off with your client list, or the head of Accounting embezzling funds, or the Stock manager from running off with half the inventory, largely: Trust, but verify.\nAt the very least, I would require that all passwords for all Administrator accounts on systems and services under IT be kept in a password safe (either digitally like KeePass, or a literal piece of paper kept in a safe).  Periodically you will need to verify that these accounts are still active and have appropriate access rights.  Most experienced IT people call this the \"if I'm hit by a bus\" scenario, and it's part of the general idea of eliminating points of failure.\nAt the one business I worked at where I was the sole IT Admin, we maintained a relationship with an external IT consultant who handed this, primarily because the company had been burned in the past (by incompetence more than malice).  They had remote access passwords and could, when asked, reset the essential administrator passwords.  They did not have direct access to any company data, however.  They could only reset passwords.  Of course, since they could reset enterprise admin passwords, they could take control of the systems.  Again, it became \"Trust but Verify\".  They made sure they could access the systems.  I made sure they didn't change anything without us knowing about it.\nAnd remember: the easiest way to make sure a person doesn't burn your company is to make sure they're happy.  Make sure your pay is at least at the median value.  I've heard of too many situations where IT personnel have damaged a company out of spite.  Treat your employees right and they'll do the same.",
        "url": "https://serverfault.com/questions/283909/how-do-i-protect-my-company-from-my-it-guy"
    },
    {
        "title": "Do you skip a rack unit between servers?",
        "question": "It seems like there's a lot of disagreement in mindsets when it comes to installing rackmount servers. There have been threads discussing cable arms and other rackmount accessories, but I'm curious: \nDo you leave an empty rack unit between your servers when you install them? Why or why not? Do you have any empirical evidence to support your ideas? Is anyone aware of a study which proves conclusively whether one is better or not? ",
        "top_answer": "If your servers use front to back flow-through cooling, as most rack mounted servers do, leaving gaps can actually hurt cooling.  You don't want the cold air to have any way to get to the hot aisle except through the server itself.  If you need to leave gaps (for power concerns, floor weight issues, etc) you should use blanking panels so air can't pass between the servers.",
        "url": "https://serverfault.com/questions/27726/do-you-skip-a-rack-unit-between-servers"
    },
    {
        "title": "How to check disk I/O utilization per process?",
        "question": "I'm having a problem with a Linux system and I have found sysstat and sar to report huge peaks of disk I/O, average service time as well as average wait time.\nHow could I determine which process is causing these peaks the next time it happen?\nIs it possible to do with sar? Can I find this info from the already recorded sar files?\nOutput of sar -d, system stall happened around 12.58-13.01pm.   \n12:40:01          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util\n12:40:01       dev8-0     11.57      0.11    710.08     61.36      0.01      0.97      0.37      0.43\n12:45:01       dev8-0     13.36      0.00    972.93     72.82      0.01      1.00      0.32      0.43\n12:50:01       dev8-0     13.55      0.03    616.56     45.49      0.01      0.70      0.35      0.47\n12:55:01       dev8-0     13.99      0.08    917.00     65.55      0.01      0.86      0.37      0.52\n13:01:02       dev8-0      6.28      0.00    400.53     63.81      0.89    141.87    141.12     88.59\n13:05:01       dev8-0     22.75      0.03    932.13     40.97      0.01      0.65      0.27      0.62\n13:10:01       dev8-0     13.11      0.00    634.55     48.42      0.01      0.71      0.38      0.50\n\nI also have this follow-up question to another thread I started yesterday: \n\nSudden peaks in load and disk block wait\n",
        "top_answer": "If you are lucky enough to catch the next peak utilization period, you can study per-process I/O stats interactively, using iotop.",
        "url": "https://serverfault.com/questions/169676/how-to-check-disk-i-o-utilization-per-process"
    },
    {
        "title": "How do I remove login credentials for a network location in Win7?",
        "question": "I just tried to access a folder like so:\n\\\\somecomputeronmynetwork\\somelocation$\n\nWhen going to this location I'm prompted for a user name and password.\nI put one in, and it let me in fine.\nNow I need to remove that login, so I can try a different user name and password.\nWhat's the easiest way to do this?",
        "top_answer": "Open a command prompt or from start/run type:\nnet use \\\\somecomputeronmynetwork\\somelocation$ /delete\n\nYou can also use the following command to list \"remembered\" connections:\nnet use\n",
        "url": "https://serverfault.com/questions/213765/how-do-i-remove-login-credentials-for-a-network-location-in-win7"
    },
    {
        "title": "SSD or HDD for server",
        "question": "Issue\nI have read many discussions about storage, and whether SSDs or classic HDDs are better. I am quite confused. HDDs are still quite preferred, but why?\nWhich is better for active storage? For example for databases, where the disk is active all the time?\nAbout SSD.\nPros.\n\nThey are quiet.\nNot mechanical.\nFastest.\n\nCons.\n\nMore expensive.\n\nQuestion.\n\nWhen the life cycle for one cell of a SSD is used, what happens then? Is the disk reduced by only this cell and works normally?\nWhat is the best filesystem to write? Is ext4 good because it saves to cells consecutively?\n\nAbout HDD.\nPros.\n\nCheaper.\n\nCons.\n\nIn case of mechanical fault, I believe there is usually no way to repair it. (Please confirm.)\nSlowest, although I think HDD speed is usually sufficient for servers.\n\nIs it just about price? Why are HDDs preferred? And are SSDs really useful for servers?",
        "top_answer": "One aspect of my job is designing and building large-scale storage systems (often known as \"SANs\", or \"Storage Area Networks\"). Typically, we use a tiered approach with SSD's and HDD's combined.\nThat said, each one has specific benefits.\n\nSSD's almost always have a higher Cost-per-Byte. I can get 10k SAS 4kn HDD's with a cost-per-gigabyte of $0.068/GB USD. That means for roughly $280 I can get a 4TB drive. SSD's on the other hand typically have a cost-per-gigabyte in the 10's and 20's of cents, even as high as dollars-per-gigabyte.\nWhen dealing with RAID, speed becomes less important, and instead size and reliability matter much more. I can build a 12TB N+2 RAID system with HDD's far cheaper than SSD's. This is mostly due to point 1.\nWhen dealt with properly, HDD's are extremely cheap to replace and maintain. Because the cost-per-byte is lower, replacing an HDD with another due to failure is cheaper. And, because HDD failures are typically related to time vs. data-written, replacing it doesn't automatically start using up TBW when it rebuilds the RAID array. (Granted, TBW percentage used for a rebuild is tiny overall, but the point stands.)\nThe SSD market is relatively complex. There are four (current, at the time of this writing) major types of SSD's, rated from highest number of total writes supported to lowest: SLC, MLC, TLC, QLC. The SLC typically supports the largest numbers of total writes (the major limiting factor of SSD lifetimes), whereas the QLC typically supports the lowest numbers of total writes.\n\nThat said, the most successful storage systems I've seen are tiered with both drives in use. Personally, all the storage systems I recommend to clients generally follow the following tiers:\n\nTier 1 is typically a (or several) RAID 10 SSD-only tier. Data is always written to Tier 1.\nTier 2 is typically a (or several) RAID 50 or 5 SSD-only tier. Data is aged out of Tier 1 to Tier 2.\nTier 3 is typically a (or several) RAID 10 HDD-only tier. Data is aged out of Tier 2 to Tier 3.\nTier 4 is typically several groups of RAID 6 HDD-only tiers. Data is aged out of Tier 3 to Tier 4. We make the RAID 6 groups as small as possible, so that there is a maximal support of drive-failure.\n\nRead/Write performance drops as you increase tiers, data will propagate down to a tier where most of the data shares the same access-/modification-frequency. (That is, the more frequently data is read/written, the higher the tier it resides on.)\nSprinkle some well-designed fibre-channel in there, and you can actually build a SAN that has a higher throughput than on-board drives would.\nNow, to some specific items you mention:\nYour SSD Questions\n\nHow SSD exactly works, when life cycle for one cell is out, what then? Disk is reduced by only this cell and works normally? Or what happened then?\n\n\nBoth drive-types are typically designed with a number of \"spare\" cells. That is, they have \"extra\" space on them you cannot access that supports failing-to if a cell dies. (IIRC it's like 7-10%.) This means if a single \"cell\" (sector on HDD) dies, a \"spare\" is used. You can check the status of this via the S.M.A.R.T. diagnostics utility on both drives.\n\n\nWhat is best solution (filesystem) to write? I think ext4 is good, because it saves to cells consecutively?\n\n\nFor SSD's this is entirely irrelevant. Cell-positioning does not matter, as access time is typically linear.\n\nYour HDD Questions\n\nIn case of mechanical fault, no way to repair it (is it right)?\n\n\nPartially incorrect. HDD's are actually easier to recover data from in most failure situations. (Note: I said easier, not easy.) There is specialized equipment required, but success-rates here seem pretty high. The platters can often be read out of the HDD itself by special equipment, which allows data-recovery if the drive is dead.\n\n\nSlowest, but I think speed is not so important, because speed of HDD is absolutely sufficient for server using?\n\n\nTypically, when using RAID, single-drive speed becomes less a factor as you can use speed-pairing RAID setups that allow you to increase the overall speed. (RAID 0, 5, 6 are frequently used, often in tandem.) For a database with high IO's, HDD's are typically not sufficient unless designed very deliberately. You would want SLC write-intensive grade SSD's for database-grade IO.\n",
        "url": "https://serverfault.com/questions/986772/ssd-or-hdd-for-server"
    },
    {
        "title": "Block range of IP Addresses",
        "question": "I am getting bombarded with attempted hacks from China all with similar IPs.\nHow would I block the IP range with something like 116.10.191.* etc.\nI am running Ubuntu Server 13.10.\nThe current line I am using is:\nsudo /sbin/iptables -A INPUT -s 116.10.191.207 -j DROP\n\nThis only lets me block each one at a time but the hackers are changing the IPs at every attempt.",
        "top_answer": "To block 116.10.191.* addresses:\n$ sudo iptables -A INPUT -s 116.10.191.0/24 -j DROP\nTo block 116.10.*.* addresses:\n$ sudo iptables -A INPUT -s 116.10.0.0/16 -j DROP\nTo block 116.*.*.* addresses:\n$ sudo iptables -A INPUT -s 116.0.0.0/8 -j DROP\nBut be careful what you block using this method. You don't want to prevent legitmate traffic from reaching the host. \nedit: as pointed out, iptables evaluates rules in sequential order.  Rules higher in the ruleset are applied before rules lower in the ruleset.  So if there's a rule higher in your ruleset that allows said traffic, then appending (iptables -A) the DROP rule will not produce the intended blocking result.  In this case, insert (iptables -I) the rule either:\n\nas the first rule\n\nsudo iptables -I ...\n\nor before the allow rule\n\nsudo iptables --line-numbers -vnL\nsay that shows rule number 3 allows ssh traffic and you want to block ssh for an ip range. -I takes an argument of an integer that's the location in your ruleset you want the new rule to be inserted\niptables -I 2 ...",
        "url": "https://serverfault.com/questions/592061/block-range-of-ip-addresses"
    },
    {
        "title": "Why isn't ifconfig available in Ubuntu Docker container?",
        "question": "With the base ubuntu:12.04, ifconfig is not available in the container, though the ip command is available, why is this? and, how to get ifconfig in the container?",
        "top_answer": "You can install ifconfig with apt-get install net-tools. (Specifically, by adding RUN apt-get install -y net-tools to your Dockerfile.)\nBased on my test, ifconfig is included in ubuntu:14.04.",
        "url": "https://serverfault.com/questions/613528/why-isnt-ifconfig-available-in-ubuntu-docker-container"
    },
    {
        "title": "Why disable swap on kubernetes",
        "question": "Since Kubernetes 1.8, it seems I need to disable swap on my nodes (or set --fail-swap-on to false).\nI cannot find the technical reason why Kubernetes insists on the swap being disabled. Is this for performance reasons? Security reasons? Why is the reason for this not documented?",
        "top_answer": "The idea of kubernetes is to tightly pack instances to as close to 100% utilized as possible. All deployments should be pinned with CPU/memory limits. So if the scheduler sends a pod to a machine it should never use swap at all. You don't want to swap since it'll slow things down.\nIts mainly for performance. ",
        "url": "https://serverfault.com/questions/881517/why-disable-swap-on-kubernetes"
    },
    {
        "title": "How can I monitor what logrotate is doing?",
        "question": "How can I monitor what logrotate is doing in Ubuntu? Can the activity of logrotate be monitored?",
        "top_answer": "cat /var/lib/logrotate/status \n\n\nTo verify if a particular log is indeed rotating or not and to check the last date and time of its rotation, check the /var/lib/logrotate/status file. This is a neatly formatted file that contains the log file name and the date on which it was last rotated.\n\nTaken From: \nhttps://www.digitalocean.com/community/articles/how-to-manage-log-files-with-logrotate-on-ubuntu-12-10",
        "url": "https://serverfault.com/questions/189320/how-can-i-monitor-what-logrotate-is-doing"
    },
    {
        "title": "Multiple public keys for one user",
        "question": "This question is similar to SSH public key authentication - can one public key be used for multiple users? but it's the other way around.\nI'm experimenting on using ssh so any ssh server would work for your answers.\nCan I have multiple public keys link to the same user? What are the benefits of doing so? Also, can different home directories be set for different keys used (all of which link to the same user)?\nPlease let me know if I'm unclear.\nThanks.",
        "top_answer": "You can have as many keys as you desire.  It's good practice to use separate private/public key sets for different realms anyway, like one set for your personal use, one for your work, etc.\nFirst, generate two separate keypairs, one for home and one for work:\nssh-keygen -t rsa -f ~/.ssh/id_rsa.home\nssh-keygen -t rsa -f ~/.ssh/id_rsa.work\n\nNext, add an entry to your ~/.ssh/config file to pick the key to use based on the server you connect to:\nHost home\nHostname home.example.com\nIdentityFile ~/.ssh/id_rsa.home\nUser <your home acct>\n\nHost work\nHostname work.example.com\nIdentityFile ~/.ssh/id_rsa.work\nUser <your work acct>\n\nNext, append the contents of your id_rsa.work.pub into ~/.ssh/authorized_keys on the work machine, and do the same for the home key on your home machine.\nThen when you connect to the home server you use one of the keys, and the work server you use another.\nNote you probably want to add both keys to your ssh-agent so you don't have to type your passphrase all the time.",
        "url": "https://serverfault.com/questions/221760/multiple-public-keys-for-one-user"
    },
    {
        "title": "Why do I get sqlite error, \"unable to open database file\"?",
        "question": "Using my Django app, I'm able to read from the database just fine. When the application didn't have permission to access the file, it gave me this error:\n\nattempt to write a readonly database\n\nWhich made sense. So I edited the permissions on the file, so that the Apache process had write permissions. However, instead of it being able to write, I get this cryptic error:\n\nunable to open database file\n\nIf it's useful, here's the entire output:\nRequest Method: POST\nRequest URL:    http://home-sv-1/hellodjango1/polls/1/vote/\nException Type: OperationalError\nException Value:    \nunable to open database file\nException Location: /usr/lib/pymodules/python2.5/django/db/backends/sqlite3/base.py in execute, line 193\nPython Executable:  /usr/bin/python\nPython Version: 2.5.2\nPython Path:    ['/var/www', '/usr/lib/python2.5', '/usr/lib/python2.5/plat-linux2', '/usr/lib/python2.5/lib-tk', '/usr/lib/python2.5/lib-dynload', '/usr/local/lib/python2.5/site-packages', '/usr/lib/python2.5/site-packages', '/usr/lib/pymodules/python2.5', '/usr/lib/pymodules/python2.5/gtk-2.0']\nServer time:    Sun, 23 Aug 2009 07:06:08 -0500\n\nLet me know if a stack trace is necessary.",
        "top_answer": "Aha, just stumbled across an article explaining this. Also Django have info on their NewbieMistakes page.\nThe solution is to make sure the directory containing the database file also has write access allowed to the process.\nIn my case, running this command fixed the problem:\nsudo chown www-data .\n",
        "url": "https://serverfault.com/questions/57596/why-do-i-get-sqlite-error-unable-to-open-database-file"
    },
    {
        "title": "Must CSRs be generated on the server that will host the SSL certificate?",
        "question": "Is it necessary to generate the CSR (Certificate Signing Request) on the same machine that will host my web application and SSL certificate?\nThis page on SSL Shopper says so, but I'm not sure if that's true, because it would mean I'd have to buy a separate SSL certificate for each server in my cluster.\n\nWhat is a CSR? A CSR or Certificate Signing request is a block of\n  encrypted text that is generated on the server that the certificate\n  will be used on.\n",
        "top_answer": "No. It is not necessary to generate the CSR on the machine that you want to host the resulting certificate on. The CSR does need to be generated either using the existing private key that the certificate will be eventually paired with or its matching private key is generated as part of the CSR creation process.\nWhat's important is not so much the originating host but that the private key and resulting public key are a matching pair. ",
        "url": "https://serverfault.com/questions/471289/must-csrs-be-generated-on-the-server-that-will-host-the-ssl-certificate"
    },
    {
        "title": "What is the difference between unlink and rm?",
        "question": "Is unlink any faster than rm?",
        "top_answer": "Both are a wrapper to the same fundamental function which is an unlink() system call.\nTo weigh up the differences between the userland utilies.\nrm(1):\n\nMore options.\nMore feedback.\nSanity checking.\nA bit slower for single calls as a result of the above.\nCan be called with multiple arguments at the same time.\n\nunlink(1):\n\nLess sanity checking.\nUnable to delete directories.\nUnable to recurse.\nCan only take one argument at a time.\nMarginally leaner for single calls due to it's simplicity.\nSlower when compared with giving rm(1) multiple arguments.\n\nYou could demonstrate the difference with:\n$ touch $(seq 1 100)\n$ unlink $(seq 1 100)\nunlink: extra operand `2'\n\n$ touch $(seq 1 100)\n$ time rm $(seq 1 100)\n\nreal    0m0.048s\nuser    0m0.004s\nsys     0m0.008s\n\n$ touch $(seq 1 100)\n$ time for i in $(seq 1 100); do rm $i; done\n\nreal    0m0.207s\nuser    0m0.044s\nsys     0m0.112s\n\n$ touch $(seq 1 100)\n$ time for i in $(seq 1 100); do unlink $i; done\n\nreal    0m0.167s\nuser    0m0.048s\nsys     0m0.120s\n\nIf however we're talking about an unadulterated call to the system unlink(2) function, which I now realise is probably not what you're accounting for.\nYou can perform a system unlink() on directories and files alike. But if the directory is a parent to other directories and files, then the link to that parent would be removed, but the children would be left dangling. Which is less than ideal.\nEdit:\nSorry, clarified the difference between unlink(1) and unlink(2). Semantics are still going to differ between platform.",
        "url": "https://serverfault.com/questions/38816/what-is-the-difference-between-unlink-and-rm"
    },
    {
        "title": "Alternatives to Splunk?",
        "question": "I'm pretty impressed with Splunk, especially version 4. Pretty graphs, alerting (Enterprise only), and fast, accurate, searching. It's a great product.\nHowever, the cost just way too high to consider for full production use for our company. All we really need is to be able to index different logs in a central place, and have reasonable searching on that. Having alerts based on a saved search is also really nice. We don't really go beyond that.\nIn fact, our biggest usage has been in deploying new applications. Everything gets logged via log4net to either the Event log on Windows or a text file on Linux. Splunk makes it pretty easy to quickly search across those to make sure all the parts of the app are working ok -- that's saved us tons of time versus hunting down individual logging sources.\nWhat alternatives exist in this market? I have a sinking feeling Splunk's pricing is so high because they have the best product by far, and they know it. We want the server to run on Windows.\nI'd be open to a split model, using one product for general logs (collect via syslog/Snare), and a dedicated product for our custom apps (like Log4Net Dashboard). \nWould using a simple syslog server such as Kiwi, sent to SQL Server (perhaps with fulltext enabled) work? \nI'd hope the cost should be well under 5 figures, USD. (And yes, I know, we're cheap. We're a startup with little money, and BizSpark takes care of all our MS licensing.)\nEdit: I should add, we have about 10 physical servers, 20 VMs, and a couple firewalls and switches. 90% is Windows.",
        "top_answer": "Note : This is all regarding Linux and free software, as that's what I mostly use, but you should be fine with a syslog client on Windows to send the logs to a Linux syslog server.\nLogging to an SQL server:\nWith only ~30 machines, you should be fine with pretty much any centralised syslog-alike and an SQL backend.  I use syslog-ng and MySQL on Linux for this very thing.\nPretty frontends for graphing are the main problem -- It seems that there is a lot of hacked-up front-ends which will grab items from the logs and show how many hits, alerts etc but I've not found anything integrated and clean.  Admittedly this is the main thing that you're looking for... (If I find anything good then I'll update this section!)\nAlerting: I use SEC on a Linux server to find bad things happening in the logs and alert me via various methods.  It's incredibly flexible and not as clicky as Splunk.  There's a nice tutorial here which guides through a lot of the possible features.\nI also use Nagios for graphs of various stats and some alerting which I don't get from the logs (such as when services are down etc).  This can be easily customized to add graphs of anything you like.  I have added graphs of items such as the number of hits made to an http server, by having the agent use the check_logfiles plugin to count the number of hits in the logs (it saves the position it gets up to for each check period).\nOverall, it depends on how much your time will cost to set this up, as there are many options which you can use but they aren't as integrated as Splunk and will probably require more effort to get doing what you want.  The Nagios graphs are straightforward to set up but don't give you historical data from before you add the graph, whereas with Splunk (and presumably other front-ends) you can look back at the past logs and graph things you've only just thought of to look at from them.\nNote also that the SQL database format and indexing will have a huge effect on the speed of queries, so your idea of fulltext indexing will make a tremendous increase to the speed of searches.  I'm not sure if MySQL or PostgreSQL will do something similar.\nEdit : MySQL will do fulltext indexing, but  only on MyISAM tables prior to MySQL 5.6.  In 5.6 Support was added for InnoDB.\nEdit: Postgresql can do full text search of course: http://www.postgresql.org/docs/9.0/static/textsearch.html",
        "url": "https://serverfault.com/questions/62687/alternatives-to-splunk"
    },
    {
        "title": "Quickest way to transfer 55GB of images to new server",
        "question": "I currently have two CentOS servers. I need to know how and what the quickest way would be to \"tar\" up the images directory and SCP it over?\nIs that the quickest way that I just suggested, because tarring is taking forever... I ran the command:\ntar cvf imagesbackup.tar images\n\nAnd I was going to just scp it over.\nLet me know if there is a quicker way. I have remote/SSH access to both machines.",
        "top_answer": "Instead of using tar to write to your local disk, you can write directly to the remote server over the network using ssh.\nserver1$ tar -zc ./path | ssh server2 \"cat > ~/file.tar.gz\"\n\nAny string that follows your \"ssh\" command will be run on the remote server instead of the interactive logon. You can pipe input/output to and from those remote commands through SSH as if they were local. Putting the command in quotes avoids any confusion, especially when using redirection.\nOr, you can extract the tar file on the other server directly:\nserver1$ tar -zc ./path | ssh server2 \"tar -zx -C /destination\"\n\nNote the seldom-used -C option. It means \"change to this directory first before doing anything.\"\nOr, perhaps you want to \"pull\" from the destination server:\nserver2$ tar -zx -C /destination < <(ssh server1 \"tar -zc -C /srcdir ./path\")\n\nNote that the <(cmd) construct is new to bash and doesn't work on older systems. It runs a program and sends the output to a pipe, and substitutes that pipe into the command as if it was a file.\nI could just have easily have written the above as follows:\nserver2$ tar -zx -C /destination -f <(ssh server1 \"tar -zc -C /srcdir ./path\")\n\nOr as follows:\nserver2$ ssh server1 \"tar -zc -C /srcdir ./path\" | tar -zx -C /destination\n\nOr, you can save yourself some grief and just use rsync:\nserver1$ rsync -az ./path server2:/destination/\n\nFinally, remember that compressing the data before transfer will reduce your bandwidth, but on a very fast connection, it may actually make the operation take more time. This is because your computer may not be able to compress fast enough to keep up: if compressing 100MB takes longer than it would take to send 100MB, then it's faster to send it uncompressed. \nAlternately, you may want to consider piping to gzip yourself (rather than using the -z option) so that you can specify a compression level. It's been my experience that on fast network connections with compressible data, using gzip at level 2 or 3 (the default is 6) gives the best overall throughput in most cases. Like so:\nserver1$ tar -c ./path | gzip -2 | ssh server2 \"cat > ~/file.tar.gz\"\n",
        "url": "https://serverfault.com/questions/208300/quickest-way-to-transfer-55gb-of-images-to-new-server"
    },
    {
        "title": "How to capture ack or syn packets by Tcpdump?",
        "question": "I want to use a filter rule to capture only ack or syn packets. How do I do this?",
        "top_answer": "The pcap filter syntax used for tcpdump should work exactly the same way on wireshark capture filter.\nWith tcpdump I would use a filter like this.\ntcpdump \"tcp[tcpflags] & (tcp-syn|tcp-ack) != 0\"\n\nCheck out the tcpdump man page, and pay close attention to the tcpflags.\nBe sure to also check out the sections in the Wireshark Wiki about capture and display filters.  Unfortunately the two types of filters use a completely different syntax, and different names for the same thing.\nIf you wanted a display filter instead of capture filter you would probably need to build an expression combining tcp.flags.ack, and tcp.flags.syn.  I am far more familiar with capture filters though, so you'll have to work that out on your own.\n\nhttp://wiki.wireshark.org/DisplayFilters\n\nDisplay filter ref: http://www.wireshark.org/docs/dfref/\nTCP display ref: http://www.wireshark.org/docs/dfref/t/tcp.html\n\nhttp://wiki.wireshark.org/CaptureFilters\n",
        "url": "https://serverfault.com/questions/217605/how-to-capture-ack-or-syn-packets-by-tcpdump"
    },
    {
        "title": "Why are CA root certificates all SHA-1 signed (since SHA-1 is deprecated)?",
        "question": "I understand that SSL certs cannot be signed using SHA-1 anymore. Yet, all CA root certificates are SHA-1 signed (mostly). Does it mean the same algorithm that is no longer trusted for \"you grandma SSL shop\" is fine for the uttermost top secured certificate of the world?\nAm I missing something?  (key usage? key size?)",
        "top_answer": "The signature of the root CA certificates do not matter at all, since there is no need to verify them. They are all self-signed.\nIf you trust a root CA certificate, there\u2019s no need to verify its signature. If you don\u2019t trust it, its signature is worthless for you.\nEdit: there are some very relevant comments below. I don\u2019t feel comfortable copying or rephrasing them and taking credit for them instead of their authors. But I welcome people to add explanations to this answer.",
        "url": "https://serverfault.com/questions/837994/why-are-ca-root-certificates-all-sha-1-signed-since-sha-1-is-deprecated"
    },
    {
        "title": "How to remove missing systemd units?",
        "question": "I'm having trouble figuring out how to remove systemd units that no longer have files. They still seem to linger in the system somehow.\nThe old broken units I am trying to remove:\ncore@ip-172-16-32-83 ~ $ systemctl list-units --all firehose-router*\n  UNIT                       LOAD      ACTIVE SUB    DESCRIPTION\n<E2><97><8F> [email\u00a0protected] not-found failed failed [email\u00a0protected]\n<E2><97><8F> [email\u00a0protected] not-found failed failed [email\u00a0protected]\n\nLOAD   = Reflects whether the unit definition was properly loaded.\nACTIVE = The high-level unit activation state, i.e. generalization of SUB.\nSUB    = The low-level unit activation state, values depend on unit type.\n\n2 loaded units listed.\nTo show all installed unit files use 'systemctl list-unit-files'.\n\nThe files do not exist, yet a reload still has these units lingering:\ncore@ip-172-16-32-83 ~ $ systemctl list-unit-files [email\u00a0protected]\ncore@ip-172-16-32-83 ~ $ sudo systemctl daemon-reload\ncore@ip-172-16-32-83 ~ $ systemctl list-units --all firehose-router*\n  UNIT                       LOAD      ACTIVE SUB    DESCRIPTION\n<E2><97><8F> [email\u00a0protected] not-found failed failed [email\u00a0protected]\n<E2><97><8F> [email\u00a0protected] not-found failed failed [email\u00a0protected]\n\nLOAD   = Reflects whether the unit definition was properly loaded.\nACTIVE = The high-level unit activation state, i.e. generalization of SUB.\nSUB    = The low-level unit activation state, values depend on unit type.\n\n2 loaded units listed.\nTo show all installed unit files use 'systemctl list-unit-files'.\n\nThere are no files related to them that I can find:\ncore@ip-172-16-32-83 ~ $ sudo find /var/run/systemd -name \"*firehose-router*\"\ncore@ip-172-16-32-83 ~ $ find /etc/systemd/ -name \"*firehose-router*\"\ncore@ip-172-16-32-83 ~ $ find /usr/lib/systemd/ -name \"*firehose-router*\"\ncore@ip-172-16-32-83 ~ $\n\nSo how do I get rid of these?",
        "top_answer": "The command you're after is systemctl reset-failed",
        "url": "https://serverfault.com/questions/606520/how-to-remove-missing-systemd-units"
    },
    {
        "title": "How do I tell when/if/why a container in a kubernetes cluster restarts?",
        "question": "I have a single node kubernetes cluster in google container engine to play around with.\nTwice now, a small personal website I host in it has gone offline for a couple minutes.  When I view the logs of the container, I see the normal startup sequence recently completed, so I assume a container died (or was killed?) and restarted.\nHow can I figure out the how & why of this happening?\nIs there a way to get an alert whenever a container starts/stops unexpectedly?",
        "top_answer": "You can view the last restart logs of a container using:\n\nkubectl logs podname -c containername --previous\n\nAs described by Sreekanth, kubectl get pods should show you number of restarts, but you can also run\n\nkubectl describe pod podname\n\nAnd it will show you events sent by the kubelet to the apiserver about the lifecycled events of the pod.\nYou can also write a final message to /dev/termination-log, and this will show up as described in the docs.",
        "url": "https://serverfault.com/questions/727104/how-do-i-tell-when-if-why-a-container-in-a-kubernetes-cluster-restarts"
    },
    {
        "title": "Why is the response on localhost so slow?",
        "question": "I am working on a tiny little PHP project for a friend of mine, and I have a WAMP environment setup for local development. I remember the days when the response from my local Apache 2.2 was immediate. Alas, now that I got back from a long, long holiday, I find the responses from localhost painfully slow.\nIt takes around 5 seconds to get a 300B HTML page served out.\nWhen I look at the task manager, the httpd processes (2) are using up 0% of the CPU and overall my computer is not under load (0-2% CPU usage).\nWhy is the latency so high? Is there any Apache setting that I could tweak to perhaps make its thread run with a higher priority or something? It seems like it's simply sleeping  before it's serving out the response.",
        "top_answer": "For me, setting the ServerName property in httpd.conf fixed the delays (they were up to 10 seconds at worst):\n# ServerName gives the name and port that the server uses to identify itself.\n# This can often be determined automatically, but we recommend you specify\n# it explicitly to prevent problems during startup.\n#\n# If your host doesn't have a registered DNS name, enter its IP address here.\nServerName 127.0.0.1:80\n",
        "url": "https://serverfault.com/questions/66347/why-is-the-response-on-localhost-so-slow"
    },
    {
        "title": "Do you have any useful awk and grep scripts for parsing apache logs? [closed]",
        "question": "I can use log analyzers, but often I need to parse recent web logs to see what's happening at the moment. \nI sometimes do things like to figure out top 10 ips that request a certain file\ncat foo.log | grep request_to_file_foo | awk '{print $1}' |  sort -n | uniq -c | sort -rn | head\n\nWhat do you have in your toolbox?",
        "top_answer": "You can do pretty much anything with apache log files with awk alone. Apache log files are basically whitespace separated, and you can pretend the quotes don't exist, and access whatever information you are interested in by column number. The only time this breaks down is if you have the combined log format and are interested in user agents, at which point you have to use quotes (\") as the separator and run a separate awk command. The following will show you the IPs of every user who requests the index page sorted by the number of hits:\nawk -F'[ \"]+' '$7 == \"/\" { ipcount[$1]++ }\n    END { for (i in ipcount) {\n        printf \"%15s - %d\\n\", i, ipcount[i] } }' logfile.log\n\n$7 is the requested url. You can add whatever conditions you want at the beginning. Replace the '$7 == \"/\" with whatever information you want.\nIf you replace the $1 in (ipcount[$1]++), then you can group the results by other criteria. Using $7 would show what pages were accessed and how often. Of course then you would want to change the condition at the beginning. The following would show what pages were accessed by a user from a specific IP:\nawk -F'[ \"]+' '$1 == \"1.2.3.4\" { pagecount[$7]++ }\n    END { for (i in pagecount) {\n        printf \"%15s - %d\\n\", i, pagecount[i] } }' logfile.log\n\nYou can also pipe the output through sort to get the results in order, either as part of the shell command, or also in the awk script itself:\nawk -F'[ \"]+' '$7 == \"/\" { ipcount[$1]++ }\n    END { for (i in ipcount) {\n        printf \"%15s - %d\\n\", i, ipcount[i] | sort } }' logfile.log\n\nThe latter would be useful if you decided to expand the awk script to print out other information. It's all a matter of what you want to find out. These should serve as a starting point for whatever you are interested in.",
        "url": "https://serverfault.com/questions/11028/do-you-have-any-useful-awk-and-grep-scripts-for-parsing-apache-logs"
    },
    {
        "title": "How should an IT department choose a standard Linux distribution?",
        "question": "There is a lot of community feeling about what Linux distributions are appropriate for production server environments and which aren't, however, a lot of this feeling seems religiously based, and seldom presented with supporting evidence.\nAssuming that we were trying to select a Linux distribution to standardize on (because we have an interest in keeping our environments as homogeneous as possible), what criteria are important, and how do you make determinations about how well different distributions meet those criteria?",
        "top_answer": "I currently work in an environment that has used Linux for more than a decade.  Everybody in the office uses different distros on their desktops as well as the servers.  As such, the choices of distribution tend to revolve around a number of things in no particular order:\n\nHistory - Obviously systems like RedHat and Debian have been around for a long time.  As such, the adage \"if it ain't broke, don't fix it\" can be used for these.  Upgrading becomes easier if the software is supported well on a distro.\nFamiliarity - Similar to History, however we all have our favourites.  I cut my teeth on Debian, and migrated to Ubuntu (a hard decision at the time because I tend to commit to a community).  Conversely, it's a pain to have to remember how to do things on a dozen different distros (not to mention the scratch-built ones).\nSupport - I migrated to Ubuntu mainly because I appreciated what they were doing as far as offering paid support.  That was a selling point if ever a client had a concern about running a system long-term.  Similar to RedHat's approach (but RPM hell was going on at the time).  We have a number of RedHat servers for this reason also.\nDependencies - Some softwares are easier to use on some distros simply because the dependent packages are more easily obtainable or buildable.  As example of this would be oVirt on RedHat.  There are no packages for some softwares on some distros.  And you could compile it, but why would you if the package was right there on another distro?\nGranularity - Distros like Gentoo offer finer control over versioning and software-switch granularity.  Other distros have \"pinning\" in various forms, but that's still not as controllable or reliable.\nBinding - While it's possible to compile from source on most distros, some distros are better at it than others.  This can have an effect, say, if your project patches existing libraries for extended functionality.\nPrettiness - Some distros are just better-looking.  Every geek knows it's just fluff (and you could probably get away with doing it as a web app these days) but some clients are wowed by this stuff, and we all know it.\nStability - Some distros stream \"stable\" versions of software as opposed to \"testing\", \"experimental\", etc.  This can mean alot if you know that the version you're building on will eventually reach a consensus on stability.  You may develop on \"experimental\" knowing that by the time your project is finished it will have reached \"stable\" and be good to rely on.\nPackage management - If you're developing something on a daily basis, and it's going to go out to 1000s of machines in one hit, then you probably want something that makes it easy to build, maintain, and track packages across those systems.\nConsistency - This is more an argument for the same distro.  Less mistakes get made (and less errors in security) when people can focus on one distro as opposed to several.\nPredictable release schedule - If you want to be sure that your software stays supported, planned upgrades offer a certain type of stability.\nSecurity - Some distros have active security teams whose job it is to respond immediately to genuine security risks in any approved package.\n\nThose are just a few things that come off the top of my head regarding reasons why each system was chosen.  I don't see any one guiding light or preference of one distro over another in this decision.  Diversity and choice can be great and offer you some really good options to get a project started quickly, but it's also the noose that can hang you.  Make sure you think ahead of what you're going to need.  Plan what the system's needs are as well as when the system is going to be upgraded or retired.  Don't assume you'll always be the one maintaining it.",
        "url": "https://serverfault.com/questions/461271/how-should-an-it-department-choose-a-standard-linux-distribution"
    },
    {
        "title": "How to correct Postfix' 'Relay Access Denied'?",
        "question": "This morning, in order to correct a problem with a name mismatch in the security certificate, I followed the recommended steps from How to fix mail server SSL?, but now, when attempting to send an email from a client (in this case the client is Windows Mail), I receive the following error.\n\nThe rejected e-mail address was\n  '[email\u00a0protected]'. Subject 'This is a\n  test. ', Account: 'mail.domain.com',\n  Server: 'mail.domain.com', Protocol:\n  SMTP, Server Response: '554 5.7.1\n  : Relay access\n  denied', Port: 25, Secure(SSL): No,\n  Server Error: 554, Error Number:\n  0x800CCC79\n\nEdit: I can still retrieve emails from this account, and I send emails to other accounts at the same domain. I just can't send emails to recipients outside of our domain.\nI tried disabling TLS altogether but no dice, I still get the same error.\nWhen I check file mail.log, I see the following.\nJul 18 08:24:41 company imapd: LOGIN, [email\u00a0protected], ip=[::ffff:111.111.11.11], protocol=IMAP\nJul 18 08:24:42 company imapd: DISCONNECTED, [email\u00a0protected], ip=[::ffff:111.111.11.11], headers=0, body=0, rcvd=83, sent=409, time=1\nJul 18 08:25:19 company postfix/smtpd[29282]: connect from company.university.edu[111.111.11.11]\nJul 18 08:25:19 company postfix/smtpd[29282]: NOQUEUE: reject: RCPT from company.university.edu[111.111.11.11]: 554 5.7.1 <[email\u00a0protected]>: Relay access denied; from=<[email\u00a0protected]> to=<[email\u00a0protected]> proto=ESMTP helo=<UserPC>\nJul 18 08:25:19 company postfix/smtpd[29282]: disconnect from company.university.edu[111.111.11.11]\nJul 18 08:25:22 company imapd: DISCONNECTED, [email\u00a0protected], ip=[::ffff:111.111.11.11], headers=13, body=142579, rcvd=3289, sent=215892, time=79\n\nFile main.cf looks like this:\n#\n# Postfix MTA Manager Main Configuration File;\n#\n# Please do NOT edit this file manually;\n#\n\n#\n# Postfix directory settings; These are critical for normal Postfix MTA functionallity;\n#\n\ncommand_directory = /usr/sbin\ndaemon_directory = /usr/lib/postfix\nprogram_directory = /usr/lib/postfix\n\n#\n# Some common configuration parameters;\n#\n\ninet_interfaces = all\nmynetworks = 127.0.0.0/8\nmynetworks_style = host\n\nmyhostname = mail.domain.com\nmydomain = domain.com\nmyorigin = $mydomain\n\nsmtpd_banner = $myhostname ESMTP 2.4.7.1 (Debian/GNU)\nsetgid_group = postdrop\n\n#\n# Receiving messages parameters;\n#\n\nmydestination = localhost, company \nappend_dot_mydomain = no\nappend_at_myorigin = yes\ntransport_maps = mysql:/etc/postfix/transport.cf\n\n#\n# Delivering local messages parameters;\n#\n\nmail_spool_directory = /var/spool/mail\nmailbox_size_limit = 0\nmailbox_command = procmail -a \"$EXTENSION\"\n\nbiff = no\n\nalias_database = hash:/etc/aliases\n\nlocal_recipient_maps =\n\n#\n# Delivering virtual messages parameters;\n#\nvirtual_mailbox_maps=mysql:/etc/postfix/mysql_virt.cf\nvirtual_uid_maps=mysql:/etc/postfix/uids.cf\nvirtual_gid_maps=mysql:/etc/postfix/gids.cf\nvirtual_mailbox_base=/usr/local/virtual\nvirtual_maps=mysql:/etc/postfix/virtual.cf\nvirtual_mailbox_domains=mysql:/etc/postfix/virtual_domains.cf\n\n\n#\n# SASL paramters;\n#\nsmtp_use_tls = yes\nsmtpd_use_tls = yes\nsmtpd_tls_auth_only = yes\nsmtpd_tls_loglevel = 1\nsmtpd_tls_received_header = yes\nsmtpd_tls_session_cache_timeout = 3600s\n\nsmtp_tls_CAfile = /etc/postfix/ssl/smptd.pem\nsmtp_tls_cert_file = /etc/postfix/ssl/smptd.crt\nsmtp_tls_key_file = /etc/postfix/ssl/smptd.key\n\nsmtpd_tls_CAfile = /etc/postfix/ssl/smptd.pem\nsmtpd_tls_cert_file = /etc/postfix/ssl/smptd.crt\nsmtpd_tls_key_file = /etc/postfix/ssl/smptd.key\n\nsmtpd_sasl_auth_enable = yes\n\nsmtpd_sasl_security_options = noanonymous\n\nsmtpd_sasl_local_domain =\n\nbroken_sasl_auth_clients = yes\n\nsmtpd_sender_restrictions =\n        permit_sasl_authenticated\n        permit_mynetworks\n\nsmtpd_recipient_restrictions =\n        permit_sasl_authenticated\n        check_recipient_access hash:/etc/postfix/filtered_domains\n        permit_mynetworks\n        reject_unauth_destination\n\nAs a side note, my employer wants to be able to send emails from clients (Thunderbird and Outlook) both from within our local network and outside it. ",
        "top_answer": "TLS just enables encryption on the smtp session and doesn't directly affect whether or not Postfix will be allowed to relay a message.\nThe relaying denied message occurs because the smtpd_recipient_restrictions rules was not matched.  One of those conditions must be fulfilled to allow the message to go through:\nsmtpd_recipient_restrictions =\n    permit_sasl_authenticated\n    check_recipient_access hash:/etc/postfix/filtered_domains\n    permit_mynetworks\n    reject_unauth_destination\n\nTo explain those rules:\npermit_sasl_authenticated\n\npermits authenticated senders through SASL.  This will be necessary to authenticate users outside of your network which are normally blocked.\ncheck_recipient_access\n\nThis will cause postfix to look in /etc/postfix/filtered_domains for rules based on the recipient address.  (Judging by the file name, it is probably just blocking specific domains... Check to see if gmail.com is listed in there?)\npermit_mynetworks\n\nThis will permit hosts by IP address that match IP ranges specified in $mynetworks.  In the main.cf you posted, $mynetworks was set to 127.0.0.1, so it will only relay emails generated by the server itself.\nBased on that configuration, your mail client will need to use SMTP Authentication before being allowed to relay messages.  I'm not sure what database SASL is using.  That is specified in /usr/lib/sasl2/smtpd.conf.   Presumably it also uses the same database as your virtual mailboxes, so you should be able to enable SMTP authentication in your mail client and be all set.",
        "url": "https://serverfault.com/questions/42519/how-to-correct-postfix-relay-access-denied"
    },
    {
        "title": "How to send the output from a cronjob to multiple e-mail addresses?",
        "question": "In the beginning of a crontab file you could use the MAILTO instruction to indicate you want the output to be sent as an e-mail to an e-mail address. I would like to send the output to multiple addresses. Is it possible (and how) to specify multiple addresses?",
        "top_answer": "It may differ depending exactly which cron daemon package you use, but this is from the manpage of Vixie Cron on Ubuntu Hardy:\n\nIf MAILTO is defined (and non-empty),\n  mail is sent to the user so named. \n  MAILTO may also be used to direct mail\n  to multiple recipients by separating \n  recipient  users with a comma. If\n  MAILTO is defined but empty\n  (MAILTO=\"\"), no mail will be sent. \n  Otherwise mail is sent to the owner of\n  the crontab.\n\nIf you're not using Vixie Cron, or aren't sure, try the manual page for the crontab file: man 5 crontab\nExample\nMAILTO=\"[email\u00a0protected],[email\u00a0protected]\"\n",
        "url": "https://serverfault.com/questions/133058/how-to-send-the-output-from-a-cronjob-to-multiple-e-mail-addresses"
    },
    {
        "title": "iptables equivalent for mac os x",
        "question": "I want to forward requests from 192.168.99.100:80 to 127.0.0.1:8000. This is how I'd do it in linux using iptables:\niptables -t nat -A OUTPUT -p tcp --dport 80 -d 192.168.99.100 -j DNAT --to-destination 127.0.0.1:8000\n\nHow do I do the same thing in MacOS X? I tried out a combination of ipfw commands without much success:\nipfw add fwd 127.0.0.1,8000 tcp from any to 192.168.99.100 80\n\n(Success for me is pointing a browser at http://192.168.99.100 and getting a response back from a development server that I have running on localhost:8000)",
        "top_answer": "So I found out a way to do this. I'm not sure if it's the preferred way but it works! At your favourite shell:\nsudo ifconfig lo0 10.0.0.1 alias\nsudo ipfw add fwd 127.0.0.1,9090 tcp from me to 10.0.0.1 dst-port 80\n\n(The alias to lo0 seems to be the missing part)\nIf you'd like a (fake) domain to point to this new alias then make sure /etc/hosts contains the line:\n10.0.0.1 www.your-domain.com\n",
        "url": "https://serverfault.com/questions/102416/iptables-equivalent-for-mac-os-x"
    },
    {
        "title": "What does \"Error: Cycle\". means in Terraform?",
        "question": "The Terraform docs for some weird reason do not explain what \"Error: Cycle\" means. I've looked everywhere but there is no mention of it on the official docs. (Turns out it is well-known term, a circular dependency, that someone apparently renamed thinking it would make them sound cool...)",
        "top_answer": "As part of Terraform's work, it analyzes the dependencies between your resource blocks, data blocks, and other configuration constructs in order to determine a suitable order to process them so that the necessary input data will be available.\nFor example, consider the following contrived simple configuration:\nresource \"null_resource\" \"foo\" {\n}\n\nresource \"null_resource\" \"bar\" {\n  triggers = {\n    foo = null_resource.foo.id\n  }\n}\n\nTerraform will analyze the above and notice that the configuration of null_resource.bar contains a reference to null_resource.foo, and therefore the operations related to null_resource.foo must happen before null_resource.bar. We can visualize that as a graph where an arrow represents \"must happen after\", or \"depends on\":\n\n\nOperations for null_resource.bar must happen after operations for null_resource.foo.\n\n\nConsider now what happens if we modify that configuration like this:\nresource \"null_resource\" \"foo\" {\n  triggers = {\n    bar = null_resource.bar.id\n  }\n}\n\nresource \"null_resource\" \"bar\" {\n  triggers = {\n    foo = null_resource.foo.id\n  }\n}\n\nNow null_resource.foo also refers to null_resource.bar. There are now two \"must happen after\" relationships implied by this configuration:\n\n\nOperations for null_resource.bar must happen after operations for null_resource.foo.\nOperations for null_resource.foo must happen after operations for null_resource.bar.\n\nThe two statements above contradict one another: null_resource.bar cannot be processed both before and after null_resource.foo. Terraform will respond to this situation by reporting a dependency cycle, using the error message you've seen:\nCycle: null_resource.foo, null_resource.bar\n\nWhen Terraform returns this error, the solution is to remove at least one of the \"must happen after\" arrows (dependencies) from the configuration so that it's no longer contradictory. Without seeing your configuration I can't suggest what specific change might achieve that in your case, but it's likely that somewhere in your configuration you have two mutually-dependent resources like this, or perhaps a resource referring to itself.\n\nIf you are sighted, depending on how complicated your configuration is, it might help to ask Terraform to produce a graph similar to the ones I've included above in this answer, but highlighting the cycles. To do this, you can use the terraform graph command, like this:\nterraform graph -draw-cycles\n\nThe output of this command is a description of the graph in the format accepted by Graphviz. If you don't have Graphviz installed on your local computer, you could instead copy-paste the output into Graphviz Online to produce a graph image. The -draw-cycles command causes Terraform to mark the arrows that are related to the cycle being reported using the color red. If you cannot visually distinguish red from black, you may wish to first edit the generated Graphviz code to replace red with some other color you can distinguish.\nThe graph visualization of the configuration tends to become unusable for non-trivial configurations because there are so many graphs and edges, so if your configuration has many objects it may be better to follow the dependencies through the configuration itself.",
        "url": "https://serverfault.com/questions/1005761/what-does-error-cycle-means-in-terraform"
    },
    {
        "title": "Capturing STDERR and STDOUT to a file using 'tee'",
        "question": "I'm unclear what the best order is to capture both STDERR and STDOUT to the same file using tee. I know that if I want to pipe to a file I have to map the file handle after the redirect, i.e.,\nfind . >/tmp/output.txt 2>&1\n\nThis instructs the shell to send STDOUT to /tmp/output.txt and then to send STDERR to STDOUT (which is now sending to /tmp/output.txt).\nAttempting to perform the 2>&1 before redirecting the file will not have the desired effect.\nHowever, when I want to pipe using tee should it be:\nfind . |tee /tmp/output.txt 2>&1   # or\nfind . 2>&1 |tee /tmp/output.txt   # ?\n",
        "top_answer": "The latter (find . 2>&1 | tee /tmp/output.txt); it makes sure STDOUT and STDERR of the original command go to the same fd, and then feeds them jointly into tee.  In the former case, it's the STDERR of the tee command that you'd be joining with its STDOUT.",
        "url": "https://serverfault.com/questions/201061/capturing-stderr-and-stdout-to-a-file-using-tee"
    },
    {
        "title": "NMAP: Check if port 80 and 8080 is open",
        "question": "In our company, I want to check if users are running web servers on port 80 and 8080.\nI downloaded nmap and ran this command:\nnmap -p 80,8080 192.168.1.0-255\n\nI got a list of IPs and tried to access them in my browser (EG: 192.168.1.1:8080) but wasn't able to connect.\nIs my nmap command correct?\nThanks!\nEDIT:\nHere's the response:\nHost is up (0.050s latency).\nPORT     STATE SERVICE\n80/tcp   open  http\n8080/tcp open  http-proxy\n",
        "top_answer": "Your NMAP command is fine. It's the service you're trying to connect to that is the \"problem\". \nYou can find that a \"server\" has an open port on 80 or 8080 but still not be able to connect it.  For instance, I have about three dozen polycom phones that are accessible at port 8080 but they have bum config files.  When someone tries to access them at that port they aren't able to connect.  \nAnd it may depend on how you're trying to connect.  Even though its port 8080 maybe you aren't supposed to use a browser to connect to it.(I know - crazy idea).\nFind out what type of device you're trying to connect to using this (you'll need sudo or root):\nnmap -sS -O -p80,8080 192.168.1.0/24\n",
        "url": "https://serverfault.com/questions/115063/nmap-check-if-port-80-and-8080-is-open"
    },
    {
        "title": "How to force MySQL to connect by TCP instead of a Unix socket?",
        "question": "I would like to analyze mysql traffic. Right now, all mysql requests are sent to the MySQL unix socket:\nunix  2      [ ACC ]     STREAM     LISTENING     3734388  15304/mysqld        /var/run/mysqld/mysqld.sock\n\nI'm trying to disable that socket to force MySQL to use the network socket instead on the loopback. I tried commenting out all the socket directives in the my.cnf and debian.cnf files and restarted MySQL but it made no difference.\nHow can I disable the MySQL unix socket to force MySQL over the network?\nadditional info: I'm running MySQL 5.1 on ubuntu 10.04.\nPrecisions on the question\nSince plenty of people suggested enabling the network socket I would like to clarify my question by pointing out that the bind address was already enabled with bind-address = 127.0.0.1 and that a listening connection is available:\ntcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      15601/mysqld \n\nStill I see no connections attempt to 127.0.0.1:3306 coming from my webapp (Drupal website).\nUpdated with the answer\nIt appears indeed that the issue is coming from the mysqli connector that Drupal uses (in .ht_config.php for those who are interested). It was set:  mysqli://drupal:***@localhost/drupal, changing localhost to 127.0.0.1 fixed the issue (i.e. Drupal is now making connections to the network socket).",
        "top_answer": "In Linux and other *nixes, MySQL will assume you want to use a socket (Unix domain socket) if you connect to the host \"localhost\" (which would be the default hostname).\nYou can override this in 3 ways:\n\nSpecify a different hostname like 127.0.0.1 (mysql -h 127.0.0.1) or your server's real hostname\nSpecify that you want to use TCP and not a socket (mysql --protocol tcp)\n\nYou can also easily make that the default my editing your my.cnf so it has this ([client] means any client:\n[client]\nprotocol=tcp\n\nYou can see the full description of how MySQL decides how to connect here:\nhttp://dev.mysql.com/doc/refman/5.5/en/connecting.html",
        "url": "https://serverfault.com/questions/337818/how-to-force-mysql-to-connect-by-tcp-instead-of-a-unix-socket"
    },
    {
        "title": "Deleting All Partitions From the Command Line",
        "question": "How do you delete all partitions on a device from the command line on Linux (specifically Ubuntu)? I tried looking at fdisk, but it presents an interactive prompt. I'm looking for a single command, which I can give a device path (e.g. /dev/sda) and it'll delete the ext4, linux-swap, and whatever other partitions it finds. Essentially, this would be the same thing as if I were to open GParted, and manually select and delete all partitions. This seems fairly simple, but unfortunately, I haven't been able to find anything through Google.",
        "top_answer": "Would this suffice?\ndd if=/dev/zero of=/dev/sda bs=512 count=1 conv=notrunc\n",
        "url": "https://serverfault.com/questions/250839/deleting-all-partitions-from-the-command-line"
    },
    {
        "title": "DNS A vs NS record",
        "question": "I'm trying to understand DNS a bit better, but I still don't get A and NS records completely.\nAs far as I understood, the A record tells which IP-address belongs to a (sub) domain, so far it was still clear to me. But as I understood, the NS record tells which nameserver points belongs to a (sub) domain, and that nameserver should tell which IP-address belongs to a (sub) domain. But that was already specified in the A record in the same DNS file. So can someone explain to me what the NS records and nameservers exactly do, because probably I understood something wrong.\nedit: As I understand you correctly, a NS record tells you were to find the DNS server with the A record for a certain domain, and the A record tells you which ip-address belongs to a domain. But what is the use of putting an A and an NS record in the same DNS file? If there is already an A record for a certain domain, then why do you need to point to another DNS server, which would probably give you the same information?",
        "top_answer": "Some examples out of the fictitious foo.com zone file\n ....... SOA record & lots more stuff .......\n foo.com.      IN        NS        ns1.bar.com.\n\n foo.com.      IN        A         192.168.100.1\n ....... More A/CNAME/AAAA/etc. records .......\n\nA Record  = \"The host called foo.com lives at address 192.168.100.1\"\nNS Record = \"If you want to know about hosts in the foo.com zone, ask the name server ns1.bar.com\"",
        "url": "https://serverfault.com/questions/224920/dns-a-vs-ns-record"
    },
    {
        "title": "Apache Default/Catch-All Virtual Host?",
        "question": "If I have 3 domains, domain1.com, domain2.com, and domain3.com, is it possible to set up a default virtual host to domains not listed? For example, if I would have:\n<VirtualHost 192.168.1.2 204.255.176.199>\nDocumentRoot /www/docs/domain1\nServerName domain1\nServerAlias host\n</VirtualHost>\n\n<VirtualHost 192.168.1.2 204.255.176.199>\nDocumentRoot /www/docs/domain2\nServerName domain2\nServerAlias host\n</VirtualHost>\n\n<VirtualHost 192.168.1.2 204.255.176.199>\nDocumentRoot /www/docs/everythingelse\nServerName *\nServerAlias host\n</VirtualHost>\n\nIf you register a domain and point it to my server, it would default to everythingelse showing the same as domain3. Is that possible?",
        "top_answer": "Yes, that should work, except ServerAlias should be \"*\", with ServerName set to an actual hostname. You might need to make sure that VirtualHost is the very last loaded... ",
        "url": "https://serverfault.com/questions/82306/apache-default-catch-all-virtual-host"
    },
    {
        "title": "xvda1 is 100% full, What is it? how to fix?",
        "question": "I'm running a Linux instance on EC2 (I have MongoDB and node.js installed) and I'm getting this error:\nCannot write: No space left on device\n\nI think I've tracked it down to this file, here is the df output\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/xvda1             1032088   1032088         0 100% /\n\nThe problem is, I don't know what this file is and I also don't know if this file is even the problem.\nSo my question is: How do I fix the \"No space left on device\" error?",
        "top_answer": "That file, / is your root directory. If it's the only filesystem you see in df, then it's everything. You have a 1GB filesystem and it's 100% full. You can start to figure out how it's used like this:\nsudo du -x / | sort -n | tail -40\n\nYou can then replace / with the paths that are taking up the most space. (They'll be at the end, thanks to the sort. The command may take awhile.)",
        "url": "https://serverfault.com/questions/330532/xvda1-is-100-full-what-is-it-how-to-fix"
    },
    {
        "title": "Forcing the from address when postfix relays over smtp",
        "question": "I'm trying to get email reports from our AWS EC2 instances.  We're using Exchange Online (part of Microsoft Online Services).  I've setup a user account specifically for SMTP relaying, and I've setup Postfix to meet all the requirements to relay messages through this server.  However, Exchange Online's SMTP server will reject messages unless the From address exactly matches the authentication address (the error message is 550 5.7.1 Client does not have permissions to send as this sender).\nWith careful configuration, I can setup my services to send as this user.  But I'm not a huge fan of being careful - I'd rather have postfix force the issue.  Is there a way to do this?",
        "top_answer": "This is how to really do it in postfix.\nThis config changes sender addresses from both local originated, and relayed SMTP mail traffic:\n/etc/postfix/main.cf:\nsender_canonical_classes = envelope_sender, header_sender\nsender_canonical_maps =  regexp:/etc/postfix/sender_canonical_maps\nsmtp_header_checks = regexp:/etc/postfix/header_check\n\nRewrite envelope address from email originating from the server itself\n/etc/postfix/sender_canonical_maps:\n/.+/    [email\u00a0protected]\n\nRewrite from address in SMTP relayed e-mail\n/etc/postfix/header_check:\n/From:.*/ REPLACE From: [email\u00a0protected]\n\nThats very useful if you're for instance using a local relay smtp server which is used by all your multifunctionals and several applications.\nIf you use Office 365 SMTP server, any mail with a different sender address than the email from the authenticated user itself will simply be denied. The above config prevents this.",
        "url": "https://serverfault.com/questions/147921/forcing-the-from-address-when-postfix-relays-over-smtp"
    },
    {
        "title": "How can I choose between using my ISP's DNS, or Google's 8.8.8.8? [closed]",
        "question": "It seems like a good idea to use Google's public DNS 8.8.8.8 and 8.8.4.4 because it's really fast -- much faster than my own ISP's DNS! -- and probably more reliable, too. That seems like a ridiculously quick win for me, and much easier to remember.\nAssuming we're not all \"tin foil hat\" about Google, why shouldn't everybody use Google DNS?  How can I determine which DNS server would be the fastest, most reliable, or what would generally be considered the best?\nNote: I've seen this question, but I don't want a comparison to OpenDNS. This is about everyday use by everyday people in their homes.\nUpdate: I seem to have put my hand in a wasps' nest of privacy concerns. I appreciate the issue, but I was expecting a more technology-oriented discussion...",
        "top_answer": "There is a useful tool that test the different DNS nameservers available (your ISP, current configuration, DynDNS, Google Public DNS and other one).\nFrom my point of view Google DNS are pretty fast but depending on the load GoogleDNS supports my ISP Dns is sometimes faster.\nNameBench (Linux/Windows/Mac OS X)\nOutput : \n\n(source: googlecode.com) ",
        "url": "https://serverfault.com/questions/169279/how-can-i-choose-between-using-my-isps-dns-or-googles-8-8-8-8"
    },
    {
        "title": "The right way to keep docker container started when it used for periodic tasks",
        "question": "I have docker container with installed and configured software.\nThere is no any programm supposed to be started/runned all the time.\nWhat I want - its ability to start some command depending on external events. like:\ndocker exec mysupercont /path/to/mycommand -bla -for\n\nand \ndocker exec mysupercont /path/to/myothercommand \n\nBut \"exec\" impossible when container is stopped, and also this container have some \"working\" data inside, which used for that commands, so I can't use\ndocker run ...\n\neach time, because it recreate container from image and destroy my data.\nWhat is the \"right\" and the \"best\" way to keep such container runned? \nWhich command I can start inside?",
        "top_answer": "You do not need to perform each time docker run.\ndocker run is actually a sequence of two commands: \"create\" and \"start\".\nWhen you run the container, you must specify the \"-it\":\n\n-i, --interactive=false    Keep STDIN open even if not attached\n  -t, --tty=false            Allocate a pseudo-TTY\n\nExample:\ndocker run -it debian:stable bash\n\nAfter the work was completed command specified at startup (in my example bash). For example, you perform the \"exit\". Container stops:\nCONTAINER ID        IMAGE                      COMMAND                CREATED             STATUS                     PORTS               NAMES\n1329c99a831b        debian:stable              \"bash\"                 51 seconds ago      Exited (0) 1 seconds ago                       goofy_bardeen\n\nNow you can start it again\ndocker start 1329c99a831b\n\nThe container is started and again executes the command \"bash\".\nConnect to this session \"bash\" with the command\ndocker attach 1329c99a831b\n\nTo sum up: you have to understand the difference between the run and start container.\nBesides, look at the documentation for the role of parameters \"-i t\" and \"-d\" for the \"Run\"",
        "url": "https://serverfault.com/questions/661909/the-right-way-to-keep-docker-container-started-when-it-used-for-periodic-tasks"
    },
    {
        "title": "Active Directory explained",
        "question": "If you had to explain Active Directory to someone how would you explain it?",
        "top_answer": "I'm glossing over quite a bit here, of course, but it's a decent semi-technical summary that would be suitable for communicating to others who are not familiar with Active Directory itself, but generally familiar with computers and the issues associated with authentication and authorization.\nActive Directory is, at its heart, a database management system. This database can be replicated amongst an arbitrary number of server computers (called Domain Controllers) in a multi-master manner (meaning that changes can be made to each independent copy, and eventually they'll be replicated to all the other copies).\nThe Active Directory database in an enterprise can be broken up into units of replication called \"Domains\". The system of replication between server computers can be configured in a very flexible manner to permit replication even in the face of failures of connectivity between domain controller computers, and to replicate efficiently between locations that might be connected with low-bandwidth WAN connectivity.\nWindows uses the Active Directory as a repository for configuration information. Chief amongst these uses is the storage of user logon credentials (usernames / password hashes) such that computers can be configured to refer to this database to provide a centralized single sign-on capability for large numbers of machines (called \"members\" of the \"Domain\").\nPermissions to access resources hosted by servers that are members of an Active Directory domain can be controlled through explicit naming of user accounts from the Active Directory domain in permissions called Access Control Lists (ACLs), or by creating logical groupings of user accounts into Security Groups. The information about the names and membership of these security groups are stored in the Active Directory.\nThe ability to modify records stored in the Active Directory database is controlled through security permissions that, themselves, refer to the Active Directory database. In this way, enterprises can provide \"Delegation of Control\" functionality to allow certain authorized users (or members of security groups) to perform administrative functions on the Active Directory of a limited and defined scope. This would allow, for example, a helpdesk employee to change the password of another user, but not to place his own account into security groups that might grant him permission to access sensitive resources.\nVersions of the Windows operating system also can perform installations of software, make modifications to the user's environment (desktop, Start menu, behaviour of application programs, etc) by using the Group Policy. The back-end storage of the data that drives this Group Policy system is stored in Active Directory, and thus is given replication and security functionality.\nFinally, other software applications, both from Microsoft and from third-parties, store additional configuration information in the Active Directory database. Microsoft Exchange Server, for example, makes heavy use of the Active Directory. Applications use Active Directory to gain the benefits of replication, security, and delegation of control described above.\nWhew! Not too bad, I don't think, for a stream of consciousness!\nSuper short answer: AD is a database to store user logon and group information, and configuration information that drives group policy and other application software.",
        "url": "https://serverfault.com/questions/18339/active-directory-explained"
    },
    {
        "title": "What is the difference between a public and private subnet in a Amazon VPC?",
        "question": "When I launch a server with a security group that allows all traffic into my private subnet, it displays a warning that it may be open to the world.  \nIf it is a private subnet, how can that be?",
        "top_answer": "The main difference is the route for 0.0.0.0/0 in the associated route table.\nA private subnet sets that route to a NAT gateway/instance. Private subnet instances only need a private ip and internet traffic is routed through the NAT in the public subnet.  You could also have no route to 0.0.0.0/0 to make it a truly private subnet with no internet access in or out.\nA public subnet routes 0.0.0.0/0 through an Internet Gateway (igw).  Instances in a public subnet require public IPs to talk to the internet.\nThe warning appears even for private subnets, but the instance is only accessible inside your vpc.",
        "url": "https://serverfault.com/questions/556363/what-is-the-difference-between-a-public-and-private-subnet-in-a-amazon-vpc"
    },
    {
        "title": "How do I change the privileges for MySQL user that is already created?",
        "question": "I understand how to create a new user with privileges, but what is the correct way to change privileges for users that are already created?\nWe are running a DB audit and some of the users have way more access then is needed. Plus I don't know the passwords for most of these MySQL users, so I don't want to delete them and create new ones.",
        "top_answer": "To list users:\nselect user,host from mysql.user;\n\nTo show privileges:\nshow grants for 'user'@'host';\n\nTo change privileges, first revoke.  Such as:\nrevoke all privileges on *.* from 'user'@'host';\n\nThen grant the appropriate privileges as desired:\ngrant SELECT,INSERT,UPDATE,DELETE ON `db`.* TO 'user'@'host';\n\nFinally, flush:\nflush privileges;\n\nThe MySQL documentation is excellent:\n\nAccess Control and Account Management\n",
        "url": "https://serverfault.com/questions/115950/how-do-i-change-the-privileges-for-mysql-user-that-is-already-created"
    },
    {
        "title": "apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1 for ServerName",
        "question": "I keep getting this warning when I (re)start Apache.\n\n* Restarting web server apache2\napache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1 for ServerName\n... waiting apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1 for ServerName\n[ OK ]\n\nThis is the content of my etc/hosts file:\n#127.0.0.1  hpdtp-ubuntu910\n#testproject.localhost  localhost.localdomain   localhost\n#127.0.1.1  hpdtp-ubuntu910\n\n127.0.0.1   localhost\n127.0.0.1   testproject.localhost\n127.0.1.1   hpdtp-ubuntu910\n\n\n\n# The following lines are desirable for IPv6 capable hosts\n::1     localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\nff02::3 ip6-allhosts\n\nThis is the content of my /etc/apache2/sites-enabled/000-default file:\n<VirtualHost *:80>\n  ServerName testproject.localhost\n  DocumentRoot \"/home/morpheous/work/websites/testproject/web\"\n  DirectoryIndex index.php\n  <Directory \"/home/morpheous/work/websites/testproject/web\">\n    AllowOverride All\n    Allow from All\n  </Directory>\n\n  Alias /sf /lib/vendor/symfony/symfony-1.3.2/data/web/sf\n  <Directory \"/lib/vendor/symfony/symfony-1.3.2/data/web/sf\">\n    AllowOverride All\n    Allow from All\n  </Directory>\n</VirtualHost>\n\nWhen I go to http://testproject.localhost, I get a blank page.\nCan anyone spot what I am doing wrong?",
        "top_answer": "By default Ubuntu doesn't specify a ServerName in the Apache configuration, because it doesn't know what the name of your server is.  It tries a reverse lookup on your IP address, which returns nothing, so it just has to use the IP address as the ServerName.\nTo fix it, either add a ServerName directive outside of any virtual host - e.g. in /etc/apache2/httpd.conf, or set up a reverse DNS response for your primary IP address - in this case, 127.0.1.1 \nIt's perfectly fine to ignore it also.",
        "url": "https://serverfault.com/questions/103426/apache2-could-not-reliably-determine-the-servers-fully-qualified-domain-name"
    },
    {
        "title": "Set gitlab external web port number",
        "question": "How do I change gitlab's default port 80 to a custom port number?\nThere are two approaches I've tried: \nSet the port in /etc/gitlab/gitlab.rb\nexternal_port \"8888\"\n\nThen run reconfigure:\ngitlab-ctl reconfigure\n\nSet port in /var/opt/gitlab/gitlab-rails/etc/gitlab.yml\nproduction: &base\n  #\n  # 1. GitLab app settings\n  # ==========================\n\n  ## GitLab settings\n  gitlab:\n    ## Web server settings (note: host is the FQDN, do not include http://)\n    host: gitlab.blitting.com\n    port: 8888\n    https: false\n\nThen restart gitlab\ngitlab-ctl stop\ngitlab-ctl start\n\nWith both of these, gitlab continues to run on the default 80 port number.",
        "top_answer": "Chad Carbert answer still applies but just want to add extra for version 7.0.0. \nOpen \"/etc/gitlab/gitlab.rb\" in your text editor where currently I have external_url http://127.0.0.1/ or similar to that. I may need to change external_url with dns including port number (eg. 'http://gitlab.com.local:81/') then reconfigure using command \"sudo gitlab-ctl reconfigure\"  Gitlab is now working on port 81.\nStep by step:\n\nsudo -e /etc/gitlab/gitlab.rb\nChange external_url from yourdomain.com to yourdomain.com:9999\n9999 -> Port you want it to run\nsudo gitlab-ctl reconfigure\n",
        "url": "https://serverfault.com/questions/585528/set-gitlab-external-web-port-number"
    },
    {
        "title": "Newline-separated xargs",
        "question": "Is it possible to make xargs use only newline as separator? (in bash on Linux and OS X if that matters)\nI know -0 can be used, but it's PITA as not every command supports NUL-delimited output.",
        "top_answer": "Something along the lines of\nalias myxargs='perl -p -e \"s/\\n/\\0/;\" | xargs -0'\ncat nonzerofile | myxargs command\n\nshould work.",
        "url": "https://serverfault.com/questions/127904/newline-separated-xargs"
    },
    {
        "title": "Check OpenSSH version and update instructions on FreeBSD system",
        "question": "On a FreeBSD system (8.1), I am looking for instructions on how to check the running version of OpenSSH and also instructions on the best way to download install an update of OpenSSH",
        "top_answer": "Run sshd -V or ssh -V and they'll return the version and usage information.\nNote: These are capital \"V\" now, when I originally wrote this answer they were lower case.\nThere's a dozen ways to upgrade.\n\npkg-add -r openssh-portable\ncd /usr/ports/security/openssh && make install clean\nportupgrade security/openssh-portable\npart of the makeworld/buildworld process\nfreebsd-upgrade \nand the list goes on...\n\nI'm not aware of any issues with the 5.2p1 version that shipped with 8.1-RELEASE. I have seen hoax e-mails flying around for over a year now announcing the imminent release of a zero day hack (note that it's been a year and a half since release, so 'zero' day was a heck of a long time ago).",
        "url": "https://serverfault.com/questions/213224/check-openssh-version-and-update-instructions-on-freebsd-system"
    },
    {
        "title": "best way to debug nginx rewrite rules in config file?",
        "question": "I have a bunch of rewrite rules that I have to port from apache to nginx.\nIt's a rather painful process because I'm not able to see if my rewrite rules and \"if\" conditions are working as I want them to.\nApache did have debugging for its rewrite module. Whats can I do for nginx?",
        "top_answer": "Enable rewrite_log:\nrewrite_log on;\n\nand set debug level in error_log directive:\nerror_log /var/log/nginx/localhost.error_log notice;\n",
        "url": "https://serverfault.com/questions/333048/best-way-to-debug-nginx-rewrite-rules-in-config-file"
    },
    {
        "title": "How to test a LDAP connection from a client",
        "question": "How to check the LDAP connection from a client to server. I'm working on the LDAP authentication and this client desktop needs to authenticate via a LDAP server. I can SSH to the LDAP server using LDAP user but When in desktop login prompt, I can't login. It says Authentication failure. \nClient machine has Cent OS 6.3 and LDAP server has Cent OS 5.5\nLDAP software is Openldap.\nLDAP servers logs doesn't even show any messages.\nSo, how to test whether the client can successfully connect to LDAP or not.",
        "top_answer": "Use ldapsearch. It will return an error if you cannot query the LDAP Server.\nThe syntax for using ldapsearch:\nldapsearch -x -LLL -h [host] -D [user] -w [password] -b [base DN] -s sub \"([filter])\" [attribute list]\n\nA simple example\n$ ldapsearch -x -LLL -h host.example.com -D user -w password -b\"dc=ad,dc=example,dc=com\" -s sub \"(objectClass=user)\" givenName\n\nPlease see this link: http://randomerror.wordpress.com/2009/10/16/quick-tip-how-to-search-in-windows-active-directory-from-linux-with-ldapsearch/\nEdit: It seems you don't have pam configured corectlly for gdm/xdm here is an example how to do it: http://pastebin.com/TDK4KWRV\nNote for ldapsearch >= 2.5: If using ldapsearch from openldap, the options -h and -p were dropped in version 2.5. Use -H instead:\n$ ldapsearch -H ldapuri -D binddn -w password -b searchbase filter\nwhere ldapuri could contain protocol/host/port fields, e.g.: ldaps://ldap.example.org:636",
        "url": "https://serverfault.com/questions/452935/how-to-test-a-ldap-connection-from-a-client"
    },
    {
        "title": "Docker: failed to add the pair interfaces (operation not supported)",
        "question": "After installing Docker, I am getting an error when I try to run the Hello World example:\nError response from daemon: Cannot start container 4145d0fccd96b904e4ab4413735f1129b8765429bad5be71dc8d5f4c0760666d:\nfailed to create endpoint high_saha on network bridge:\nfailed to add the host (veth7f6f907) <=> sandbox (veth788d9dc) pair interfaces: operation not supported\n\n(I have just upgraded my Debian server from Wheezy to Jessie)\nDoes anyone has an idea why I get this error? Did I missed something during the upgrade?\nThanks for your help.",
        "top_answer": "You've booted your system with OVH's custom kernel, which is built to OVH's specifications, and may not be suitable for running software you want to run.\nChange your dedicated server configuration to boot from the kernel installed on the hard drive provided by your Linux distribution.\n",
        "url": "https://serverfault.com/questions/738773/docker-failed-to-add-the-pair-interfaces-operation-not-supported"
    },
    {
        "title": "Linux: How to use a file as input and output at the same time?",
        "question": "I've just run the following in bash:\nuniq .bash_history > .bash_history\n\nand my history file ended up completely empty.\nI guess I need a way to read the whole file before writing to it.\nHow is that done?\nPS: I obviously thought of using a temporary file, but I'm looking for a more elegant solution.",
        "top_answer": "I recommend using sponge from moreutils. From the manpage:\nDESCRIPTION\n  sponge  reads  standard  input  and writes it out to the specified file. Unlike\n  a shell redirect, sponge soaks up all its input before opening the output file.\n  This allows for constructing pipelines that read from and write to the same \n  file.\n\nTo apply this to your problem, try:\nuniq .bash_history | sponge .bash_history\n",
        "url": "https://serverfault.com/questions/135507/linux-how-to-use-a-file-as-input-and-output-at-the-same-time"
    },
    {
        "title": "How do I know if I'm working on a Virtual Machine or not?",
        "question": "Is there a way to know if the Windows machine I'm working on is virtual or physical?\n(I'm connecting with RDP to the machine. If it's a virtual machine it is working and handled by VMWare).",
        "top_answer": "If it's Windows, just have a look at the hardware screens. It'll have a billion and five VMWare-branded virtual devices.",
        "url": "https://serverfault.com/questions/109154/how-do-i-know-if-im-working-on-a-virtual-machine-or-not"
    },
    {
        "title": "Is there a difference between how two ampersands and a semi-colon operate in bash?",
        "question": "If I wanted to run two separate commands on one line, I could do this:\ncd /home; ls -al\n\nor this:\ncd /home && ls -al\n\nAnd I get the same results. However, what is going on in the background with these two methods? What is the functional difference between them?",
        "top_answer": "The ; just separates one command from another. The && says only run the following command if the previous was successful\ncd /home; ls -al\n\nThis will cd /home and even if the cd command fails (/home doesn't exist, you don't have permission to traverse it, etc.), it will run ls -al.\ncd /home && ls -al\n\nThis will only run the ls -al if the cd /home was successful. ",
        "url": "https://serverfault.com/questions/373052/is-there-a-difference-between-how-two-ampersands-and-a-semi-colon-operate-in-bas"
    },
    {
        "title": "How to create an SHA-512 hashed password for shadow?",
        "question": "The previous SF questions I've seen have lead to answers that produce MD5 hashed password.\nDoes anyone have a suggestion on to produce an SHA-512 hashed password? I'd prefer a one liner instead of a script but, if a script is the only solution, that's fine as well.\nUpdate\nReplacing previous py2 versions with this one:\npython3 -c \"import crypt;print(crypt.crypt(input('clear-text pw: '), crypt.mksalt(crypt.METHOD_SHA512)))\"\n",
        "top_answer": "Edit: Please note this answer is 10+ years old.\nHere's a one liner:\npython -c 'import crypt; print crypt.crypt(\"test\", \"$6$random_salt\")'\n\nPython 3.3+ includes mksalt in crypt, which makes it much easier (and more secure) to use:\npython3 -c 'import crypt; print(crypt.crypt(\"test\", crypt.mksalt(crypt.METHOD_SHA512)))'\n\nIf you don't provide an argument to crypt.mksalt (it could accept crypt.METHOD_CRYPT, ...MD5, SHA256, and SHA512), it will use the strongest available.\nThe ID of the hash (number after the first $) is related to the method used:\n\n1 -> MD5\n2a -> Blowfish (not in mainline glibc; added in some Linux distributions)\n5 -> SHA-256 (since glibc 2.7)\n6 -> SHA-512 (since glibc 2.7)\n\nI'd recommend you look up what salts are and such and as per smallclamgers comment the difference between encryption and hashing.\nUpdate 1: The string produced is suitable for shadow and kickstart scripts.\nUpdate 2: Warning. If you are using a Mac, see the comment about using this in python on a mac where it doesn't seem to work as expected.\nOn macOS you should not use the versions above, because Python uses the system's version of crypt() which does not behave the same and uses insecure DES encryption. You can use this platform independent one liner (requires passlib \u2013 install with pip3 install passlib):\npython3 -c 'import passlib.hash; print(passlib.hash.sha512_crypt.hash(\"test\"))'\n",
        "url": "https://serverfault.com/questions/330069/how-to-create-an-sha-512-hashed-password-for-shadow"
    },
    {
        "title": "Convert from P7B to PEM via OpenSSL",
        "question": "On Ubuntu, I cannot convert certificate using openssl successfully.\nvagrant@dev:/vagrant/keys$ openssl pkcs7 -print_certs -in a.p7b -out a.cer \nunable to load PKCS7 object <blah blah>:PEM\nroutines:PEM_read_bio:no start line:pem_lib.c:696:Expecting: PKCS7\n\nHave you seen this error before?",
        "top_answer": "Try this:\n$ openssl pkcs7 -inform der -in a.p7b -out a.cer\n\nIf it doesn't work, brings to a Windows machine and export follow this guide.",
        "url": "https://serverfault.com/questions/417140/convert-from-p7b-to-pem-via-openssl"
    },
    {
        "title": "how to fix \"send-mail: Authorization failed 534 5.7.14 \"",
        "question": "I am trying to create e-mail alert on ssh root login so I had to install ssmtp and mail utility.\nThen I configured ssmtp.conf file as follows :\n# Config file for sSMTP sendmail\n# The person who gets all mail for userids < 1000\n# Make this empty to disable rewriting.\n        #root=postmaster\n        #Adding  email id to receive system information\nroot = [email\u00a0protected]\n# The place where the mail goes. The actual machine name is required no\n# MX records are consulted. Commonly mailhosts are named mail.domain.com\n        #mailhub=mail\n\nmailhub = smtp.gmail.com:587\n\n[email\u00a0protected]\nAuthPass=plaintext password\nUseTLS=YES\nUseSTARTTLS=YES\n\n# Where will the mail seem to come from?\nrewriteDomain=gmail.com\n\n# The full hostname\n\nhostname = mailserver\n# Are users allowed to set their own From: address?\n# YES - Allow the user to specify their own From: address\n# NO - Use the system generated From: address\nFromLineOverride=YES\n\nas well as revaliases as follows:\n# Format:       local_account:outgoing_address:mailhub\n# Example: root:[email\u00a0protected]:mailhub.your.domain[:port]\n\nroot:[email\u00a0protected]:smtp.gmail.com:25\n\nand I am getting this error:\nsend-mail: Authorization failed (534 5.7.14 https://support.google.com/mail/bin/answer.py?answer=78754 ni5sm3908366pbc.83 - gsmtp)\nCan't send mail: sendmail process failed with error code 1\n\nbut it didn't work.\nPlease help me to sort out this",
        "top_answer": "It may take more than one step to fix this issue\n\nTake the step mentioned earlier.  Log into your google email account and then go to this link: https://www.google.com/settings/security/lesssecureapps and set \"Access for less secure apps\" to ON.  Test to see if your issue is resolved.  If it isn't resolved, as it wasn't for me, continue to Step #2.\nGo to https://support.google.com/accounts/answer/6009563 (Titled: \"Password incorrect error\").  This page says \"There are several reasons why you might see a \u201cPassword incorrect\u201d error (aka 534-5.7.14) when signing in to Google using third-party apps. In some cases even if you type your password correctly.\"  This page gives 4 suggestions of things to try.\n\nFor me, the first suggestion worked:\n\nGo to https://g.co/allowaccess from a different device you have previously used to access your Google account and follow the instructions.\nTry signing in again from the blocked app.\n\nThere were three more suggestions on the page given in step #2 but I didn't try them because after going to the redacted link and following the instructions, everything began to work as it should.",
        "url": "https://serverfault.com/questions/635139/how-to-fix-send-mail-authorization-failed-534-5-7-14"
    },
    {
        "title": "Redirect URL within Apache VirtualHost?",
        "question": "I have a dedicated server with Apache, on which I've set up some VirtualHosts. I've set up one to handle the www domain as well as the non-www domain.\nMy VH .conf file for the www:\n<VirtualHost *>\n  DocumentRoot /var/www/site\n  ServerName www.example.com\n  <Directory \"/var/www/site\">\n    allow from all\n  </Directory>\n</VirtualHost>\n\nWith this .htaccess:\nRewriteEngine on\nRewriteBase /\nRewriteCond %{HTTP_HOST} ^www.example.com [NC]\nRewriteRule ^(.*)$ http://example.com/$1 [L,R=301]\n\nIs there a simple way to redirect the www to the non-www version? Currently I'm sending both versions to the same DocumentRoot and using .htaccess but I'm sure I must be able to do it in the VirtualHost file.",
        "top_answer": "Turns out mod_rewrite rules are fine in the VirtualHosts file, apart from the RewriteBase rule. I ended up with this:\n<VirtualHost *>\n  ServerName www.example.com\n  RewriteEngine on\n  RewriteCond %{HTTP_HOST} ^www.example.com\n  RewriteRule ^/(.*)$ http://example.com/$1 [L,R=301]\n</VirtualHost>\n\nEDIT: on the advice of joschi in the comments, I'm now using this simplified version using the Redirect directive from mod_alias:\n<VirtualHost *>\n  ServerName www.example.com\n  Redirect 301 / http://example.com/\n</VirtualHost>\n",
        "url": "https://serverfault.com/questions/120488/redirect-url-within-apache-virtualhost"
    },
    {
        "title": "How to restart rsyslog daemon on ubuntu",
        "question": "how can i restart rsyslog or rsyslogd on ubuntu 10\nroot@terminator:/etc/init.d# service rsyslog status\nrsyslog stop/waiting\nroot@terminator:/etc/init.d# service rsyslog stop\nstop: Unknown instance:\nroot@terminator:/etc/init.d# service rsyslog restart\nrestart: Unknown instance:\nroot@terminator:/etc/init.d# service rsyslog start\nstart: Job failed to start\nroot@terminator:/etc/init.d# service rsyslogd start\nrsyslogd: unrecognized service\n",
        "top_answer": "sudo service rsyslog restart\nThat should do it.",
        "url": "https://serverfault.com/questions/176747/how-to-restart-rsyslog-daemon-on-ubuntu"
    },
    {
        "title": "How do I enable non-free packages on Debian?",
        "question": "How can I enable non-free packages on Debian?  I want to install Sun's Java JDK but it's not available to me.",
        "top_answer": "Open up /etc/apt/sources.list, and you should see lines like the following (URLs will likely vary):\ndeb http://http.us.debian.org/debian stable main contrib\n\nSimply add non-free to the respective URLs you wish to use, i.e.:\n deb http://http.us.debian.org/debian stable main contrib non-free\n\nRunning apt-get update will update your local repo with the package listing.",
        "url": "https://serverfault.com/questions/240920/how-do-i-enable-non-free-packages-on-debian"
    },
    {
        "title": "Unknown/unsupported storage engine: InnoDB | MySQL Ubuntu",
        "question": "I recently upgraded from the previous LTS Ubuntu to Precise and now mysql refuses to start. It complains of the following when I attempt to start it:\n\u2570$ sudo service mysql restart\nstop: Unknown instance:\nstart: Job failed to start\n\nAnd this shows in \"/var/log/mysql/error.log\":\n120415 23:01:09 [Note] Plugin 'InnoDB' is disabled.\n120415 23:01:09 [Note] Plugin 'FEDERATED' is disabled.\n120415 23:01:09 [ERROR] Unknown/unsupported storage engine: InnoDB\n120415 23:01:09 [ERROR] Aborting\n\n120415 23:01:09 [Note] /usr/sbin/mysqld: Shutdown complete\n\nI've checked permissions on all the mysql directories to make sure it had ownership and I also renamed the previou ib_logs so that it could remake them. I'm just getting no where with this issue right now, after looking at google results for 2 hours.",
        "top_answer": "After checking the logs I found the following error:\n[ERROR] Unknown/unsupported storage engine: InnoDB\n\nI removed these files:\nrm /var/lib/mysql/ib_logfile0\nrm /var/lib/mysql/ib_logfile1 \n\nat /var/lib/mysql\nThis resolved my problem after restart.",
        "url": "https://serverfault.com/questions/379714/unknown-unsupported-storage-engine-innodb-mysql-ubuntu"
    },
    {
        "title": "Is there a way to redirect output to a file without buffering on unix/linux?",
        "question": "I have a long running batch process that outputs some debug and process information to stdout.\nIf I just run from a terminal I can keep track of 'where it is' but then the data gets too much and scrolls off the screen.\nIf I redirect to output to a file '> out.txt' I get the whole output eventually but it is buffered so I can no longer see what it is doing right now.\nIs there a way to redirect the output but make it not buffer its writes?",
        "top_answer": "You can explicitly set the buffering options of the standard streams using a setvbuf call in C (see this link), but if you're trying to modify the behaviour of an existing program try stdbuf (part of coreutils starting with version 7.5 apparently).\nThis buffers stdout up to a line:\nstdbuf -oL command > output\n\nThis disables stdout buffering altogether:\nstdbuf -o0 command > output\n",
        "url": "https://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux"
    },
    {
        "title": "How to read memory usage in htop?",
        "question": "Here is my htop output:\n\nFor example, I'm confused by this ruby script:\n\nHow much physical memory is it using? 3+1+8+51+51? 51? 51+51?",
        "top_answer": "Hide user threads (shift + H) and close the process tree view (F5), then you can sort out the process of your interest by PID and read the RES column (sort by MEM% by pressing shift + M, or F3 to search in cmd line)",
        "url": "https://serverfault.com/questions/517483/how-to-read-memory-usage-in-htop"
    },
    {
        "title": "What's the easiest way to make my old init script work in systemd?",
        "question": "I don't want to do the right thing by creating a new systemd script, I just want my old init script to work again now that I've upgraded my system to an OS that's using systemd. \nI've briefly researched how to convert init scripts and how to write systemd scripts, but I'm sure learning it properly and doing it right would take me several hours. \nThe current situation is:\nsystemctl start solr\nFailed to start solr.service: Unit solr.service failed to load: No such file or directory.\n\nAnd:\nsudo service solr start\nFailed to start solr.service: Unit solr.service failed to load: No such file or directory.\n\nRight now, I just want to get back to work. What's the path of least resistance to getting this working again? \nUpdates\nI didn't want to figure this all out \u2013 I really didn't \u2013 but I have to and I've unearthed my first clue:\nsudo systemctl enable solr\nSynchronizing state for solr.service with sysvinit using update-rc.d...\nExecuting /usr/sbin/update-rc.d solr defaults\ninsserv: warning: script 'K01solr' missing LSB tags and overrides\ninsserv: warning: script 'solr' missing LSB tags and overrides\nExecuting /usr/sbin/update-rc.d solr enable\nupdate-rc.d: error: solr Default-Start contains no runlevels, aborting.\n\nThe incompatibilities page for systemd says that:\n\nLSB header dependency information matters. The SysV implementations on many distributions did not use the dependency information encoded in LSB init script headers, or used them only in very limited ways. Due to that they are often incorrect or incomplete. systemd however fully interprets these headers and follows them closely at runtime\n\nI think that means my script won't work until that's fixed.\nThe script in question:\n#!/bin/sh\n\n# Prerequisites:\n# 1. Solr needs to be installed at /usr/local/solr/example\n# 2. daemon needs to be installed\n# 3. Script needs to be executed by root\n# 4. $INSTALL_ROOT must be set\n\n# This script will launch Solr in a mode that will automatically respawn if it\n# crashes. Output will be sent to /var/log/solr/solr.log. A pid file will be\n# created in the standard location.\n\nstart () {\n    echo -n \"Starting solr...\"\n\n    # Reset ulimit or else get issues with too many open files (https://issues.apache.org/jira/browse/SOLR-4)\n    ulimit -n 10000\n\n    # start daemon\n    daemon --chdir='/usr/local/solr/example' --command \"java -jar -server start.jar -DINSTALL_ROOT=$INSTALL_ROOT\" --respawn --output=/var/log/solr/solr.log --name=solr --verbose\n\n    RETVAL=$?\n    if [ $RETVAL = 0 ]\n    then\n        echo \"done.\"\n    else\n        echo \"failed. See error code for more information.\"\n    fi\n    return $RETVAL\n}\n\nstop () {\n    # stop daemon\n    echo -n \"Stopping solr...\"\n\n    daemon --stop --name=solr  --verbose\n    RETVAL=$?\n\n    if [ $RETVAL = 0 ]\n    then\n        echo \"done.\"\n    else\n        echo \"failed. See error code for more information.\"\n    fi\n    return $RETVAL\n}\n\n\nrestart () {\n    daemon --restart --name=solr  --verbose\n}\n\n\nstatus () {\n    # report on the status of the daemon\n    daemon --running --verbose --name=solr\n    return $?\n}\n\n\ncase \"$1\" in\n    start)\n        start\n    ;;\n    status)\n        status\n    ;;\n    stop)\n        stop\n    ;;\n    restart)\n        stop\n        sleep 15\n        start\n    ;;\n    *)\n        echo $\"Usage: solr {start|status|stop|restart}\"\n        exit 3\n    ;;\nesac\n\nexit $RETVAL\n",
        "top_answer": "Seriously, a systemd unit file is trivial to write for a service like this...or for most services.\nThis ought to get you about 95% of the way there. Put this in, for example, /etc/systemd/system/solr.service\n[Unit]\nDescription=Apache Solr\nAfter=syslog.target network.target remote-fs.target nss-lookup.target\n\n[Service]\nType=simple\nEnvironmentFile=/etc/courtlistener\nWorkingDirectory=/usr/local/solr/example\nExecStart=/usr/bin/java -jar -server -Xmx${CL_SOLR_XMX} start.jar -DINSTALL_ROOT=${INSTALL_ROOT}\nRestart=on-failure\nLimitNOFILE=10000\n\n[Install]\nWantedBy=multi-user.target\n\nNote the stuff that isn't here, like the log file and such; systemd will automatically capture and log the service output under the service's name.",
        "url": "https://serverfault.com/questions/690155/whats-the-easiest-way-to-make-my-old-init-script-work-in-systemd"
    },
    {
        "title": "Linux: how to explicitly unswap everything possible?",
        "question": "I've launched something that took lots of memory and now everything lags a lot.\nI guess all applications' memory has gone to swap in order to free some space for the memory-intensive process, and now everything is slowly returning to RAM when accessed.\nIs there a way to explicitly move everything possible from swap back to RAM? Or maybe not everything, but just some particular processes data?",
        "top_answer": "I'd recommend allowing the normal Linux memory control swap in the things that are actually used, as they are used.\nThe only thing I can think off is to turn swap off, then on again\nsudo swapoff -a\nsudo swapon -a\n\nThat assumes you have enough spare physical memory to contain everything in swap...",
        "url": "https://serverfault.com/questions/110436/linux-how-to-explicitly-unswap-everything-possible"
    },
    {
        "title": "How long does it take for DNS records to propagate?",
        "question": "\nThis is a Canonical Question about DNS Propagation\n\nHow long does it take for an the various types of records to propagate?\nDo some propagate faster than others? \nWhy does it take time for DNS records to propagate and how does it work?",
        "top_answer": "\"DNS propagation\" isn't a real phenomenon, per se. Rather, it is the manifest effect of the caching functionality specified in the DNS protocol. Saying that changes \"propagate\" between DNS servers is a convenient falsehood that's, arguably, easier to explain to non-technical users than describing all of the details of the DNS protocol. It's not really how the protocol works, though.\nRecursive DNS servers make queries on behalf of clients. Recursive DNS servers, typically run by ISPs or IT departments, are used by client computers to resolve names of Internet resources. Recursive DNS servers cache the results of queries they make to improve efficiency. Queries for already-cached information can be answered without making any additional queries. The duration, in seconds, that a result is cached is supposed to be based on a configurable value called the Time To Live (TTL). This value is specified by the authoritative DNS server for the record queried. \nThere is no one answer to all the questions being asked because DNS is a distributed protocol. The behavior of DNS depends on the configuration of the authoritative DNS server for a given record, the configuration of recursive DNS servers making queries on behalf of client computers, and DNS caching functionality built-in to the client computers' operating systems.\nIt's good practice to specify a TTL value short enough to accommodate neecssary day-to-day changes to DNS records, but long enough so to create a \"win\" in caching (i.e. not so short as to age-out of cache too quickly to provide any efficiency improvement). Employing a balanced strategy with TTL values results in a \"win\" for everyone. It reduces both the load and bandwidth utilization for the authoritative DNS servers for a given domain, the root servers, and the TLD servers. It reduces the upstream bandwidth utilization for the operator of the recursive DNS server. It results in quicker query responses for client computers. \nAs a DNS record's TTL is set lower load and bandwidth utilization on the authoritative DNS servers will increase because recursive DNS servers will not be able to cache the result for a long duration. As a record's TTL is higher changes to records will not appear to \"take effect\" quickly because client computers will continue to receive cached results stored on their recursive DNS servers. Setting the optimal TTL comes down to a balancing act between utilization and ability to change records quickly and see those changes reflected on clients.\nIt is worth noting that some ISPs are abusive and ignore the TTL values specified by the authoritative DNS servers (substituting their own administrative override, which is a violation of RFC). There's nothing to be done about this, from a technical perspective. If the operators of abusive DNS servers can be located complaints to their systems administrators might result in their implementing best practices (arguably what amounts to common sense for any network engineer familiar with DNS). This particular type of abuse isn't a technical problem. \nIf everybody \"plays by the rules\" changes to DNS records can \"take effect\" very quickly. In the case of changing the IP address assigned to an \"A\" record, for example, an exponential backoff of the TTL value would be performed, leading up to the time the change will be made. The TTL might start at 1 day, for example, and be decreased to 12 hours for a 24 hour period, then 6 hours for a 12 hour period, 3 hours for a 6 hour period, etc, down to some suitably small interval. Once the TTL has been backed-off the record can be changed and the TTL brought back up to the desired value for day-to-day operations. (It is not necessary to use an exponential backoff, however this strategy minimizes the time the record will have a low TTL and decreases load on the authoritative DNS server.)\nAfter making a DNS record change logs should be monitored for access attempts being made as a result of the old DNS record. In the example of changing an \"A\" record to refer to a new IP address a server should remain present at the old IP address to handle access attempts resulting from client computers still using the old \"A\" record. Once access attempts based on the old record have reached an acceptably low level the old IP address can be disused. If the requests related to an old record are not abating quickly it is possible that (as described above) a recursive DNS server is ignoring the authoritative TTL. Knowing the source IP address of an access attempt, however, does not provide direct information as to the recursive DNS server responsible for supplying an old record. If the IP addresses of errant access attempts are all related to a single ISP it may be possible to locate the offending DNS server and contact its operator.\nPersonally, I've seen changes \"take effect\" immediately, in a few hours, and in some cases with a particular brain-damaged ISP, after several days. Doing a backoff of your TTL and being mindful of how the process works will increase your changes for success, but you can't ever be sure what some well-meaning idiot might be doing with their recursive DNS servers.",
        "url": "https://serverfault.com/questions/125371/how-long-does-it-take-for-dns-records-to-propagate"
    },
    {
        "title": "How can I get more info on open pipes show in /proc in Linux?",
        "question": "In Linux if you go digging in /proc/<pid>/fd often you'll see output like:\nlrwx------ 1 root root 64 Jul 30 15:14 0 -> /dev/null\nlrwx------ 1 root root 64 Jul 30 15:14 1 -> /dev/null\nl-wx------ 1 root root 64 Jul 30 15:14 10 -> pipe:[90222668]\nlr-x------ 1 root root 64 Jul 30 15:14 11 -> pipe:[90222669]\nl-wx------ 1 root root 64 Jul 30 15:14 13 -> pipe:[90225058]\nlr-x------ 1 root root 64 Jul 30 15:14 14 -> pipe:[90225059]\n\nHow do I get more info about the open pipes, such as which process is on the other end?",
        "top_answer": "Similiar to other answers, but:\nlsof | grep 90222668\n\nWill show you both ends, because both ends share the 'pipe number'.",
        "url": "https://serverfault.com/questions/48330/how-can-i-get-more-info-on-open-pipes-show-in-proc-in-linux"
    },
    {
        "title": "Why is it so difficult to upgrade between major versions of Red Hat and CentOS?",
        "question": "\"Can we upgrade our existing production EL5 servers to EL6?\"\nA simple-sounding request from two customers with completely different environments prompted my usual best-practices answer of \"yes, but it will require a coordinated rebuild of all of your systems\"...\nBoth clients feel that a complete rebuild of their systems is an unacceptable option for downtime and resource reasons... When asked why it was necessary to fully reinstall the systems, I didn't have a good answer beyond, \"that's the way it is...\"\nI'm not trying to elicit responses about configuration management (\"Puppetize everything\" doesn't always apply) or how the clients should have planned better. This is a real-world example of environments that have grown and thrived in a production capacity, but don't see a clean path to move to the next version of their OS.\nEnvironment A:\nNon-profit organization with 40 x Red Hat Enterprise Linux 5.4 and 5.5 web, database servers and mail servers, running a Java web application stack, software load balancers and Postgres databases. All systems are virtualized on two VMWare vSphere clusters in different locations, each with HA, DRS, etc.\nEnvironment B:\nHigh-frequency financial trading firm with 200 x CentOS 5.x systems in multiple co-location facilities running production trading operations, supporting in-house development and back-office functions. The trading servers are running on bare-metal commodity server hardware. They have numerous sysctl.conf, rtctl, interrupt binding and driver tweaks in place to lower messaging latency. Some have custom and/or realtime kernels. The developer workstations are also running a similar version(s) of CentOS. \n\nIn both cases, the environments are running well as-is. The desire to upgrade comes from a need for a newer application or feature available in EL6. \n\nFor the non-profit firm, it's tied to Apache, the kernel and some things that will make the developers happy. \nIn the trading firm, it's about some enhancements in the kernel, networking stack and GLIBC, which will make the developers happy. \n\nBoth are things that can't be easily packaged or updated without drastically altering the operating system.\nAs a systems engineer, I appreciate that Red Hat recommends full rebuilds when moving between major version releases. A clean start forces you to refactor and pay attention to configs along the way.\nBeing sensitive to business needs of clients, I wonder why this needs to be such an onerous task. The RPM packaging system is more than capable of handling in-place upgrades, but it's the little details that get you: /boot requiring more space, new default filesystems, RPM possibly breaking mid-upgrade, deprecated and defunct packages...\nWhat's the answer here? Other distributions (.deb-based, Arch and Gentoo) seem to have this ability or a better path. Let's say we find the downtime to accomplish this task the right way: \n\nWhat should these clients do to avoid the same problem when EL7 is released and stabilizes? \nOr is this a case where people need to resign themselves to full rebuilds every few years?\nThis seems to have gotten worse as Enterprise Linux has evolved... Or am I just imagining that?\nHas this dissuaded anyone from using Red Hat and derivative operating systems?\n\nI suppose there's the configuration management angle, but most Puppet installations I see do not translate well into environments with highly-customized application servers (Environment B could have a single server whose ifconfig output looks like this). I'd be interesting in hearing suggestions on how configuration management can be used to help organizations get across the RHEL major version bump, though.",
        "top_answer": "(Author's Note: This answer refers to RHEL 6 and prior versions. RHEL 7 now has a fully supported upgrade path from RHEL 6, the details of which are at the end.)\n\nTo start, I should note that there are two ways to do the in-place upgrade:\n\nDrop in the installation DVD (or use the DVD image via iLO/iDRAC), boot from it and choose Upgrade, e.g. linux upgradeany.\nUpdate the redhat-release RPM manually, run yum distro-sync (this is oversimplified a bit) and reboot.\n\nMethod 1 is merely unsupported. Method 2 is for Real Cowboys. In addition to the recommended fresh installs, I have done both of these...\n\nDo I need support?\nSupport has two complementary meanings in our world. The first is that a product has a given feature (e.g. \"Postfix supports SMTP\"). The second is that the vendor will talk to you about it. Which definition is meant is not always clear from context.\nTo accomplish a task, you obviously need support in the first sense. Where vendor support comes in is to assist you in resolving issues and giving the vendor feedback as to what features need to exist or be improved. Many sites pay a fortune for vendor support when they have the in-house expertise to resolve any issues that may arise, faster and even cheaper than the vendor could. Whether to buy vendor support is ultimately a business decision you will have to make (or advise management on).\n\nWhy not do an in-place upgrade?\nThis is what Red Hat says about it:\n\nRed Hat does not support in-place upgrades between any major versions of Red Hat Enterprise Linux. A major version is denoted by a whole number version change. For example, Red Hat Enterprise Linux 5 and Red Hat Enterprise Linux 6 are both major versions of Red Hat Enterprise Linux.\nIn-place upgrades across major releases do not preserve all system settings, services or custom configurations. Consequently, Red Hat strongly recommends fresh installations when upgrading from one major version to another.\n\nThey further warn:\n\nHowever, note the following limitations before you choose to upgrade your system:\n\nIndividual package configuration files may or may not work after performing an upgrade due to changes in various configuration file formats or layouts.\nIf you have one of Red Hat's layered products (such as the Cluster Suite) installed, it may need to be manually upgraded after the Red Hat Enterprise Linux upgrade has been completed.\nThird party or ISV applications may not work correctly following the upgrade.\n\n\nOf course, they then describe how to do an in-place upgrade via method 1, just in case you really want to do it. The feature exists and Red Hat puts development time into it, so it is supported in that the feature exists. But if something goes wrong, Red Hat will tell you to install fresh; they will not provide vendor support for things that break as a result of the upgrade.\nFor the record, I've never actually had a problem with an in-place upgrade of a RHEL/CentOS or Fedora system that I couldn't resolve myself. The typical problems come from renamed packages, third party repositories and the occasional version mismatch between the i386 and x86_64 architectures of a package. The installer is a bit better at handling these than yum, I think.\n\nHow should I upgrade?\nI generally warn people that they should plan on a maintenance window every 3-4 years to update RHEL systems from one major version to the next. While upgrades generally go smoothly, the unexpected can always happen.\nFor both of your environments, I expect an in-place upgrade would work, though I strongly recommend testing it thoroughly first. P2V a representative sample of the servers and run through the in-place upgrade on the virtual systems to see what problems you're going to run into. You can then plan the actual production upgrade based on better knowledge of what will happen.\nFor a large deployment such as you have here, consider using Limoncelli's \"one-some-many\" approach. Upgrade one machine, see what problems occur, solve them, then use lessons learned when upgrading a small batch of machines, repeat the lessons learned thing, then when you believe you have all the kinks worked out, upgrade large batches of them.\nAt a time like this, I also recommend taking a long hard look at your application deployment process. If it isn't sufficiently automated that you can kick it off with a single command and be reasonably sure that the app will be deployed correctly, then perhaps the developers need to get to work on that. Having such a deployment process would make it much easier to do a fresh installation of the newer version of EL and then deploy onto it.\n\nWill switching distributions help?\nDebian-based distributions do have a supported in-place upgrade method, and it mostly works, but it is not immune from problems. Lots of things broke for people upgrading from Ubuntu 10.04 LTS to 12.04 LTS via the supported method, for instance. It's not clear that Debian or Canonical are putting a sufficient amount of development time into \"supporting\" this feature, i.e., making sure it works. And you still actually have to buy vendor support for this distribution if you want someone to hold your hand. So I doubt you will gain much from switching to such a distribution.\nYou may gain by switching to a rolling-release distribution such as Gentoo or Arch. However, this also doesn't make you immune to problems; it just means you have to deal with the upgrade problems continuously over the life of the server (e.g. whenever you or the developers decide to update something on the system), rather than all at once at a well-planned distribution upgrade time. You also have no vendor to provide support.\n\nWhat does the future hold?\nThe Fedora Project is working on a tool to improve in-place upgrades. They had a tool called preupgrade which was abandoned and replaced with a new tool called fedup beginning with Fedora 18. This was added to RHEL7 and now in-place upgrades have full support, at least from RHEL 6 to RHEL 7. From my own experience I can say that while fedup still has some kinks, it is shaping up to be a very useful tool.\nCentOS is also experimenting with a rolling-release type of repository, but it only applies between minor versions (e.g. 6.3-6.4).",
        "url": "https://serverfault.com/questions/449048/why-is-it-so-difficult-to-upgrade-between-major-versions-of-red-hat-and-centos"
    },
    {
        "title": "What are your favorite open source tools?",
        "question": "I believe every system administrator is used to open source by now. From Apache to Firefox or Linux, everyone uses it at least a little bit.\nHowever, most open source developers are not good in marketing, so I know that there are hundreds of very good tools out there that very few people know.\nTo fill this gap, share your favorite open source tool that you use in your day-to-day work.\n*I will post mine in the comments.",
        "top_answer": "I love PuTTY !\nThe PuTTY executables and source code are distributed under the MIT licence, which is similar in effect to the BSD licence. (This licence is Open Source certified and complies with the Debian Free Software Guidelines.)",
        "url": "https://serverfault.com/questions/34465/what-are-your-favorite-open-source-tools"
    },
    {
        "title": "How do I check the postfix queue size?",
        "question": "What's the postfix equivalent to sendmail -bp?",
        "top_answer": "Or, less typing:\nmailq\n",
        "url": "https://serverfault.com/questions/58196/how-do-i-check-the-postfix-queue-size"
    },
    {
        "title": "IIS complains about a locked section - how can I find out where it's locked?",
        "question": "I have this section in my web.config:\n<system.webServer>\n    <modules runAllManagedModulesForAllRequests=\"true\" />\n    <security>\n        <authentication>\n            <anonymousAuthentication enabled=\"true\" />\n            <windowsAuthentication enabled=\"true\" />\n        </authentication>\n    </security>\n</system.webServer>\n\nIIS7 crashes and complains about the autientication section:\n\nModule AnonymousAuthenticationModule\n     Notification   AuthenticateRequest\n     Handler    StaticFile\n     Error Code 0x80070021\n     Config Error   This configuration section cannot be used at this path. This happens when the section is locked at a parent level. Locking is either by default (overrideModeDefault=\"Deny\"), or set explicitly by a location tag with overrideMode=\"Deny\" or the legacy allowOverride=\"false\".  \n\nConfig Source  \n   69:  <authentication>\n   70:    <anonymousAuthentication enabled=\"true\" />\n\nSo the usual way to solve this is to go into %windir%\\system32\\inetsrv\\config\\applicationHost.config and unlock the section:\n    <sectionGroup name=\"system.webServer\">\n        <sectionGroup name=\"security\">\n            <section name=\"access\" overrideModeDefault=\"Deny\" />\n            <section name=\"applicationDependencies\" overrideModeDefault=\"Deny\" />\n            <sectionGroup name=\"authentication\">\n                <section name=\"anonymousAuthentication\" overrideModeDefault=\"Allow\" />\n                <section name=\"basicAuthentication\" overrideModeDefault=\"Allow\" />\n                <section name=\"clientCertificateMappingAuthentication\" overrideModeDefault=\"Allow\" />\n                <section name=\"digestAuthentication\" overrideModeDefault=\"Allow\" />\n                <section name=\"iisClientCertificateMappingAuthentication\" overrideModeDefault=\"Allow\" />\n                <section name=\"windowsAuthentication\" overrideModeDefault=\"Allow\" />\n            </sectionGroup>\n\n(alternatively, appcmd unlock config).\nThe weird thing: I've done that and it still complains.\nI looked for Locations (MVC is the name of my website that's the root of all sites I'm using):\n<location path=\"MVC\" overrideMode=\"Allow\">\n    <system.webServer overrideMode=\"Allow\">\n        <security overrideMode=\"Allow\">\n            <authentication overrideMode=\"Allow\">\n                <windowsAuthentication enabled=\"true\" />\n                <anonymousAuthentication enabled=\"true\" />\n            </authentication>\n        </security>\n    </system.webServer>\n</location>\n\nStill it blows up. I'm puzzled as to why this happens. I cannot remove it from the web.config, I want to find the root problem.\nIs there a way to get specific information from IIS which rule is eventually denying me?\nEdit: I was able to fix this using the IIS7 management console by going to the very root (my machine) and clicking \"Edit Configuration\" and unlocking the section there. Still I'd like to know if there is a better way since I can't find the file it actually modifies.",
        "top_answer": "Worked out these steps which fix the issue for me:\n\nOpen IIS Manager\nClick the server name in the tree on the left\nRight hand pane, Management section, double click Configuration Editor\nAt the top, choose the section system.webServer/security/authentication/anonymousAuthentication\nRight hand pane, click Unlock Section\nAt the top, choose the section system.webServer/security/authentication/windowsAuthentication\nRight hand pane, click Unlock Section\n",
        "url": "https://serverfault.com/questions/360438/iis-complains-about-a-locked-section-how-can-i-find-out-where-its-locked"
    },
    {
        "title": "gpg --gen-key hangs at gaining enough entropy on centos 6",
        "question": "Trying to generate a key for a server. \ngpg --gen-key\n\n\nWe need to generate a lot of random bytes. It is a good idea to perform\n  some other action (type on the keyboard, move the mouse, utilize the\n  disks) during the prime generation; this gives the random number\n  generator a better chance to gain enough entropy.\n\nand it just hangs there. \nThere is another error:\n\ncan't connect to `/root/.gnupg/S.gpg-agent': No such file or directory\n\nwhich seems to go away after:\n\ngpg-agent --daemon\n  GPG_AGENT_INFO=/tmp/gpg-4c5hyT/S.gpg-agent:1397:1; export GPG_AGENT_INFO;\n\n#GPG_AGENT_INFO=/tmp/gpg-4c5hyT/S.gpg-agent:1397:1; export GPG_AGENT_INFO;\ngpg --gen-key\n...\n\nbut again, it hangs at \"...gain enough entropy\". \nThere are no \"++++++++++++++++++++++++++++++++++++++++++\"'s which from forum posts looks like should be expected as the key is generated. \nI have tried reinstalling the package, but seemingly everything depends on gpg. \nI've read other people having problems with this on centos 6 too (whereas centos 5 works fine). \nThere is nothing remarkable in /var/log/*.\nAny ideas on where to go from here?\nThanks.",
        "top_answer": "When the gpg --gen-key command hangs like this, log in to another shell and perform the following command:\ndd if=/dev/sda of=/dev/zero\n\n(This command basically reads from your hard drive and discards the output, because writing to /dev/zero will do nothing.)\nAfter a few seconds / minutes, the key generation command should complete.",
        "url": "https://serverfault.com/questions/471412/gpg-gen-key-hangs-at-gaining-enough-entropy-on-centos-6"
    },
    {
        "title": "How to check if a port is open for remote system(ubuntu) [closed]",
        "question": "Is it possible to check that if the ports are open for the remote system on ubuntu server?\nI should able to check if a port(eg:ssh) on my machine is open for the remote machine.",
        "top_answer": "use good old telnet:\n[user@lappie ~]$ telnet host 22\nTrying ip.adr.tld ...\nConnected to host  (ip.addr.tld).\nEscape character is '^]'.\nSSH-2.0-OpenSSH_5.1p1 Debian-5\n\nthis is a successful attempt. An unsuccessful one should look like this;\n[user@lappie ~]$ telnet host 23\nTrying ip.adr.tld ...\ntelnet: connect to address ip.adr.tld: Connection refused\ntelnet: Unable to connect to remote host: Connection refused\n\nor with nmap \n[user@lappie ~]$ nmap host\n\nStarting Nmap 5.21 ( http://nmap.org ) at 2010-10-07 11:25 CEST\nNmap scan report for host (ip.adr.tld)\nHost is up (0.0052s latency).\nrDNS record for ip.adr.tld : host.domain.tld\nNot shown: 995 closed ports\nPORT     STATE SERVICE\n22/tcp   open  ssh\n80/tcp   open  http\n111/tcp  open  rpcbind\n3000/tcp open  ppp\n5666/tcp open  nrpe\n\nNmap done: 1 IP address (1 host up) scanned in 0.18 seconds\n",
        "url": "https://serverfault.com/questions/188429/how-to-check-if-a-port-is-open-for-remote-systemubuntu"
    },
    {
        "title": "Redis (error) NOAUTH Authentication required",
        "question": "I get the error:\n(error) NOAUTH Authentication required.\n\nWhen in redis-cli and trying to display the KEYS *. I've only set a requirepass not an auth afaiac. I'm in the redis.conf but do not know what to do.",
        "top_answer": "Setting the requirepass configuration directive causes the server to require password authentication with the AUTH command before sending other commands. The redis.conf file states that clearly:\n\nRequire clients to issue AUTH  before processing any other\n  commands.  This might be useful in environments in which you do not trust\n  others with access to the host running redis-server.\n",
        "url": "https://serverfault.com/questions/722803/redis-error-noauth-authentication-required"
    },
    {
        "title": "`mysql_upgrade` is failing with no real reason given",
        "question": "I'm upgrading from MySQL 5.1 to 5.5, running mysql_upgrade and getting this output:\n# mysql_upgrade\nLooking for 'mysql' as: mysql\nLooking for 'mysqlcheck' as: mysqlcheck\nFATAL ERROR: Upgrade failed\n\nAny ideas on where to look for what's happening (or, not happening?) so I can fix whatever is wrong and actually run mysql_upgrade?\nThanks! \nMore output:\n# mysql_upgrade --verbose\nLooking for 'mysql' as: mysql\nLooking for 'mysqlcheck' as: mysqlcheck\nFATAL ERROR: Upgrade failed\n\n# mysql_upgrade --debug-check --debug-info\nLooking for 'mysql' as: mysql\nLooking for 'mysqlcheck' as: mysqlcheck\nFATAL ERROR: Upgrade failed\n\n# mysql_upgrade --debug-info\nLooking for 'mysql' as: mysql\nLooking for 'mysqlcheck' as: mysqlcheck\nFATAL ERROR: Upgrade failed\n\nUser time 0.00, System time 0.00\nMaximum resident set size 1260, Integral resident set size 0\nNon-physical pagefaults 447, Physical pagefaults 0, Swaps 0\nBlocks in 0 out 16, Messages in 0 out 0, Signals 0\nVoluntary context switches 9, Involuntary context switches 5\n\n# mysql_upgrade --debug-check\nLooking for 'mysql' as: mysql\nLooking for 'mysqlcheck' as: mysqlcheck\nFATAL ERROR: Upgrade failed\n\nAfter shutting down mysqld --skip-grant-tables via mysqladmin shutdown and restarting mysql via service mysql start, the error log loops through this set of errors over and over:\n130730 21:03:27 [Note] Plugin 'FEDERATED' is disabled.\n/usr/sbin/mysqld: Table 'mysql.plugin' doesn't exist\n130730 21:03:27 [ERROR] Can't open the mysql.plugin table. Please run mysql_upgrade to create it.\n130730 21:03:27 InnoDB: The InnoDB memory heap is disabled\n130730 21:03:27 InnoDB: Mutexes and rw_locks use GCC atomic builtins\n130730 21:03:27 InnoDB: Compressed tables use zlib 1.2.3.4\n130730 21:03:27 InnoDB: Initializing buffer pool, size = 20.0G\n130730 21:03:29 InnoDB: Completed initialization of buffer pool\n130730 21:03:30 InnoDB: highest supported file format is Barracuda.\nInnoDB: Log scan progressed past the checkpoint lsn 588190222435\n130730 21:03:30  InnoDB: Database was not shut down normally!\nInnoDB: Starting crash recovery.\nInnoDB: Reading tablespace information from the .ibd files...\nInnoDB: Restoring possible half-written data pages from the doublewrite\nInnoDB: buffer...\nInnoDB: Doing recovery: scanned up to log sequence number 588192055067\n130730 21:03:30  InnoDB: Starting an apply batch of log records to the database...\nInnoDB: Progress in percents: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 \nInnoDB: Apply batch completed\nInnoDB: Last MySQL binlog file position 0 81298895, file name /var/log/mysql/mysql-bin.006008\n130730 21:03:33  InnoDB: Waiting for the background threads to start\n130730 21:03:34 InnoDB: 5.5.32 started; log sequence number 588192055067\n130730 21:03:34 [Note] Recovering after a crash using /var/log/mysql/mysql-bin\n130730 21:03:34 [Note] Starting crash recovery...\n130730 21:03:34 [Note] Crash recovery finished.\n130730 21:03:34 [Note] Server hostname (bind-address): '0.0.0.0'; port: 3306\n130730 21:03:34 [Note]   - '0.0.0.0' resolves to '0.0.0.0';\n130730 21:03:34 [Note] Server socket created on IP: '0.0.0.0'.\n130730 21:03:34 [ERROR] Fatal error: Can't open and lock privilege tables: Table 'mysql.host' doesn't exist\n\nMySQL log during start up via mysqld_safe --skip-grant-tables\n130730 21:19:36 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql\n130730 21:19:36 [Note] Plugin 'FEDERATED' is disabled.\n130730 21:19:36 InnoDB: The InnoDB memory heap is disabled\n130730 21:19:36 InnoDB: Mutexes and rw_locks use GCC atomic builtins\n130730 21:19:36 InnoDB: Compressed tables use zlib 1.2.3.4\n130730 21:19:37 InnoDB: Initializing buffer pool, size = 20.0G\n130730 21:19:39 InnoDB: Completed initialization of buffer pool\n130730 21:19:39 InnoDB: highest supported file format is Barracuda.\n130730 21:19:42  InnoDB: Warning: allocated tablespace 566, old maximum was 0\n130730 21:19:42  InnoDB: Waiting for the background threads to start\n130730 21:19:43 InnoDB: 5.5.32 started; log sequence number 588192055067\n130730 21:19:43 [Note] Server hostname (bind-address): '0.0.0.0'; port: 3306\n130730 21:19:43 [Note]   - '0.0.0.0' resolves to '0.0.0.0';\n130730 21:19:43 [Note] Server socket created on IP: '0.0.0.0'.\n130730 21:19:43 [Warning] Can't open and lock time zone table: Table 'mysql.time_zone_leap_second' doesn't exist trying to live without them\n130730 21:19:43 [ERROR] Can't open and lock privilege tables: Table 'mysql.servers' doesn't exist\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'events_waits_current' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'events_waits_history' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'events_waits_history_long' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'setup_consumers' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'setup_instruments' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'setup_timers' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'performance_timers' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'threads' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'events_waits_summary_by_thread_by_event_name' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'events_waits_summary_by_instance' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'events_waits_summary_global_by_event_name' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'file_summary_by_event_name' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'file_summary_by_instance' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'mutex_instances' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'rwlock_instances' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'cond_instances' has the wrong structure\n130730 21:19:43 [ERROR] Native table 'performance_schema'.'file_instances' has the wrong structure\n130730 21:19:43 [Note] /usr/sbin/mysqld: ready for connections.\nVersion: '5.5.32-0ubuntu0.12.04.1-log'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  (Ubuntu)\n\nAs I understand it, all the table structure/existence issues (as it relates to mysql system tables) should be corrected by running mysql_upgrade :",
        "top_answer": "I think that it needs username and password\nmysql_upgrade -u root -p\n\nIf I don't pass them I get your error\nEdit: thanks to the comments now I know that there are other reasons, maybe less frequent but it's best to be aware of them too\nSo you get that error when\n\nyou didn't pass username and password\nyou passed your credentials, but they were wrong\nthe MySQL server isn't running\nthe permissions' tables are ruined (then you must restart MySQL with mysqld --skip-grant-table)\nthe table mysql.plugin is missing (you'll see an error about that when starting MySQL which suggests to run... mysql_upgrade, and that fails. You probably have some obsolete configuration in my.cnf)\n",
        "url": "https://serverfault.com/questions/527422/mysql-upgrade-is-failing-with-no-real-reason-given"
    },
    {
        "title": "Search ec2 instance by its name from aws command line tool",
        "question": "I have an instance named dev-server-03. Now how can I search all dev-server-* instances from command line?\nI am using aws cli tool.",
        "top_answer": "Assuming that you are using the convention of putting the name of the instance in a tag with the key of \"Name\" (this is what the AWS Console does when you enter a name), then you can use the --filters option to list those instances with aws-cli:\naws ec2 describe-instances --filters 'Name=tag:Name,Values=dev-server-*'\n\nIf you just wanted the instance ids of those instances, you could use:\naws ec2 describe-instances --filters 'Name=tag:Name,Values=dev-server-*' \\\n  --output text --query 'Reservations[*].Instances[*].InstanceId'\n\nNote: --query may require a recent version of aws-cli but it's worth getting.",
        "url": "https://serverfault.com/questions/560337/search-ec2-instance-by-its-name-from-aws-command-line-tool"
    },
    {
        "title": "Better logging for cronjobs? Send cron output to syslog?",
        "question": "I am looking for a better way to log cronjobs. Most cronjobs tend to spam email or the console, get ignored, or create yet another logfile.\nIn this case, I have a Nagios NSCA script which sends data to a central Nagios sever. This send_nsca script also prints a single status line to STDOUT, indicating success or failure.\n0 * * * * root    /usr/local/nagios/sbin/nsca_check_disk\n\nThis emails the following message to root@localhost, which is then forwarded to my team of sysadmins. Spam.\nforwarded   nsca_check_disk: 1 data packet(s) sent to host successfully.\nI'm looking for a logging method which:\n\nDoesn't spam the messages to email or the console\nDon't create yet another krufty logfile which requires cleanup months or years later.\nCapture the log information somewhere, so it can be viewed later if desired.\nWorks on most unixes\nFits into an existing log infrastructure.\nUses common syslog conventions like 'facility' and 'priority'\nCan work with third party scripts which don't always do logging internally.\n",
        "top_answer": "In the process of writing this question, I answered myself. So I'll answer myself \"Jeopardy-style\". This expands on the answer provided by Dennis Williamson.\nThe following will send any Cron output to /usr/bin/logger (including stderr, which is converted to stdout using 2>&1), which will send to syslog, with a 'tag' of nsca_check_disk. Syslog handles it from there. Since these systems (CentOS and FreeBSD) already have built-in log rotation mechanisms, I don't need to worry about a log like /var/log/mycustom.log filling up a disk.\n*/5 * * * * root    /usr/local/nagios/sbin/nsca_check_disk 2>&1 | /usr/bin/logger -t nsca_check_disk\n\n/var/log/messages now has one additional message which says this:\nApr 29, 17:40:00 192.168.6.19 nsca_check_disk: 1 data packet(s) sent to host successfully.\n\nI like /usr/bin/logger , because it works well with an existing syslog configuration and infrastructure, and is included with most Unix distros. Most *nix distributions already do log rotation and do it well.",
        "url": "https://serverfault.com/questions/137468/better-logging-for-cronjobs-send-cron-output-to-syslog"
    },
    {
        "title": "How can I automatically restart a Windows service if it crashes?",
        "question": "I have a Windows service that exits unexpectedly every few days. Is there a simple way to monitor it to make sure it gets restarted quickly if it crashes?",
        "top_answer": "Under the Services application, select the properties of the service in question.\nView the recovery tab - there are all sorts of options - I'd set First & Second Failure to Restart the Service, Third to run a batch program that BLAT's out an email with the third failure notification.\nYou should also set the Reset Fail Count to 1 to reset the fail count daily.\nEDIT:\nLooks like you can do this via a command line:\nSC failure w3svc reset= 432000  actions= restart/30000/restart/60000/run/60000\nSC failure w3svc command= \"MyBatchFile.cmd\"\n\nYour MyBatchFile.CMD file can look like this:\nblat - -body \"Service W3svc Failed\" -subject \"SERVICE ERROR\" -to [email\u00a0protected] -server SMTP.Example.com -f [email\u00a0protected]\n",
        "url": "https://serverfault.com/questions/48600/how-can-i-automatically-restart-a-windows-service-if-it-crashes"
    },
    {
        "title": "How to mount a CD/DVD ISO in Windows 7?",
        "question": "Does Windows 7 have native support for mounting CD/DVD ISO images?  If not, what is the best tool to use for that under Windows 7 64-bit?  I am looking for a solution to allow installing MSDN downloads without burning them to CD/DVD.",
        "top_answer": "My preference is Slysoft Virtual Clone Drive. It's great because:\n\nyou can mount/dismount by right-clicking on the drive\nthe drive remembers what has been mounted before\nyou can mount an iso by right-clicking the ISO itself.\n\nNo issues with device driver signing, etc. I dumped Daemon Tools a while ago. Using it on Win7 7100 64 bit with no problem. Feels quite fast.",
        "url": "https://serverfault.com/questions/1797/how-to-mount-a-cd-dvd-iso-in-windows-7"
    },
    {
        "title": "How the heck is http://to./ a valid domain name?",
        "question": "Apparently it's a URL shortener. It resolves just fine in Chrome and Firefox. How is this a valid top-level domain?\nUpdate: for the people saying it's browser shenanigans, why is it that: http://com./ does not take me to: http://www.com/?\nAnd, do browsers ever send you a response from some place other than what's actually up in the address bar? Aside from framesets and things like that, I thought browsers tried really hard to send you content only from the site in the address bar, to help guard against phishing.",
        "top_answer": "Basically, someone has managed to convince the owners of the ccTLD 'to.' (Tonga?) to assign the A record to their own IP address. Quite a coup in the strange old world of URL shorteners.\nNormally these top-levels would not have IP addresses assigned via a standard A record, but there is nothing to say that the same could not be done to .uk, .com, .eu, etc.\nStrictly speaking there is no reason to have the '.' specified, though it should prevent your browser from trying other combinations like 'to.yourdomain.com' first, and speed up the resolution of the address. It might also confuse browsers, as there is no dot, but Safari at least seems to work ok with it.",
        "url": "https://serverfault.com/questions/90737/how-the-heck-is-http-to-a-valid-domain-name"
    },
    {
        "title": "Bash: Difference between > and >> operator? [closed]",
        "question": "I  don't know much about bash.\nMy instructor asked me to make a cat script and to observe the output and then tell what is the operator > and what is the difference between the operators > & >>.\nI am unable to find any justifications. Could you help?",
        "top_answer": "The > sign is used for redirecting the output of a program to something other than stdout (standard output, which is the terminal by default).\nThe >> appends to a file or creates the file if it doesn't exist.\nThe > overwrites the file if it exists or creates it if it doesn't exist.\nIn either case, the output of the program is stored in the file whose name is provided after the redirection operator.\nExamples:\n$ ls > allmyfiles.txt creates the file \"allmyfiles.txt\" and fills it with the directory listing from the ls command\n$ echo \"End of directory listing\" >> allmyfiles.txt adds \"End of directory listing\" to the end of the file \"allmyfiles.txt\"\n$ > newzerobytefile creates a new zero byte file with the name \"newzerobytefile\" or overwrites an existing file of the same name (making it zero bytes in size)",
        "url": "https://serverfault.com/questions/196734/bash-difference-between-and-operator"
    },
    {
        "title": "What is the difference between iisreset, recycle, refresh and restart?",
        "question": "In IIS7, there are numerous things you can do that seem to restart the website. I am unclear about exactly how they are all related.\n\nrun iisreset from the command line\nrefresh a website\nrecycle an app pool\nrestart a website\n\nCan someone explain exactly what each one does please? ",
        "top_answer": "iisreset will stop and start the World Wide Web Publishing Service. This, of course, applies to all of your application pools. I'm sure you noticed a process being created for each application pool. This process will handle requests for all websites associated with it. When you recycle an application pool, IIS will create a new process (keeping the old one) to serve requests. Then it tries to move all requests on the new process. After a timeout the old process will be killed automatically. You usually recycle your application pool to get rid of leaked memory (you might have a problem in your application if this needs to be a regular operation, even though it is recommended to have a scheduled recycle). As for restarting a website, it just stops and restarts serving requests for that particular website. It will continue to serve other websites on the same app pool with no interruptions.\nIf you have a session oriented application, all of the above will cause loss of session objects.\nRefreshing a website has no effect on the service/process/website and is merely a UI command to refresh the treeview (maybe you added a directory you don't see in the management console).",
        "url": "https://serverfault.com/questions/247425/what-is-the-difference-between-iisreset-recycle-refresh-and-restart"
    },
    {
        "title": "What is the \"slash\" after the IP? [duplicate]",
        "question": "In Amazon EC2, where I set \"security groups\", It says: Source:\n0.0.0.0/0\nAnd then it gives an example of: 192.168.2.0/24\nWhat is \"/24\"?\nI know what port and IP is.",
        "top_answer": "It represents the CIDR netmask - after the slash you see the number of bits the netmask has set to 1. So the /24 on your example is equivalent to 255.255.255.0.\nThis defines the subnet the IP is in - IPs in the same subnet will be identical after applying the netmask. Take AND to mean bitwise &. Then:\n192.168.2.5 AND 255.255.255.0 = 192.168.2.0\n192.168.2.100 AND 255.255.255.0 = 192.168.2.0\n\nbut, for example:\n192.168.3.100 AND 255.255.255.0 = 192.168.3.0 != 192.168.2.0\n\nThe most common CIDR netmasks are probably /32 (255.255.255.255 - a single host); /24 (255.255.255.0); /16 (255.255.0.0); and /8 (255.0.0.0).\nI think it's easier to make sense of the numbers if you remember that 255.255.255.255 can be written as FF.FF.FF.FF - and F is of course the same as binary 1111. So you substract as many 1's as the difference between 32 and the CIDR netmask to know how much of the IP address \"belongs\" to its subnet. If this is confusing you can probably skip it and keep to the previously mentioned common ones for the time being, it's just the way I prefer to think about this.\nVery simply, it is the number of most significant bits that would remain same in the network. Alternately it is (32 less the specified number) of least significant bits that would change in the network.\nhttps://www.rfc-editor.org/rfc/rfc1878",
        "url": "https://serverfault.com/questions/270005/what-is-the-slash-after-the-ip"
    },
    {
        "title": "How to diagnose a 500 Internal Server Error on IIS 7.5 when nothing is written to the event log?",
        "question": "I've just deployed an update to an existing ASP.NET MVC3 site (it was already configured) and I'm getting the IIS blue screen of death stating \n\nHTTP Error 500.0 - Internal Server Error\n  The page cannot be displayed because an internal server error has occurred.\n\nHowever; there is nothing showing up in the Application Event Log where I would expect to see a (more) detailed description of the entry.\nHow can I go about diagnosing this issue?",
        "top_answer": "Take a look at IIS7's Failed Request Tracing feature:\n\nTroubleshooting Failed Requests Using Tracing in IIS 7\nTroubleshoot with Failed Request Tracing\n\nThe other thing I would do is tweak your <httpErrors> setting because IIS may be swallowing an error message from further up the pipeline:\n<configuration>\n  <system.webServer>\n    <httpErrors existingResponse=\"PassThrough\" />\n  </system.webServer>\n</configuration>\n\nIf the site is written in Classic ASP then be sure to turn on the Send Errors to Browser setting in the ASP configuration feature:\n\nAnd finally, if you're using Internet Explorer then make sure you've turned off Show friendly HTTP error messages in the Advanced settings (though I suspect you've done that already or are using a different browser).",
        "url": "https://serverfault.com/questions/407954/how-to-diagnose-a-500-internal-server-error-on-iis-7-5-when-nothing-is-written-t"
    },
    {
        "title": "104: Connection reset by peer while reading response header from upstream (Nginx)",
        "question": "I have a server which was working ok until 3rd Oct 2013 at 10:50am when it began to intermittently return \"502 Bad Gateway\" errors to the client.\nApproximately 4 out of 5 browser requests succeed but about 1 in 5 fail with a 502.\nThe nginx error log contains many hundreds of these errors;\n2013/10/05 06:28:17 [error] 3111#0: *54528 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 66.249.66.75, server: www.bec-components.co.uk  request: \"\"GET /?_n=Fridgefreezer/Hotpoint/8591P;_i=x8078 HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"www.bec-components.co.uk\"\n\nHowever the PHP error log does not contain any matching errors.\nIs there a way to get PHP to give me more info about why it is resetting the connection?\nThis is nginx.conf;\nuser              www-data;\nworker_processes  4;\nerror_log         /var/log/nginx/error.log;\npid               /var/run/nginx.pid;\n\nevents {\n   worker_connections  1024;\n}\n\nhttp {\n  include          /etc/nginx/mime.types;\n  access_log       /var/log/nginx/access.log;\n\n  sendfile               on;\n  keepalive_timeout      30;\n  tcp_nodelay            on;\n  client_max_body_size   100m;\n\n  gzip         on;\n  gzip_types   text/plain application/xml text/javascript application/x-javascript text/css;\n  gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\";\n\n  include /gvol/sites/*/nginx.conf;\n\n}\n\nAnd this is the .conf for this site;\nserver {\n\n  server_name   www.bec-components.co.uk bec3.uk.to bec4.uk.to bec.home;\n  root          /gvol/sites/bec/www/;\n  index         index.php index.html;\n\n  location ~ \\.(js|css|png|jpg|jpeg|gif|ico)$ {\n    expires        2592000;   # 30 days\n    log_not_found  off;\n  }\n\n  ## Trigger client to download instead of display '.xml' files.\n  location ~ \\.xml$ {\n    add_header Content-disposition \"attachment; filename=$1\";\n  }\n\n   location ~ \\.php$ {\n      fastcgi_read_timeout  3600;\n      include               /etc/nginx/fastcgi_params;\n      keepalive_timeout     0;\n      fastcgi_param         SCRIPT_FILENAME  $document_root$fastcgi_script_name;\n      fastcgi_pass          127.0.0.1:9000;\n      fastcgi_index         index.php;\n   }\n}\n\n## bec-components.co.uk ##\nserver {\n   server_name   bec-components.co.uk;\n   rewrite       ^/(.*) http://www.bec-components.co.uk$1 permanent;\n}\n",
        "top_answer": "I'd always trust if my web servers are telling me 502 Bad Gateway.\n\nWhat is the uptime of your FastCGI/NGINX process?\nDo you monitor network connections?\nCan you confirm/deny a change of visitors count around that day?\n\nWhat the error means\nYour FastCGI process is not accessible by NGINX; either to slow or not corresponding at all. Bad gateway means that NGINX cannot complete the fastcgi_pass step to that defined resources listening on 127.0.0.1:9000 and at that very specific moment.\nYour inital error logs tells it all:\nrecv() failed \n    -> nginx failed\n\n(104: Connection reset by peer) while reading response header from upstream, \n    -> no complete answer, or no answer at all\nupstream: \"fastcgi://127.0.0.1:9000\", \n    -> who is he, who failed???\n\nFrom my limited point of view, I'd suggest:\n\nTo restart your FastCGI process or server\nTo check your access.log\nTo enable enable debug.log\n",
        "url": "https://serverfault.com/questions/543999/104-connection-reset-by-peer-while-reading-response-header-from-upstream-nginx"
    },
    {
        "title": "Does orientation affect hard drive lifespan?",
        "question": "Are there any studies or evidence which show that mounting a hard drive horizontally is better than vertically for the lifespan of the device? Or upside down, or any direction for that matter.",
        "top_answer": "The quotes in this thread from WD and Seagate suggest not.\nTo precis the link:  Seagate, Maxtor and WD drives can be used in any orientation including upside down.",
        "url": "https://serverfault.com/questions/13839/does-orientation-affect-hard-drive-lifespan"
    },
    {
        "title": "What should NOT be managed by puppet?",
        "question": "I'm learning my way through configuration management in general and using puppet to implement it in particular, and I'm wondering what aspects of a system, if any, should not be managed with puppet?\nAs an example we usually take for granted that hostnames are already set up before lending the system to puppet's management. Basic IP connectivity, at least on the network used to reach the puppetmaster, has to be working. Using puppet to automatically create dns zone files is tempting, but DNS reverse pointers ought to be already in place before starting up the thing or certificates are going to be funny.\nSo should I leave out IP configuration from puppet? Or should I set it up prior to starting puppet for the first time but manage ip addresses with puppet nonetheless? What about systems with multiple IPs (eg. for WAN, LAN and SAN)?\nWhat about IPMI? You can configure most, if not all, of it with ipmitool, saving you from getting console access (physical, serial-over-lan, remote KVM, whatever) so it could be automated with puppet. But re-checking its state at every puppet agent run doesn't sound cool to me, and basic lights out access to the system is something I'd like to have before doing anything else.\nAnother whole story is about installing updates. I'm not going in this specific point, there are already many questions on SF and many different philosophies between different sysadmins. Myself, I decided to not let puppet update things (eg. only ensure => installed) and do updates manually as we are already used to, leaving the automation of this task to a later day when we are more confident with puppet (eg. by adding MCollective to the mix).\nThose were just a couple of examples I got right now on my mind.\nIs there any aspect of the system that should be left out of reach from puppet?\nOr, said another way, where is the line between what should be set up at provisioning time and \"statically\" configured in the system, and what is handled through centralized configuration management?",
        "top_answer": "General rule: If you're using configuration management, manage every aspect of the configuration that you can. The more you centralize the easier it will be to scale your environment out.\nSpecific examples (cribbed from the question, all \"This is why you want to manage it\" narratives):\n\nIP Network configuration\nOK, sure, you configured an address/gateway/NS on the machine before you dropped it in the rack.  I mean if you didn't how would you run puppet to do the rest of the config?\nBut say now you add another nameserver to your environment and you need to update all your machines -- Don't you want your configuration management system to do that for you?\nOr say your company gets acquired, and your new parent company demands that you change from your 192.168.0.0/24 addressing to 10.11.12.0/24 to fit into their numbering system.\nOr you suddenly get a massive government contract -- Only catch is you have to turn up IPv6 RIGHT FREAKIN' NOW or the deal is blown....\nLooks like network configuration is something we'd like to manage...\n\nIPMI Configuration\nJust like with IP addresses, I'm sure you set this up before you put the machine in the rack -- It's just good common sense to enable IPMI, remote console, etc. on any machine that has the capability, and those configurations don't change much...\n... Until that hypothetical acquisition I mentioned in IP Configuration above -- The reason you were forced to vacate those 192.168-net addresses is because that's IPMI-land according to your new corporate overlords, and you need to go update all your IPMI cards NOW because they're gonna be trampling on someone's reserved IP space.\nOK, it's a bit of a stretch here, but like you said - all of it can be managed with ipmitool, so why not have Puppet run the tool and confirm the configuration while it's doing all of its other stuff? I mean it's not going to hurt anything, so we may as well include IPMI too...\n\nUpdates\nSoftware updates are more of a gray area -- In my organization we evaluated puppet for this and found it \"sorely lacking\", so we use radmind for this purpose.  There's no reason Puppet can't call radmind though -- In fact if/when we migrate to Puppet for configuration management that's exactly what's going to happen!\nThe important thing here is to have all your updates installed in a standard way (either standard across the organization, or standard within platforms) -- There's no reason Puppet shouldn't be launching your update process, as long as you've thoroughly tested everything to ensure that Puppet won't mess up anything.\nThere's also no reason why Puppet can't call out to a tool that's better suited for this task if you've determined that Puppet can't do a good job on its own...",
        "url": "https://serverfault.com/questions/414758/what-should-not-be-managed-by-puppet"
    },
    {
        "title": "Are IP addresses \"trivial to forge\"?",
        "question": "I was reading through some of the notes on Google's new public DNS service:\n\nPerformance Benefits\nSecurity Benefits\n\nI noticed under the security section this paragraph:\n\nUntil a standard system-wide solution to DNS vulnerabilities is universally implemented, such as the DNSSEC2 protocol, open DNS resolvers need to independently take some measures to mitigate against known threats. Many techniques have been proposed; see IETF RFC 4542: Measures for making DNS more resilient against forged answers for an overview of most of them. In Google Public DNS, we have implemented, and we recommend, the following approaches:\n\nOverprovisioning machine resources to protect against direct DoS attacks on the resolvers themselves. Since IP addresses are trivial for attackers to forge, it's impossible to block queries based on IP address or subnet; the only effective way to handle such attacks is to simply absorb the load.\n\n\nThat is a depressing realization; even on Stack Overflow / Server Fault / Super User, we frequently use IP addresses as the basis for bans and blocks of all kinds.\nTo think that a \"talented\" attacker could trivially use whatever IP address they want, and synthesize as many unique fake IP addresses as they want, is really scary!\nSo my question(s):\n\nIs it really that easy for an attacker to forge an IP address in the wild?\nIf so, what mitigations are possible?\n",
        "top_answer": "As stated by many others, IP headers are trivial to forge, as long as one doesn't care about receiving a response. This is why it is mostly seen with UDP, as TCP requires a 3-way handshake. One notable exception is the SYN flood, which uses TCP and attempts to tie up resources on a receiving host; again, as the replies are discarded, the source address does not matter.\nA particularly nasty side-effect of the ability of attackers to spoof source addresses is a backscatter attack. There is an excellent description here, but briefly, it is the inverse of a traditional DDoS attack:\n\nGain control of a botnet.\nConfigure all your nodes to use the same source IP address for malicious packets. This IP address will be your eventual victim.\nSend packets from all of your controlled nodes to various addresses across the internet, targeting ports that generally are not open, or connecting to valid ports (TCP/80) claiming to be part of an already existing transaction.\n\nIn either of the cases mentioned in (3), many hosts will respond with an ICMP unreachable or a TCP reset, targeted at the source address of the malicious packet. The attacker now has potentially thousands of uncompromised machines on the network performing a DDoS attack on his/her chosen victim, all through the use of a spoofed source IP address.\nIn terms of mitigation, this risk is really one that only ISPs (and particularly ISPs providing customer access, rather than transit) can address. There are two main methods of doing this:\n\nIngress filtering - ensuring packets coming in to your network are sourced from address ranges that live on the far side of the incoming interface. Many router vendors implement features such as unicast reverse path forwarding, which use the router's routing and forwarding tables to verify that the next hop of the source address of an incoming packet is the incoming interface. This is best performed at the first layer 3 hop in the network (i.e. your default gateway.)\nEgress filtering - ensuring that packets leaving your network only source from address ranges you own. This is the natural complement to ingress filtering, and is essentially part of being a 'good neighbor'; ensuring that even if your network is compromised by malicious traffic, that traffic is not forwarded to networks you peer with.\n\nBoth of these techniques are most effective and easily implemented when done so in 'edge' or 'access' networks, where clients interface with the provider. Implementing ingress/egress filtering above the access layer becomes more difficult, due to the complexities of multiple paths and asymmetric routing.\nI have seen these techniques (particularly ingress filtering) used to great effect within an enterprise network. Perhaps someone with more service provider experience can give more insight into the challenges of deploying ingress/egress filtering on the internet at large. I imagine hardware/firmware support to be a big challenge, as well as being unable to force upstream providers in other countries to implement similar policies...",
        "url": "https://serverfault.com/questions/90725/are-ip-addresses-trivial-to-forge"
    },
    {
        "title": "How can I get a list of shared directories on local Windows server?",
        "question": "If I have a Windows server (typically 2000, 2003 or 2008), is there a simple way to list all local directories shared on that server?\nI can find the shares themselves easily enough, but I would love a quick way to find the local directories they represent on disk.\nThanks!",
        "top_answer": "You can go into computer management (right click my computer, select manage), expand the Shared Folders node and see a list of all shares, connected sessions and open files.\nFor W2K8, you do this in Server Manager instead: Roles -> File Services -> Share and Storage Management; the Shares tab in the center of the window.\nFor listing shares of remote servers, note that NET VIEW svr_name will only show user shares, no admin or hidden shares.  Adding the /all switch at the end will show these others (for W2K8).\nC:\\>net view sx1\nShared resources at sx1\n\nShare name    Type  Used as  Comment\n --------------------------------------------\nSHARE_CIFS    Disk\nThe command completed successfully.\n\nC:\\>net view sx1 /all\nShared resources at sx1\n\nShare name    Type  Used as  Comment\n --------------------------------------------\nADMIN$        Disk           Remote Admin\nSHARE_CIFS    Disk\nC$            Disk           Default share\nIPC$          IPC            Remote IPC\nThe command completed successfully.\n",
        "url": "https://serverfault.com/questions/17710/how-can-i-get-a-list-of-shared-directories-on-local-windows-server"
    },
    {
        "title": "How do I determine the block size of an ext3 partition on Linux?",
        "question": "How do I determine the block size of an ext3 partition on Linux?",
        "top_answer": "# tune2fs -l /dev/sda1 | grep -i 'block size'\nBlock size:               1024\n\nReplace /dev/sda1 with the partition you want to check.",
        "url": "https://serverfault.com/questions/29887/how-do-i-determine-the-block-size-of-an-ext3-partition-on-linux"
    },
    {
        "title": "The following signatures couldn't be verified because the public key is not available: NO_PUBKEY",
        "question": "My environment:\n# uname -a\nLinux app11 4.9.0-5-amd64 #1 SMP Debian 4.9.65-3+deb9u2 (2018-01-04) x86_64 GNU/Linux\n# \n# cat /etc/*release\nPRETTY_NAME=\"Debian GNU/Linux 9 (stretch)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"9\"\nVERSION=\"9 (stretch)\"\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n# \n\nwhile trying to run apt-get update, I get bunch of errors:\n# apt-get update\nIgn:1 http://deb.debian.org/debian stretch InRelease\nHit:2 http://security.debian.org stretch/updates InRelease\nHit:3 http://deb.debian.org/debian stretch-updates InRelease             \nHit:4 http://deb.debian.org/debian stretch-backports InRelease           \nHit:5 http://deb.debian.org/debian stretch Release \nGet:6 http://packages.cloud.google.com/apt cloud-sdk-stretch InRelease [6,377 B]\nIgn:7 https://artifacts.elastic.co/packages/6.x/apt stable InRelease\nHit:8 https://artifacts.elastic.co/packages/6.x/apt stable Release\nGet:9 http://packages.cloud.google.com/apt google-compute-engine-stretch-stable InRelease [3,843 B]\nGet:10 http://packages.cloud.google.com/apt google-cloud-packages-archive-keyring-stretch InRelease [3,876 B]\nHit:11 https://download.docker.com/linux/debian stretch InRelease\nErr:6 http://packages.cloud.google.com/apt cloud-sdk-stretch InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nErr:9 http://packages.cloud.google.com/apt google-compute-engine-stretch-stable InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nErr:10 http://packages.cloud.google.com/apt google-cloud-packages-archive-keyring-stretch InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nFetched 6,377 B in 0s (7,132 B/s)\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://packages.cloud.google.com/apt cloud-sdk-stretch InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://packages.cloud.google.com/apt google-compute-engine-stretch-stable InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://packages.cloud.google.com/apt google-cloud-packages-archive-keyring-stretch InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-stretch/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: Failed to fetch http://packages.cloud.google.com/apt/dists/google-compute-engine-stretch-stable/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: Failed to fetch http://packages.cloud.google.com/apt/dists/google-cloud-packages-archive-keyring-stretch/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: Some index files failed to download. They have been ignored, or old ones used instead.\n# \n\nPlease advise.",
        "top_answer": "Per Installing Google Cloud SDK \u00a0|\u00a0 Cloud SDK Documentation - Debian/Ubuntu:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n\nOR\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n\nfollow by:\nsudo apt-get update\n",
        "url": "https://serverfault.com/questions/906972/the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-avai"
    },
    {
        "title": "Disable a service from starting at all runlevels?",
        "question": "I have a service foo which currently starts at runlevel 3 and above. How can I stop it from doing so, without using update-rc.d foo stop 3 ., which (if I understand it correctly) would turn off the service at each runlevel change. \n(ie., if I was at runlevel 1 and enabled foo, then when I changed to runlevel 3 it would be disabled, no?)\nRunning Debian GNU/Linux Lenny.",
        "top_answer": "The \"stop\" term does not prevent the daemon from starting but rather shuts it down while entering the specified runlevel.\nIf you just want to remove a service/daemon from a single runlevel, update-rc.d as pointed out bei freiheit or simply remove the symlink from /etc/rcX.d/, where X is your runlevel. If you don't want the service to start automatically, update-rc.d -f foo remove will do the trick.",
        "url": "https://serverfault.com/questions/32438/disable-a-service-from-starting-at-all-runlevels"
    },
    {
        "title": "php cli memory limit",
        "question": "I am getting a memory error in a php cron job:\nFatal error: Allowed memory size of 67108864 bytes exhausted (tried to allocate 71 bytes)   in /opt/matrix/core/lib/DAL/DAL.inc on line 830\nThe applicable parts of the crontab are:\n$ sudo crontab -u www-data -l\nMAILTO=root\n# m h  dom mon dow   command\n*/15 * * * * php /opt/matrix/core/cron/run.php /opt/matrix\n\nI am running on Debian Squeeze, fully updated.\nThe obvious solution would be that the cli has a low memory limit (of 64MB).  However, /etc/php5/cli/php.ini says it's unlimited.\n$ cat /etc/php5/cli/php.ini | grep memory_limit\nmemory_limit = -1\n\nI read somewhere that it could be different for different users, and since the process is running as www-data, i ran:\n$ sudo -u www-data -s\n$ php -i | grep memory_limit\nmemory_limit => -1 => -1\nsuhosin.memory_limit => 0 => 0\n\nEven the apache/php.ini has a higher limit than the error is claiming:\n$ sudo cat /etc/php5/apache2/php.ini | grep memory_limit\nmemory_limit = 128M\n\nWhat am I missing?  Where is this memory limit?",
        "top_answer": "IIRC, an unlimited memory_limit isn't supported by the CLI (I'll try to find a source for this) but for now, try passing it into the command:\nphp -d memory_limit=128M my_script.php\n\nUPDATE\nApparently I was dreaming about the unlimited memory_limit not being supported for php cli. Regardless, it looks like the value from the ini is ignored. The simplest solution should then be to specifically set it in the php command calling the script.\nUPDATE2\nTo answer the question of where the memory limit is coming from, it's most likely being set in the script itself using 'ini_set'.",
        "url": "https://serverfault.com/questions/295584/php-cli-memory-limit"
    },
    {
        "title": "Is Round-Robin DNS \"good enough\" for load balancing static content?",
        "question": "We have a set of shared, static content that we serve up between our websites at http://sstatic.net. Unfortunately, this content is not currently load balanced at all -- it's served from a single server. If that server has problems, all the sites that rely on it are effectively down because the shared resources are essential shared javascript libraries and images.\nWe are looking at ways to load balance the static content on this server, to avoid the single server dependency.\nI realize that round-robin DNS is, at best, a low end (some might even say ghetto) solution, but I can't help wondering -- is round robin DNS a \"good enough\" solution for basic load balancing of static content?\nThere is some discussion of this in the [dns] [load-balancing] tags, and I've read through some great posts on the topic.  \nI am aware of the common downsides of DNS load balancing through multiple round-robin A records:\n\nthere's typically no heartbeats or failure detection with DNS records, so if a given server in the rotation goes down, its A record must manually be removed from the DNS entries\nthe time to live (TTL) must necessarily be set quite low for this to work at all, since DNS entries are cached aggressively throughout the internet\nthe client computers are responsible for seeing that there are multiple A records and picking the correct one\n\nBut, is round robin DNS good enough as a starter, better than nothing, \"while we research and implement better alternatives\" form of load balancing for our static content? Or is DNS round robin pretty much worthless under any circumstances?",
        "top_answer": "Jeff, I disagree, load balancing does not imply redundancy, it's quite the opposite in fact. The more servers you have, the more likely you'll have a failure at a given instant. That's why redundancy IS mandatory when doing load balancing, but unfortunately there are a lot of solutions which only provide load balancing without performing any health check, resulting in a less reliable service.\nDNS roundrobin is excellent to increase capacity, by distributing the load across multiple points (potentially geographically distributed). But it does not provide fail-over. You must first describe what type of failure you are trying to cover. A server failure must be covered locally using a standard IP address takeover mechanism (VRRP, CARP, ...). A switch failure is covered by resilient links on the server to two switches. A WAN link failure can be covered by a multi-link setup between you and your provider, using either a routing protocol or a layer2 solution (eg: multi-link PPP). A site failure should be covered by BGP : your IP addresses are replicated over multiple sites and you announce them to the net only where they are available.\nFrom your question, it seems that you only need to provide a server fail-over solution, which is the easiest solution since it does not involve any hardware nor contract with any ISP. You just have to setup the appropriate software on your server for that, and it's by far the cheapest and most reliable solution.\nYou asked \"what if an haproxy machine fails ?\". It's the same. All people I know who use haproxy for load balancing and high availability have two machines and run either ucarp, keepalived or heartbeat on them to ensure that one of them is always available.\nHoping this helps!",
        "url": "https://serverfault.com/questions/101053/is-round-robin-dns-good-enough-for-load-balancing-static-content"
    },
    {
        "title": "crontab running as a specific user",
        "question": "I need to run a script daily. The script should be run as a specific user (ex. user1) not as root. So I put the cron file at /etc/cron.d and put the user name in the line (2nd column). But it gives an error saying that the command is not found. I suspect that the script was not run as user1's environment. Did I miss something?",
        "top_answer": "Only /etc/crontab and the files in /etc/cron.d/ have a username field.  In that file you can do this:\n1 1 * * * username /path/to/your/script.sh\n\nFrom root's crontab sudo crontab -e you can use:\n1 1 * * * su username -c \"/path/to/your/script.sh\"\n\nOr you can use the user's actual crontab like this:\nsudo crontab -u username -e\n\nThe second column in any crontab file is for the hour that you want the job to run at.  Did you mean the sixth field?",
        "url": "https://serverfault.com/questions/352835/crontab-running-as-a-specific-user"
    },
    {
        "title": "Choosing the shell that SSH uses?",
        "question": "How do I set the shell that is used when a user SSHs to a server. For example I can't stand BASH and need to use ZSH, how do I make it so ZSH is loaded along with my profile (.zsh_profile) when I ssh to the machine. \nI dont want to have to pass a bunch of parameters with ssh either, can't I set the default shell?",
        "top_answer": "Assuming you're running on Linux, you can use the chsh command.\nchsh -s /bin/ksh foo\nchsh -s /bin/bash username\n",
        "url": "https://serverfault.com/questions/106722/choosing-the-shell-that-ssh-uses"
    },
    {
        "title": "Is the PHP option 'cgi.fix_pathinfo' really dangerous with Nginx + PHP-FPM?",
        "question": "There has been a lot of talking about a security issue relative to the cgi.fix_pathinfo PHP option used with Nginx (usually PHP-FPM, fast CGI). \nAs a result, the default nginx configuration file used to say: \n# NOTE: You should have \"cgi.fix_pathinfo = 0;\" in php.ini\n\nHowever, now, the \"official\" Nginx wiki states that PATH_INFO can be handled correctly without disabling the above PHP option. So what?\nQuestions\n\nCan you explain clearly what does cgi.fix_pathinfo do? (official doc just says: \"For more information on PATH_INFO, see the CGI specs\")\nWhat will PHP really do with these PATH_INFO and SCRIPT_FILENAME variables? \nWhy and how can it be dangerous with Nginx? (detailed examples)\nDoes the issue still exist in recent versions of these programs?\nIs Apache vulnerable?\n\nI'm trying to understand the issue at each step. For example, I don't understand why using the php-fpm Unix socket could avoid this problem. ",
        "top_answer": "TL;DR - the fix (which you may not even need) is VERY SIMPLE and at the end of this answer.\nI'll try to address your specific questions, but your misunderstanding of what PATH_INFO is makes the questions themselves a little bit wrong.\n\nFirst question should be \"What is this path info business?\"\n\nPath info is stuff after the script in a URI (should start with a forward slash, but ends before the query arguments, which start with a ?). The last paragraph in the overview section of the Wikipedia article about CGI sums it up nicely. Below the PATH_INFO is \"/THIS/IS/PATH/INFO\":\nhttp://example.com/path/to/script.php/THIS/IS/PATH/INFO?query_args=foo\n\nYour next question should have been: \"How does PHP determine what PATH_INFO and SCRIPT_FILENAME are?\"\n\nEarlier versions of PHP were naive and technically didn't even support PATH_INFO, so what was supposed to be PATH_INFO was munged onto SCRIPT_FILENAME which, yes, is broken in many cases. I don't have an old enough version of PHP to test with, but I believe it saw SCRIPT_FILENAME as the whole shebang: \"/path/to/script.php/THIS/IS/PATH/INFO\" in the above example (prefixed with the docroot as usual).\nWith cgi.fix_pathinfo enabled, PHP now correctly finds \"/THIS/IS/PATH/INFO\" for the above example and puts it into PATH_INFO and SCRIPT_FILENAME gets just the part that points to the script being requested (prefixed with the docroot of course).\nNote: when PHP got around to actually supporting PATH_INFO, they had to add a configuration setting for the new feature so people running scripts that depended on the old behavior could run new PHP versions. That's why there's even a configuration switch for it. It should have been built-in (with the \"dangerous\" behavior) from the start.\n\nBut how does PHP know what part is the script and what it path info? What if the URI is something like:\nhttp://example.com/path/to/script.php/THIS/IS/PATH/INFO.php?q=foo\n\nThat can be a complex question in some environments. What happens in PHP is that it finds the first part of the URI path that does not correspond to anything under the server's docroot. For this example, it sees that on your server you don't have \"/docroot/path/to/script.php/THIS\" but you most certainly do have \"/docroot/path/to/script.php\" so now the SCRIPT_FILENAME has been determined and PATH_INFO gets the rest.\nSo now the good example of the danger that is nicely detailed in the Nginx docs and in Hrvoje \u0160poljar's answer (you can't be fussy about such a clear example) becomes even more clear: given Hrvoje's example (\"http://example.com/foo.jpg/nonexistent.php \"), PHP sees a file on your docroot \"/foo.jpg\" but it does not see anything called \"/foo.jpg/nonexistent.php\" so SCRIPT_FILENAME gets \"/foo.jpg\" (again, prefixed with docroot) and PATH_INFO gets \"/nonexistent.php\".\n\nWhy and how it can be dangerous should now be clear:\n\nThe web server really isn't at fault - it's merely proxying the URI to PHP, which innocently finds that \"foo.jpg\" actually contains PHP content, so it executes it (now you've been pwned!). This is NOT particular to Nginx per se.\n\nThe REAL problem is that you let untrusted content be uploaded somewhere without sanitizing and you allow other arbitrary requests to the same location, which PHP happily executes when it can.\nNginx and Apache could be built or configured to prevent requests using this trickery, and there are plenty of examples for how to do that, including in user2372674's answer. This blog article explains the problem nicely, but it's missing the right solution.\nHowever, the best solution is to just make sure PHP-FPM is configured correctly so that it will never execute a file unless it ends with \".php\". It's worth noting that recent versions of PHP-FPM (~5.3.9+?) have this as default, so this danger isn't so much problem any more.\n\nThe Solution\nIf you have a recent version of PHP-FPM (~5.3.9+?), then you need to do nothing, as the safe behaviour below is already the default.\nOtherwise, find php-fpm's www.conf file (maybe /etc/php-fpm.d/www.conf, depends on your system). Make sure you have this:\nsecurity.limit_extensions = .php\n\nAgain, that's default in many places these days.\nNote that this doesn't prevent an attacker from uploading a \".php\" file to a WordPress uploads folder and executing that using the same technique. You still need to have good security for your applications.",
        "url": "https://serverfault.com/questions/627903/is-the-php-option-cgi-fix-pathinfo-really-dangerous-with-nginx-php-fpm"
    },
    {
        "title": "What does [::] mean as an ip address? Bracket colon colon bracket",
        "question": "When I run netstat there are some entries such as TCP  [::]:8010  computername LISTENING\nWhat does that mean? It is impossible to search for...",
        "top_answer": ":: can be used once in an IPv6 address to replace a consecutive blocks of zeroes. It can be any length of zeroes as long as it is greater than a single block. All zeroes in a single block can be represented by :0: instead of writing out all four zeroes.\nIn this case, it means all zeroes, or the IPv6 equivalent of the IPv4 0.0.0.0\n\nAs an example of something that is not all zeroes:\nfe80:0000:0000:0000:34cb:9850:4868:9d2c\nWhich is properly \"reduced\" to:\nfe80::34cb:9850:4868:9d2c\nAs an example, it can also be written as:\nfe80:0:0:0:34cb:9850:4868:9d2c\nbut that is far less common than just \"double coloning\" it.",
        "url": "https://serverfault.com/questions/444554/what-does-mean-as-an-ip-address-bracket-colon-colon-bracket"
    },
    {
        "title": "Question marks showing in ls of directory. IO errors too",
        "question": "Has anyone seen this before?   I've got a raid 5 mounted on my server and for whatever reason it started showing this: \n\njason@box2:/mnt/raid1/cra$ ls -alh\nls: cannot access e6eacc985fea729b2d5bc74078632738: Input/output error\nls: cannot access 257ad35ee0b12a714530c30dccf9210f: Input/output error\ntotal 0\ndrwxr-xr-x 5 root root 123 2009-08-19 16:33 .\ndrwxr-xr-x 3 root root  16 2009-08-14 17:15 ..\n?????????? ? ?    ?      ?                ? 257ad35ee0b12a714530c30dccf9210f\ndrwxr-xr-x 3 root root  57 2009-08-19 16:58 9c89a78e93ae6738e01136db9153361b\n?????????? ? ?    ?      ?                ? e6eacc985fea729b2d5bc74078632738\n\nThe md5 strings are actual directory names and not part of the error.  The question marks are odd, and any directory with a question mark throws an io error when you attempt to use/delete/etc it.\nI was unable to umount the drive due to \"busy\".  Rebooting the server \"fixed\" it but it was throwing some raid errors on shutdown.  I have configured two raid 5 arrays and both started doing this on random files.  Both are using the following config:\n\nmkfs.xfs -l size=128m -d agcount=32\nmount -t xfs -o noatime,logbufs=8\n\nNothing too fancy, but part of an optimized config for this box.  We're not partitioning the drives and that was suggested as a possible issue.  Could this be the culprit?",
        "top_answer": "I had a similar problem because my directory had read (r) but not execute (x) rights.\nMy directory listing showed:\nmyname@srv:/home$ ls -l service/mail/\nls: cannot access service/mail/001_SERVICE INBOX: Permission denied\ntotal 0\n-????????? ? ? ? ?                ? 001_SERVICE INBOX\nd????????? ? ? ? ?                ? 01_CURRENT SERVICE\n\nThe mail directory had the r bit set, but not the x that you need for listing or search and access.\nDoing sudo chmod -R g+x mail solved this problem.",
        "url": "https://serverfault.com/questions/65616/question-marks-showing-in-ls-of-directory-io-errors-too"
    },
    {
        "title": "How can I query my system via command line to see if a KB patch is installed?",
        "question": "I'm looking to find out if a KB is installed via command line.",
        "top_answer": "In addition to systeminfo there is also\nwmic qfe\nExample: \nwmic qfe get hotfixid | find \"KB99999\"\nwmic qfe | find \"KB99999\"\n\nThere is also update.exe\nOr from powershell, just adjust it for your needs:\nGet-WmiObject -query 'select * from win32_quickfixengineering' | foreach {$_.hotfixid}\n",
        "url": "https://serverfault.com/questions/263847/how-can-i-query-my-system-via-command-line-to-see-if-a-kb-patch-is-installed"
    },
    {
        "title": "Is there a maximum size for content of an HTTP POST?",
        "question": "Is there a maximum size for an HTTP POST? And if there is a max size, is it determined by the protocol or is it at the discretion of the server?",
        "top_answer": "The HTTP specification doesn't impose a specific size limit for posts.  They will usually be limited by either the web server or the programming technology used to process the form submission.",
        "url": "https://serverfault.com/questions/151090/is-there-a-maximum-size-for-content-of-an-http-post"
    },
    {
        "title": "Is FreeNAS reliable?",
        "question": "FreeNAS seems like a great product with a full checklist of features, even iSCSI. But how reliable is it? There are a few scary stories about lost data, for example here.\nHere is another example.\nIf you have used freeNAS for a longer period of time or even in a production setting, please share your experiences, good or bad. It would be great if you could also describe the setup, ie which hardware and features (software raid, zfs, iscsi etc) you are using.",
        "top_answer": "I have been using freenas on a spare machine with 4x 1TB hard drives (2 raid 1's, so 2TB usable). It has been up 24/7 for 6 months.\nI find it brilliant!\nI tested many NAS's devices and only got a maximum of 10Mb/s on a gigabit port, and that was rare, typically it was around 3-4. My main reason for a device was to save energy, however 2x 2 drive nas's = more than a 80+% psu on a celeron system.\nOn freenas, I have a celeron based machine that cost me under \u00a370, and on the internal 100Mb card, I can easily push 70Mb/s on samba.\nThe most expensive part was I bought a 4 drive enclosure to add/remove hard drives easily! Was a bit of a waste of money, but looks cool!\nI can not complain at all about it and love the system. I did look at openfiler, but it seemed a bit OTT and freenas did what I needed... \nTo the others who recommended it, not saying Openfiler is bad, but freenas suited my needs perfectly, I boot the machine off of a USB stick and works well... The question was \"is FreeNAS reliable\" and my answer has to be yes.\nThe system is using software raid and even though the celeron is a single core 64 bit one, even during a raid rebuild + watching a HDTV episode across the network, it never goes above 60% cpu\nTo get it working, I downloaded the full iso, put a 1GB usb stick in my laptop, used usb pass through on Vmware Workstation and booted from the iso. I then used the install option and chose the USB stick. (You can do this on the actual machine and I have since however this was my first time using it and I couldn't find a blank cd!)\nI put the usb stick in to the machine and booted. It worked fine first time!\nSteps to actually get it usable as a nas were the following:\n\nGo in to disk management and add each of the 4 drives.\nGo to format and format all drives to software raid\nGo to software raid and add disks 1 and 2, 3 and 4 to a new raid 1\nGo to format and format both the new raid's to the standard os\nMount both raids\nSet up Samba and choose both of the mount points as shares\nSet up a couple of users\n\nThen it was accessible over windows by \\\\ip and using the username and password I chose.\nI will be looking at openfiler again soon as AD support is lacking a bit, however for a SOHO / domainless environment, you can not go wrong with freenas.\nedit - Via request - Was to big to fit in comments",
        "url": "https://serverfault.com/questions/47021/is-freenas-reliable"
    },
    {
        "title": "Why can't I cd to a directory with docker run?",
        "question": "I need to run an application from a specific directory.\n$ sudo docker run -P ubuntu/decomposer 'cd /local/deploy/decomposer; ./decomposer-4-15-2014'\n2014/10/09 21:30:03 exec: \"cd /local/deploy/decomposer; ./decomposer-4-15-2014\": stat cd /local/deploy/decomposer; ./decomposer-4-15-2014: no such file or directory\n\nThat directory definitely exists, and if I connect to docker by running bash interactively I can run the above command.\n$ sudo docker run -i -t ubuntu/decomposer /bin/bash\n# cd /local/deploy/decomposer; ./decomposer-4-15-2014\n\nI can run my program by specifying the full path, but then it crashes as it expects to be launched from the current directory.  What can I do?",
        "top_answer": "Pass your command as an argument to /bin/sh like this:\nsudo docker run -P ubuntu/decomposer /bin/sh -c 'cd /local/deploy/decomposer; ./decomposer-4-15-2014'\n",
        "url": "https://serverfault.com/questions/634883/why-cant-i-cd-to-a-directory-with-docker-run"
    },
    {
        "title": "A previous IT worker probably left some backdoors. How can I eliminate them? [duplicate]",
        "question": "I started working for a company that fired a previous IT worker for leaking data.\nI can only say the following things:\nWe use a Firebird DB with an application written by another company, Proxmox, for virtualization of Windows Server 2008 R2, SQL Server, cloud core Mikrotik router, and a few other Mikrotik devices.\nI am not 100% sure, but is there some quick way to check if there are some backdoors left, without interrupting internal processes and reformatting everything?\nThis previous guy was really good, having written software in C++ and C#. I also know that he did some assembler and cracked a few programs in ollydbg.",
        "top_answer": "The only way to be absolutely certain is to wipe every system clean and to reinstall from scratch. You will also need to audit all of the locally generated software and configurations to ensure that they do not contain backdoors. This is a non trivial task which comes with a non trivial cost. \nBeyond that then there isn't really much you can do. \nObviously while you're deciding what to do \n\nAudit all firewall rules for validity\nAudit all accounts for validity\nAudit all sudoers files for validity\nChange all passwords and keys\n\nbut that is only scratching the surface.",
        "url": "https://serverfault.com/questions/828480/a-previous-it-worker-probably-left-some-backdoors-how-can-i-eliminate-them"
    },
    {
        "title": "Linux: How to know where a process was started and how it was started?",
        "question": "I was checking a Linux box and found a perl process running and taking a good share of cpu usage. With top, i could only perl in process name.\nWhen i pressed c, to view the command-line, it showed /var/spool/mail. Which does not make sense, since this is directory.\nMy questions are:\n1) Why did this happen? How this perl process could mask its command-line?\n2) What is the most reliable way of finding out where and how a process was started?\nThanks!",
        "top_answer": "In most cases just running ps is usually sufficient, along with your favorite flags to enable wide output.  I lean towards ps -feww, but the other suggestions here will work.  Note that if a program was started out of someone's $PATH, you're only going to see the executable name, not the full path.  For example, try this:\n$ lftp &\n$ ps -feww | grep ftp\nlars      9600  9504  0 11:30 pts/10   00:00:00 lftp\nlars      9620  9504  0 11:31 pts/10   00:00:00 grep ftp\n\nIt's important to note that the information visible in ps can be completely overwritten by the running program.  For example, this code:\nint main (int argc, char **argv) {\n        memset(argv[0], ' ', strlen(argv[0]));\n        strcpy(argv[0], \"foobar\");\n        sleep(30);\n        return(0);\n}\n\nIf I compile this into a file called \"myprogram\" and run it:\n$ gcc -o myprogram myprogram.c\n$ ./myprogram &\n[1] 10201\n\nAnd then run ps, I'll see a different process name:\n$ ps -f -p 10201\nUID        PID  PPID  C STIME TTY          TIME CMD\nlars     10201  9734  0 11:37 pts/10   00:00:00 foobar\n\nYou can also look directly at /proc/<pid>/exe, which may be a symlink to the appropriate executable.  In the above example, this gives you much more useful information than ps:\n$ls -l /proc/9600/exe\nlrwxrwxrwx. 1 lars lars 0 Feb  8 11:31 /proc/9600/exe -> /usr/bin/lftp\n",
        "url": "https://serverfault.com/questions/232762/linux-how-to-know-where-a-process-was-started-and-how-it-was-started"
    },
    {
        "title": "How can I set environment variable for just one command in fish shell?",
        "question": "In bash, I can do EDITOR=vim crontab -e. Can I get similar effect in Fish shell?",
        "top_answer": "begin; set -lx EDITOR vim; crontab -e; end\n",
        "url": "https://serverfault.com/questions/164305/how-can-i-set-environment-variable-for-just-one-command-in-fish-shell"
    },
    {
        "title": "How do I change the NGINX user?",
        "question": "I have a PHP script that creates a directory and outputs an image to the directory. This was working just fine under Apache but we recently decided to switch to NGINX to make more use of our limited RAM. I'm using the PHP mkdir() command to create the directory:\nmkdir(dirname($path['image']['server']), 0755, true);\n\nAfter the switch to NGINX, I'm getting the following warning:\nWarning: mkdir(): Permission denied in ...\n\nI've already checked all the permissions of the parent directories, so I've determined that I probably need to change the NGINX or PHP-FPM 'user' but I'm not sure how to do that (I never had to specify user permissions for APACHE). I can't seem to find much information on this. Any help would be great!\n(Note: Besides this little hang-up, the switch to NGINX has been pretty seamless; I'm using it for the first time and it literally only took about 10 minutes to get up and running with NGINX. Now I'm just ironing out the kinks.)",
        "top_answer": "Run nginx & php-fpm as www:www\n###1. Nginx\nEdit nginx.conf and set user to www www;:\nuser www www;\n\n\nIf the master process is run as root, then nginx will\nsetuid()/setgid() to USER/GROUP. If GROUP is not specified, then nginx\nuses the same name as USER. By default it's nobody user and nobody or\nnogroup group or the --user=USER and --group=GROUP from the\n./configure script.\n\n###2. PHP-FPM\nEdit php-fpm.conf and set user and group to www:\n[www]\nuser=www\ngroup=www\n\n\nuser - Unix user of processes. Default \"www-data\"\n\n\ngroup - Unix group of processes. Default \"www-data\"\n",
        "url": "https://serverfault.com/questions/433265/how-do-i-change-the-nginx-user"
    },
    {
        "title": "Executing a command as a nologin user",
        "question": "I've recently set up my server so that my suPHP 'virtual' users can't be logged into by using this article: Linux shell restricting access and disable shell with nologin\nMy issue now is that before when I ran a rake command for my Ruby on Rails application running on the server, I used su to go into www-data and execute the command from there - obviously I can't do that anymore because of the nologin.\nSo as a root user, how can I execute commands as other user's, even if they are nologin?",
        "top_answer": "One way is to launch a shell for that user (explicitly specifying the shell): \nsudo -u www-data bash \n\nThis will launch a (bash) shell as the specified user. You can then execute your command(s) and logout (to return to your previous shell)",
        "url": "https://serverfault.com/questions/333321/executing-a-command-as-a-nologin-user"
    },
    {
        "title": "automate dpkg-reconfigure tzdata",
        "question": "I'm using puppet to admin a cluster of debian servers.  I need to change the timezone of each machine on the cluster.  The proper debian way to do this is to use dpkg-reconfigure tzdata. But I can only seem to change it if I use the dialog.  Is there some way to automate this from the shell so I can just write an Exec to make this easy?\nIf not, I think the next best way would probably be to have puppet distribute /etc/timezone and /etc/localtime with the correct data across the cluster.\nAny input appreciated!",
        "top_answer": "You need to specify the frontend as `noninteractive' and it will save your current settings.\ndpkg-reconfigure will take the current system settings as gospel, so simply change your timezone the way you would normally and run it with the non-interactive flag\ne.g. for me to change to \"Europe/Dublin\" where I am:\n# echo \"Europe/Dublin\" > /etc/timezone    \n# dpkg-reconfigure -f noninteractive tzdata\n\nObviously this allows you to use puppet/cfengine as you like to distribute /etc/timezone also.\nEDIT:\nafter @gertvdijk comment pointing to https://bugs.launchpad.net/ubuntu/+source/tzdata/+bug/1554806 and @scruss answer you will probably have to do it like this in most modern distributions:\n$ sudo ln -fs /usr/share/zoneinfo/Europe/Dublin /etc/localtime\n$ sudo dpkg-reconfigure -f noninteractive tzdata\n",
        "url": "https://serverfault.com/questions/84521/automate-dpkg-reconfigure-tzdata"
    },
    {
        "title": "How can I check from the command line if a reboot is required on RHEL or CentOS?",
        "question": "I'm using CentOS and Red Hat Enterprise Linux on a few machines without the GUI. How can I check if recently installed updates require a reboot? In Ubuntu, I'm used to checking if /var/run/reboot-required is present.",
        "top_answer": "https://access.redhat.com/discussions/3106621#comment-1196821\n\nDon't forget that you might need to reboot because of core library updates, at least if it is glibc. (And also, services may need to be restarted after updates).\nIf you install the yum-utils package, you can use a command called needs-restarting.\nYou can use it both for checking if a full reboot is required because of kernel or core libraries updates (using the -r option), or what services need to be restarted (using the -s option).\nneeds-restarting -r returns 0 if reboot is not needed, and 1 if it is, so it is perfect to use in a script.\nAn example:\n\nroot@server1:~> needs-restarting  -r ; echo $?\nCore libraries or services have been updated:\n  openssl-libs -> 1:1.0.1e-60.el7_3.1\n  systemd -> 219-30.el7_3.9\n\nReboot is required to ensure that your system benefits from these updates.\n\nMore information:\nhttps://access.redhat.com/solutions/27943\n1\n",
        "url": "https://serverfault.com/questions/122178/how-can-i-check-from-the-command-line-if-a-reboot-is-required-on-rhel-or-centos"
    },
    {
        "title": "Should websites live in /var/ or /usr/ according to recommended usage?",
        "question": "According to a guide on the Linux directory structure, /usr/ is for application files, and /var/ is for files that change (I assume this means \"files that belong to the applications\"). Is this correct?\nIf this is the case then I'm a little torn between using either. A website is an application (if it's dynamic, so to speak), but in other cases it is just a collection of files used by Apache.\nThe default www dir lives in /var/www/, so should we follow suit by using /var/websites/ (or something similar), or choose /usr/websites/ since they could be applications?\nThis is a very trivial question, but it's bugging me nonetheless. For our case, I'm leaning toward /usr/web or something like that, since our websites are all applications.\nUpdate:\nThis is for our company websites; it's not a shared hosting server, so we don't need to worry about separating them in /home/ or anything like that.",
        "top_answer": "According to the FHS, /usr is for shareable, read-only data - not where you want to put the website. This is where you should put your code (for example Fedora does this for Wordpress). See also the web assets packaging guide for Fedora.\n/var is \"variable data files. This includes spool directories and files, administrative and logging data, and transient and temporary files.\" -- better, but still not quite right -- but a lot of systems will use /var/www, so even if you're wrong to put it there you're in good company.\n/srv is for \"site-specific data which is served by this system.\" -- which seems like a good match, but is much less common than /var/www.\nThe other common place to put the site files is under /home -- by creating a special user called website or such, then placing the files inside that user's homedir (e.g., /home/website).",
        "url": "https://serverfault.com/questions/102569/should-websites-live-in-var-or-usr-according-to-recommended-usage"
    },
    {
        "title": "Get a list of SQL Server Agent Jobs",
        "question": "I've got an extensive selection of these to add to a spreadsheet and don't want to go through by hand.\nWhat it the T-SQL command(s) to generate a list of SQL Server Agent Jobs?",
        "top_answer": "On each server, you can query the sysjobs table in the msdb. For instance:\nSELECT job_id, [name] FROM msdb.dbo.sysjobs;\n",
        "url": "https://serverfault.com/questions/14524/get-a-list-of-sql-server-agent-jobs"
    },
    {
        "title": "How do I check if Log4j is installed on my server?",
        "question": "I have read about security vulnerabilities related to Log4j.\nHow do I check if Log4j is installed on my server?\nMy specific servers use Ubuntu 18.04.6 LTS.\nI have installed many third-party packages and maybe some of them contain it.\nIs there a command to run on my server to check if Log4j is installed?",
        "top_answer": "Try this script to get a hint:\necho \"checking for log4j vulnerability...\"\nOUTPUT=\"$(locate log4j|grep -v log4js)\"\nif [ \"$OUTPUT\" ]; then\n  echo \"[WARNING] maybe vulnerable, those files contain the name:\"\n  echo \"$OUTPUT\"\nfi\nOUTPUT=\"$(dpkg -l|grep log4j|grep -v log4js)\"\nif [ \"$OUTPUT\" ]; then\n  echo \"[WARNING] maybe vulnerable, dpkg installed packages:\"\n  echo \"$OUTPUT\"\nfi\nif [ \"$(command -v java)\" ]; then\n  echo \"java is installed, so note that Java applications often bundle their libraries inside jar/war/ear files, so there still could be log4j in such applications.\"\nfi\necho \"If you see no output above this line, you are safe. Otherwise check the listed files and packages.\"\n\nMake sure your locate database is up to date with updatedb.\nOr better check and run the enhanced script from GitHub which also searches inside packed Java files. Run in one line with\nwget https://raw.githubusercontent.com/rubo77/log4j_checker_beta/main/log4j_checker_beta.sh -q -O - | bash   \n\nI am not sure if there could be compiled Java Programs running on the server without java being installed though?\nOr even compiled versions where the source files aren't even found inside packed archives any more?\n\nThere is also a lot development on GitHub, where you find attacks and countermeasures.\n\nThis takes to much time for me. I am looking for someone I can transfer the ownership of the repository on GitHub",
        "url": "https://serverfault.com/questions/1086065/how-do-i-check-if-log4j-is-installed-on-my-server"
    },
    {
        "title": "iptables: difference between NEW, ESTABLISHED and RELATED packets",
        "question": "Part of a firewall on a server :\niptables -A INPUT -p tcp --dport 22 -m state NEW --state -m recent --set\n\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 100 --hitcount 10 -j DROP\n\nWhen I search online I always see NEW being used in that rule but I'm having a hard time understanding why ESTABLISHED and RELATED aren't being used.\nLike this :\niptables -A INPUT -p tcp --dport 22 -m state NEW,ESTABLISHED,RELATED --state -m recent --set\n\niptables -A INPUT -p tcp --dport 22 -m state --state NEW,ESTABLISHED,RELATED -m recent --update --seconds 100 --hitcount 10 -j DROP\n\nCan someone explain to me when exactly a NEW packet changes into ESTABLISHED and RELATED ? ",
        "top_answer": "Consider a NEW packet a telephone call before the receiver has picked up.  An ESTABLISHED packet is their, \"Hello.\"  And a RELATED packet would be if you were calling to tell them about an e-mail you were about to send them. (The e-mail being RELATED.)\nIn case my analogy isn't so great, I personlly think the man pages handles it well:\n\nNEW -- meaning that the packet has started a new connection, or otherwise\n  associated with a connection which has not seen packets in both\n  directions, and\nESTABLISHED -- meaning that the packet is associated with a connection\n  which has seen packets in both directions, \nRELATED -- meaning that the packet is starting a new connection, but is\n  associated with an existing connection, such as an FTP data transfer,\n  or an ICMP error.\n\niptables(8) - Linux man page",
        "url": "https://serverfault.com/questions/371316/iptables-difference-between-new-established-and-related-packets"
    },
    {
        "title": "_default_ VirtualHost overlap on port 443, the first has precedence",
        "question": "I have two ruby on rails 3  applications running on same server, (ubuntu 10.04), both with SSL.\nHere is my apache config file:\n<VirtualHost *:80>\nServerName example1.com\nDocumentRoot /home/me/example1/production/current/public\n</VirtualHost>\n<VirtualHost *:443>\nServerName example1.com\nDocumentRoot /home/me/example1/production/current/public\nSSLEngine on\nSSLCertificateFile /home/me/example1/production/shared/example1.crt\nSSLCertificateKeyFile /home/me/example1/production/shared/example1.key\nSSLCertificateChainFile /home/me/example1/production/shared/gd_bundle.crt\nSSLProtocol -all +TLSv1 +SSLv3\nSSLCipherSuite HIGH:MEDIUM:!aNULL:+SHA1:+MD5:+HIGH:+MEDIUM\n</VirtualHost>\n\n\n<VirtualHost *:80>\nServerName example2.com\nDocumentRoot /home/me/example2/production/current/public\n</VirtualHost>\n<VirtualHost *:443>\nServerName example2.com\nDocumentRoot /home/me/example2/production/current/public\nSSLEngine on\nSSLCertificateFile /home/me/example2/production/shared/iwanto.crt\nSSLCertificateKeyFile /home/me/example2/production/shared/iwanto.key\nSSLCertificateChainFile /home/me/example2/production/shared/gd_bundle.crt\nSSLProtocol -all +TLSv1 +SSLv3\nSSLCipherSuite HIGH:MEDIUM:!aNULL:+SHA1:+MD5:+HIGH:+MEDIUM\n</VirtualHost>\n\nWhats the issue:\nOn restarting my server it gives me some output like this:\n * Restarting web server apache2                                   \n [Sun Jun 17 17:57:49 2012] [warn] _default_ VirtualHost overlap on port 443, the first has precedence\n ... waiting [Sun Jun 17 17:57:50 2012] [warn] _default_ VirtualHost overlap on port 443, the first has precedence\n\nOn googling why this issue is coming I got something like this:\nYou cannot use name based virtual hosts with SSL because the SSL handshake (when the browser accepts the secure Web server's certificate) occurs before the HTTP request, which identifies the appropriate name based virtual host. If you plan to use name-based virtual hosts, remember that they only work with your non-secure Web server.\nBut not able to figure out how to run two ssl application on same server.\nCan any one help me?",
        "top_answer": "Almost there!\nAdd this to ports.conf or http.conf and keep your above config.\n<IfModule mod_ssl.c>\n    # If you add NameVirtualHost *:443 here, you will also have to change\n    # the VirtualHost statement in /etc/apache2/sites-available/default-ssl\n    # to <VirtualHost *:443>\n    # Server Name Indication for SSL named virtual hosts is currently not\n    # supported by MSIE on Windows XP.\n\n    # !important below!\n    NameVirtualHost *:443 \n    Listen 443\n</IfModule>\n",
        "url": "https://serverfault.com/questions/399616/default-virtualhost-overlap-on-port-443-the-first-has-precedence"
    },
    {
        "title": "Exchange server replacement that runs on Linux",
        "question": "I've worked as a sysadmin for some years and what I keep coming back to is that users like Microsoft Outlook and want to use its Exchange features. I have tried my fair share of commercial alternatives but usually there is either a fundamental feature missing or there are stability issues.\nIn short I am looking for a Microsoft Exchange Alternative with the following features:\n\nAuthentication through SQL or LDAP\nHas a solid, comfortable web interface for the users when they are off-site\nSupports replication and load balancing (if one fails, the second one should be already running)\nOutlook client support (or a really good alternative client)\nResource booking (meeting rooms, projectors, company jet, etc)\nCalendar (shared/private) and Email (if that wasn't obvious)\n(Optional) A cross-platform client for us *nix users.\n(Optional) Corporate support contracts available\n(Optional) An open-source software is a plus\n\nPlease keep your answers as detailed as possible to determine that you've successfully deployed the software and it fulfills the needs. If I wanted a list of claimed alternatives , I would simply Google it.\nI've personally tried Binary Server, Novell Groupwise, homegrown Postfix/Cyrus stuff and in the end the 'real thing' because those users just love Exchange.\nPlease help me find a good alternative.",
        "top_answer": "Zimbra is an excellent opensource, linux based alternative to Exchange. It combines Apache Tomcat, Postfix, MySQL, OpenLDAP and Lucene in a single, well defined package. It offers:\n\nLDAP Authentication\nCalendaring, resource booking and free/busy info\nAbility to connect using outlook, and its own Zimbra client\nExcellent web interface\nAllows multi server setup and replication\n\nIt is available for free, with the option of paying for a supported version. I have used Zimbra for a number of organisations who did not, or could not pay for Exchange, and they have all been very happy with it.",
        "url": "https://serverfault.com/questions/35842/exchange-server-replacement-that-runs-on-linux"
    },
    {
        "title": "Apache ProxyPass with SSL",
        "question": "I want to proxy requests from an SSL site via a non-SSL site. My Apache httpd.conf looks like this:\n<VirtualHost 1.2.3.4:80>\n    ServerName foo.com\n    ProxyPass / https://bar.com/\n</VirtualHost>\n\nSo, when I visit http://foo.com, I expect apache to make a request to https://bar.com and send me the the page it fetched.\nInstead, I get a 500 error, and in the error log, I see:\n[error] proxy: HTTPS: failed to enable ssl support for 4.3.2.1:443 (bar.com)\n\nPresumably I'm missing a directive here. Which might it be?\nNever mind the security implications. I fully understand the risks.",
        "top_answer": "You'll need mod_ssl, mod_proxy and optionally mod_rewrite. Depending on your distribution and Apache version you may have to check if mod_proxy_connect and mod_proxy_http are loaded as well.\nThe directives for enabling SSL proxy support are in mod_ssl:\n<VirtualHost 1.2.3.4:80>\n    ServerName foo.com\n    SSLProxyEngine On\n    SSLProxyCheckPeerCN on\n    SSLProxyCheckPeerExpire on\n    ProxyPass / https://secure.bar.com\n    ProxyPassReverse / https://secure.bar.com\n</VirtualHost>\n\nIIRC you can also use:\n    RewriteRule / https://secure.bar.com [P]    # don't forget to setup SSLProxy* as well\n",
        "url": "https://serverfault.com/questions/84821/apache-proxypass-with-ssl"
    },
    {
        "title": "Extracting files from Clonezilla images",
        "question": "Is there a way to browse Clonezilla images and extract individual files from them without restoring the whole image?",
        "top_answer": "Better use the partclone utility instead:\n\ncd /home/partimag/YOURIMAGE/\nDepending on compression method used:\n\nIf the image is compressed with gzip: cat dir/hda2.ntfs-ptcl-img.gz.* | gunzip | partclone.restore --restore_raw_file -C -s - -o hda2.img\n\n\nIf the image is compressed with zstd: zstdcat dir/hda2.ntfs-ptcl-img.zst.* | partclone.restore --restore_raw_file -C -s - -o hda2.img\n\n\nmount -o loop hda2.img /mnt -t ntfs -o ro\n\nNote: This needs to be done as root, since partclone requires root permissions to write the image, and the mount command will likely only work as root.\nFor zst images you'll need to install zstd (e.g. apt install zstd)\nSee also CloneZilla FAQ Entry: \"How can I restore those *-ptcl-img.* images into a file manually?\"",
        "url": "https://serverfault.com/questions/35639/extracting-files-from-clonezilla-images"
    },
    {
        "title": "Could not resolve host: mirrorlist.centos.org Centos 7",
        "question": "If you are reading this after 2024 mirrorlist.centos.org doesn't exist anymore.\nAs noted by @user202729 in notes check this reply.\n\nI have a fresh install of latest centos 7\n[root@localhost ~]# cat /etc/centos-release\nCentOS Linux release 7.4.1708 (Core) \n[root@localhost ~]# \n\nI wanted to install something and wget was not installed so when I tried to install wget I saw tha yum is giving error.\nI saw maybe all the topics about this problem on the internet but no luck I cant find my solution.\n[root@localhost ~]# yum update\nLoaded plugins: fastestmirror\nCould not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=stock error was\n14: curl#6 - \"Could not resolve host: mirrorlist.centos.org; Unknown error\"\nhttp://mirror.centos.org/centos/7/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - \"Could not resolve host: mirror.centos.org; Unknown error\"\nTrying other mirror.\n\n One of the configured repositories failed (CentOS-7 - Base),\n and yum doesn't have enough cached data to continue. At this point the only\n safe thing yum can do is fail. There are a few ways to work \"fix\" this:\n\n     1. Contact the upstream for the repository and get them to fix the problem.\n\n     2. Reconfigure the baseurl/etc. for the repository, to point to a working\n        upstream. This is most often useful if you are using a newer\n        distribution release than is supported by the repository (and the\n        packages for the previous distribution release still work).\n\n     3. Run the command with the repository temporarily disabled\n            yum --disablerepo=base ...\n\n     4. Disable the repository permanently, so yum won't use it by default. Yum\n        will then just ignore the repository until you permanently enable it\n        again or use --enablerepo for temporary usage:\n\n            yum-config-manager --disable base\n        or\n            subscription-manager repos --disable=base\n\n     5. Configure the failing repository to be skipped, if it is unavailable.\n        Note that yum will try to contact the repo. when it runs most commands,\n        so will have to try and fail each time (and thus. yum will be be much\n        slower). If it is a very temporary problem though, this is often a nice\n        compromise:\n\n            yum-config-manager --save --setopt=base.skip_if_unavailable=true\n\nfailure: repodata/repomd.xml from base: [Errno 256] No more mirrors to try.\nhttp://mirror.centos.org/centos/7/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - \"Could not resolve host: mirror.centos.org; Unknown error\"\n[root@localhost ~]# \n\nSo when I list the repos I get this:\n[root@localhost ~]# yum repolist all\nLoaded plugins: fastestmirror\nCould not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=stock error was\n14: curl#6 - \"Could not resolve host: mirrorlist.centos.org; Unknown error\"\nhttp://mirror.centos.org/centos/7/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - \"Could not resolve host: mirror.centos.org; Unknown error\"\nTrying other mirror.\nhttp://mirror.centos.org/centos/7/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - \"Could not resolve host: mirror.centos.org; Unknown error\"\nTrying other mirror.\nCould not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=centosplus&infra=stock error was\n14: curl#6 - \"Could not resolve host: mirrorlist.centos.org; Unknown error\"\nhttp://mirror.centos.org/centos/7/centosplus/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - \"Could not resolve host: mirror.centos.org; Unknown error\"\nTrying other mirror.\nCould not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=extras&infra=stock error was\n14: curl#6 - \"Could not resolve host: mirrorlist.centos.org; Unknown error\"\nhttp://mirror.centos.org/centos/7/extras/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - \"Could not resolve host: mirror.centos.org; Unknown error\"\nTrying other mirror.\nCould not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates&infra=stock error was\n14: curl#6 - \"Could not resolve host: mirrorlist.centos.org; Unknown error\"\nhttp://mirror.centos.org/centos/7/updates/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - \"Could not resolve host: mirror.centos.org; Unknown error\"\nTrying other mirror.\nrepo id                                                                                         repo name                                                                                         status\nC7.0.1406-base/x86_64                                                                           CentOS-7.0.1406 - Base                                                                            disabled\nC7.0.1406-centosplus/x86_64                                                                     CentOS-7.0.1406 - CentOSPlus                                                                      disabled\nC7.0.1406-extras/x86_64                                                                         CentOS-7.0.1406 - Extras                                                                          disabled\nC7.0.1406-fasttrack/x86_64                                                                      CentOS-7.0.1406 - CentOSPlus                                                                      disabled\nC7.0.1406-updates/x86_64                                                                        CentOS-7.0.1406 - Updates                                                                         disabled\nC7.1.1503-base/x86_64                                                                           CentOS-7.1.1503 - Base                                                                            disabled\nC7.1.1503-centosplus/x86_64                                                                     CentOS-7.1.1503 - CentOSPlus                                                                      disabled\nC7.1.1503-extras/x86_64                                                                         CentOS-7.1.1503 - Extras                                                                          disabled\nC7.1.1503-fasttrack/x86_64                                                                      CentOS-7.1.1503 - CentOSPlus                                                                      disabled\nC7.1.1503-updates/x86_64                                                                        CentOS-7.1.1503 - Updates                                                                         disabled\nC7.2.1511-base/x86_64                                                                           CentOS-7.2.1511 - Base                                                                            disabled\nC7.2.1511-centosplus/x86_64                                                                     CentOS-7.2.1511 - CentOSPlus                                                                      disabled\nC7.2.1511-extras/x86_64                                                                         CentOS-7.2.1511 - Extras                                                                          disabled\nC7.2.1511-fasttrack/x86_64                                                                      CentOS-7.2.1511 - CentOSPlus                                                                      disabled\nC7.2.1511-updates/x86_64                                                                        CentOS-7.2.1511 - Updates                                                                         disabled\nC7.3.1611-base/x86_64                                                                           CentOS-7.3.1611 - Base                                                                            disabled\nC7.3.1611-centosplus/x86_64                                                                     CentOS-7.3.1611 - CentOSPlus                                                                      disabled\nC7.3.1611-extras/x86_64                                                                         CentOS-7.3.1611 - Extras                                                                          disabled\nC7.3.1611-fasttrack/x86_64                                                                      CentOS-7.3.1611 - CentOSPlus                                                                      disabled\nC7.3.1611-updates/x86_64                                                                        CentOS-7.3.1611 - Updates                                                                         disabled\nbase/7/x86_64                                                                                   CentOS-7 - Base                                                                                   enabled: 0\nbase-debuginfo/x86_64                                                                           CentOS-7 - Debuginfo                                                                              disabled\nbase-source/7                                                                                   CentOS-7 - Base Sources                                                                           disabled\nc7-media                                                                                        CentOS-7 - Media                                                                                  disabled\ncentosplus/7/x86_64                                                                             CentOS-7 - Plus                                                                                   enabled: 0\ncentosplus-source/7                                                                             CentOS-7 - Plus Sources                                                                           disabled\ncr/7/x86_64                                                                                     CentOS-7 - cr                                                                                     disabled\nextras/7/x86_64                                                                                 CentOS-7 - Extras                                                                                 enabled: 0\nextras-source/7                                                                                 CentOS-7 - Extras Sources                                                                         disabled\nfasttrack/7/x86_64                                                                              CentOS-7 - fasttrack                                                                              disabled\nupdates/7/x86_64                                                                                CentOS-7 - Updates                                                                                enabled: 0\nupdates-source/7                                                                                CentOS-7 - Updates Sources                                                                        disabled\nrepolist: 0\n[root@localhost ~]# \n\nIm not sure where can be the problem its a fresh install on my vmware/OVH dedicated server.\nI have another server installed and working fine but this time I got this problem .\nAny one can help me?I have also tried to enable all disabled lines in etc/yum.repo.d\nmy /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the \n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra\nbaseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#released updates \n[updates]\nname=CentOS-$releasever - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra\nbaseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra\nbaseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus&infra=$infra\nbaseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n",
        "top_answer": "Could not resolve host: mirrorlist.centos.org; Unknown error\n\nThis indicates that you either (a) don't have a properly configured DNS server or (b) your network configuration isn't correct and you can't connect to a DNS server to check the hostname mirrorlist.centos.org.\nTry using ping 8.8.8.8. If this fails, try ping <local-gateway-ip>. If that also fails, your local network configuration is wrong and you'll have to check the configuration.\nIf you can ping 8.8.8.8, try using host, nslookup or dig to check the DNS settings like host google.com or dig google.com. If these fail, you need to check your DNS settings. Check /etc/resolv.conf to see what's configured.\nUPDATE\nSince /etc/resolv.conf is blank, you need to setup a DNS resolver. I would suggest entering the following into the file using nano or vi (or whatever your comfortable using):\nnameserver 9.9.9.9\nSave this file, then try yum update again.\nYou can also try other DNS hosts if you would rather, such as 8.8.8.8 or 8.8.4.4 or any of the OpenDNS hosts.",
        "url": "https://serverfault.com/questions/904304/could-not-resolve-host-mirrorlist-centos-org-centos-7"
    },
    {
        "title": "How do I force Linux to reacquire a new IP address from the DHCP server?",
        "question": "The environment is Debian, although the answer will apply to all distributions.",
        "top_answer": "You can also use this command:\ndhclient -r interface\n\nWhere interface is the device you want to get a new address for. \ndhclient -r eth0\n\nThe -r flag forces dhclient to first release any leases you have, you can then use this command to request a new lease:\ndhclient eth0\n\nFrom man dhclient:\n   -r     Tell  dhclient  to  release the current lease it has from the \n          server.  This is not required by the DHCP protocol, but some \n          ISPs require their clients to notify the server if they wish \n          to release an assigned IP address.\n",
        "url": "https://serverfault.com/questions/42799/how-do-i-force-linux-to-reacquire-a-new-ip-address-from-the-dhcp-server"
    },
    {
        "title": "MySQL keeps crashing: InnoDB: Unable to lock ./ibdata1, error: 11",
        "question": "I have a simple webserver (Debian 6.0 x86, DirectAdmin with 1 GB of memory and still 10 GB free space, mySQl version 5.5.9), however the mySQL server keeps crashing and I need to kill all mySQL processes to be able to restart it again.\n/var/log/mysql-error.log output:\n130210 21:04:26 InnoDB: Using Linux native AIO\n130210 21:04:34 InnoDB: Initializing buffer pool, size = 128.0M\n130210 21:05:42 InnoDB: Completed initialization of buffer pool\n130210 21:05:48 InnoDB: Initializing buffer pool, size = 128.0M\n130210 21:06:22 InnoDB: Initializing buffer pool, size = 128.0M\n130210 21:06:27 mysqld_safe mysqld from pid file /usr/local/mysql/data/website.pid ended\n130210 21:06:29 mysqld_safe mysqld from pid file /usr/local/mysql/data/website.pid ended\n130210 21:07:22 InnoDB: Completed initialization of buffer pool\n130210 21:07:51 mysqld_safe mysqld from pid file /usr/local/mysql/data/website.pid ended\n130210 21:08:33 InnoDB: Completed initialization of buffer pool\n130210 21:12:03 [Note] Plugin 'FEDERATED' is disabled.\n130210 21:12:47 InnoDB: The InnoDB memory heap is disabled\n130210 21:12:47 InnoDB: Mutexes and rw_locks use InnoDB's own implementation\n130210 21:12:47 InnoDB: Compressed tables use zlib 1.2.3\n130210 21:12:47 InnoDB: Using Linux native AIO\n130210 21:13:11 InnoDB: highest supported file format is Barracuda.\n130210 21:13:23 InnoDB: Initializing buffer pool, size = 128.0M\nInnoDB: The log sequence number in ibdata files does not match\nInnoDB: the log sequence number in the ib_logfiles!\n130210 21:14:05  InnoDB: Database was not shut down normally!\nInnoDB: Starting crash recovery.\nInnoDB: Unable to lock ./ibdata1, error: 11\nInnoDB: Check that you do not already have another mysqld process\nInnoDB: using the same InnoDB data or log files.\nInnoDB: Unable to lock ./ibdata1, error: 11\nInnoDB: Check that you do not already have another mysqld process\nInnoDB: using the same InnoDB data or log files.\nInnoDB: Unable to lock ./ibdata1, error: 11\nInnoDB: Check that you do not already have another mysqld process\nInnoDB: using the same InnoDB data or log files.\n130210 21:17:53  InnoDB: Unable to open the first data file\nInnoDB: Error in opening ./ibdata1\n130210 21:17:53  InnoDB: Operating system error number 11 in a file operation.\n\nI have found a topic on the mySQL website here however there's no solution for it.\nAny ideas anyone?",
        "top_answer": "another approach from one comment in the same blog:\n\nthis helped me:\nlsof -i:3306\nThen kill it (the process number)\nkill -9 PROCESS\ne.g. kill -9 13498\nThen try to restart MySQL again.\n\nvia http://www.webhostingtalk.com/archive/index.php/t-1070293.html",
        "url": "https://serverfault.com/questions/477448/mysql-keeps-crashing-innodb-unable-to-lock-ibdata1-error-11"
    },
    {
        "title": "Install a newer version of Git on CentOS 7",
        "question": "I like to enable Git \"Push to Deploy\" on my CentOS 7 server. Currently I only can get Git 1.8.3.1 via yum. I need a newer version.\nDo I have to build it from source or is there any repo I can use? I alreay added EPEL and elrepo but yum still gives me Git 1.8.3.1. ",
        "top_answer": "You could use a IUS repository (https://ius.io/) as provided on Git official site here or here. It contains prebuilt binaries for x86_64.\nTo do that, run (as root):\nyum install epel-release\nyum remove git\nrpm -U https://centos7.iuscommunity.org/ius-release.rpm\nyum install git2u\n\n(centos7 can be replaced with centos6 or rhel{6,7} if you are not using CentOS).\nNote: some users report that there is no more package called git2u. You can also try packages git222 or git224 in that case.\nNote: git2u is no longer valid. You can check in github for the latest IUS Git version. https://github.com/search?q=org%3Aiusrepo+topic%3Arpm+git&s=updated&type=repositories\nIt worked for me using \"sudo yum install git236\".\nAnother option would be to use another RPM repository (i386 & x86_64):\nsudo yum -y install https://packages.endpointdev.com/rhel/7/os/x86_64/endpoint-repo.x86_64.rpm\nsudo yum install git\n\nNote 2: as reported @alaindeseine in comments there is an issue accessing https://centos7.iuscommunity.org/ius-release.rpm. In that case use https://repo.ius.io/ius-release-el7.rpm",
        "url": "https://serverfault.com/questions/709433/install-a-newer-version-of-git-on-centos-7"
    },
    {
        "title": "Rsync : copying over timestamps only",
        "question": "Currently I have two directories A/ and B/ which are identical in every respect, with the exception of the timestamps. Therefore if I  run the command :\nrsync --dry-run -crvv A/ B/\n\nthen all files are marked \"uptodate\", whereas the command : \nrsync --dry-run -rvv A/ B/\n\nshows that all files are to be copied over from A/ to B/.\nMy question is this : given that I know the files are identical (in respect to contents), then is there any way (via rsync or otherwise) to set the timestamps for files in B/ to be identical to the timestamps of the files in A/, without copying over all the files from A/ to B/ ?\nThanks",
        "top_answer": "Using -t (preserve timestamps) and --size-only will only compare files on size. If the size matches, rsync will not copy the file but since -t is specified, it will update the timestamp on the destination file without recopying it.\nMake sure to not use -u (update) as this will skip files that already exist and completely skip updating the timestamp.\nI had the problem of originally not using rsync to copy a bunch of files to a new drive, and therefore the timestamps were updated to current time. I used the command below to sync everything correctly in a decent amount of time:\nrsync -vrt --size-only /src /dest\n",
        "url": "https://serverfault.com/questions/344731/rsync-copying-over-timestamps-only"
    },
    {
        "title": "Why does AWS recommend against public S3 buckets?",
        "question": "\"We highly recommend that you never grant any kind of public access to your S3 bucket.\"\nI have set a very granular public policy (s3:GetObject) for one bucket that I use to host a website. Route53 explicitly supports aliasing a bucket for this purpose. Is this warning just redundant, or am I doing something wrong?",
        "top_answer": "Yes, if you know what you're doing (edit: and everyone else with access to it does, too...), you can ignore this warning.\nIt exists because even large organizations  who should know better have accidentally placed private data into public buckets. Amazon will also send you heads-up emails if you leave buckets public in addition to the in-console warnings.\n\nAccenture, Verizon, Viacom, Illinois voter information and military information has all been found inadvertently left open to everyone online due to IT bods misconfiguring their S3 silos.\n\nIf you are absolutely, 100% certain that everything in the bucket should be public and that no one's going to accidentally put private data in it - a static HTML site's a good example - then by all means, leave it public.",
        "url": "https://serverfault.com/questions/888487/why-does-aws-recommend-against-public-s3-buckets"
    },
    {
        "title": "Nginx location regex doesn't work with proxy pass",
        "question": "I'm trying to get these 2 location directives working in Nginx but I'm getting some errors back when booting Nginx.\n   location ~ ^/smx/(test|production) {\n        proxy_pass   http://localhost:8181/cxf;\n    }\n\n    location ~ ^/es/(test|production) {\n        proxy_pass   http://localhost:9200/;\n    }\n\nThis is the error I'm receiving:\nnginx: [emerg] \"proxy_pass\" cannot have URI part in location given by regular expression, or inside named location, or inside \"if\" statement, or inside \"limit_except\" block\n\nDoes it sounds familiar to anyone? What I'm I missing here?",
        "top_answer": "A small addition to the great answer from Xaviar:\nIf you happen to be not so well acquainted with nginx, there's an important difference between adding the slash to the end of the proxy_pass directive.\nThe following does not work:\nlocation ~* ^/dir/ {\n  rewrite ^/dir/(.*) /$1 break;\n  proxy_pass http://backend/;\n\nbut this one does:\nlocation ~* ^/dir/ {\n  rewrite ^/dir/(.*) /$1 break;\n  proxy_pass http://backend;\n\nThe difference being the / at the end of the proxy_pass directive.",
        "url": "https://serverfault.com/questions/649151/nginx-location-regex-doesnt-work-with-proxy-pass"
    },
    {
        "title": "How to zero fill a virtual disk's free space on windows for better compression?",
        "question": "How to zero fill a virtual disk's free space on windows for better compression?\nI would like a simple open source tool (or at least free) for that. It should probably write an as big as possible file full of 0and erase it afterwards. Only one pass (this is not for security reasons but for compression, we are backing up virtual machines).\nShould run from inside windows and not from a disk.\nOn Linux I do it like this (as a user):\ncd\nmkdir wipe\nsudo sfill -f -l -l -z ./wipe/\n\nEdit 1: I decided to use sdelete from the accepted answer. I had a look at the sdelete's help:\nC:\\WINDOWS\\system32>sdelete /?\n\nSDelete - Secure Delete v1.51\nCopyright (C) 1999-2005 Mark Russinovich\nSysinternals - www.sysinternals.com\n\nusage: sdelete [-p passes] [-s] [-q] <file or directory>\n       sdelete [-p passes] [-z|-c] [drive letter]\n   -c         Zero free space (good for virtual disk optimization)\n   -p passes  Specifies number of overwrite passes (default is 1)\n   -q         Don't print errors (Quiet)\n   -s         Recurse subdirectories\n   -z         Clean free space\n\nThis is an old version. I used the -c switch from the 2nd invocation and this was quite fast (syntax only valid for older versions before V1.6):\nc:\\>sdelete -c c: (OUTDATED!)\n\nI have the impression this does what I want. The sdelete tool is easy to use and easy to get.\nEdit 2: As scottbb pointed out in his answer below, there was a September 2011 change to the tool (version 1.6) The -c and -z options have changed meanings. The correct usage from 1.6 onwards is\nc:\\>sdelete -z c:\n\nEdit 3: There is a 2.0 version of sdelete and sdelete64. They appear to be buggy when zeroing. It will appear to be stuck at 100% for extremely long times. Some people have reported 10 - 40 times longer. The older version 1.61 does not have this issue. See https://social.technet.microsoft.com/Forums/en-US/2ffb2539-34ba-4378-aa8a-941d243f117e/sdelete-hangs-at-100?forum=miscutils\nEdit 4: Now there's the issue of dynamically allocated virtual disc space. If you have a 100GB disk that is not full and uses only 30GB on the host, zero filling should not increase dramatically the size of the disc because that contradicts the purpose of dynamic allocation. There is an answer for Oracle VM VirtualBox https://superuser.com/q/907196/44402 - but on other stacks like VMWare, Xen, XenServer, etc., this needs to be answered separately.",
        "top_answer": "On windows use the sysinternals tool sdelete to zero out all the empty space.  The command you want would look like this sdelete -z c:.\n\nUsage: sdelete [-p passes] [-s] [-q]  ...  \nsdelete [-p passes] [-z|-c] [drive letter] ...  \n-a         Remove Read-Only attribute.  \n-c         Clean free space.  \n-p passes  Specifies number of overwrite passes (default is 1).  \n-q         Don't print errors (Quiet).  \n-s or -r   Recurse subdirectories.  \n-z         Zero free space (good for virtual disk optimization).\n\nFor Linux I suggest you use zerofree.",
        "url": "https://serverfault.com/questions/165070/how-to-zero-fill-a-virtual-disks-free-space-on-windows-for-better-compression"
    },
    {
        "title": "GNU less: How can I search while ignoring case sensitivity without using less -I option?",
        "question": "From GNU less manpage\n\n-i or --ignore-case\nCauses searches to ignore case; that is, uppercase and lowercase are considered identical.\nThis option is ignored if any uppercase letters appear in the search pattern; in other words, if a pattern contains uppercase letters, then that search does not ignore case.\n-I or --IGNORE-CASE\nLike -i, but searches ignore case even if the pattern contains uppercase letters.\n\nThis is a great way of searching in GNU less, while ignoring case sensitivity. However, you must know in advance that you'd like to search while ignoring case sensitivity and indicate it in the command line.\nvim solves this problem by letting the user specify \\c before a search, to indicate that the pattern should be searched while ignoring case sensitivity.\nIs there a way to do the same in less (without specifying -I in the command line)?",
        "top_answer": "You can set it from within less by typing -i and then doing the normal search procedure.\nHave a look in the help for less by pressing h",
        "url": "https://serverfault.com/questions/11736/gnu-less-how-can-i-search-while-ignoring-case-sensitivity-without-using-less-i"
    },
    {
        "title": "Can we have multiple CNAMES for a single Name?",
        "question": "I need this for load balancing. For example, I've two azure storage accounts (say a and b) and the blob addresses for those are a.blob.core.windows.net and b.blob.core.windows.net. Both of them store identical data. Now I need to provide a single external name (say example.com) which points to both the storage accounts and should work in round robin. This can be achieved if I create two CNAME entries in DNS as following and it resolves to one of them in round robing.\n\nexample.com CNAME a.blob.core.windows.net \nexample.com CNAME b.blob.core.windows.net\n\nBut I can not create two CNAME records for a single name in Windows DNS server.\nSo is it ever possible?",
        "top_answer": "Multiple CNAME records for the same fully-qualified domain name is a violation of the specs for DNS. Some versions of BIND would allow you to do this (some only if you specified the multiple-cnames yes option) and would round-robin load-balance between then but it's not technically legal. \nThere are not supposed to be resource records (RRs) with the same name as a CNAME and, to pick nits, that would include multiple identical CNAMEs. Quoth RFC 1034, Section 3.6.2:\n\nIf a CNAME RR is present at a node, no other data should be present;\n  this ensures that the data for a canonical name and its aliases cannot\n  be different. This rule also insures that a cached CNAME can be used\n  without checking with an authoritative server for other RR types.\n\nThe letter-of-the RFC method to handle what you're doing would be with a single CNAME referring to a load-balanced \"A\" record. ",
        "url": "https://serverfault.com/questions/574072/can-we-have-multiple-cnames-for-a-single-name"
    },
    {
        "title": "What is the difference between a managed and unmanaged switch?",
        "question": "I've seen some documentation discussing the use of an unmanaged switch. What is the difference in functionality/performance/etc. between an unmanaged and managed switch?",
        "top_answer": "Unmanaged switches \u2014 These switches have no configuration interface or options. They are plug-and-play. They are typically the least expensive switches, found in home, SOHO, or small businesses. They can be desktop or rack mounted. \nManaged switches \u2014 These switches have one or more ways, or interfaces, to modify the operation of the switch. Common management methods include: a serial console or Command Line Interface accessed via telnet or Secure Shell; an embedded Simple Network Management Protocol SNMP agent allowing management from a remote console or management station; a web interface for management from a web browser. Examples of configuration changes that one can do from a managed switch include: enable features such as Spanning Tree Protocol; set port speed; create or modify VLANs, etc. \nTwo sub-classes of managed switches are marketed today: \nSmart (or intelligent) switches \u2014 These are managed switches with a limited set of management features. Likewise \"web-managed\" switches are switches which fall in a market niche between unmanaged and managed. For a price much lower than a fully managed switch they provide a web interface (and usually no CLI access) and allow configuration of basic settings, such as VLANs, port-speed and duplex.[10] \nEnterprise Managed (or fully managed) switches - These have a full set of management features, including Command Line Interface, SNMP agent, and web interface. They may have additional features to manipulate configurations, such as the ability to display, modify, backup and restore configurations. Compared with smart switches, enterprise switches have more features that can be customized or optimized, and are generally more expensive than \"smart\" switches. Enterprise switches are typically found in networks with larger number of switches and connections, where centralized management is a significant savings in administrative time and effort. A Stackable switch is a version of enterprise-managed switch.\nSource: http://en.wikipedia.org/wiki/Network_switch\nI would explain in more personal detail, but the wiki explains it pretty well.",
        "url": "https://serverfault.com/questions/47324/what-is-the-difference-between-a-managed-and-unmanaged-switch"
    },
    {
        "title": "Linux FHS: /srv vs /var ... where do I put stuff?",
        "question": "My web development experience has started with Fedora and RHEL but I'm transitioning to Ubuntu. In Fedora/RHEL, the default seems to be using the /var folder while Ubuntu uses /srv.\nIs there any reason to use one over the other and where does the line split? (It confused me so much that until very recently, I thought /srv was /svr for server/service)\nMy main concern deals with two types of folders\n\ndefault www and ftp directories \nspecific application folders like:\n\n\nsamba shares (possibly grouped under a smb folder)\nweb applications (should these go in www folder, or do can I do a symlink to its own directory like \"___/www/wordpress\" -> \"/srv/wordpress\")\n\n\nI'm looking for best practice, industry standards, and qualitative reasons for which approach is best (or at least why its favored).",
        "top_answer": "This stems from LSB which says \"/var contains variable data files. This includes spool directories and files, administrative and logging data, and transient and temporary files.\" but says this for /srv: \"/srv contains site-specific data which is served by this system.\"\nSuSE was one of the first disto's that I used that kept webroot's in /srv - typically Debian/Ubuntu/RHEL use /var/www - but also be aware that if you install a web application using yum or apt then they will likely end up in /usr/share. Also the packaging guidelines for Fedora say that a \"package,  once installed and configured by a user, can use /srv as a location for data. The package simply must not do this out of the box\".\nOn balanced reflection I would say keep to /var/www - or you can do both by making /var/www a symlink to /srv/www. I know that on oracle RDBMS systems that I build I often create /u01 /u02 etc as symlinks to /home/oracle. The reason for this is that many DBA's expect to find things in /u01 and many others expect /home/oracle. The same can be said of Sysadmins in general - some will instinctively look in /var/www and some in /srv/www while others like myself will grep the apache config for the DocumentRoot.\nHope this provides some guidance for you.",
        "url": "https://serverfault.com/questions/124127/linux-fhs-srv-vs-var-where-do-i-put-stuff"
    },
    {
        "title": "Windows equivalent of iptables?",
        "question": "Dumb question:\nIs there an equivalent of iptables on Windows? Could I install one via cygwin?\nThe real question: how can I accomplish on Windows what I can accomplish via iptables?\nJust looking for basic firewall functionality (e.g. blocking certain IP addresses)",
        "top_answer": "One way would be with the netsh command:\n\nnetsh firewall (deprecated after XP and 2003)\nnetsh advfirewall (Vista, 7, and 2008)\n",
        "url": "https://serverfault.com/questions/207620/windows-equivalent-of-iptables"
    },
    {
        "title": "How to ping in linux until host is known?",
        "question": "How can I ping a certain address and when found, stop pinging.\nI want to use it in a bash script, so when the host is starting up, the script keeps on pinging and from the moment the host is available, the script continues...",
        "top_answer": "A further simplification of Martynas' answer:\nuntil ping -c1 www.google.com >/dev/null 2>&1; do :; done\n\nnote that ping itself is used as the loop test; as soon as it succeeds, the loop ends.  The loop body is empty, with the null command \":\" used to prevent a syntax error.\nUpdate: I thought of a way to make Control-C exit the ping loop cleanly. This will run the loop in the background, trap the interrupt (Control-C) signal, and kill the background loop if it occurs:\nping_cancelled=false    # Keep track of whether the loop was cancelled, or succeeded\nuntil ping -c1 \"$1\" >/dev/null 2>&1; do :; done &    # The \"&\" backgrounds it\ntrap \"kill $!; ping_cancelled=true\" SIGINT\nwait $!          # Wait for the loop to exit, one way or another\ntrap - SIGINT    # Remove the trap, now we're done with it\necho \"Done pinging, cancelled=$ping_cancelled\"\n\nIt's a bit circuitous, but if you want the loop to be cancellable it should do the trick.",
        "url": "https://serverfault.com/questions/42021/how-to-ping-in-linux-until-host-is-known"
    },
    {
        "title": "Display output with Ansible",
        "question": "I have a Ansible play for PGBouncer that displays some output from a stats module built into PGBouncer.\nMy issue is that when Ansible prints the output to the terminal it mangles the newlines. Instead of seeing\n----------\n| OUTPUT |\n----------\n\nI see\n----------\\n| OUTPUT |\\n----------\n\nDoes anyone know how to get Ansible to \"pretty print\" the output?",
        "top_answer": "There isn't a way to do what you want natively in Ansible.\nYou can do this as a workaround:\nansible-playbook ... | sed 's/\\\\n/\\n/g'\n",
        "url": "https://serverfault.com/questions/640130/display-output-with-ansible"
    },
    {
        "title": "Rsync triggered Linux OOM killer on a single 50 GB file",
        "question": "I have a single 50 GB file on server_A, and I'm copying it to server_B. I run\nserver_A$ rsync --partial --progress --inplace --append-verify 50GB_file root@server_B:50GB_file\n\nServer_B has 32\u00a0GB of RAM with 2\u00a0GB swap. It is mostly idle and should have had lots of free RAM. It has plenty of disk space. At about 32\u00a0GB, the transfer aborts because the remote side closed the connection.\nServer_B has now dropped off the network. We ask the data center to reboot it. When I look at the kernel log from before it crashed, I see that it was using 0 bytes of swap, and the process list was using very little memory (the rsync process was listed as using 600\u00a0KB of RAM), but the oom_killer was going wild, and the last thing in the log is where it kills metalog's kernel reader process.\nThis is kernel 3.2.59, 32-bit (so no process can map more than 4\u00a0GB anyway).\nIt's almost as if Linux gave more priority to caching than to long-lived running daemons. What gives?? And how can I stop it from happening again?\nHere is the output of the oom_killer:\nSep 23 02:04:16 [kernel] [1772321.850644] clamd invoked oom-killer: gfp_mask=0x84d0, order=0, oom_adj=0, oom_score_adj=0\nSep 23 02:04:16 [kernel] [1772321.850649] Pid: 21832, comm: clamd Tainted: G         C   3.2.59 #21\nSep 23 02:04:16 [kernel] [1772321.850651] Call Trace:\nSep 23 02:04:16 [kernel] [1772321.850659]  [<c01739ac>] ? dump_header+0x4d/0x160\nSep 23 02:04:16 [kernel] [1772321.850662]  [<c0173bf3>] ? oom_kill_process+0x2e/0x20e\nSep 23 02:04:16 [kernel] [1772321.850665]  [<c0173ff8>] ? out_of_memory+0x225/0x283\nSep 23 02:04:16 [kernel] [1772321.850668]  [<c0176438>] ? __alloc_pages_nodemask+0x446/0x4f4\nSep 23 02:04:16 [kernel] [1772321.850672]  [<c0126525>] ? pte_alloc_one+0x14/0x2f\nSep 23 02:04:16 [kernel] [1772321.850675]  [<c0185578>] ? __pte_alloc+0x16/0xc0\nSep 23 02:04:16 [kernel] [1772321.850678]  [<c0189e74>] ? vma_merge+0x18d/0x1cc\nSep 23 02:04:16 [kernel] [1772321.850681]  [<c01856fa>] ? handle_mm_fault+0xd8/0x15d\nSep 23 02:04:16 [kernel] [1772321.850685]  [<c012305a>] ? do_page_fault+0x20e/0x361\nSep 23 02:04:16 [kernel] [1772321.850688]  [<c018a9c4>] ? sys_mmap_pgoff+0xa2/0xc9\nSep 23 02:04:16 [kernel] [1772321.850690]  [<c0122e4c>] ? vmalloc_fault+0x237/0x237\nSep 23 02:04:16 [kernel] [1772321.850694]  [<c08ba7e6>] ? error_code+0x5a/0x60\nSep 23 02:04:16 [kernel] [1772321.850697]  [<c08b0000>] ? cpuid4_cache_lookup_regs+0x372/0x3b2\nSep 23 02:04:16 [kernel] [1772321.850700]  [<c0122e4c>] ? vmalloc_fault+0x237/0x237\nSep 23 02:04:16 [kernel] [1772321.850701] Mem-Info:\nSep 23 02:04:16 [kernel] [1772321.850703] DMA per-cpu:\nSep 23 02:04:16 [kernel] [1772321.850704] CPU    0: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850706] CPU    1: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850707] CPU    2: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850709] CPU    3: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850711] CPU    4: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850713] CPU    5: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850714] CPU    6: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850716] CPU    7: hi:    0, btch:   1 usd:   0\nSep 23 02:04:16 [kernel] [1772321.850718] Normal per-cpu:\nSep 23 02:04:16 [kernel] [1772321.850719] CPU    0: hi:  186, btch:  31 usd:  70\nSep 23 02:04:16 [kernel] [1772321.850721] CPU    1: hi:  186, btch:  31 usd: 116\nSep 23 02:04:16 [kernel] [1772321.850723] CPU    2: hi:  186, btch:  31 usd: 131\nSep 23 02:04:16 [kernel] [1772321.850724] CPU    3: hi:  186, btch:  31 usd:  76\nSep 23 02:04:16 [kernel] [1772321.850726] CPU    4: hi:  186, btch:  31 usd:  29\nSep 23 02:04:16 [kernel] [1772321.850728] CPU    5: hi:  186, btch:  31 usd:  61\nSep 23 02:04:16 [kernel] [1772321.850731] CPU    7: hi:  186, btch:  31 usd:  17\nSep 23 02:04:16 [kernel] [1772321.850733] HighMem per-cpu:\nSep 23 02:04:16 [kernel] [1772321.850734] CPU    0: hi:  186, btch:  31 usd:   2\nSep 23 02:04:16 [kernel] [1772321.850736] CPU    1: hi:  186, btch:  31 usd:  69\nSep 23 02:04:16 [kernel] [1772321.850738] CPU    2: hi:  186, btch:  31 usd:  25\nSep 23 02:04:16 [kernel] [1772321.850739] CPU    3: hi:  186, btch:  31 usd:  27\nSep 23 02:04:16 [kernel] [1772321.850741] CPU    4: hi:  186, btch:  31 usd:   7\nSep 23 02:04:16 [kernel] [1772321.850743] CPU    5: hi:  186, btch:  31 usd: 188\nSep 23 02:04:16 [kernel] [1772321.850744] CPU    6: hi:  186, btch:  31 usd:  25\nSep 23 02:04:16 [kernel] [1772321.850746] CPU    7: hi:  186, btch:  31 usd: 158\nSep 23 02:04:16 [kernel] [1772321.850750] active_anon:117913 inactive_anon:9942 isolated_anon:0\nSep 23 02:04:16 [kernel] [1772321.850751]  active_file:106466 inactive_file:7784521 isolated_file:0\nSep 23 02:04:16 [kernel] [1772321.850752]  unevictable:40 dirty:0 writeback:61 unstable:0\nSep 23 02:04:16 [kernel] [1772321.850753]  free:143494 slab_reclaimable:128312 slab_unreclaimable:4089\nSep 23 02:04:16 [kernel] [1772321.850754]  mapped:6706 shmem:308 pagetables:915 bounce:0\nSep 23 02:04:16 [kernel] [1772321.850759] DMA free:3624kB min:140kB low:172kB high:208kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolate\nd(file):0kB present:15808kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:240kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB writeback_tm\np:0kB pages_scanned:0 all_unreclaimable? yes\nSep 23 02:04:16 [kernel] [1772321.850763] lowmem_reserve[]: 0 869 32487 32487\nSep 23 02:04:16 [kernel] [1772321.850770] Normal free:8056kB min:8048kB low:10060kB high:12072kB active_anon:0kB inactive_anon:0kB active_file:248kB inactive_file:388kB unevictable:0kB isolated(anon)\n:0kB isolated(file):0kB present:890008kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:513008kB slab_unreclaimable:16356kB kernel_stack:1888kB pagetables:3660kB unstable:0\nkB bounce:0kB writeback_tmp:0kB pages_scanned:1015 all_unreclaimable? yes\nSep 23 02:04:16 [kernel] [1772321.850774] lowmem_reserve[]: 0 0 252949 252949\nSep 23 02:04:16 [kernel] [1772321.850785] lowmem_reserve[]: 0 0 0 0\nSep 23 02:04:16 [kernel] [1772321.850788] DMA: 0*4kB 7*8kB 3*16kB 6*32kB 4*64kB 6*128kB 5*256kB 2*512kB 0*1024kB 0*2048kB 0*4096kB = 3624kB\nSep 23 02:04:16 [kernel] [1772321.850795] Normal: 830*4kB 80*8kB 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 1*4096kB = 8056kB\nSep 23 02:04:16 [kernel] [1772321.850802] HighMem: 13*4kB 14*8kB 2*16kB 2*32kB 0*64kB 0*128kB 2*256kB 2*512kB 3*1024kB 0*2048kB 136*4096kB = 561924kB\nSep 23 02:04:16 [kernel] [1772321.850809] 7891360 total pagecache pages\nSep 23 02:04:16 [kernel] [1772321.850811] 0 pages in swap cache\nSep 23 02:04:16 [kernel] [1772321.850812] Swap cache stats: add 0, delete 0, find 0/0\nSep 23 02:04:16 [kernel] [1772321.850814] Free swap  = 1959892kB\nSep 23 02:04:16 [kernel] [1772321.850815] Total swap = 1959892kB\nSep 23 02:04:16 [kernel] [1772321.949081] 8650736 pages RAM\nSep 23 02:04:16 [kernel] [1772321.949084] 8422402 pages HighMem\nSep 23 02:04:16 [kernel] [1772321.949085] 349626 pages reserved\nSep 23 02:04:16 [kernel] [1772321.949086] 7885006 pages shared\nSep 23 02:04:16 [kernel] [1772321.949087] 316864 pages non-shared\nSep 23 02:04:16 [kernel] [1772321.949089] [ pid ]   uid  tgid total_vm      rss cpu oom_adj oom_score_adj name\n            (rest of process list omitted)\nSep 23 02:04:16 [kernel] [1772321.949656] [14579]     0 14579      579      171   5       0             0 rsync\nSep 23 02:04:16 [kernel] [1772321.949662] [14580]     0 14580      677      215   5       0             0 rsync\nSep 23 02:04:16 [kernel] [1772321.949669] [21832]   113 21832    42469    37403   0       0             0 clamd\nSep 23 02:04:16 [kernel] [1772321.949674] Out of memory: Kill process 21832 (clamd) score 4 or sacrifice child\nSep 23 02:04:16 [kernel] [1772321.949679] Killed process 21832 (clamd) total-vm:169876kB, anon-rss:146900kB, file-rss:2712kB\n\n\nHere is the 'top' output after repeating my rsync command as a non-root user:\ntop - 03:05:55 up  8:43,  2 users,  load average: 0.04, 0.08, 0.09\nTasks: 224 total,   1 running, 223 sleeping,   0 stopped,   0 zombie\nCpu(s):  0.0% us,  0.0% sy,  0.0% ni, 99.9% id,  0.0% wa,  0.0% hi,  0.0% si\nMem:  33204440k total, 32688600k used,   515840k free,   108124k buffers\nSwap:  1959892k total,        0k used,  1959892k free, 31648080k cached\n\n\nHere are the sysctl vm parameters:\n# sysctl -a | grep '^vm'\nvm.overcommit_memory = 0\nvm.panic_on_oom = 0\nvm.oom_kill_allocating_task = 0\nvm.oom_dump_tasks = 1\nvm.overcommit_ratio = 50\nvm.page-cluster = 3\nvm.dirty_background_ratio = 1\nvm.dirty_background_bytes = 0\nvm.dirty_ratio = 0\nvm.dirty_bytes = 15728640\nvm.dirty_writeback_centisecs = 500\nvm.dirty_expire_centisecs = 3000\nvm.nr_pdflush_threads = 0\nvm.swappiness = 60\nvm.lowmem_reserve_ratio = 256   32      32\nvm.drop_caches = 0\nvm.min_free_kbytes = 8192\nvm.percpu_pagelist_fraction = 0\nvm.max_map_count = 65530\nvm.laptop_mode = 0\nvm.block_dump = 0\nvm.vfs_cache_pressure = 100\nvm.legacy_va_layout = 0\nvm.stat_interval = 1\nvm.mmap_min_addr = 4096\nvm.vdso_enabled = 2\nvm.highmem_is_dirtyable = 0\nvm.scan_unevictable_pages = 0\n",
        "top_answer": "So let us read the oom-killer output and see what can be learned from there.\nWhen analyzing OOM killer logs, it is important to look at what triggered it. The first line of your log gives us some clues:\n\n[kernel] [1772321.850644] clamd invoked oom-killer: gfp_mask=0x84d0, order=0\n\norder=0 is telling us how much memory is being requested. The kernel's memory management is only able to manage page numbers in the powers of 2, so clamd has requested 20 pages of memory or 4KB.\nThe lowest two bits of the GFP_MASK (get free page mask) constitute the so-called zone mask telling the allocator which zone to get the memory from:\nFlag            value      Description\n                0x00u      0 implicitly means allocate from ZONE_NORMAL\n__GFP_DMA       0x01u      Allocate from ZONE_DMA if possible\n__GFP_HIGHMEM   0x02u      Allocate from ZONE_HIGHMEM if possible\n\nMemory zones is a concept created mainly for compatibility reasons. In a simplified view, there are three zones for an x86 Kernel: \nMemory range   Zone       Purpose \n\n0-16 MB        DMA        Hardware compatibility (devices)\n16 - 896 MB    NORMAL     space directly addressable by the Kernel, userland \n> 896 MB       HIGHMEM    userland, space addressable by the Kernel via kmap() calls\n\nIn your case, the zonemask is 0, meaning clamd is requesting memory from ZONE_NORMAL. \nThe other flags are resolving to\n/*\n * Action modifiers - doesn't change the zoning\n *\n * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt\n * _might_ fail.  This depends upon the particular VM implementation.\n *\n * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller\n * cannot handle allocation failures.\n *\n * __GFP_NORETRY: The VM implementation must not retry indefinitely.\n */\n#define __GFP_WAIT      0x10u   /* Can wait and reschedule? */\n#define __GFP_HIGH      0x20u   /* Should access emergency pools? */\n#define __GFP_IO        0x40u   /* Can start physical IO? */\n#define __GFP_FS        0x80u   /* Can call down to low-level FS? */\n#define __GFP_COLD      0x100u  /* Cache-cold page required */\n#define __GFP_NOWARN    0x200u  /* Suppress page allocation failure warning */\n#define __GFP_REPEAT    0x400u  /* Retry the allocation.  Might fail */\n#define __GFP_NOFAIL    0x800u  /* Retry for ever.  Cannot fail */\n#define __GFP_NORETRY   0x1000u /* Do not retry.  Might fail */\n#define __GFP_NO_GROW   0x2000u /* Slab internal usage */\n#define __GFP_COMP      0x4000u /* Add compound page metadata */\n#define __GFP_ZERO      0x8000u /* Return zeroed page on success */\n#define __GFP_NOMEMALLOC 0x10000u /* Don't use emergency reserves */\n#define __GFP_NORECLAIM  0x20000u /* No realy zone reclaim during allocation */\n\naccording to the Linux MM documentation, so your requst has the flags for GFP_ZERO, GFP_REPEAT, GFP_FS, GFP_IO and GFP_WAIT, thus being not particularly picky. \nSo what's up with ZONE_NORMAL? Some generic stats can be found further on in the OOM output:\n\n[kernel] [1772321.850770] Normal free:8056kB min:8048kB low:10060kB high:12072kB active_anon:0kB inactive_anon:0kB active_file:248kB inactive_file:388kB unevictable:0kB isolated(anon)\n  :0kB isolated(file):0kB present:890008kB\n\nNoticeable here is that free is just 8K from min and way under low. This means your host's memory manager is somewhat in distress and kswapd should be swapping out pages already as it is in the yellow phase of the graph below:\n\nSome more information on the memory fragmentation of the zone is given here:\n\n[kernel] [1772321.850795] Normal: 830*4kB 80*8kB 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 1*4096kB = 8056kB\n\nbasically stating that you have a single contiguous page of 4MB with the rest heavily fragmented into mainly 4KB pages. \nSo let's recapitulate: \n\nyou have a userland process (clamd) getting memory from ZONE_NORMAL whereas non-privileged memory allocation usually would be performed from ZONE_HIMEM\nthe memory manager should at this stage have been able to serve the requested 4K page, although you seem to have significant memory pressure in ZONE_NORMAL\nthe system, by kswapd's rules, should have seen some paging activity beforehand, but nothing is being swapped out, even under memory pressure in ZONE_NORMAL, without apparent cause\nNone of the above gives a definite reason as for why oom-killer has been invoked\n\nAll of this seems rather odd, but is at least to be related to what is described in section 2.5 of John O'Gorman's excellent \"Understanding the Linux Virtual Memory Manager\" book:\n\nAs the addresses space usable by the kernel (ZONE_NORMAL) is limited in size, the kernel has support for the concept of High Memory. [...] To access memory between the range of 1GiB and 4GiB, the kernel temporarily maps pages from high memory into ZONE_NORMAL with kmap(). [...]\nThat means that to describe 1GiB of memory, approximately 11MiB of kernel memory is required. Thus, with 16GiB, 176MiB of memory is consumed, putting significant pressure on ZONE_NORMAL. This does not sound too bad until other structures are taken into account which use ZONE_NORMAL. Even very small structures such as Page Table Entries (PTEs) require about 16MiB in the worst case. This makes 16GiB about the practical limit for available physical memory Linux on an x86.\n\n(emphasis is mine)\nSince 3.2 has numerous advancements in memory management over 2.6, this is not a definite answer, but a really strong hint I would pursue first. Reduce the host's usable memory to at most 16G by either using the mem= kernel parameter or by ripping half of the DIMMs out of the server. \nUltimately, use a 64-bit Kernel.\nDude, it's 2015.",
        "url": "https://serverfault.com/questions/724469/rsync-triggered-linux-oom-killer-on-a-single-50-gb-file"
    },
    {
        "title": "What a beginner should know/learn for sysadmin job?",
        "question": "\nThis is a Canonical Question about System Administration Careers\n\nWhen I start my job as System Administrator, what basics skills should I know/learn?\nAre there any key differences for Network, Storage, Database, and other Administrators?",
        "top_answer": "There is a lot of overlap with existing questions, I am creating a wiki here with links. Please feel free to update.\n\nHow to make (and restore) backups!\nCustomer service skills\nTroubleshooting\n\n\nYour troubleshooting rules, approach to troubleshooting?\nEtiquette of Troubleshooting Problems In The Workspaces Of Others\n\nHow to respond when there is a crisis\n\n\nWhat\u2019s your checklist for when everything blows up?\n\nThe OSI Model, and IP networking.\n\n\nWhat is the OSI model and how does it apply to todays networks?\nPractical implications of OSI vs TCP/IP networking.\nHow does Subnetting Work?\nWhat is the difference between a port and a socket?\nWhat are routers, hubs, and switches?\nWhat is the difference between UDP and TCP?\n\nHow to document their network\n\n\nHow are you documenting your work, processes and environment?\n\nHow to ask for help in a way that will get you useful results.\n\n\nHow To Ask Questions The Smart Way\nHow to ask a question\n\nSecurity\n\n\nHow to respond to a compromised system\n\nHow to use the CLI\n\n\nUseful Commandline Commands on Windows\nUseful Commandline Commands on Linux\nUseful Commandline Commands on Mac OS\n\nHow to monitor the systems you will be responsible for\nAlso see\n\n\nWhat makes a \u201cGood\u201d or \u201cGreat\u201d Administrator?\nCheat Sheets for System Administrators?\nWhat tools should you absolutely know as a Windows/Linux Sysadmin?\nWhat should every sysadmin know before administrating a public server?\nWhat sysadmin things should every programmer know?\nWhat is the single most influential book every sysadmin should read?\n\n",
        "url": "https://serverfault.com/questions/9766/what-a-beginner-should-know-learn-for-sysadmin-job"
    },
    {
        "title": "How to manage my .ssh/known_hosts file",
        "question": "I run an Ubuntu desktop with a bunch of virtual servers in Virtual Box to test stuff out, etc. In the past I have also been connecting to other kinds of remote VPS Linux boxes. Currently my .ssh/known_hosts file has a whole bunch of keys in it, most of which are not being used any more.\nI want to clean up my .ssh/known_hosts file, but how do I know which key belongs to what host? I.e. how do I know which keys I can safely remove and which ones I should leave alone?",
        "top_answer": "To find out which entry is for a known hostname in known_hosts:\n # ssh-keygen -H  -F <hostname or IP address>\n\nTo delete a single entry from known_hosts:\n # ssh-keygen -R <hostname or IP address>\n",
        "url": "https://serverfault.com/questions/29262/how-to-manage-my-ssh-known-hosts-file"
    },
    {
        "title": "How can I launch a screen session with a command over ssh on a remote server from my desktop?",
        "question": "I want to be able to launch screen sessions on remote servers from a single ssh command on my desktop.  However, screen seems to need a terminal, which is not available when running a command through ssh.\nSo the obvious\nssh [email\u00a0protected] screen \"tail -f /var/log/messages\"\n\n(as an example) does not work, and gives\nMust be connected to a terminal.\n\nI want ssh to launch the command under a screen so I can log in later and attach as I would to a screen session I would have launched manually.",
        "top_answer": "Try using the -t option to ssh\nssh -t [email\u00a0protected] screen \"tail -f /var/log/messages\"\n\nFrom man ssh\n\n-t      Force pseudo-tty allocation.  This can be used to execute arbi-\n        trary screen-based programs on a remote machine, which can be\n        very useful, e.g., when implementing menu services.  Multiple -t\n        options force tty allocation, even if ssh has no local tty.\n",
        "url": "https://serverfault.com/questions/21806/how-can-i-launch-a-screen-session-with-a-command-over-ssh-on-a-remote-server-fro"
    },
    {
        "title": "Best Practices in Username Standards: Avoiding Problems",
        "question": "I'm interested in finding out what people's experiences with standard usernames is. I've always been in places that used {firstInitial}{lastname} (sometimes with a length-limit). Now I've users that want {firstname}.{lastname} - and now it comes up that the period may cause problems.\nSpecifically:\n\nWhat is the best username length limit to use to maintain compatibility across all uses?\nWhat characters should be avoided?\n\nUPDATE: The reason I didn't mention specifics is that I wanted to be general enough to handle anything that might come up in the future. However, that may be too general of a requirement (anything can happen, right?).\nThis is our environment: Ubuntu Server Lucid Lynx 10.04 LTS, Red Hat Enterprise Linux 5.6 and up, Windows Server 2003 and Windows 2000 Server (with Active Directory in Windows 2000 Native Mode), Zimbra 7.x for mail, and OpenLDAP in the near future.\nUPDATE: I should mention (for completeness) that I saw this question (though it didn't answer my asked question) and also this web post, both of which were very informative.",
        "top_answer": "This is a chronic problem with large Identity Management systems attempting to glue together heterogeneous systems. Invariably, you'll be limited to the lowest common denominator, which all too often is an 8-character ASCII-alpha-numeric limit thanks to some (probably legacy) Unix-like system somewhere in the bowels of the datacenter. Those fancy modern systems can take arbitrary length UTF8 usernames are unlikely to get used.\nI spent 7 years at an institution of higher education where we had to figure out 8-character usernames for 5000 new students every year. We had managed to come up with unique names for 15 years of students by the time I left. This can be done, Mr. smitj510\nThings that will make your life immeasurably easier:\n\nFigure out what your lowest-common-denominator is, which requires analyzing every part of your identity-management system to discover what the limits are.\n\n\nThat old Solaris 7 system is forcing the 8-character limit.\nCritical applications that use identity data have their own limits you will have to consider.\n\n\nPerhaps they expect user data from LDAP to conform to a unique-to-them 'standard'.\nPerhaps the authentication database they use can only handle certain formatted data.\nPerhaps that Windows-compatible system still uses SAMAccountName two decades after that stopped being a good idea.\n\n\nHave a database table with a list of the One True Identifier (that 8-character account-name), with links/fields listing alternate ID's like firstname.lastname or anything else that might come up.\n\n\nOff-the-shelf software can do some really weird and IDM-unfriendly things like use a numerical ID for account name, or auto-generate account IDs based on profile data. All that goes into the database table too.\nThis also helps people with non-[a-z|0-9] characters in their names like Harry O'Neil, or non-ASCII ones like Al\u017eb\u00eata.\n\nWhen you build your account synchronization processes, leverage that database table to ensure that the right accounts are getting the right updates. When names change (marriage, divorce, others) you want those changes to propagate to the right places.\n\n\nConfigure the actual identity databases themselves to prevent local-changes where possible, and business process to strongly discourage that when it isn't possible. Rely on the central account-sync process for everything you can.\n\nLeverage alias systems wherever you can, such as in email. \nConsider the 8-char ID immutable, since changing that field can trigger a LOT of heart-ache among IT staff as accounts have to be recreated.\n\n\nThis suggests an account-ID not derived from name data, since marriage/divorce/court-order can change the name-data over time. \n\nHave a system in place for exceptions, since there will always be some.\n\n\nHorrible divorce and that name-data generated 8-char UID brings wrenching memories every time you have to enter it? Be nice to your users and allow a mechanism for these changes, but keep it quiet. \n\nDo what you can to allow multiple username logins in the systems where that's an option\n\n\nSome people like their 8-character uid, others like [email\u00a0protected]. Be flexible, make friends.\nSometimes this requires fronting your web-based systems with a single-sign-on framework like CAS or leverage saml. You will be surprised at how many off the shelf systems can support SSO frameworks like this, so don't be discouraged.\n\n\nWhich is to say, treat it like a databasing problem because that's what it is. Pick a primary key for maximum compatibility with your systems (likely 8 characters), build a lookup-table to allow systems to translate local ID's to the primary key, and engineer your data synchronization systems to handle various IDs. ",
        "url": "https://serverfault.com/questions/348912/best-practices-in-username-standards-avoiding-problems"
    },
    {
        "title": "associate name with ip for ssh?",
        "question": "Let's say I just have an ip address for a server and I don't have a domain with it (it's just a database server, so it doesn't need a domain). I don't want to have to remember the ip address every time, so is there a way I could still use the syntax like ssh username@database or something?",
        "top_answer": "If you only want the name for ssh and ssh only, you can add a name to your ssh config in ~/.ssh/config\nAs an example, your config file could look like this:\nHost database\n    HostName <real IP address or hostname here>\n    User username\n\nThen you can type ssh database on the command line and ssh will automatically do ssh [email\u00a0protected] for you.",
        "url": "https://serverfault.com/questions/215007/associate-name-with-ip-for-ssh"
    },
    {
        "title": "bash: variable loses value at end of while read loop",
        "question": "I have a problem in one of my shell scripts. Asked a few colleagues, but they all just shake their heads (after some scratching), so I've come here for an answer.\nAccording to my understanding the following shell script should print \"Count is 5\" as the last line. Except it doesn't. It prints \"Count is 0\". If the \"while read\" is replaced with any other kind of loop, it works just fine. Here's the script:\necho \"1\">input.data\necho \"2\">>input.data\necho \"3\">>input.data\necho \"4\">>input.data\necho \"5\">>input.data\n\nCNT=0 \n\ncat input.data | while read ;\ndo\n  let CNT++;\n  echo \"Counting to $CNT\"\ndone \necho \"Count is $CNT\"\n\nWhy does this happen and how can I prevent it? I've tried this in Debian Lenny and Squeeze, same result (i.e. bash 3.2.39 and bash 4.1.5.\nI fully admit to not being a shell script wizard, so any pointers would be appreciated.",
        "top_answer": "See argument @ Bash FAQ entry #24: \"I set variables in a loop. Why do they suddenly disappear after the loop terminates? Or, why can't I pipe data to read?\" (most recently archived here).\nSummary:\nThis is only supported from bash 4.2 and up.\nYou need to use different ways like command substitutions instead of a pipe if you are using bash.",
        "url": "https://serverfault.com/questions/259339/bash-variable-loses-value-at-end-of-while-read-loop"
    },
    {
        "title": "How to rename a MySQL database?",
        "question": "How to rename a MySQL database?\nThe MySQL online manual has said about the RENAME DATABASE command (this documentation page has been removed by Oracle some time ago):\n\nThis statement was added in MySQL 5.1.7 but was found to be dangerous and\n  was removed in MySQL 5.1.23.\n\nSo, how to proceed? The rationale: We started with a code name for the project and want the database name now to reflect the definitive name of the project.",
        "top_answer": "From this blog post by Ilan Hazan:\nIn MySQL there is no support for database renaming.\nIn order to rename a MySQL database you can do one of the following:\n\nCreate new database and rename all tables in the old database to be in the new database:\n CREATE database new_db_name;\n RENAME TABLE db_name.table1 TO new_db_name.table1, db_name.table2 TO new_db_name.table2;\n DROP database db_name;\n\n\nIn Linux shell, use mysqldump to back up the old database, then restore the dumped database under a new name using the MySQL utility. Finally, use the drop database command to drop the old database. This option can perform badly for large database.\n mysqldump -uxxxx -pxxxx -h xxxx db_name > db_name_dump.sql\n mysql -uxxxx -pxxxx -h xxxx -e \"CREATE DATABASE new_db_name\"\n mysql -uxxxx -pxxxx -h xxxx new_db_name < db_name_dump.sql\n mysql -uxxxx -pxxxx -h xxxx -e \"DROP DATABASE db_name\"\n\n\nWrite a simple Linux script (my favorite solution)\n #!/bin/bash\n\n dbuser=xxxx\n dbpass=xxxx\n olddb=xxxx\n newdb=xxxx\n\n mysqlconn=\"mysql -u $dbuser -p$dbpass -h localhost\"\n\n $mysqlconn -e \"CREATE DATABASE \\`$newdb\\`\"\n params=$($mysqlconn -N -e \"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE table_schema='$olddb'\")\n\n for name in $params; do\n       sql=\"RENAME TABLE \\`$olddb\\`.\\`$name\\` to \\`$newdb\\`.\\`$name\\`\" \n       echo \"$sql\";\n       $mysqlconn -e \"$sql\";\n       echo \"Renamed $olddb.$name to $newdb.$name\";\n done;\n\n #$mysqlconn -e \"DROP DATABASE \\`$olddb\\`\"\n\n\nIf all your tables are MyISAM, you can rename the old database folder name:\n\nShut down the MySQL server,\nRename the database folder name to the new name,\nStart the MySQL server.\n\n\n",
        "url": "https://serverfault.com/questions/195221/how-to-rename-a-mysql-database"
    },
    {
        "title": "Prevent Windows Server 2012 from forcing a reboot after updates",
        "question": "I have a domain controller with Windows Server 2012 on it. After updates, the server does not reboot immediately. However if I remote into the server I will be presented with a countdown for a reboot. The only options are to restart now or to close the notification. However the countdown still continues and the server eventually reboots without my permission. How can I stop this from occurring?",
        "top_answer": "There is a Local Group Policies you can set to disable the automatic restarts. This should only be done on Windows Servers assuming a sysadmin is going to RDP into the server on a regular schedule and install updates and restart the server (see Patch Tuesday).\n\nPress Windows Key+R to open the run prompt.\nType \"gpedit.msc\" and press enter.\nIn the \"Local Group Policy Editor\", navigate to Computer Configuration > Administrative Templates > Windows Components > Windows Update.\nEnable the \"Configure Automatic Updates\" policy and set it to \"2\".\nEnable the \"No auto-restart with logged on users for scheduled automatic updates installations\" policy.\n",
        "url": "https://serverfault.com/questions/465473/prevent-windows-server-2012-from-forcing-a-reboot-after-updates"
    },
    {
        "title": "How to ignore an error in Powershell and let it continue?",
        "question": "I am trying to see if a process is running on multiple servers and then format it into a table.\nget-process -ComputerName server1,server2,server3 -name explorer | Select-Object processname,machinename\n\nThats the easy part - When the process does not exist or if the server is unavailable, powershell outputs a big ugly error, messes up the the table and doesn't continue.  Example\nGet-Process : Couldn't connect to remote machine.At line:1 char:12 + get-process <<<<  -ComputerName server1,server2,server3 -name explorer | format-table processname,machinename\n+ CategoryInfo          : NotSpecified: (:) [Get-Process], InvalidOperatio   nException    + FullyQualifiedErrorId : System.InvalidOperationException,Microsoft.Power   Shell.Commands.GetProcessCommand\n\nHow do I get around this? If the I would still like to get notified if the process isn't available or Running. ",
        "top_answer": "Add -ErrorAction SilentlyContinue to your command.\nWhen it's not an error, but an unhandled Exception, you should add -EV Err -EA SilentlyContinue, in order to catch the exception. (EA is an alias for ErrorAction)\nYou can then evaluate the error in your script, by having a look at $Err[0]",
        "url": "https://serverfault.com/questions/336121/how-to-ignore-an-error-in-powershell-and-let-it-continue"
    },
    {
        "title": "Supervisor HTTP Server Port Issue",
        "question": "I have supervisor setup to manage a few processes. It works perfectly fine when I boot my server, however when I stop it and try to start it again it fails and give's me this error msg:\n\n * Starting Supervisor daemon manager...\nError: Another program is already listening on a port that one of our HTTP servers is configured to use.  Shut this program down first before starting supervisord.\nFor help, use /usr/bin/supervisord -h\n   ...fail!\n\nI'm running nginx on port 80 and 4 web servers on ports 8000, 8001, 8002, 8003\nDoes anyone have any idea of what is going on?\nWhen I reboot everything works fine.",
        "top_answer": "Just ran into this as well.  I fixed it by doing either of these:\nsudo unlink /tmp/supervisor.sock\n\nsudo unlink /var/run/supervisor.sock\n\nThis .sock file is defined in /etc/supervisord.conf [unix_http_server] file config value (default is /tmp/supervisor.sock or /var/run/supervisor.sock).\nHope this helps someone in the future.",
        "url": "https://serverfault.com/questions/114477/supervisor-http-server-port-issue"
    },
    {
        "title": "What is the difference between syslog, rsyslog and syslog-ng? [closed]",
        "question": "I am a bit confused in syslog, rsyslog and syslog-ng.\nFrom where can I get the source code for syslog()?\nIs there any difference between rsyslog and rsyslogd?",
        "top_answer": "Basically, they are all the same, in the way they all permit the logging of data from different types of systems in a central repository.\nBut they are three different project, each project trying to improve the previous one with more reliability and functionalities.\nThe Syslog project was the very first project. It started in 1980. It is the root project to Syslog protocol. At this time Syslog is a very simple protocol. At the beginning it only supports UDP for transport, so that it does not guarantee the delivery of the messages.\nNext came syslog-ng in 1998. It extends basic syslog protocol with new features like:\n\ncontent-based filtering\nLogging directly into a database\nTCP for transport\nTLS encryption\n\nNext came Rsyslog in 2004. It extends syslog protocol with new features like:\n\nRELP Protocol support\nBuffered operation support\n\nLet's say that today they are three concurrent projects that have grown separately upon versions, but also grown in parallel regarding what the neighbors was doing.\nI personally think that today syslog-ng is the reference in most cases, as it is the most mature project offering the main features you may need, in addition to an easy and comprehensive setup and configuration.",
        "url": "https://serverfault.com/questions/692309/what-is-the-difference-between-syslog-rsyslog-and-syslog-ng"
    },
    {
        "title": "How do I grant start/stop/restart permissions on a service to an arbitrary user or group on a non-domain-member server?",
        "question": "We have a suite of Windows Services running on our servers which perform a bunch of automated tasks independently of one another, with the exception of one service which looks after the other services.\nIn the event that one of the services should fail to respond or hang, this service attempts to restart the service and, if an exception is thrown during the attempt, emails the support team instead, so that they can restart the service themselves.\nHaving done a little research, I've come across a few 'solutions' which range from the workaround mentioned in KB907460 to giving the account under which the service is running administrator rights.\nI'm not comfortable with either of these methods - I don't understand the consequences of the first method as outlined in Microsoft's knowledge base article, but I definitely don't want to give administrator access to the account under which the service is running.\nI've taken a quick look through the Local Security Policy and other than the policy which defines whether or not an account can log on as a service, I can't see anything else which looks like it refers to services.\nWe're running this on Server 2003 and Server 2008, so any ideas or pointers would be graciously received! \n\nClarification: I don't want to grant the ability to start/stop/restart ALL services to a given user or group - I want to be able to grant the permission to do so on specific services only, to a given user or group.\n\nFurther Clarification: The servers I need to grant these permissions on do not belong to a domain - they are two internet-facing servers which receive files, process them and send them on to third parties, as well as serving a couple of websites, so Active Directory Group Policy isn't possible. Sorry that I didn't make this clearer.",
        "top_answer": "There doesn't appear to be a GUI-based way of doing this unless you're joined to a domain - at least not one I could find anywhere - so I did a bit more digging and I've found an answer that works for our situation.\nI didn't understand what the string representation meant in the knowledge base article, but doing a bit of digging led me to discover that it's SDDL syntax. Further digging led me to this article by Alun Jones which explains how to get the security descriptor for a service and what each bit means. MS KB914392 has more details.\nTo append to the service's existing security descriptor, use sc sdshow \"Service Name\" to get the existing descriptor. If this is a plain old .NET Windows Service - as is the case with ours - the security descriptor should look something like this:\nD:(A;;CCLCSWRPWPDTLOCRRC;;;SY)(A;;CCDCLCSWRPWPDTLOCRSDRCWDWO;;;BA)(A;;CCLCSWLOC\nRRC;;;IU)(A;;CCLCSWLOCRRC;;;SU)(A;;CR;;;AU)(A;;CCLCSWRPWPDTLOCRRC;;;PU)S:(AU;FA\n;CCDCLCSWRPWPDTLOCRSDRCWDWO;;;WD)\n\nWe needed to grant permissions RP (to start the service), WP (to stop the service), DT (to pause/continue the service) and LO (to query the service's current status). This could be done by adding our service account to the Power Users group, but I only want to grant individual access to the account under which the maintenance service runs.\nUsing runas to open a command prompt under the service account, I ran whoami /all which gave me the SID of the service account, and then constructed the additional SDDL below:\n(A;;RPWPDTLO;;;S-x-x-xx-xxxxxxxxxx-xxxxxxxxxx-xxxxxxxxx-xxxx)\n\nThis then gets added to the D: section of the SDDL string above:\nD:(A;;CCLCSWRPWPDTLOCRRC;;;SY)(A;;CCDCLCSWRPWPDTLOCRSDRCWDWO;;;BA)(A;;CCLCSWLOC\nRRC;;;IU)(A;;CCLCSWLOCRRC;;;SU)(A;;CR;;;AU)(A;;CCLCSWRPWPDTLOCRRC;;;PU)(A;;RPWP\nDTLO;;;S-x-x-xx-xxxxxxxxxx-xxxxxxxxxx-xxxxxxxxx-xxxx)S:(AU;FA;CCDCLCSWRPWPDTLOC\nRSDRCWDWO;;;WD)\n\nThis is then applied to the service using the sc sdset command (before the S: text):\nsc sdset \"Service Name\" D:(A;;CCLCSWRPWPDTLOCRRC;;;SY)(A;;CCDCLCSWRPWPDTLOCRSDRCWDWO;;;BA)(A;;\nCCLCSWLOCRRC;;;IU)(A;;CCLCSWLOCRRC;;;SU)(A;;CR;;;AU)(A;;CCLCSWRPWPDTLOCRRC;;;PU\n)(A;;RPWPDTLO;;;S-x-x-xx-xxxxxxxxxx-xxxxxxxxxx-xxxxxxxxx-xxxx)S:(AU;FA;CCDCLCSW\nRPWPDTLOCRSDRCWDWO;;;WD)\n\nIf all goes according to plan, the service can then be started, stopped, paused and have it's status queried by the user defined by the SID above.",
        "url": "https://serverfault.com/questions/187302/how-do-i-grant-start-stop-restart-permissions-on-a-service-to-an-arbitrary-user"
    },
    {
        "title": "Drawbacks of mounting a filesystem with noatime?",
        "question": "Having every file be updated just when accessing them sounds like a waste.\nWhat's the catch with mounting a file system with the noatime option. What kind of applications/servers relies on the access time?",
        "top_answer": "Consider relatime:\nIf you have a newish install (~2008), you can use the relatime mount option.  This is a good compromise for atime I think.  From the kerneltrap discussion about implementing this new option:\n\n\"relative atime only updates the atime\n  if the previous atime is older than\n  the mtime or ctime. Like noatime, but\n  useful for applications like mutt that\n  need to know when a file has been read\n  since it was last modified.\"\n\nThis makes it so most of the applications that need atime will still work, but lessens the disk load -- so it is a compromise. This is the default with recent Ubuntu desktop distributions.\nRegarding noatime and nodiratime:\nIf you are going noatime for files, I wonder if there is a reason not to use nodiratime in addition to noatime so you are not updating the access time on directories as well.\nThe other reason to keep atime enabled which wasn't mentioned is for auditing purposes.  But since who accessed it is not kept and only when, it is probably not that useful for an audit trail.\nAll of these options can be found in 'man mount 8'.",
        "url": "https://serverfault.com/questions/47466/drawbacks-of-mounting-a-filesystem-with-noatime"
    },
    {
        "title": "Unique Features of bash compared to zsh",
        "question": "I have been a zsh user for quite some time (before that tcsh and before that csh). I am quite happy with it, but was wondering if there are any compelling features of bash that do not exist in zsh. And conversely, are there zsh features which do not exist in bash. My current feel is that bash is better:\n\nIf you are familiar with it already and don't want to learn new syntax.\nIt is going to exist on most all *nix machines by default, whereas zsh may be an extra install.\n\nNot trying to start a religious battle here, which is why I'm just looking for features which exist in only one of the shells.",
        "top_answer": "zsh is for vulcans. ;-)\nSeriously:\nbash 4.0 has some features previously only found in zsh, like ** globbing:\n% ls /usr/src/**/Makefile\n\nis equivalent to:\n% find /usr/src -name \"Makefile\"\n\nbut obviously more powerful.\nIn my experience bash's programmable completion performs a bit better than zsh's, at least for some cases (completing debian packages for aptitude for example).\nbash has Alt + . to insert !$\n\nzsh has expansion of all variables, so you can use e.g.\n% rm !$<Tab>\n\nfor this. zsh can also expand a command in backtics, so\n% cat `echo blubb | sed 's/u/a/'`<Tab>\n\nyields\n% cat blabb\n\nI find it very useful to expand rm *, as you can see what would be removed and can maybe remove one or two files from the commmand to prevent them from being deleted.\nAlso nice: using the output from commands for other commands that do not read from stdin but expect a filename:\n% diff <(sort foo) <(sort bar)\n\nFrom what I read bash-completion also supports completing remote filenames over ssh if you use ssh-agent, which used to be a good reason to switch to zsh.\nAliases in zsh can be defined to work on the whole line instead of just at the beginning:\n% alias -g ...=\"../..\"\n% cd ...\n\n",
        "url": "https://serverfault.com/questions/4993/unique-features-of-bash-compared-to-zsh"
    },
    {
        "title": "Is a VM with 2 CPUs really faster than one with 4 CPUs?",
        "question": "Our IT created a VM with 2 CPUs allocated rather than the 4 I requested.  Their reason is that the VM performs better with 2 CPUs rather than 4 (according to them).  The rationale is that the VM hypervisor (VMWare in this case) waits for all the CPUs to be available before engaging any of them. Thus, it takes longer to wait for 4 rather than 2 CPUs.\nDoes this statement make sense?",
        "top_answer": "This used to be true, but is no longer exclusively true.\nWhat they are referring to is Strict Co-Scheduling. \n\nMost important of all, while in the strict co-scheduling algorithm, the existence of a lagging vCPU causes the\n  entire virtual machine to be co-stopped. In the relaxed co-scheduling algorithm, a leading vCPU decides whether\n  it should co-stop itself based on the skew against the slowest sibling vCPU\n\nNow, if the host only has 4 threads, then you'd be silly to allocate all of them. If it has two processors and 4 threads per processor, then you might not want to allocate all of the contents of a single processor, as your hypervisor should try to keep vCPUs on the same NUMA node to make memory access faster, and you're making this job more difficult by allocating a whole socket to a single VM (See page 12 of that PDF above).\nSo there are scenarios where fewer vCPUs can perform better than more, but it's not true  100% of the time. \nAll that said and done, I very rarely allocate more than 3 vCPUs per guest. Everyone gets 2 by default, 3 if it's a heavy workload, and 4 for things like SQL Servers or really heavy batch processing VMs, or a terminal server with a lot of users.",
        "url": "https://serverfault.com/questions/686655/is-a-vm-with-2-cpus-really-faster-than-one-with-4-cpus"
    },
    {
        "title": "Is there a way to validate /etc/crontab\u2019s format?",
        "question": "I prefer to stick scheduled tasks in /etc/crontab so I can see at a glance what's scheduled to run, regardless of which user the task runs as.\nThe only gotcha is that the format isn't validated on save, unlike crontab -e - so a stray character can quietly break the entire cron.\nIs there a way to validate the /etc/crontab format before/after save?",
        "top_answer": "The only reliable way I found is to check the log.\ncron checks /etc/crontab every minute, and logs a message indicating that it has reloaded it, or that it found an error.\nSo after editing, run this:\nsleep 60; grep crontab /var/log/syslog | tail\n\nOr, to not wait a full minute, but only until the next minute + 5 seconds:\nsleep $(( 60 - $(date +%S) + 5 )) && grep cron /var/log/syslog | tail\n\nExample output with an error:\nJan  9 19:10:57 r530a cron[107258]: Error: bad minute; while reading /etc/crontab\nJan  9 19:10:57 r530a cron[107258]: (*system*) ERROR (Syntax error, this crontab file will be ignored)\n\nGood output:\nJan  9 19:19:01 r530a cron[107258]: (*system*) RELOAD (/etc/crontab)\n\nThat's on Debian 8. On other systems, cron might log to a different file.\n(I thought I could avoid hunting for the right log file by using systemd's journalctl -u cron, but that didn't show me these log entries, and actually seems to have stopped logging cron events 2 days ago for some reason)",
        "url": "https://serverfault.com/questions/43733/is-there-a-way-to-validate-etc-crontab-s-format"
    },
    {
        "title": "Why does Heroku warn against \"naked\" domain names?",
        "question": "I ran across this page in the Heroku docs...\n\nNaked domains, also called bare or apex domains, are configured in DNS via A-records and have serious availability implications when used in highly available environments such as massive on-premise datacenters, cloud infrastructure services, and platforms like Heroku.\nFor maximum scalability and resiliency applications should avoid naked domains and instead rely solely on subdomain-based hostnames.\n\nDoes anyone here speak Enterprise? What are the \"availability implications\" they're warning about?\n(I notice that http://stackoverflow.com works no problem, so evidently there are viable alternate philosophies on this issue.)",
        "top_answer": "What they're talking about is that when you use a CNAME to point to their services (which is only possible on subdomain, not the zone root - it can't coexist with the SOA and NS records that are required on the root of your zone), they can make a change to their own DNS records to work around some kind of availability issue.\nWith a zone root, you must use an A record to point to a specific IP address for the service.  If they have an issue with routing, or some kind of denial of service against that specific address, they're not able to update your zone's A record to point to a different IP on the fly; they can update their own, though, and that's what a CNAME allows them to do.\nThis doesn't apply to Stack Exchange because they aren't using a third party's platform; they'll be the ones responding to an availability issue, so whether it's a CNAME or an A makes no difference to them.",
        "url": "https://serverfault.com/questions/408017/why-does-heroku-warn-against-naked-domain-names"
    },
    {
        "title": "The perfect server room?",
        "question": "What do I have to consider when I'm planning a new server room for a small company (30 PCs, 5 servers, a couple of switches, routers, UPS...)?\nWhat are the most important aspects in order to protect the hardware?\nWhat things do not belong in a server closet?\nEdit: You may also be interested in this question: Server Room Survival Kit.",
        "top_answer": "\nEnough space for expansion\nPlenty of network ports\nSufficient network bandwidth\nPlenty of dedicated power sockets\nShould not be on the ground floor (risk of flooding + less secure)\nFire suppression facilities + smoke alarms\nIP KVM for remote access\nTelephone (so the operator can call a support line while looking at the hardware)\nPens + paper\nA label printer - label everything!\nA standard printer (nice to have)\nSpare network and power cables\nAir conditioning (also dehumidifies)\nGood UPS (with automated/controlled shutdown functionality)\nSufficient power to run everything (and enough for expansion)\nEntrance security (preferably also with logging)\nPhysical security (security on windows, entrance, etc.)\nWhiteboard (nice to have)\nFireproof safe (for storing backup tapes, passwords and installation media)\nGood server racks - well maintained (cabling)\nEnough space to work comfortably behind the servers\nA table large enough to build/dismantle a server on (plus monitor, keyboard and mouse)\nAt least 1 chair\nTidy patch panel (especially if you patch to PC's and telephones in the office)\nGood lighting\n",
        "url": "https://serverfault.com/questions/331/the-perfect-server-room"
    },
    {
        "title": "Suspicious symbols on nginx config",
        "question": "I have the following nginx config, e.g.\nserver {\n        listen   80;\n        server_name example.com\n        allow 127.0.0.0/8;\n\nWhen I restart, it warn me:\nRestarting nginx: nginx: [warn] server name \"127.0.0.0/8\" has suspicious \nsymbols in /etc/nginx/sites-enabled/xxx\n\nAny idea?",
        "top_answer": "I guess you are missing the ; at the end of the server_name directive so it interprets  the allow line as part of the server name. \nserver {\n        listen   80;\n        server_name example.com;\n        allow 127.0.0.0/8;\n",
        "url": "https://serverfault.com/questions/447871/suspicious-symbols-on-nginx-config"
    },
    {
        "title": "How to delete domain user profile from a computer?",
        "question": "I'm wondering how to safely remove a domain user profile from a computer that is a part of a domain. I don't want to delete the account from the domain itself, I just need to remove the profile from this computer, to do some cleanup.\nI'm currently on a Vista Business computer, but we also have Win XP Pro and Win 7 Pro.",
        "top_answer": "Method 1 (easy and safe)\n\nOpen up \"Control Panel | System and Security | System\"\nIn the dialog click on \"Advanced system settings\" (requires Admin rights)\nThe \"System Properties\" dialog will be displayed\nMake sure you are in the \"Advanced\" register\nIn the \"User Profiles\" section click on \"Settings\"\nThe \"User Profiles\" dialog is displayed\nSelect the account. Hit Delete.\n\nMethod 2 (slight variation of method 1)\n\nStart | Run\nsysdm.cpl\nswitch to register \"Advanced\"\nIn the \"User Profiles\" section click on \"Settings\"\nThe \"User Profiles\" dialog is displayed\nSelect the account. Hit Delete.\n\nThe greyed out button possibly means that the registry hive has not been released by the operating system, as pointed out by @joeqwerty in the comments.\nMethod 3 (manual and prone to errors)\nDelete the C:\\Users\\[ACCOUNT] directory. That leaves some registry entries behind that have to be manually deleted as follows.\n\nOpen Regedit with Administrator Permissions (Runas Administrator)\nSelect the HKEY_USERS branch\nSearch for the Domain Account without the domain (e.g. login = DOMAIN\\ACCOUNT then search for ACCOUNT)\nKeep on searching until the status bar shows Computer\\HKEY_USERS\\[SID]\\Software\\Microsoft\\Windwos\\CurrentVersion\\Explorer\\Shell Folders\nThere should be a large list of your ACCOUNTs folders e.g. C:\\Users\\ACCOUNT\\Desktop\n\nYou are in the right HKEY_USERS\\[SID]\\Software\\Microsoft\\Windwos\\CurrentVersion\\Explorer\\Shell Folders\nbranch if the ACCOUNT in \"Shell Folders\" matches the ACCOUNT you just manually deleted form the C:\\Users\\[ACCOUNT] directory. This branch [SID] can be exported and/or deleted to clean up the last of the user profile.",
        "url": "https://serverfault.com/questions/450389/how-to-delete-domain-user-profile-from-a-computer"
    },
    {
        "title": "Can I run a cron job more frequently than every minute?",
        "question": "Is it possible to run a cron job every 30 seconds without a sleep command?",
        "top_answer": "If your task needs to run that frequently, cron is the wrong tool. Aside from the fact that it simply won't launch jobs that frequently, you also risk some serious problems if the job takes longer to run than the interval between launches. Rewrite your task to daemonize and run persistently, then launch it from cron if necessary (while making sure that it won't relaunch if it's already running).",
        "url": "https://serverfault.com/questions/49082/can-i-run-a-cron-job-more-frequently-than-every-minute"
    },
    {
        "title": "DNS failing to propagate worldwide",
        "question": "I haven't changed anything related to the DNS entry for serverfault.com, but some users were reporting today that the serverfault.com DNS fails to resolve for them.\nI ran a justping query and I can sort of confirm this -- serverfault.com dns appears to be failing to resolve in a handful of countries, for no particular reason that I can discern. (also confirmed via What's My DNS which does some worldwide pings in a similar fashion, so it's confirmed as an issue by two different sources.)\n\n\nWhy would this be happening, if I haven't touched the DNS for serverfault.com ?\n\nour registrar is (gag) GoDaddy, and I use default DNS settings for the most part without incident. Am I doing something wrong? Have the gods of DNS forsaken me?\n\nis there anything I can do to fix this? Any way to goose the DNS along, or force the DNS to propagate correctly worldwide?\n\n\nUpdate: as of Monday at 3:30 am PST, everything looks correct.. JustPing reports site is reachable from all locations. Thank you for the many very informative responses, I learned a lot and will refer to this Q the next time this happens..",
        "top_answer": "This is not directly a DNS problem, it's a network routing problem between some parts of the internet and the DNS servers for serverfault.com.  Since the nameservers can't be reached the domain stops resolving.\nAs far as I can tell the routing problem is on the (Global Crossing?) router with IP address 204.245.39.50.\nAs shown by @radius, packets to ns52 (as used by stackoverflow.com) pass from here to 208.109.115.121 and from there work correctly.  However packets to ns22 go instead to 208.109.115.201.\nSince those two addresses are both in the same /24 and the corresponding BGP announcement is also for a /24 this shouldn't happen.\nI've done traceroutes via my network which ultimately uses MFN Above.net instead of Global Crossing to get to GoDaddy and there's no sign of any routing trickery below the /24 level - both name servers have identical traceroutes from here.\nThe only times I've ever seen something like this it was broken Cisco Express Forwarding (CEF).  This is a hardware level cache used to accelerate packet routing.  Unfortunately just occasionally it gets out of sync with the real routing table, and tries to forward packets via the wrong interface.  CEF entries can go down to the /32 level even if the underlying routing table entry is for a /24.  It's tricky to find these sorts of problems, but once identified they're normally easy to fix.\nI've e-mailed GC and also tried to speak to them, but they won't create a ticket for non-customers.  If any of you are a customer of GC, please try and report this...\nUPDATE at 10:38 UTC  As Jeff has noted the problem has now cleared.  Traceroutes to both servers mentioned above now go via the 208.109.115.121 next hop.",
        "url": "https://serverfault.com/questions/42678/dns-failing-to-propagate-worldwide"
    },
    {
        "title": "How to convert 'dmesg' time format to 'real' time format",
        "question": "I have (for example) this log entry in dmesg output:\n[600711.395348] do_trap: 6 callbacks suppressed\n\nIs there a possibility to convert this 'dmesg' time to 'real' time to know, when this event happend?",
        "top_answer": "It looks as if it was implemented recently for Quantal (12.10) : see http://brainstorm.ubuntu.com/idea/17829/ .\nBasically, dmesg is reported to have a new switch -T, --ctime.\n\nEdit. As another extension on Ignacio's answer, here are some scripts to enhance dmesg output on older systems.\n( Note: for the python version of the code shown there, one will want to replace &lt; and &gt; back to <> to make it usable again. )\n\nFinally, for a single value like 600711.395348 one could do\nut=`cut -d' ' -f1 </proc/uptime` \nts=`date +%s` \ndate -d\"70-1-1 + $ts sec - $ut sec + $(date +%:::z) hour + 600711.395348 sec\" +\"%F %T\"\n\nand get the event date and time in the local time zone.\n( Please note that due to round-off errors the last second digit probably won't be accurate. ) .\nEdit(2): Please note that -- as per Womble's comment below, -- this will only work if the machine was not hibernated etc. ( In that case, one shall better look at syslog configs at /etc/*syslog* and check the appropriate files. See also: dmesg vs /var/messages . )",
        "url": "https://serverfault.com/questions/366392/how-to-convert-dmesg-time-format-to-real-time-format"
    },
    {
        "title": "How to manage DNS in NetworkManager via console (nmcli)?",
        "question": "I have CentOS 7.2 (guest in VirtualBox, vagrant box centos/7, no GUI).\nI see there is a nameserver in file:\n$ cat /etc/resolv.conf\n# Generated by NetworkManager\nnameserver 10.0.2.3\n\nBut how to add or replace with new one?\nI have done this manually directly in the network:\n$ vi /etc/sysconfig/network-scripts/ifcfg-eth0\nPEERDS=no\nDNS1=8.8.4.4\nDNS2=8.8.8.8\n\nAnd it works.\nBut is there any way to do this through nmcli?\nP.S. No nmtui installed (in a selected system).",
        "top_answer": "Here is the command to modify an existing connection.\nnmcli con mod \"$connectionName\" ipv4.dns \"8.8.8.8 8.8.4.4\"\n\nconnectionName can be found by command: nmcli con. In the question case, it will be \"System eth0\"\nIf you want to ignore automatically configured nameservers and search domains, ie the settings passed from DHCP.\nnmcli con mod \"$connectionName\" ipv4.ignore-auto-dns yes\n\nFinally, to enable the changes, bring the connection down then up:\nnmcli con down \"$connectionName\"\nnmcli con up \"$connectionName\"\n\nservice NetworkManager restart\n\nVerify with cat /etc/resolv.conf. You should not edit /etc/resolv.conf manually as it is generated by NetworkManager service, it is likely to get overridden at any given time.\nUseful nmcli manual",
        "url": "https://serverfault.com/questions/810636/how-to-manage-dns-in-networkmanager-via-console-nmcli"
    },
    {
        "title": "Apache 2 startup warning: NameVirtualHost *:80 has no VirtualHosts",
        "question": "When my Ubuntu Apache server (Apache 2) starts up I get a warning message that reads:\n\n[warn] NameVirtualHost *:80 has no VirtualHosts\n\nHowever, the web server is working fine. What might I have wrong in my site's configuration to make it give me this warning?\nThe configuration file in question (located in /etc/apache2/sites-available) reads like (details removed for brevity)\n<VirtualHost *>\n    <Location /mysite>\n        # Configuration details here...\n    </Location>\n\n    # Use the following for authorization.\n    <LocationMatch \"/mysite/login\">\n        AuthType Basic\n        AuthName \"My Site\"\n        AuthUserFile /etc/sitepasswords/passwd\n        Require valid-user\n    </LocationMatch>\n</VirtualHost>\n\nCould the fact that I'm using <Location> be a part of the problem?",
        "top_answer": "Change\n<VirtualHost *>\n\nto read\n<VirtualHost *:80>\n\nOr its (NameVirtualHost *:80) added twice in your apache2 Confing  file.  ( By Default its added in ports.conf file ) \nThis should clear the error.\nAside: you shouldn't ignore this error. Apache's config, especially when globbing virtual hosts (eg Include /etc/httpd/vhosts.d/*) is not stable. That means you don't control the order of loading the hosts explicitly so the default vhost for an IP becomes the one that is loaded first, which can lead to unintended consequences.\nOne example of this is the default vhost for an IP will also be available on that IP, rather than its name. This can cause information to leak onto google referring to your sites IP rather than name, which can be confusing for customers.\nThe NameVirtualHost error above can be a hint that apache has loaded things in a non optimal way, so you shouldn't ignore it.",
        "url": "https://serverfault.com/questions/1405/apache-2-startup-warning-namevirtualhost-80-has-no-virtualhosts"
    },
    {
        "title": "How to split a PEM file",
        "question": "Note : This is not really a question because I already found the answer but since I didn't find it easily here I will post it so that it can benefit others.\nQuestion : How to read a concatenated PEM file as the one used by apache/mod_ssl directive SSLCACertificateFile  ?\nAnswer (original) (source) : \ncat $file|awk 'split_after==1{n++;split_after=0} /-----END CERTIFICATE-----/ {split_after=1} {print > \"cert\" n \".pem\"}'\n\nThis can leave an empty file if there's a blank line at the end, such as with openssl pkcs7 -outform PEM -in my-chain-file -print_certs. To prevent that, check the length of the line before printing:\ncat $file|awk 'split_after==1{n++;split_after=0}\n   /-----END CERTIFICATE-----/ {split_after=1}\n   {if(length($0) > 0) print > \"cert\" n \".pem\"}' \n\nAnswer 29/03/2016 :\nFollowing @slugchewer answer, csplit might be a clearer option with :\ncsplit -f cert- $file '/-----BEGIN CERTIFICATE-----/' '{*}'\n",
        "top_answer": "The awk snippet works for extracting the different parts, but you still need to know which section is the key / cert / chain.\nI needed to extract a specific section, and found this on the OpenSSL mailinglist: http://openssl.6102.n7.nabble.com/Convert-pem-to-crt-and-key-files-tp47681p47697.html\n# Extract key\nopenssl pkey -in foo.pem -out foo-key.pem\n\n# Extract all the certs\nopenssl crl2pkcs7 -nocrl -certfile foo.pem |\n  openssl pkcs7 -print_certs -out foo-certs.pem\n\n# Extract the textually first cert as DER\nopenssl x509 -in foo.pem -outform DER -out first-cert.der\n",
        "url": "https://serverfault.com/questions/391396/how-to-split-a-pem-file"
    },
    {
        "title": "How to use nginx to proxy to a host requiring authentication?",
        "question": "How can I setup an nginx proxy_pass directive that will also include HTTP Basic authentication information sent to the proxy host?  \nThis is an example of the URL I need to proxy to:\nhttp://username:[email\u00a0protected]/export?uuid=1234567890\n\nThe end goal is to allow 1 server present files from another server (the one we're proxying to) without exposing the URI of the proxy server.  I have this working 90% correct now from following the Nginx config found here:\nhttp://kovyrin.net/2010/07/24/nginx-fu-x-accel-redirect-remote/\nI just need to add in the HTTP Basic authentication to send to the proxy server",
        "top_answer": "I did a writeup on this a while ago. See the details here:\nhttp://shairosenfeld.blogspot.com/2011/03/authorization-header-in-nginx-for.html\nFor example:\n location / {\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_pass http://6.6.6.6:80;\n    proxy_set_header Authorization \"Basic a2luZzppc25ha2Vk\";\n }\n\n\"a2luZzppc25ha2Vk\" is \"king:isnaked\" base64 encoded, so that would work for \nhttp://king:[email\u00a0protected]\nFeel free to check out blog post for more details.",
        "url": "https://serverfault.com/questions/230749/how-to-use-nginx-to-proxy-to-a-host-requiring-authentication"
    },
    {
        "title": "Correct way to move kvm vm",
        "question": "I'm wondering what is the correct way of moving a VM between two KVM hosts without using any kind of shared storage\nWould copying the disk files and the XML dump from the source KVM machine to the destination one suffice? If so, what commands need to be run to import the vm on the destination?\nOS is Ubuntu on both the Dom0's and DomU.\nThanks in advance",
        "top_answer": "\ncopy the VM's disks from /var/lib/libvirt/images on src host to the same dir on destination host\non the source host run virsh dumpxml VMNAME > domxml.xml and copy this xml to the destination host\non the destination host run virsh define domxml.xml\n\nstart the VM.\n\nIf the disk location differs, you need to edit the xml's devices/disk node to point to the image on the destination host\nIf the VM is attached to custom defined networks, you'll need to either edit them out of the xml on the destination host or redefine them as well \n\nOn source machine virsh net-dumpxml NETNAME > netxml.xml\ncopy netxml.xml to target machine\nOn target machine virsh net-define netxml.xml && virsh net-start NETNAME & virsh net-autostart NETNAME)\n\n",
        "url": "https://serverfault.com/questions/434064/correct-way-to-move-kvm-vm"
    },
    {
        "title": "Can rsync display current average speed?",
        "question": "When I'm transferring large quantities of data using rsync, it would be helpful if I could have the average speed up until now at a glance, rather than a bunch of different speeds for each file.",
        "top_answer": "Yes. Starting with rsync version 3.1.0 the --info=progress2 argument will give you progress on the entire transfer, including speed of the entire transfer. You can see a little bit of detail on the rsync man page.",
        "url": "https://serverfault.com/questions/607689/can-rsync-display-current-average-speed"
    },
    {
        "title": "Optimal value for Nginx worker_connections",
        "question": "Nginx worker_connections \"sets the maximum number of simultaneous connections that can be opened by a worker process. This number includes all connections (e.g. connections with proxied servers, among others), not only connections with clients. Another consideration is that the actual number of simultaneous connections cannot exceed the current limit on the maximum number of open files\". I have few queries around this:\n\nWhat should be the optimal or recommended value for this?\nWhat are the downsides of using a high number of worker connections?\n",
        "top_answer": "Let's take the pragmatic approach.\nAll these limits are things that were hardcoded and designed in the past century when hardware was slow and expensive. We're in 2016 now, an average wall-mart toaster can process more requests than the default values. \nThe default settings are actually dangerous. Having hundreds of users on a website is nothing impressive. \nworker_process\nA related setting, let's explain it while we're on the topic.\nnginx as load balancer:\n\n1 worker for HTTP load balancing.\n1 worker per core for HTTPS load balancing. \n\nnginx as webservers:\nThis one is tricky.\nSome applications/frameworks/middleware (e.g. php-fpm) are run outside of nginx. In that case, 1 nginx worker is enough because it's usually the external application that is doing the heavy processing and eating the resources.\nAlso, some applications/frameworks/middleware can only process one request at a time and it is backfiring to overload them.\nGenerally speaking, 1 worker is always a safe bet.\nOtherwise, you may put one worker per core if you know what you're doing. I'd consider that route to be an optimization and advise proper benchmarking and testing.\nworker_connections\nThe total amount of connections is worker_process * worker_connections. Half in load balancer mode.\nNow we're reaching the toaster part. There are many seriously underrated system limits:\n\nulimits is 1k max open files per process on linux (1k soft, 4k hard on some distro)\nsystemd limits is about the same as ulimits.\nnginx default is 512 connections per worker.\nThere might be more: SELinux, sysctl, supervisord (each distro+version is slightly different)\n\n1k worker_connections\nThe safe default is to put 1k everywhere. \nIt's high enough to be more than most internal and unknown sites will ever encounter. It's low enough to not hit any other system limits.\n10k worker_connections\nIt's very common to have thousands of clients, especially for a public website. I stopped counting the amount of websites I've seen went down because of the low defaults.\nThe minimum acceptable for production is 10k. Related system limits must be increased to allow it.\nThere is no such thing as a too-high limit (a limit simply has no effect if there are no users). However a too-low limit is a very real thing that results in rejected users and a dead site.\nMore than 10k\n10k is nice and easy. \nWe could set an arbitrary 1000kk limits (it's only a limit after all) but that doesn't make much practical sense, we never get that traffic and couldn't take it anyway.\nLet's stick to 10k as a reasonable setting. The services which are going for (and can really do) more will require special tuning and benchmarking.\nSpecial Scenario: Advanced Usage\nSometimes, we know that the server doesn't have much resources and we expect spikes that we can't do much about. We'd rather refuse users than try. In that case, put a reasonable connection limit and configure nice error messages and handling.\nSometimes, the backend servers are working good and well but only up to some load, anything more and everything goes south quickly. We'd rather slow down than have the servers crash. In that case, configure queuing with strict limits, let nginx buffer all the heat while requests are being drained at a capped pace.",
        "url": "https://serverfault.com/questions/787919/optimal-value-for-nginx-worker-connections"
    },
    {
        "title": "What permissions are needed to write a PID file in /var/run?",
        "question": "On Ubuntu:\ntouch: cannot touch `/var/run/test.pid': Permission denied\n\nI am starting start-stop-daemon and like to write the PID file in /var/run\nstart-stop-daemon is run as my-program-user\n/var/run setting is drwxr-xr-x  9 root  root\n\nI like to avoid putting my-program-user in the root group.",
        "top_answer": "By default, you can only write to /var/run as a user with an effective user ID of 0 (ie as root).  This is for good reasons, so whatever you do, don't go and change the permissions of /var/run... Instead, as root, create a directory under /var/run:\n# mkdir /var/run/mydaemon\nThen change its ownership to the user/group under which you wish to run your process:\n# chown myuser:myuser /var/run/mydaemon\nNow specify to use /var/run/mydaemon rather than /var/run.\nYou can always test this by running a test as the user in question.",
        "url": "https://serverfault.com/questions/159334/what-permissions-are-needed-to-write-a-pid-file-in-var-run"
    },
    {
        "title": "How should I setup separate MX records for a subdomain?",
        "question": "Let's say I have a domain that I run a web application on, for example cranketywidgets.com, and I'm using Google Apps for handling email for people working on that domain, for example, support@ cranketywidgets.com, [email\u00a0protected], [email\u00a0protected] and so on.\nGoogle's own mail services aren't always the best for sending automated reminder emails, comment notifications and so on, so the current solution I plan to pursue is to create a separate subdomain called mailer.cranketywidgets.com, run a mail server off it, and create a few accounts specifically for sending these kinds of emails.\nWhat should the MX records and A records look like here for this?\nI'm somewhat confused by the fact that MX records can be names, but that they must eventually resolve to an A record. What should the records look like here?\ncranketywidgets.com - A record to actual server like 10.24.233.214\ncranketywidgets.com - MX records for Google's email applications\nmailer.cranketywidgets.com - MX name pointing to server's IP address\nI would greatly appeciate some help on this - the answer seems like it'll be obvious, but email spam is a difficult problem to solve.",
        "top_answer": "You should never point your MX to a IP address to be RFC compliant. Make an A record for the IP address instead and point the MX record to it.\nThen the zone should look like this,\n    @               IN      MX      1       ASPMX.L.GOOGLE.COM.\n    @               IN      MX      5       ALT1.ASPMX.L.GOOGLE.COM.\n    @               IN      MX      5       ALT2.ASPMX.L.GOOGLE.COM.\n    @               IN      MX      10      ASPMX2.GOOGLEMAIL.COM.\n    @               IN      MX      10      ASPMX3.GOOGLEMAIL.COM.\n    @               IN      MX      10      ASPMX4.GOOGLEMAIL.COM.\n    @               IN      MX      10      ASPMX5.GOOGLEMAIL.COM.\n    @               IN      A       10.24.233.214\n    mailer          IN      A       10.24.233.214\n    mailer          IN      MX      10      mailer.cranketywidgets.com.\n",
        "url": "https://serverfault.com/questions/54800/how-should-i-setup-separate-mx-records-for-a-subdomain"
    },
    {
        "title": "How to: 1 Cron Job every ODD minutes and 1 other every EVEN minutes?",
        "question": "I have 2 cron jobs, i want one of them to run every odd minute (1,3,5,7,9,11....57,59)\nand i want the other one to run every even minute (0,2,4,6,8,10,12...58)\nhow can i do it in an easy way? (no scripting - just cron job rules)",
        "top_answer": "*/2 * * * * date >>/tmp/even\n1-59/2 * * * * date >>/tmp/odd\n",
        "url": "https://serverfault.com/questions/220006/how-to-1-cron-job-every-odd-minutes-and-1-other-every-even-minutes"
    },
    {
        "title": "What steps do you take to secure a Debian server? [closed]",
        "question": "I am installing a Debian server which is connected directly to the Internet. Obviously I want to make it as secure as possible. I would like you guys/gals to add your ideas to secure it and what programs you use for it.\nI want part of this question to cover what do you use as a firewall? Just iptables manually configured or do you use some kind of software to aid you? What's the best way? Block everything and allow only what is needed? Are there maybe good tutorials for beginners to this topic?\nDo you change your SSH port? Do you use software like Fail2Ban to prevent bruteforce attacks?",
        "top_answer": "Obligatory:\n\ninstallation of system with expert mode, only packages that I need\nhand written firewall with default policy on iptables'input: drop, permitting access to SSH, HTTP or whatever else given server is running\nFail2Ban for SSH [ and sometimes FTP / HTTP / other - depending on context ]\ndisable root logins, force using normal user and sudo\ncustom kernel [ just old habit ]\nscheduled system upgrade\n\nDepending on level of paranoia additionally:\n\ndrop policy on output except a couple of allowed destinations / ports\nintegrit for checking if some parts of file system ware not modified [with checksum kept outside of the machine], for example Tripwire \nscheduled scan at least with nmap of system from the outside\nautomated log checking for unknown patterns [but that's mostly to detect hardware malfunction or some minor crashes]\nscheduled run of chkrootkit\nimmutable attribute for /etc/passwd so adding new users is slightly more difficult\n/tmp mounted with noexec\nport knocker or other non-standard way of opening SSH ports [e.g. visiting 'secret' web page on web server allows incoming SSH connection for a limited period of time from an IP address that viewed the page. If you get connected, -m state --satete ESTABLISHED takes care of allowing packet flow as long as you use a single SSH session]\n\nThings I do not do myself but make sense:\n\ngrsecurity for kernel\nremote syslog so logs cannot be overwritten when system gets compromised\nalerting about any SSH logins\nconfigure rkhunter and set it up to run from time to time\n",
        "url": "https://serverfault.com/questions/11659/what-steps-do-you-take-to-secure-a-debian-server"
    },
    {
        "title": "nginx set variable in location",
        "question": "I am trying to optimize my nginx configs, so it would be possible to set one variable, and all location paths would update automatically. I have four lines in question:\nserver_name php.domain.com;\nroot /srv/web/vhosts/php/web;\nerror_log /srv/web/vhosts/php/logs/error.log;\naccess_log /srv/web/vhosts/php/logs/access.log;\n\nWhat I would like to achieve is to set one variable (in this case 'php') and include it to config.\nset $variable \"php\";\nserver_name $variable.domain.com;\nroot /srv/web/vhosts/$variable/web;\nerror_log /srv/web/vhosts/$variable/logs/error.log;\naccess_log /srv/web/vhosts/$variable/logs/access.log;\n\nHowever it seams that nginx ignores variables in this config. Am I doing something wrong or it is not possible to use variable in location paths?",
        "top_answer": "Variables can't be declared anywhere nor be used in any directive.\nAs the documentation of set directive is :\n\nSyntax:   set $variable value;\nDefault:  \u2014\nContext:  server, location, if\n\n\nThe immediate consequence is that you can't use custom variables in an http block.\nUpdate : after a discussion and experiments with AlexeyTen in this chatroom.\n\naccess_log can contain variables with restrictions. Among them, the lack of buffering and the fact that the leading slash must not be declared in a variable.\nerror_log won't work with variables at all.\nroot directive can contains variables.\nserver_name directive only allows strict $hostname value as a variable-like notation.\n",
        "url": "https://serverfault.com/questions/644892/nginx-set-variable-in-location"
    },
    {
        "title": "Why do ethernet cables have 8 wires?",
        "question": "This might seem a stupid question but why do Ethernet cables have 8 wires? Cat5 cables were just using 4 of the 8 wires, so only 4 are actualy 'needed'. Why not 12 or 16 wires?",
        "top_answer": "This is an interesting question since I've never seen anything that authoritatively states the design decisions behind that choice. Everything that I've come across, whether on the Interwebs or from conversation with people smarter than me in this area, seem to indicate two possibilities:\n\nFuture proofing\nExtra shielding\n\nFuture Proofing\nBy the time of the Cat5 spec we had seen the explosion of data cable runs. Telephone had been using Cat3, or something similar for some time, serial connections had been run throughout University campuses, ThickNet had spidered its way around, ThinNet had started to see significant use in microcomputer labs and in some cases offices. It was obvious that networking computing equipment was the wave of the future. We had also learned the terrible costs of changing out cabling to meet the demands of longer segments or higher speeds. Let's face it, replacing cabling is a nightmarish chore and expensive.\nThe notion of limiting this cost by developing a cable that could be run, and left in place for some length of time, was definitely an appealing one. So forward thinking engineers, who were probably tired of replacing wiring, could easily have found it worthwhile to design extra pairs into the spec. After all, especially at a time when the price of bulk copper was relatively low. Which is more expensive - adding 4 extra wires or having a team of people remove old wiring and add new?\nExtra Shielding\nSince typical Cat5 is UTP (unshielded twisted pair) it does not contain the extra grounded foil to slough off the extraneous electro-magnetic interference. It has been described to me that, when properly grounded, the unused wires will help buffer the in-use pairs in a similar, albeit less effective way, than actual shielding. This could have been an important feature in the long runs and (electrically) noisy environments we were accustomed to running cabling at the time.\nTo me the future proofing argument is the most compelling.",
        "url": "https://serverfault.com/questions/449416/why-do-ethernet-cables-have-8-wires"
    },
    {
        "title": "How do you document a network?",
        "question": "I'm not sure how to ask this question, since I'm not in the field. Say you're a network admin and you leave your job. How does the new guy know where to start?",
        "top_answer": "It depends on the size of the network, number of users, number of nodes (computers, servers, printers, etc.) and the size of your IT staff, among other things.\nIt also depends on your goal.  Are you documenting the network for training and maintenance purposes, insurance/loss prevention, etc?\nPersonally, I document my networks in such a way that I know I can derive any missing information based on what is documented.  From a practical stance, there is a point of diminishing returns when your documentation gets too granular.\nA good rule of thumb I use is that there should be documentation in a known location that is thorough enough that if I get hit by a bus tonight, another administrator can keep the core network running while he/she fills in the missing pieces over the next few days/weeks.\nHere is an overview of what I think is most important about one of my networks.  For the record this is a Windows-only shop with about 100 users and 5 offices.\n\nAdministrator credentials for all servers.  Obviously this should be kept secure.\nIP Addresses and NetBIOS names for any node on the network with a static IP address, including servers, workstations, printers, firewalls, routers, switches, etc.\nBasic server hardware information, such as Service tags or equivalent, total disk capacity, total RAM, etc.\nMajor roles of each server, such as Domain Controller, File Server, Print Server, Terminal Server, etc.\nLocation of backup tapes/drives.\nInformation about the account numbers and credentials for services like remote office voice and data providers.\nExternal DNS for websites and routing.\n\nIf there was anything strange about a setup or workflow that would not be immediately obvious to a new administrator, I would write a short \"brief\" about it as well.",
        "url": "https://serverfault.com/questions/12378/how-do-you-document-a-network"
    },
    {
        "title": "How can I configure nginx locations to share common configuration options?",
        "question": "How can i configure a shared config block for a set of locations?\n    location / {\n\n            proxy_pass        http://127.0.0.1:9000/;\n            proxy_redirect    off;\n            proxy_set_header  Host             $http_host;\n            proxy_set_header  X-Real-IP        $remote_addr;\n            proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;\n\n            proxy_cache cache-test;\n            proxy_cache_valid 200 302 24h;\n            proxy_cache_valid 404 60s;\n            add_header X-Cache-Status $upstream_cache_status;\n\n    }\n\n\n    location /api/0.1/user{\n            proxy_cache_key /user/$http_authorization;\n    }\n\nNow if i try to access /api/0.1/user then i will get 404 because it doesn\u00b4t pass the request to 127.0.0.1:9000",
        "top_answer": "Create a common proxy config and include as-needed.\n/etc/nginx/api_proxy.conf\nproxy_pass        http://127.0.0.1:9000/;\nproxy_redirect    off;\nproxy_set_header  Host             $http_host;\nproxy_set_header  X-Real-IP        $remote_addr;\nproxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;\n\nproxy_cache cache-test;\nproxy_cache_valid 200 302 24h;\nproxy_cache_valid 404 60s;\nadd_header X-Cache-Status $upstream_cache_status;\n\nYour Host Config File\n...\nlocation /api/0.1/user {\n    include /etc/nginx/api_proxy.conf;\n    proxy_cache_key /user/$http_authorization;\n}\n...\n",
        "url": "https://serverfault.com/questions/373578/how-can-i-configure-nginx-locations-to-share-common-configuration-options"
    },
    {
        "title": "git fetch specific revision from remote repository",
        "question": "We have a remote git repo that we normally deploy from using git push on our dev server then git pull on on our live servers to get the latest pushed version of the repo.\nBut if we have committed and pushed a few revisions (without a git pull on the live servers) how can we do a git pull that is referring to the older commit that we want?\ni.e. something like git pull -r 3ef0dedda699f56dc1062b5dcc2c59f7ad93ede4",
        "top_answer": "Once you've pulled the repository you should be able to go:\ngit checkout 3ef0d...\n",
        "url": "https://serverfault.com/questions/117255/git-fetch-specific-revision-from-remote-repository"
    },
    {
        "title": "What's the best way to move c:\\users to d:\\users under vista/W7 [closed]",
        "question": "I just installed Windows 7 RC1 and want to move c:\\users to d:\\users.\nWhat's the best way to do this?\n\nDue to the fact that Windows 7 creates a reserved partition that is mounted as C:\nin the recovery console, I had to use the following commands\nrobocopy /mir /xj D:\\Users E:\\Users\nmklink D:\\Users D:\\Users /j\n\nBoth D's in the mklink command are correct.\nWhen the system reboots, the drive that was D in the recovery console becomes the C drive.",
        "top_answer": "You can move the entire C:\\Users folder to a different drive pretty easily after windows is installed:\nWarning: Doing this may cause issues if/when you need to perform a System Restore\n\nBoot to the installation media, and get to the command prompt (press Shift + F10 on the install dialog)\nUse Robocopy to copy C:\\Users to D:\\Users: robocopy c:\\Users d:\\Users /mir /xj /copyall\na. /mir tells robocopy to mirror the directories, this will copy all files\nb. /xj is very important, this tells robocopy not to follow junction points. If you forget this, you will have a lot of trouble.\nc. /copyall will copy all the attributes includings ACL and Owner info\nVerify that the files successfully copied\nDelete c:\\Users\nCreate junction that points to d:\\Users: mklink c:\\Users d:\\Users /j\n\nThat's it. I've been using this process since Vista went RTM with no problems.\nHere is an article that explains it as well. Just use robocopy instead of xcopy as he does in the article to avoid possible ntfs permissions problems.\nUpdate: Because I found out the hard way, I thought I'd also mention that if you are planning on moving \"Program Data\", or \"Program Files\" with this method, you will be disapointed to find out that everything works as expected, but windows updates will no longer install. I'm not sure if this has been fixed Win 7.\nUpdate 2: @Benjol has a blog post that details a method of moving the profiles folder that will recreate the junctions that this method leaves out. If you run into any issues with legacy apps, take a look here and see if his method resolves the issue.",
        "url": "https://serverfault.com/questions/8187/whats-the-best-way-to-move-c-users-to-d-users-under-vista-w7"
    },
    {
        "title": "Server Aliases on NGINX",
        "question": "I'm moving a site to a new server running on NGINX.  The old site's Apache2 VirtualHost has ServerAlias configured that I want to replicate in the NGINX configuration.\nFrom what I've read on the NGINX Docs this seems to be simply achieved with server_name.\nI just want to make sure what I have is correct.\nFrom APACHE2 \nServerAlias our-domain.com www1.our-domain.com our-domain.ie\n\nTo NGINX\nserver_name www.our-domain.com our-domain.com www.our-domain.ie our-domain.ie;\n",
        "top_answer": "Yes, just using server_name with serveral names is perfectly correct.  The first name will be used as canonical one (similar to Apache ServerName), and others as aliases (similar to Apache ServerAlias).",
        "url": "https://serverfault.com/questions/718801/server-aliases-on-nginx"
    },
    {
        "title": "What are the drawbacks of running a database inside a virtual machine?  How do I overcome them? [closed]",
        "question": "Running anything inside a virtual machine will have some level of performance hit, but how much does it really impact the performance of a database system?\nI found this academic reference paper with some interesting benchmarks, but it was a limited test using Xen and PostgreSQL only.  The conclusion was that using a VM does \"not come at a high cost in performance\" (although you may think the actual data says otherwise).\nWhat are the technical, administrative, and other drawbacks associated with running a database within a virtual machine?\nPlease post answers that can be backed up by objective facts, I'm not interested in speculation or any other semi-religious argument (geek passion is good in many ways, but that won't help us here).\nThat being said, \n\n\nWhat issues show up when running database in a Virtual Machine?  (please post references)\nAre those issues significant?  \n  \n  \nAre they only significant under certain scenarios?   \n\nWhat are the workarounds?\n\n",
        "top_answer": "Though many DB vendors were very slow to do this, nearly all of them now officially support their software running in a virtualized environment.\nWe run many Oracle 11g instances in linux on top of ESXi, and it is certainly possible to get very good performance. As with all hardware scaling, you just need to make sure that the virtualization host has plenty of resources (RAM, CPU), and that your disk layer is up to the task of delivering whatever IO performance you require.",
        "url": "https://serverfault.com/questions/307235/what-are-the-drawbacks-of-running-a-database-inside-a-virtual-machine-how-do-i"
    },
    {
        "title": "Why are my XFS filesystems suddenly consuming more space and full of sparse files?",
        "question": "I've run XFS filesystems as data/growth partitions for nearly 10 years across various Linux servers. \nI've noticed a strange phenomenon with recent CentOS/RHEL servers running version 6.2+. \nStable filesystem usage became highly variable following the move to the newer OS revision from EL6.0 and EL6.1. Systems initially installed with EL6.2+ exhibit the same behavior; showing wild swings in disk utilization on the XFS partitions (See the blue line in the graph below).\nBefore and after. The upgrade from 6.1 to 6.2 occurred on Saturday.\n\nThe past quarter's disk usage graph of the same system, showing the fluctuations over the last week.\n\nI started to check the filesystems for large files and runaway processes (log files, maybe?). I discovered that my largest files were reporting different values from du and ls. Running du with and without the --apparent-size switch illustrates the difference.\n# du -skh SOD0005.TXT\n29G     SOD0005.TXT\n\n# du -skh --apparent-size SOD0005.TXT\n21G     SOD0005.TXT\n\nA quick check using the ncdu utility across the entire filesystem yielded:\nTotal disk usage: 436.8GiB  Apparent size: 365.2GiB  Items: 863258\n\nThe filesystem is full of sparse files, with nearly 70GB of lost space compared to the previous version of the OS/kernel!\nI pored through the Red Hat Bugzilla and change logs to see if there were any reports of the same behavior or new announcements regarding XFS. \nNada.\nI went from kernel version 2.6.32-131.17.1.el6 to 2.6.32-220.23.1.el6 during the upgrade; no change in minor version number.\nI checked file fragmentation with the filefrag tool. Some of the biggest files on the XFS partition had thousands of extents. Running on online defrag with xfs_fsr -v during a slow period of activity helped reduce disk usage temporarily (See Wednesday in the first graph above). However, usage ballooned as soon as heavy system activity resumed.\nWhat is happening here?",
        "top_answer": "I traced this issue back to a discussion about a commit to the XFS source tree from December 2010. The patch was introduced in Kernel 2.6.38 (and obviously, later backported into some popular Linux distribution kernels). \nThe observed fluctuations in disk usage are a result of a new feature; XFS Dynamic Speculative EOF Preallocation.\nThis is a move to reduce file fragmentation during streaming writes by speculatively allocating space as file sizes increase. The amount of space preallocated per file is dynamic and is primarily a function of the free space available on the filesystem (to preclude running out of space entirely).\nIt follows this schedule:\nfreespace       max prealloc size\n  >5%             full extent (8GB)\n  4-5%             2GB (8GB >> 2)\n  3-4%             1GB (8GB >> 3)\n  2-3%           512MB (8GB >> 4)\n  1-2%           256MB (8GB >> 5)\n  <1%            128MB (8GB >> 6)\n\nThis is an interesting addition to the filesystem as it may help with some of the massively fragmented files I deal with. \nThe additional space can be reclaimed temporarily by freeing the pagecache, dentries and inodes with:\nsync; echo 3 > /proc/sys/vm/drop_caches\n\nThe feature can be disabled entirely by defining an allocsize value during the filesystem mount. The default for XFS is allocsize=64k. \nThe impact of this change will probably be felt by monitoring/thresholding systems (which is how I caught it), but has also affected database systems and could cause unpredictable or undesired results for thin-provisioned virtual machines and storage arrays (they'll use more space than you expect). \nAll in all, it caught me off-guard because there was no clear announcement of the filesystem change at the distribution level or even in monitoring the XFS mailing list.\n\nEdit: Performance on XFS volumes with this feature is drastically improved. I'm seeing consistent < 1% fragmentation on volumes that previously displayed up to 50% fragmentation. Write performance is up globally!\nStats from the same dataset, comparing legacy XFS to the version in EL6.3.\nOld:\n# xfs_db -r -c frag /dev/cciss/c0d0p9\nactual 1874760, ideal 1256876, fragmentation factor 32.96%\n\nNew:\n# xfs_db -r -c frag /dev/sdb1\nactual 1201423, ideal 1190967, fragmentation factor 0.87%\n",
        "url": "https://serverfault.com/questions/406069/why-are-my-xfs-filesystems-suddenly-consuming-more-space-and-full-of-sparse-file"
    },
    {
        "title": "What can cause slow ssh session? [closed]",
        "question": "I ssh on remote host but terminal performance is poor. Symbols I am typing are not shown immediately, but with some delay. Sometimes two symbols are shown at one time after delay.",
        "top_answer": "High latency is another cause of poor ssh performance. I highly recommend using mtr as a better replacement for traceroute. It should be able to give you some idea of where your network problems might occur.",
        "url": "https://serverfault.com/questions/35453/what-can-cause-slow-ssh-session"
    },
    {
        "title": "Subversion error: (405 Method Not Allowed) in response to MKCOL",
        "question": "I am getting the following error while trying to commit a new directory addition.\nsvn: Commit failed (details follow):\nsvn: Server sent unexpected return value (405 Method Not Allowed) in response to MKCOL request for '....\n\nI have never seen this error before. How can I fix this problem?\nSolution\nI managed to solve the problem:\n\nDelete the parent's directory of the folder giving the problem. \nDo SVN Update.\nA folder with the same name as the new one already existed in repository.\nDelete this folder.\nSVN commit.\nCopy the new folder, schedule for addition and SVN commit.\n",
        "top_answer": "This error usually indicated the folder you're trying to add already exists in the repository - have you done an update recently, or did you do a non-recursive checkout to start with?\nI've previously encountered this error - found a blog that told me what the problem was. Solution was a little more complex, remove the directory in question from my working copy (copying it somewhere else), run a recursive update, then merge the two directories together manually.\nEdit: Opps - already answered in question body. And here I was all excited that I knew the answer to an unanswered question :)",
        "url": "https://serverfault.com/questions/11996/subversion-error-405-method-not-allowed-in-response-to-mkcol"
    },
    {
        "title": "What is the single most influential book every sysadmin should read?",
        "question": "For software developers, there are some books you must absolutely read.\nWhat is the single most influential book every programmer should read?\nHow about for sysadmins? Is there a similar list of books?",
        "top_answer": "The only essential I have is The Practice of System and Network Administration by Limoncelli, Hogan, et al.  My first edition copy lives on my desk\n",
        "url": "https://serverfault.com/questions/1046/what-is-the-single-most-influential-book-every-sysadmin-should-read"
    },
    {
        "title": "Server wiped after \"yum remove python\" [closed]",
        "question": "A disaster just occurred to me after I ran the command yum remove python and now I can't boot the server up anymore.\nHow it happened: I tried updating some apps via yum on my CentOS 5 VPS and the command was failing due to some weird python 2.4 error. I noticed that my version of python was old and I tried reinstalling it by first removing it, and so I did yum remove python.\nAfter that it asked me something about removing dependencies and it looked like nothing I could miss so I clicked Y.\nSo the aftermath of that was that I was unable to run any command what so ever. I even tried cd /var/www but it said something like \"command does not exist in /usr/bin\". When I used tab to see folder navigation suggestions, the file structure still seemed to be there (at least the /var/www bit which is really important to me).\nAfter that I tried restarting the vps (from the admin panel since reboot command did not work) and now it doesn't boot anymore.\nNow my question is: how can a command like that possibly destroy my server like this?",
        "top_answer": "I'm sincerely sorry: I can feel the pain to have a server unbootable/unserviceable.\nHowever, I'm lost when reading that:\n\nAfter that it asked me something about removing dependencies and it\n  looked like nothing I could miss so I clicked [Y]\n\nThe list of to-be-removed packages surely was really huge, as python is an essential part of RHEL/CentOS. You should never confirm some warning message you don't really understand.\nThe best thing you can do, as already suggested, is to boot via a recovery media (ie: livecd), extract the required data files, and reinstall your machine with a newer CentOS release (and as CentOS 6 is quite old, I strongly suggest you to rebase on CentOS 7).",
        "url": "https://serverfault.com/questions/844677/server-wiped-after-yum-remove-python"
    },
    {
        "title": "Is it possible to on-line shrink a EXT4 volume with LVM?",
        "question": "Today I tried this on my machine with OpenSUSE 12.3 (kernel 3.7):\n# resize2fs /dev/mapper/system-srv 2G\nresize2fs 1.42.6 (21-Sep-2012)\nFilesystem at /dev/mapper/system-srv is mounted on /srv; on-line resizing required\nresize2fs: On-line shrinking not supported\n\n/dev/mapper/system-srv is an EXT4 volume.\nIs it really unsupported or I am missing something?",
        "top_answer": "As the message said, you can only grow an ext4 filesystem on-line. If you want to shrink it, you will need to unmount it first.\nAccording to the ext4 filesystem maintainer, Ted Ts'o:\n\nSorry, on-line shrinking is not supported.\n",
        "url": "https://serverfault.com/questions/528075/is-it-possible-to-on-line-shrink-a-ext4-volume-with-lvm"
    },
    {
        "title": "How to install libpq-dev on Centos 5.5",
        "question": "In order to develop one web application based on postgresql, i need to install libpq on my centos. \nI can install it by \"apt-get install libpq-dev\" on ubuntu, but i can not install it on centos by \"yum install libpq\".\nWho can tell me how to install it, thanks!",
        "top_answer": "The package is called postgresql-libs on Red Hat and derived distributions.",
        "url": "https://serverfault.com/questions/316703/how-to-install-libpq-dev-on-centos-5-5"
    },
    {
        "title": "How do you wait for an exe to complete in batch file?",
        "question": "I'm trying to create a script to execute an exe on shutdown in order to install sp1. my script goes something like (not actual bat script).\nIf installed GOTO END\n    Install.exe\nEND:\n\nMy problem is that when it runs, it starts the installer, then finishes the script because the installer's a different process and follows up by shutting down the install process because the computer's shutting down and shutting down the system (at least, that's what i think it's doing.)\nIs there any way to tell it to wait until the process it started completes and then shutdown?",
        "top_answer": "Try running\nSTART /WAIT Install.exe\n",
        "url": "https://serverfault.com/questions/245393/how-do-you-wait-for-an-exe-to-complete-in-batch-file"
    },
    {
        "title": "dd on entire disk, but do not want empty portion",
        "question": "I have a disk, say /dev/sda. \nHere is fdisk -l:\n Disk /dev/sda: 64.0 GB, 64023257088 bytes\n255 heads, 63 sectors/track, 7783 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x0000e4b5\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *           1          27      209920   83  Linux\nPartition 1 does not end on cylinder boundary.\n/dev/sda2              27         525     4000768    5  Extended\nPartition 2 does not end on cylinder boundary.\n/dev/sda5              27         353     2621440   83  Linux\n/dev/sda6             353         405      416768   83  Linux\n/dev/sda7             405         490      675840   83  Linux\n/dev/sda8             490         525      282624   83  Linux\n\nI need to make an image to store on our file server for use in flashing other devices we are manufacturing so I only want the used space (only about 4gb). I want to keep the mbr etc... as this device should be boot ready as soon as the copy is finished. \nAny ideas? I previously had been using dd if=/dev/sda of=[//fileserver/file], but at that time, my master copy was on a 4gb flash ide.",
        "top_answer": "Back in the day I ran into a similar problem with embedded Linux distributions - get rid of all the junk before compressing the image.\ndd if=/dev/zero of=asdf.txt. Wait until it dies. Delete asdf.txt.\nYou've just written zeros to all free space on the device.\nNow take a disk image and run it through gzip.  Voila, sparse image.\nProbably doesn't scale very well and could cause problems if you actually need to write to the disk, but hey.\nYou could take an rsync snapshot of the disk to another volume, zero that, and then take that disk image.\nNote: Could be hazardous for SSD, user should consider this operation befor committing.",
        "url": "https://serverfault.com/questions/439128/dd-on-entire-disk-but-do-not-want-empty-portion"
    },
    {
        "title": "What's the best way to see logged in users in Windows Server 2012?",
        "question": "Right now I use a powershell script to see the currently logged in users. But I don't see if their session is idle, active or inactive. I can see when the session was started, that's it.\nIs there an easy way to see how many users are currently logged in to the server I am logged in and see their status? It should not be remotely executed. \nI would like to avoid third party tools if possible.",
        "top_answer": "Use the query user command  \nQuery User Command\nhttp://technet.microsoft.com/en-us/library/bb490801.aspx",
        "url": "https://serverfault.com/questions/598554/whats-the-best-way-to-see-logged-in-users-in-windows-server-2012"
    },
    {
        "title": "A better Unix 'find' with parallel processing",
        "question": "The Unix find(1) utility is very useful, allowing me to perform an action on many files that match certain specifications, e.g.,\nfind /dump -type f -name '*.xml' -exec java -jar ProcessFile.jar {} \\;\n\nThe above might run a script or tool over every XML file in a particular directory.\nLet's say my script/program takes a lot of CPU time and I have 8 processors. It would be nice to process up to 8 files at a time.\nGNU Make allows for parallel job processing with the -j flag, but find does not appear to have such functionality. Is there an alternative generic job-scheduling method of approaching this?",
        "top_answer": "xargs with the -P option (number of processes). Say I wanted to compress all the logfiles in a directory on a 4-cpu machine:\nfind . -name '*.log' -mtime +3 -print0 | xargs -0 -P 4 bzip2\n\nYou can also say -n <number> for the maximum number of work-units per process. So say I had 2500 files and I said:\nfind . -name '*.log' -mtime +3 -print0 | xargs -0 -n 500 -P 4 bzip2\n\nThis would start 4 bzip2 processes, each of which with 500 files, and then when the first one finished another would be started for the last 500 files. \nNot sure why the previous answer uses xargs and make, you have two parallel engines there!",
        "url": "https://serverfault.com/questions/193319/a-better-unix-find-with-parallel-processing"
    },
    {
        "title": "Does SpinRite do what it claims to do? [closed]",
        "question": "I don't have any real (i.e. professional) experience with Steve Gibson's SpinRite so I'd like to put this to the SF community. Does SpinRite actually do what it claims? Is it a good product to use?  With a proper backup solution and RAID fault tolerance, I've never found need for it, but I'm curious.\nThere seems to be some conflicting messages regarding it, and no hard data to be found either way.\nOn one hand, I've heard many home users claim it helped them, but I've heard home users say a lot of things -- most of the time they don't have the knowledge or experience to accurately describe what really happened. On the other hand, Steve's own description and documentation don't give me a warm fuzzy about it either.\nSo what is the truth of the matter? Would you use it?",
        "top_answer": "I've had a reasonably good experience with SpinRite, but I think it's highly overrated. In fact, it might just be too clever for its own good. There are free solutions which work just as well (actually, the free ones might work even better).\nWe had a 200 GB NTFS drive that suddenly failed catastrophically.  This was supposed to be the \"shared\" drive on which people just dumped stuff temporarily, but it ended up turning into a huge data repository that had miscellaneous backups, as well as a bunch of files that nobody bothered to back up anywhere.  When the drive died, we couldn't get it to mount, no matter how many times we ran chkdsk or other tools.\nIn the end, we purchased and ran SpinRite...which continued to run for more than 1 month.  Every time it hit a bad cluster, it spent hours trying to recover data from it.  Again, it ran nonstop for more than a month trying to recover data from a defective 200 GB drive.  (In SpinRite's defense, it can scan a drive in just a few hours if there are no physical defects.)  SpinRite was eventually able to recover all our files, although many of the larger ones turned out to be corrupt anyway.  SpinRite also made the drive mountable again.  So I'd definitely say it did something.\nHowever, despite the fact that it worked, I don't know if it helped any more than just booting off a Linux CD and running dd to copy the entire drive to a file.  There's something to be said for not running a dying disk for an entire month, as it's dying!  Physical defects seem to have a habit of spreading.  It wouldn't surprise me if the disk degraded even further while SpinRite was running.  Personally, I'd rather get the data off the disk as quickly as possible, make several backup images, and try to repair the files offline.\nWe've had to recover other data recently, and dd has done a great job.  You can tell it to copy all the good data off the drive, then you can run it a few more times to go and try harder (i.e., use smaller block sizes) trying to pull data off the bad areas.\nIf you've got an hour or so to spare, I'd say it's worth your time to learn how to use dd instead of buying SpinRite:\nhttp://www.debianadmin.com/recover-data-from-a-dead-hard-drive-using-dd.html\nOr go the slightly easier route and just download dd_rescue:\nhttp://www.garloff.de/kurt/linux/ddrescue\nIf you still want to run SpinRite, I'd highly recommend doing it AFTER you've copied all existing data off the drive, just in case running the drive for a longer period of time allows it to become further degraded.\nEvery time you get a new drive, you should boot off a Linux CD and run badblocks to check it for defects.  You should also periodically check your drives for degradation.  We've had at least 2 brand-new drives come with defects, and 3 or 4 more die within a couple of months (even though we did thorough tests before putting them into service).\nNote that you need to run badblocks as root, or prefix the commands with \"sudo \" if you're booting off an Ubuntu live CD.\nBrand-new drives (warning: destroys all data!):\nbadblocks -wvs /dev/sd#\n\nor\nbadblocks -wvs /dev/hd#\n\nIn-use drives (read-only test):\nbadblocks -vs /dev/sd#\n\nor\nbadblocks -vs /dev/hd#\n\nWhere # is the drive number in Linux.  IDE drives usually are called /dev/hd#, and SCSI (and often SATA) drives are /dev/sd#.\nMore info on badblocks here: http://en.wikipedia.org/wiki/Badblocks\nBy the way, even though dd and badblocks are Linux programs, you can use them on NTFS drives, and you can even mount NTFS partitions in Linux, regardless of whether you're using MBR partitions, dynamic disks, or GPT disks.\nSteve's documentation discusses a lot of hypothetical problems that SpinRite theoretically could help with. For example: data fading away over time and needing to be \"refreshed\" by reading every block and writing it back to the disk again, or the notion that repeatedly repositioning the read head on either side of a block will eventually permit you to statistically divine the original data stored in that block. Logically, these things make sense, but I think they are just solutions to academic problems which may not actually arise in the real world. (At least, with hard disks--maybe Zip disks and such were more susceptible to data fading.) If Steve cited papers on the subjects, or if these techniques had been experimentally proven to be effective, then I would expect for there to be many open-source or commercially-available SpinRite clones. It would be well within the capabilities of an average script programmer to write a Python, Perl, or UNIX shell script that includes all of SpinRite's documented features.",
        "url": "https://serverfault.com/questions/51681/does-spinrite-do-what-it-claims-to-do"
    },
    {
        "title": "How to move files with SCP?",
        "question": "How to not copy but move files from one server to another (both Linux)?\nman scp didn't give me anything useful. I cannot use 'scp' then 'rm' because I must make sure the file is successfully transferred. If there is any error during transfer, the file must not be deleted.\nPerhaps I should use exit code somehow, but how? Also, there are a lot of files, and if the last file fails it would be not-so-good option keep the whole bunch of successfully transferred files.\nMaybe there is something besides SCP? ",
        "top_answer": "rsync over ssh is probably your best bet with the --remove-source-files option\nrsync -avz --remove-source-files -e ssh /this/dir remoteuser@remotehost:/remote/dir \n\na quick test gives;\n[tomh@workstation001 ~]$ mkdir test1\n[tomh@workstation001 ~]$ mkdir test2\n[tomh@workstation001 ~]$ touch test1/testfile.1\n[tomh@workstation001 ~]$ ls test1/\ntestfile.1\n[tomh@workstation001 ~]$ rsync --remove-source-files -av -e ssh test1/testfile.1 tomh@localhost:/home/tomh/test2/\nsending incremental file list\n\nsent 58 bytes  received 12 bytes  10.77 bytes/sec\ntotal size is 0  speedup is 0.00\n\n[tomh@workstation001 ~]$ ls test1/\n[tomh@workstation001 ~]$\n[tomh@workstation001 ~]$ ls test2/\ntestfile.1\n\nAs @SvenW mentioned, -e ssh is the default so can be omitted.",
        "url": "https://serverfault.com/questions/363922/how-to-move-files-with-scp"
    },
    {
        "title": "Loopback to forwarded Public IP address from local network - Hairpin NAT",
        "question": "\nThis is a Canonical Question about Hairpin NAT (Loopback NAT).\n\nThe generic form of this question is:\nWe have a network with clients, a server, and a NAT Router. There is port forwarding on the router to the server so some of it's services are available externally. We have DNS pointing to the external IP. Local network clients fail to connect, but external work.\n\nWhy does this fail?\nHow can I create a unified naming scheme (DNS names which work both locally and externally)?\n\nThis question has answeres merged from multiple other questions. They originally referenced FreeBSD, D-Link, Microtik, and other equipment. They're all trying to solve the same problem however. ",
        "top_answer": "What you're looking for is called \"hairpin NAT\". Requests from the internal interface for an IP address assigned to the external interface should be NAT'ted as though they came in from the external-side interface.\nI don't have any FreeBSD familiarity at all, but reading the \"pf\" manual for OpenBSD (http://www.openbsd.org/faq/pf/rdr.html) the proposed solutions of split-horizon DNS, using a DMZ network, or TCP proxying lead me to believe that \"pf\" doesn't support hairpin NAT.\nI'd look at going the route of split-horizon DNS and not using IP addresses in URLs internally but, instead, using names.",
        "url": "https://serverfault.com/questions/55611/loopback-to-forwarded-public-ip-address-from-local-network-hairpin-nat"
    },
    {
        "title": "Denyhosts vs fail2ban vs iptables- best way to prevent brute force logons?",
        "question": "I'm setting up a LAMP server and need to prevent SSH/FTP/etc. brute-force logon attempts from succeeding. I've seen many recommendations for both denyhosts and fail2ban, but few comparisons of the two. I also read that an IPTables rule can fill the same function. \nWhy would I choose one of these methods over another? How do people on serverfault handle this problem?",
        "top_answer": "IIRC, DenyHosts will only watch your SSH service.  If you need it to protect other services as well, Fail2ban is definitely a better choice.  It is configurable to watch nearly any service if you are willing to tweak its configuration, but that shouldn't be necessary as the newer versions of Fail2ban include rulesets which are suitable for many popular server daemons.  Using fail2ban over a simple iptables rate limit has the advantage of completely blocking an attacker for a specified amount of time, instead of simply reducing how quickly he can hammer your server.  I've used fail2ban with great results on a number of production servers and have never seen one of those servers breached by a brute force attack since I've started using it.",
        "url": "https://serverfault.com/questions/128962/denyhosts-vs-fail2ban-vs-iptables-best-way-to-prevent-brute-force-logons"
    },
    {
        "title": "Is it OK to set up passwordless `sudo` on a cloud server?",
        "question": "I love the idea of accessing servers via keys, so that I don't have to type in my password every time I ssh into a box, I even lock my user's (not root) password (passwd -l username) so it's impossible to log in without a key. \nBut all of this breaks if I'm required to enter password for sudo commands. So I'm tempted to set up passwordless sudo to make things in line with passwordless login.\nHowever, I keep having a gut feeling that it may backfire at me in some unexpected way, it just seems somehow insecure. Are there any caveats with such set up? Would you recommend/not recommend doing this for a user account on a server?\nClarifications\n\nI'm talking about the use of sudo in an interactive user session here, not for services or administrative scripts\nI'm talking about using a cloud server (so I have no physical local access to a machine and can only log in remotely)\nI know that sudo has a timeout during which I don't have to re-enter my password. But my concert isn't really about wasting the extra time to physically type in a password. My idea though was to not have to deal with a password at all, because I assume that:\n\n\nIf I have to memorize it at all, it's very likely too short to be secure or reused\nIf I generate a long and unique password for my remote account, I'll have to store it somewhere (a local password manager program or a cloud service) and fetch it every time I want to use sudo. I hoped I could avoid that.\n\n\nSo with this question I wanted to better understand the risks, caveats and tradeoffs of one possible configuration over the others.\nFollow up 1\nAll answers say that passwordless sudo is insecure as it allows \"easy\" escalation of privileges if my personal user account gets compromised. I understand that. But on the other hand, if I use a password, we incur all the classic risks with passwords (too short or common string, repeated across different services, etc.). But I guess that if I disable password authentication in /etc/ssh/sshd_config so that you still have to have a key to log in, I can use a simpler password just for sudo that's easier to type in? Is that a valid strategy?\nFollow up 2\nIf I also have a key to log in as root via ssh, if somebody gets access to my computer and steal my keys (they're still protected by OS' keyring password though!), they might as well get a direct access to the root account, bypassing the sudo path. What should be the policy for accessing the root account then?",
        "top_answer": "\nI love the idea of accessing servers via keys, so that I don't have to\n  type in my password every time I ssh into a box, I even lock my user's\n  (not root) password (passwd -l username) so it's impossible to log in\n  without a key...Would you recommend/not recommend doing this for a\n  user account on a server?\n\nYou're going about disabling password-based logins the wrong way. Instead of locking a user's account, set PasswordAuthentication no in your /etc/ssh/sshd_config.\nWith that set, password authentication is disabled for ssh, but you can still use a password for sudo.\nThe only time I recommend setting NOPASSWD in sudo is for service accounts, where processes need to be able to run commands via sudo programmatically. In those circumstances, make sure that you explicitly whitelist only the specific commands that account needs to run. For interactive accounts, you should always leave passwords enabled.\nResponses to your follow-up questions:\n\nBut I guess that if I disable password authentication in\n  /etc/ssh/sshd_config so that you still have to have a key to log in, I\n  can use a simpler password just for sudo that's easier to type in? Is\n  that a valid strategy?\n\nYes, that's correct. I still recommend using relatively strong local account passwords, but not ridiculously-strong. ~8 characters, randomly generated is sufficient.\n\nIf I also have a key to log in as root via ssh, if somebody gets\n  access to my computer and steal my keys (they're still protected by\n  OS' keyring password though!), they might as well get a direct access\n  to the root account, bypassing the sudo path. \n\nRoot access via ssh should be disabled. Period. Set PermitRootLogin no in your sshd_config.\n\nWhat should be the policy for accessing the root account then?\n\nYou should always have a means of obtaining out-of-band access to the console of your server. Several VPS vendors do provide this, as do vendors of dedicated hardware. If your provider doesn't grant real console access (say, EC2 for instance), you can typically still restore access using a process like what I outline in this answer.",
        "url": "https://serverfault.com/questions/580881/is-it-ok-to-set-up-passwordless-sudo-on-a-cloud-server"
    },
    {
        "title": "How do you use apt-get to only install critical security updates on ubuntu?",
        "question": "How do you use apt-get to only install critical security updates on ubuntu?\nWe'd like to only upgrade packages that need to be upgraded for security reasons, without upgrading all other packages.",
        "top_answer": "I read the apt-get man page carefully when I got tired of manually editing the sources.list every time I wanted to only apply security updates (that means the second time).\nFigured this solution out:\nsudo cp /etc/apt/sources.list /etc/apt/security.sources.list\n\nEdit the latter to contain only security repositories, then:\nsudo apt-get upgrade -o Dir::Etc::SourceList=/etc/apt/security.sources.list\n\nTadaaaa... Scriptable stuff.",
        "url": "https://serverfault.com/questions/270260/how-do-you-use-apt-get-to-only-install-critical-security-updates-on-ubuntu"
    },
    {
        "title": "Government censors HTTPS traffic to our website. Workarounds?",
        "question": "I am helping run a website that has been blocked for political reasons by the same Russian agency that has previously tried blocking Telegram (RosKomNadzor). This is not the first time it happens, and previously we would just change the domain, but this has its own implications and loss in readership.\nThey are blocking only the domain name, not the IP (we're using Cloudflare anyways). We're using HTTPS, but ISPs are still somehow able to get the DNS information about a request coming our way from their clients. Technically, we can suggest our readers to configure their /etc/hosts, but that is not a viable option.\nIs there something that could be done on our server's side to encrypt/obfuscate the DNS information without users making any changes/installing software? Or is waiting for DNS over HTTPS to become mainstream our only option?\nFrom Russia with love.",
        "top_answer": "Unfortunately, circumventing censorship is better addressed on the client side, so there aren't many server side settings that could help with that. You could advise your users to use a VPN, Tor, and/or public DNS with DNS-over-HTTPS (RFC 8484) or DNS-over-TLS (RFC 7858).\nYou make the assumption that the censorship method has something to do with DNS, but have you actually tested this? Did you know that the server name indication (SNI, RFC 6066, 3) in the ClientHello is unencrypted and may also be used to block the TLS connection? Luckily, TLS Encrypted Client Hello (draft-ietf-tls-esni-09) is on its way and can help with that. More reading on the subject:\n\nSeth Schoen: ESNI: A Privacy-Protecting Upgrade to HTTPS (EFF)\nMatthew Prince: Encrypting SNI: Fixing One of the Core Internet Bugs (Cloudflare)\n\n(We don't usually add any greetings to our Q/A posts, but your 007 reference is golden!)",
        "url": "https://serverfault.com/questions/1050958/government-censors-https-traffic-to-our-website-workarounds"
    },
    {
        "title": "What is the correct temperature for a server room?",
        "question": "When I was working in our server room, I noticed that it was very cold.\nI know that the server room has to be cold to offset the heat of the servers, but perhaps it is TOO cold.\nWhat is an appropriate temperature to keep our server room at?",
        "top_answer": "Recommendations on server room temperature vary greatly.\nThis guide says that:\n\nGeneral recommendations suggest that you should not go below 10\u00b0C (50\u00b0F) or above 28\u00b0C (82\u00b0F). Although this seems a wide range these are the extremes and it is far more common to keep the ambient temperature around 20-21\u00b0C (68-71\u00b0F). For a variety of reasons this can sometimes be a tall order.\n\nThis discussion on Slashdot has a variety of answers but most of them within the range quoted above.\nUpdate: As others have commented below Google recommends 26.7\u00b0C (80\u00b0F) for data centres.\nAlso the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) has recently updated their recommended tempature range to be from 18\u00b0C-27\u00b0C (64.4\u00b0F-80.6\u00b0F).\nHowever this article agains highlights that there is still no consensus on the subject. As mentioned in the article I would highlight that:\n\n...nudging the thermostat higher may also leave less time to recover from a cooling failure, and is only appropriate for companies with a strong understanding of the cooling conditions in their facility.\n\nIMO most companies would not have such a strong understanding of cooling conditions and thus it would be safer in a small business environment to be running the rooms a little cooler.\nNB: It is important to note there are a lot more factors to consider in a server/data room than just the temperature, air flow & humidity for example are also important concerns.",
        "url": "https://serverfault.com/questions/11145/what-is-the-correct-temperature-for-a-server-room"
    },
    {
        "title": "How to change sshd port on Mac OS X?",
        "question": "I want to change which port sshd uses on a Mac server. For example, let's say from port 22 to port 32. \nEditing /etc/sshd_config does not seem to work.  Does anyone know how to change it?  I'd prefer a method that's compatible with all OSX versions (or as many as possible, at least).",
        "top_answer": "Every previous answer is working (as google suggest too), but they are dirty and inelegant.\n\nThe right way to change the listening port for a launchd handled service on Mac OS X is to make the changes the dedicated keys available in ssh.plist\n\nSo the solution is as simple as to use the port number instead of the service name.\nAn excerpt from my edited /System/Library/LaunchDaemons/ssh.plist:\n    <key>Sockets</key>\n    <dict>\n            <key>Listeners</key>\n            <dict>\n                    <key>SockServiceName</key>\n                    <string>22022</string>\n                    <key>SockFamily</key>\n                    <string>IPv4</string>\n                    <key>Bonjour</key>\n                    <array>\n                            <string>22022</string>\n                    </array>\n            </dict>\n    </dict>\n\n\nNote:\nTo be able to edit this file on El Capitan, Sierra and probably future versions as well, you need to disable SIP (System Integrity Protection). See How do I disable System Integrity Protection (SIP).\n\nFor Big Sur and later, this solution probably isn't possible since /System/ is now sealed and checksum-ed. See comments.\n\nFor Catalina, even after disabling SIP, the volumes are unwritable. Use sudo mount -uw / in order to enable writing to /System. Do the change then restore SIP and reboot.\n\nThe above edit will also force sshd to listen only over IPV4.\nAfter making any changes to ssh.plist, the file must be reloaded as follows:\nsudo launchctl unload /System/Library/LaunchDaemons/ssh.plist\nsudo launchctl load /System/Library/LaunchDaemons/ssh.plist\n\nNote that using launchctl stop ... and launchctl start ... will NOT reload this file.\nThe man page with more information can be found by typing man launchd.plist or using this link.",
        "url": "https://serverfault.com/questions/18761/how-to-change-sshd-port-on-mac-os-x"
    },
    {
        "title": "Heartbleed: are services other than HTTPS affected?",
        "question": "The OpenSSL 'heartbleed' vulnerability (CVE-2014-0160) affects webservers serving HTTPS. Other services also use OpenSSL. Are these services also vulnerable to heartbleed-like data leakage?\nI'm thinking in particular of\n\nsshd\nsecure SMTP, IMAP etc -- dovecot, exim & postfix\nVPN servers -- openvpn and friends\n\nall of which, on my systems at least, are linked to the OpenSSL libraries.",
        "top_answer": "Any service that uses OpenSSL for its TLS implementation is potentially vulnerable; this is a weakness in the underlying cyrptography library, not in how it's presented via a web server or email server package. You should consider all linked services vulnerable to data leakage at least.\nAs I'm sure you're aware, it's quite possible to chain attacks together. Even in the simplest attacks it's perfectly possible to, for example, use Heartbleed to compromise SSL, read webmail credentials, use webmail credentials to gain access to other systems with a quick \"Dear helpdesk, can you give me a new password for $foo, love CEO\".\nThere's more information and links in The Heartbleed Bug, and in another question maintained by a Server Fault regular, Heartbleed: What is it and what are options to mitigate it?.",
        "url": "https://serverfault.com/questions/587433/heartbleed-are-services-other-than-https-affected"
    },
    {
        "title": "What Should be the Permissions of Apache SSL Directory, Certificate, and Key?",
        "question": "I have my cert.pem and cert.key files in /etc/apache2/ssl folders.\nWhat would be the most secure permissions and ownership of:\n\n/etc/apache2/ssl directory\n/etc/apache2/ssl/cert.pem file\n/etc/apache2/ssl/cert.key file \n\n(Ensuring https:// access works of course :).\nThanks,\nJP",
        "top_answer": "The directory permissions should be 700, the file permissions on all the files should be 600, and the directory and files should be owned by root.",
        "url": "https://serverfault.com/questions/216477/what-should-be-the-permissions-of-apache-ssl-directory-certificate-and-key"
    },
    {
        "title": "Does memtest86+ test memory which is used to run itself?",
        "question": "One of the common server failure scenarios is bad DRAM, sometimes even when ECC memory is used. \nmemtest86+ is one of the most useful tools to diagnose DRAM problems. As it loads itself at the start of the memory, I've been wondering if memtest86+ checks the part of the memory which memtest86+is loaded into.\nIs the memory allocated to memtest86+so small that it doesn't matter, or is it possible that memtest86+ could miss a defect in the DRAM because it can't test the memory locations it's residing in?",
        "top_answer": "Obviously, memtest86+ cannot test the memory region which currently contains the memtest86+ executable code (but if there are memory errors in that region, it is very likely that the test itself will crash). However, memtest86+ is able to relocate its own code to a different address at runtime, and by using this trick it is able to test all memory which it is allowed to use by the firmware (BIOS) \u2014 just not all at once.\nThis code relocation is described in README.background inside the memtest86+ source code archive (the file is slightly out of date \u2014 e.g., it states that the addresses used for memtest86+ code are 0x2000 and 0x200000, but the low address as defined in the source is actually 0x10000, and the high address is either 0x2000000 or 0x300000 depending on the amount of memory in the machine).\nBut even with this relocation trick memtest86+ is not able to test all memory for the following reasons:\n\nUsually the firmware (BIOS) reserves some RAM regions for its own use (e.g., ACPI tables). While these RAM regions can be accessed by CPU, writing anything into them can result in unpredictable behavior.\nSome part of RAM is used for the System Management  Mode and is not even accessible from the CPU outside of the privileged SMM code.\nThe RAM address range between 640K and 1M is inaccessible due to quirks of the legacy PC memory layout (some of this RAM may be used as a shadow for BIOS ROM and for SMM, other parts may be completely inaccessible).\n",
        "url": "https://serverfault.com/questions/750684/does-memtest86-test-memory-which-is-used-to-run-itself"
    },
    {
        "title": "What characters should I use or not use in usernames on Linux?",
        "question": "I want to create user accounts named after a domain name. adduser complains that the usernames need to match the NAME_REGEX regular expression.\nadduser: Please enter a username matching the regular expression configured\nvia the NAME_REGEX configuration variable.  Use the `--force-badname'\noption to relax this check or reconfigure NAME_REGEX.\n\nI can add the users using useradd without complaint. \nIs there a reason that I shouldn't modify the regular expression to allow ., - and _?\nWhat characters will cause problems and shouldn't be allowed in usernames?\nThis is the default NAME_REGEX.\nNAME_REGEX=\"^[a-z][-a-z0-9]*\\$\"\n",
        "top_answer": "My advice to you is to follow the standard recommended by the default NAME_REGEX.  You can actually put nearly anything in a user name under *NIX but you may encounter odd problems with library code that makes assumptions.  Case in point:\nhttps://web.archive.org/web/20170928165345/http://blog.endpoint.com/2008/08/on-valid-unix-usernames-and-ones-sanity.html\nMy question to you: do you have a lot of domain names that would collide with each other if you stripped out the unusual punctuation?  For example, do you have both \"QUALITY-ASSURANCE\" and QUALITYASSURANCE\" as domain names?  If not, you could simply adopt a policy of stripping out the unusual characters and using what's left as the user name.\nAlso, you could use the \"real name\" section of the GECOS field in the /etc/passwd information to store the original, unmodified domain name, and scripts could extract it pretty easily.",
        "url": "https://serverfault.com/questions/73084/what-characters-should-i-use-or-not-use-in-usernames-on-linux"
    },
    {
        "title": "What's the difference between socat and netcat?",
        "question": "I understand that socat is described as a \"more advanced\" version of netcat, but what is the actual difference? \nWould it be correct to say that everything you can do in netcat you can also do in socat? What about the opposite (everything you can do with socat can also be done in netcat)?",
        "top_answer": "socat can do serial line stuff, netcat cannot.  socat can do fairly advanced functionality, like having multiple clients listen on a port, or reusing connections.",
        "url": "https://serverfault.com/questions/246347/whats-the-difference-between-socat-and-netcat"
    },
    {
        "title": "How to decide where to purchase a wildcard SSL certificate?",
        "question": "Recently I needed to purchase a wildcard SSL certificate (because I need to secure a number of subdomains), and when I first searched for where to buy one I was overwhelmed with the number of choices, marketing claims, and price range. I created a list to help me see past the marketing gimmicks that the greater majority of the Certificate Authorities (CAs) and resellers plaster all over their sites. In the end my personal conclusion is that pretty much the only things that matter are the price and the pleasantness of the CA's website.\nQuestion: Besides price and a nice website, is there anything worthy of my consideration in deciding where to purchase a wildcard SSL certificate?",
        "top_answer": "I believe that with respect to deciding where to purchase a wildcard SSL certificate, the only factors that matter are the first year's cost of an SSL certificate, and the pleasantness of the seller's website (i.e. user experience) for the purchase and setup of the certificate.\nI am aware of the following:\n\nClaims about warranties (e.g. $10K, $1.25M) are marketing gimmicks - these warranties protect the users of a given website against the possibility that the CA issues a certificate to a fraudster (e.g. phishing site) and the user loses money as a result (but, ask yourself: is someone spending/losing $10K or more on your fraudulent site? oh wait, you are not a fraudster? no point.)\nIt is necessary to generate a 2048-bit CSR (certificate signing request) private key to activate your SSL certificate. According to modern security standards using CSR codes with private key size less than 2048 bits is not allowed. Learn more here and here.\nClaims of 99+%, 99.3%, or 99.9% browser/device compatibility.\nClaims of fast issuance and easy install.\nIt is nice to have a money-back satisfaction guarantee (15 and 30 days are common).\n\nThe following list of wildcard SSL certificate base prices (not sales) and issuing authorities and resellers was updated on May 30th, 2018:\n price |\n/ year | Certificate Authority (CA) or Reseller\n($USD) |\n-------+---------------------------------------\n    $0 | DNSimple / Let's Encrypt *\n   $49 | SSL2BUY / AlphaSSL (GlobalSign) *\n   $68 | CheapSSLSecurity / PositiveSSL (Comodo) *\n   $69 | CheapSSLShop / PositiveSSL (Comodo) *\n   $94 | Namecheap / PositiveSSL (Comodo) * (Can$122)\n   $95 | sslpoint / AlphaSSL (GlobalSign) *\n  $100 | DNSimple / EssentialSSL (Comodo) *\n       |\n  $150 | AlphaSSL (GlobalSign) *\n  $208 | Gandi\n  $250 | RapidSSL\n  $450 | Comodo\n       |\n  $500 | GeoTrust\n  $600 | Thawte\n  $600 | DigiCert\n  $609 | Entrust\n  $650 | Network Solutions\n  $850 | GlobalSign\n       |\n$2,000 | Symantec\n\n* Note that DNSimple, sslpoint, Namecheap, CheapSSLShop, CheapSSLSecurity, and SSL2BUY, are resellers, not Certificate Authorities.\nNamecheap offers a choice of Comodo/PostiveSSL and Comodo/EssentialSSL (though there is no technical difference between the two, just branding/marketing - I asked both Namecheap and Comodo about this - whereas EssentialSSL costs a few dollars more (USD$100 vs $94)). DNSimple resells Comodo's EssentialSSL, which, again, is technically identical to Comodo's PositiveSSL.\nNote that SSL2BUY, CheapSSLShop, CheapSSLSecurity, Namecheap, and DNSimple provide not only the cheapest wildcard SSL certs, but they also have the least marketing gimmicks of all the sites I reviewed; and DNSimple seems to have no gimmicky stuff whatsoever. Here are links to the cheapest 1-year certificates (as I can't link to them in the table above):\n\nSSL2BUY\nCheapSSLShop\nCheapSSLSecurity\nsslpoint\nNamecheap\nDNSimple\n\nAs of March 2018 Let\u2019s Encrypt supports wildcard certificates. DNSimple supports Let's Encrypt certificates.",
        "url": "https://serverfault.com/questions/599219/how-to-decide-where-to-purchase-a-wildcard-ssl-certificate"
    },
    {
        "title": "Hostnames - What are they all about?",
        "question": "I've recently been \"forced\" to perform some sysadmin work, while this isn't something that I absolutely love doing I've been reading, experimenting and learning a lot.\nThere is one fundamental aspect of server configuration that I've not been able to grasp - hostnames.\nIn Ubuntu for instance, one should set the hostname like this (according to the Linode Library):\necho \"plato\" > /etc/hostname\nhostname -F /etc/hostname\n\nFile: /etc/hosts\n127.0.0.1        localhost.localdomain        localhost\n12.34.56.78      plato.example.com            plato\n\nI assume that plato is an arbitrary name and that plato.example.com is the FQDN.\nNow my questions are:\n\nIs it mandatory?\nTo what purpose?\nWhere is it needed / used?\nWhy can't I define \"localhost\" as the hostname for every machine?\nDo I have to set up a DNS entry for the plato.example.com FQDN?\nShould plato.example.com be used as the reverse DNS entry for my IP?\n\nAlso, are there any \"best practices\" for picking hostnames? I've seen people using Greek letters, planet names and even mythological figures... What happens when we run out of letters / planets?\nI'm sorry if this is a dumb question but I've never been too enthusiastic with network configurations.",
        "top_answer": "These days, a system may have multiple interfaces, each with multiple addresses, and each address may even have multiple DNS entries associated with it.  So what does a \"system hostname\" even mean?\nMany applications will use the system hostname as a default identifier when they communicate elsewhere.  For example, if you're collecting syslog messages at a central server, the messages will all be tagged with the hostname of the originating system.  In an ideal world you would probably ignore this (because you don't necessarily want to trust the client), but the default behavior -- if you named all your systems \"localhost\" -- would result in a bunch of log messages that you wouldn't be able to associate with a specific system.\nAs other folks have pointed out, the system hostname is also a useful identifier if you find yourself remotely accessing a number of systems.  If you've got five windows attached to a system named \"localhost\" then you're going to have a hard time keeping them straight.\nIn a similar vein, we try to make the system hostname match the hostname we use for administrative access to a system.  This helps avoid confusion when referring to the system (in email, conversations, documentation, etc).\nRegarding DNS:\nYou want to have proper forward and reverse DNS entries for your applications in order to avoid confusion.  You need some forward entry (name -> ip address) for people to be able to access your application conveniently.  Having the reverse entry match is useful for a number of reasons -- for example, it helps you correctly identify the application if you find the corresponding ip address in a log.\nNote that here I'm talking about \"applications\" and not \"systems\", because -- particularly  with web servers -- it's common to have multiple ip addresses on a system, associated with different hostnames and services.\nTrying to maintain name to ip mappings in your /etc/hosts file quickly becomes difficult as you manage an increasing number of systems.  It's very easy for the local hosts file to fall out of sync with respect to DNS, potentially leading to confusion and in some cases malfunction (because something tries to bind to an ip address that no longer exists on the system, for example).",
        "url": "https://serverfault.com/questions/228102/hostnames-what-are-they-all-about"
    },
    {
        "title": "How can I find the biggest directories in Unix / Ubuntu?",
        "question": "Is there a way in Unix to see the biggest directories on disk?\nI need to know why I'm almost out of space on the server,= and I don't know where most of the space is used.",
        "top_answer": "Try: du --max-depth=7 /* | sort -n - it won't just tell you directories, and there will be duplicates, but it will list everything 7 levels deep and sort them by size order.",
        "url": "https://serverfault.com/questions/200949/how-can-i-find-the-biggest-directories-in-unix-ubuntu"
    },
    {
        "title": "Deploying a folder of template files using ansible",
        "question": "Is there an easy way to deploy a folder full of template .j2 folder to a linux box, using the same name as the template, but without the .j2 extension, rather than using the template module for each file?\nRight now i have a long list of:\n- name: create x template\n  template:\n    src=files/x.conf.j2\n    dest=/tmp/x.conf\n    owner=root\n    group=root\n    mode=0755\n  notify:\n    - restart myService\n",
        "top_answer": "You could use with_fileglob to get the list of files from your template directory and use filters to strip the j2 extension like this:\n- name: create x template\n  template:\n    src: \"{{ item }}\"\n    dest: /tmp/{{ item | basename | regex_replace('\\\\.j2$', '') }}\n  with_fileglob:\n    - ../templates/*.j2\n",
        "url": "https://serverfault.com/questions/578544/deploying-a-folder-of-template-files-using-ansible"
    },
    {
        "title": "Linux: productive sysadmins without root (securing intellectual property)?",
        "question": "Is there any way to make a seasoned Linux syadmin productive without giving him full root access?\nThis question comes from a perspective of protecting intellectual property (IP), which in my case, is entirely code and/or configuration files (i.e. small digital files that are easily copied).  Our secret sauce has made us more successful than our smallish size would suggest.  Likewise, we are once-bitten, twice shy from a few former unscrupulous employees (not sysadmins) who tried to steal IP.  Top management's position is basically, \"We trust people, but out of self-interest, cannot afford the risk of giving any one person more access than they absolutely need to do their job.\"\nOn the developer side, it's relatively easy to partition workflows and access levels such that people can be productive but only see only what they need to see.  Only the top people (actual company owners) have the ability to combine all the ingredients and create the special sauce.\nBut I haven't been able to come up with a good way to maintain this IP secrecy on the Linux admin side.  We make extensive use of GPG for code and sensitive text files... but what's to stop an admin from (for example) su'ing to a user and hopping on their tmux or GNU Screen session and seeing what they're doing?\n(We also have Internet access disabled everywhere that could possibly come into contact with sensitive information.  But, nothing is perfect, and there could be holes open to clever sysadmins or mistakes on the network admin side.  Or even good old USB.  There are of course numerous other measures in place, but those are beyond the scope of this question.)\nThe best I can come up with is basically using personalized accounts with sudo, similar to what is described in Multiple Linux sysadmins working as root.  Specifically: no one except the company owners would actually have direct root access.  Other admins would have a personalized account and the ability to sudo into root.  Furthermore, remote logging would be instituted, and the logs would go to a server only the company owners could access.  Seeing logging turned off would set off some kind of alerts.\nA clever sysadmin could probably still find some holes in this scheme.  And that aside, it's still reactive rather than proactive.  The problem with our IP is such that competitors could make use of it very quickly, and cause a lot of damage in very short order.\nSo still better would be a mechanism that limits what the admin can do.  But I recognize that this is a delicate balance (particularly in the light of troubleshooting and fixing production issues that need to be resolved right now).\nI can't help but wonder how other organizations with very sensitive data manage this issue?  For example, military sysadmins: how do they manage servers and data without being able to see confidential information?\nEdit: In the initial posting, I meant to preemptively address the \"hiring practices\" comments that are starting to surface.  One, this is supposed to be a technical question, and hiring practices IMO tend more towards social questions.  But, two, I'll say this: I believe we do everything that's reasonable for hiring people: interview with multiple people at the firm; background and reference checks; all employees sign numerous legal documents, including one that says they've read and understood our handbook which details IP concerns in detail.  Now, it's out of the scope of this question/site, but if someone can propose \"perfect\" hiring practices that filter out 100% of the bad actors, I'm all ears.  Facts are: (1) I don't believe there is such a perfect hiring process; (2) people change - today's angel could be tomorrow's devil; (3) attempted code theft appears to be somewhat routine in this industry.",
        "top_answer": "What you are talking about is known as the \"Evil Sysadmin\" risk. The long and short of it is:\n\nA sysadmin is someone who has elevated privileges\nTechnically adept, to a level that would make them a good 'hacker'.\nInteracting with systems in anomalous scenarios. \n\nThe combination of these things makes it essentially impossible to stop malicious action. Even auditing becomes hard, because you have no 'normal' to compare with. (And frankly - a broken system may well have broken auditing too). \nThere are bunch of mitigative steps:\n\nPrivilege separation - you can't stop a guy with root from doing anything on the system. But you can make one team responsible for networking, and another team responsible for 'operating systems' (or Unix/Windows separately). \nLimit physical access to kit to a different team, who don't get admin accounts... but take care of all the 'hands' work. \nSeparate out 'desktop' and 'server' responsibility. Configure the desktop to inhibit removal of data. The desktop admins have no ability to access the sensitive, the server admins can steal it, but have to jump through hoops to get it out the building. \nAuditing to a restricted access system - syslog and event level auditing, to a relatively tamper-resistant system that they don't have privileged access to. But collecting it isn't enough, you need to monitor it - and frankly, there's a bunch of ways to 'steal' information that might not show up on an audit radar. (Poacher vs. gamekeepers)\napply 'at rest' encryption, so data isn't stored 'in the clear', and requires a live system to access. This means that people with physical access can't access a system that's not actively being monitored, and that in an 'anomalous' scenario where a sysadmin is working on it, the data is less exposed. (e.g. if the database isn't working, the data probably isn't readable)\nTwo man rule - if you're ok with your productivity being crippled, and your morale likewise. (Seriously - I've seen it done, and the persistent state of working and being watched makes it extremely difficult working conditions). \nVet your sysadmins - various records checks may exist depending on country. (Criminal records check, you might even find you can apply for a security clearance in some cases, which will trigger vetting)\nLook after your sysadmins - the absolute last thing you want to do is tell a \"trusted\" person that you don't trust them. And you certainly don't want to damage morale, because that increases the chance of malicious behaviour (or 'not-quite-negligence, but a slipping in vigilance'). But pay according to responsibility as well as skill set. And consider 'perks' - that are cheaper than salary, but probably valued more. Like free coffee, or pizza once a week. \nand you can also try and apply contract conditions to inhibit it, but be wary of the above. \n\nBut pretty fundamentally - you need to accept that this is a trust thing, not a technical thing. Your sysadmins will always be potentially very dangerous to you, as a result of this perfect storm. ",
        "url": "https://serverfault.com/questions/753268/linux-productive-sysadmins-without-root-securing-intellectual-property"
    },
    {
        "title": "How to change owner of mount point",
        "question": "We have moved mysql data directory to another disk, so now /var/lib/mysql is just a mount point to another partition. We set the owner of the /var/lib/mysql directory to mysql.mysql.\nBut everytime we mount the partition, the ownership changes to root.root. Because of this, we couldn't create additional MySQL database.\nOur fstab entry:\n/dev/mapper/db-db   /var/lib/mysql    ext3    relatime        0       2\n\nHow to change the owner of mount point to user other than root?",
        "top_answer": "You need to change the permissions of the mounted filesystem, not of the mount point when the filesystem is not mounted. So mount /var/lib/mysql then chown mysql.mysql /var/lib/mysql. This will change the permissions of the root of the MySQL DB filesystem.\nThe basic idea is that the filesystem holding the DB needs to be changed, not the mount point, unless its path has some issues, e.g. lib can only be read by root.",
        "url": "https://serverfault.com/questions/39288/how-to-change-owner-of-mount-point"
    },
    {
        "title": "linux/setfacl - Set all current/future files/directories in parent directory to 775 with specified owner/group",
        "question": "I have a directory called \"members\" and under it there are folders/files. How can I recursively set all the current folders/files and any future ones created there to by default have 775 permissions and belong to owner/group nobody/admin respectively? I enabled ACL, mounted, but can't seem to get the setfacl command to do this properly. Any idea how to accomplish this?",
        "top_answer": "I actually found something that so far does what I asked for, sharing here so anyone who runs into this issue can try out this solution:\nsudo setfacl -Rdm g:groupnamehere:rwx /base/path/members/\nsudo setfacl -Rm g:groupnamehere:rwx /base/path/members/\n\nR is recursive, which means everything under that directory will have the rule applied to it.\n\nd is default, which means for all future items created under that directory, have these rules apply by default.\nm is needed to add/modify rules.\nThe first command, is for new items (hence the d), the second command, is for old/existing items under the folder. Hope this helps someone out as this stuff is a bit complicated and not very intuitive.",
        "url": "https://serverfault.com/questions/444867/linux-setfacl-set-all-current-future-files-directories-in-parent-directory-to"
    },
    {
        "title": "Why would you use IPv6 internally?",
        "question": "Of course, I realize the need to go to IPv6 out on the open Internet since we are running out of addresses, but I really don't understand why there is any need to use it on an internal network. I have done zero with IPv6, so I also wonder: Won't modern firewalls do NAT between internal IPv4 addresses, and external IPv6 addresses?\nI was just wondering since I have seen so many people struggling with IPv6 questions here, and wonder why bother?",
        "top_answer": "There is no NAT for IPv6 (as you think of NAT anyway). NAT was an $EXPLETIVE temporary solution to IPv4 running out of addresses (a problem which didn't actually exist, and was solved before NAT was ever necessary, but history is 20/20). It adds nothing but complexity and would do little except cause headaches in IPv6 (we have so many IPv6 Address we unabashedly waste them). NAT66 does exist, and is meant to reduce the number of IPv6 addresses used by each host (it's normal for IPv6 hosts to have multiple addresses, IPv6 is somewhat different than IPv4 in many ways, this is one).\nThe Internet was supposed to be end-to-end routable, that is part of the reason IPv4 in invented and why it gained acceptance. That is not to say that all address on the Internet were supposed to be reachable. NAT breaks both. Firewalls add layers of security by breaking reachability, but normally that it's at the expense of routability.\nYou will want IPv6 in your networks as there is no way to specify an IPv6 endpoint with a IPv4 address. The other way around does work, which enables IPv6-only networks using DNS64 and NAT64 to access the IPv4 Internet still. It's actually possible today to ditch IPv4 all together, though it's a bit of hassle setting it up. It would be possible to proxy from IPv4 internal addresses to IPv6 servers. Adding and configuring a proxy server adds configuration, hardware, and maintenance costs to the network; usually much more than simply enabling IPv6.\nNAT causes it's own problems too. The Router has to be capable of coordinating every connection running through it, keeping track of endpoints, ports, timeouts, and more. All that traffic is being funneled through that single point usually. Though it's possible to build redundant NAT routers, the technology is massively complex and generally expensive. Redundant simple routers are easy and cheap (comparatively). Also, to re-establish some of the routability, forwarding and translating rules have to be established on the NAT system. This still breaks protocols which embed IP addresses, such as SIP. UPNP, STUN, and other protocols were invented to help with this problem too - more complexity, more maintenance, more that could go wrong.",
        "url": "https://serverfault.com/questions/274181/why-would-you-use-ipv6-internally"
    },
    {
        "title": "Can the environment variables tool in Windows be launched directly?",
        "question": "Is there a more direct way to the environmental variables GUI than the following?\n\nRight click 'My Computer' and select 'Properties'.\nClick 'Advanced System Settings' link.\nClick 'Advanced' tab.\nClick 'Environment Variables...' button.\n\nCan I make a shortcut to it?",
        "top_answer": "Starting with Windows\u00a0Vista, the panel can be displayed from the command line (cmd.exe) with a\nrundll32 sysdm.cpl,EditEnvironmentVariables\n\nIt is from here.",
        "url": "https://serverfault.com/questions/351129/can-the-environment-variables-tool-in-windows-be-launched-directly"
    },
    {
        "title": "How to specify multiple included domains in SPF record?",
        "question": "Our business email is hosted on Google apps.  In addition, our web server may also send email.  Currently our SPF record in DNS looks like this:\ndomain.com.    IN   TXT    \"v=spf1 a include:_spf.google.com -all\"\n\nThis is all fine, however now we've outsourced our email list management to another company and we need to include a second domain with include.  So, I'm looking for something like:\ndomain.com.    IN   TXT    \"v=spf1 a include:_spf.google.com include:otherdomain.com -all\"\n\nWhat is the correct syntax for this?\nMany thanks!",
        "top_answer": "All SPF mechanisms, including include, can be used multiple times, separated by spaces:\n\"v=spf1 include:_spf.google.com include:otherdomain.com -all\"\n\nEvaluation of include works this way:\n\nIf the included data returned PASS, then the include itself generates a result (for example, include:foo.bar generates a PASS, but -include:foo.bar generates a FAIL).\n\nIf the included data returned FAIL or NEUTRAL, then the include does not contribute to the result at all, and processing goes to your next mechanism.\n\n\nSee SPF record syntax and RFC 7208.\n(Note that redirect= is not a mechanism but a global modifier, and cannot be repeated this way.)",
        "url": "https://serverfault.com/questions/360815/how-to-specify-multiple-included-domains-in-spf-record"
    },
    {
        "title": "Fastest way to extract tar.gz",
        "question": "Is there anyway to extract a tar.gz file faster than tar -zxvf filenamehere?\nWe have large files, and trying to optimize the operation.",
        "top_answer": "pigz is a parallel version of gzip. Although it only uses a single thread for decompression, it starts 3 additional threads for reading, writing, and check calculation. Your results may vary but we have seen significant improvement in decompression of some of our datasets. Once you install pigz, the tar file can be extracted with:\npigz -dc target.tar.gz | tar xf -\n",
        "url": "https://serverfault.com/questions/270814/fastest-way-to-extract-tar-gz"
    },
    {
        "title": "Download SSL certificate from aws certificate manager",
        "question": "I am using aws certificate manager for managing SSL. Recently I purchased a wildcard ssl *.example-private.com Now I need that SSL certificate to deploy on enterprise git instance on aws. \nHow can i download ssl from aws?",
        "top_answer": "You cannot download a SSL certificate from ACM.",
        "url": "https://serverfault.com/questions/822035/download-ssl-certificate-from-aws-certificate-manager"
    },
    {
        "title": "Equivalent of logrotate on OSX",
        "question": "Is logrotate hiding somewhere on OSX, or is there an equivalent? It's not in /usr/sbin.",
        "top_answer": "Based on Brian Armstrong's answer, here's something with a little more explanation and a correction. This handles the log created by postgres on OSX installed by Homebrew. Located at /etc/newsyslog.d/postgresql.conf:\n# logfilename                           [owner:group]      mode count size(KB)  when  flags [/pid_file]                            [sig_num]\n/usr/local/var/postgres/postgresql.log  :                  600  2     2048      *     J     /usr/local/var/postgres/postmaster.pid \n\nThis will rotate the log file when it reaches 2MB in size, keep 2 archives (for a total of 6MB storage used), and bzip2-compress the archives. It will notify the postgres process to reopen the log files once rotated, which is necessary to get new log entries and to actually free the disk space without restarting the machine.\nImportant to note that size is in KB, not bytes.\nYou can test the config file (without affecting any files) using sudo newsyslog -nvv.\nnewsyslog documentation is located here: http://www.freebsd.org/cgi/man.cgi?newsyslog.conf(5).\nAlso used: http://www.redelijkheid.com/blog/2011/3/28/adding-custom-logfile-to-os-x-server-log-rotation.html",
        "url": "https://serverfault.com/questions/352942/equivalent-of-logrotate-on-osx"
    },
    {
        "title": "Overriding some DNS entries in BIND for internal networks",
        "question": "I have an internal network with a DNS server running BIND, connected to the internet through a single gateway. My domain \"example.com\" is managed by an external DNS provider. Some of the entries in that domain, say \"host1.example.com\" and \"host2.example.com\", as well as the top-level entry \"example.com\", point to the public IP address of the gateway.\nI would like hosts located on the internal network to resolve \"host1.example.com\", \"host2.example.com\" and \"example.com\" to internal IP addresses instead of that of the gateway. Other hosts like \"otherhost.example.com\" should still be resolved by the external DNS provider.\nI have succeeded in doing that for the host1 and host2 entries, by defining two single-entry zones in BIND for \"host1.example.com\" and \"host2.example.com\". However, if I add a zone for \"example.com\", all queries for that domain are resolved by my local DNS server, and e.g. querying \"otherhost.example.com\" results in an error.\nIs it possible to configure BIND to override only some entries of a domain, and to resolve the rest recursively?",
        "top_answer": "The best method is via the response policy zone in Bind 9.8.1 or newer. It allows you to override single records in arbitrary zones (and there's no need to create a whole subdomain for that, only the single record you want to change), it allows you to override CNAMEs, etc. Other solutions such as Unbound cannot override CNAMEs.\nhttps://www.redpill-linpro.com/sysadvent/2015/12/08/dns-rpz.html\n\nEDIT: Let's do this properly then. I will document what I've done based on the tutorial linked above.\nMy OS is Raspbian 4.4 for Raspberry Pi, but the technique should work without any changes on Debian and Ubuntu, or with minimal changes on other platforms.\nGo to where your Bind config files are kept on your system - here it's in /etc/bind. Create in there a file called db.rpz with the following contents:\n$TTL 60\n@            IN    SOA  localhost. root.localhost.  (\n                          2015112501   ; serial\n                          1h           ; refresh\n                          30m          ; retry\n                          1w           ; expiry\n                          30m)         ; minimum\n                   IN     NS    localhost.\n\nlocalhost       A   127.0.0.1\n\nwww.some-website.com    A        127.0.0.1\n\nwww.other-website.com   CNAME    fake-hostname.com.\n\nWhat does it do?\n\nit overrides the IP address for www.some-website.com with the fake address 127.0.0.1, effectively sending all traffic for that site to the loopback address\nit sends traffic for www.other-website.com to another site called fake-hostname.com\n\nAnything that could go in a Bind zone file you can use here.\nTo activate these changes there are a few more steps:\nEdit named.conf.local and add this section:\nzone \"rpz\" {\n  type master;\n  file \"/etc/bind/db.rpz\";\n};\n\nThe tutorial linked above tells you to add more stuff to zone \"rpz\" { } but that's not necessary in simple setups - what I've shown here is the minimum to make it work on your local resolver.\nEdit named.conf.options and somewhere in the options { } section add the response-policy option:\noptions {\n  // bunch\n  // of\n  // stuff\n  // please\n  // ignore\n\n  response-policy { zone \"rpz\"; };\n}\n\nNow restart Bind:\nservice bind9 restart\n\nThat's it. The nameserver should begin overriding those records now.\nIf you need to make changes, just edit db.rpz, then restart Bind again.\nBonus: if you want to log DNS queries to syslog, so you can keep an eye on the proceedings, edit named.conf.local and make sure there's a logging section that includes these statements:\nlogging {\n    // stuff\n    // already\n    // there\n\n    channel my_syslog {\n        syslog daemon;\n        severity info;\n    };\n    category queries { my_syslog; };\n};\n\nRestart Bind again and that's it.\nTest it on the machine running Bind:\ndig @127.0.0.1 www.other-website.com. any\n\nIf you run dig on a different machine just use @the-ip-address-of-Bind-server instead of @127.0.0.1\nI've used this technique with great success to override the CNAME for a website I was working on, sending it to a new AWS load balancer that I was just testing. A Raspberry Pi was used to run Bind, and the RPi was also configured to function as a WiFi router - so by connecting devices to the SSID running on the RPi I would get the DNS overrides I needed for testing.",
        "url": "https://serverfault.com/questions/18748/overriding-some-dns-entries-in-bind-for-internal-networks"
    },
    {
        "title": "Where are the logs for ufw located on Ubuntu Server?",
        "question": "I have an Ubuntu server where I am blocking some IPs with ufw. I enabled logging, but I don't know where to find the logs. Where might the logs be or why might ufw not be logging?",
        "top_answer": "Perform sudo ufw status verbose to see if you're even logging in the first place. If you're not, perform sudo ufw logging on if it isn't. If it is logging, check /var/log/ for files starting with ufw. For example, sudo ls /var/log/ufw*\nIf you are logging, but there are no /var/log/ufw* files, check to see if rsyslog is running: sudo service rsyslog status. If rsyslog is running, ufw is logging, and there are still no logs files, search through common log files for any mention of UFW. For example: grep -i ufw /var/log/syslog and grep -i ufw /var/log/messages as well as grep -i ufw /var/log/kern.log.\nIf you find a ton of ufw messages in the syslog, messages, and kern.log file, then rsyslog might need to be told to log all UFW messages to a separate file. Add a line to the top of /etc/rsyslog.d/50-default.conf that says the following two lines:\n:msg, contains, \u201cUFW\u201d -/var/log/ufw.log\n& ~\n\nAnd you should then have a ufw.log file that contains all ufw messages!\nNOTE:\nCheck the 50-default.conf file for pre-existing configurations.\nMake sure to backup the file before saving edits!\nUPDATE for Ubuntu Server 20.04 and later\nAs of Ubuntu 20.04 and later, the ~ is deprecated and should not be used, as reported in the ubuntu 20.04 system log:\nrsyslogd: error during parsing file /etc/rsyslog.d/50-default.conf, on or before line 6: invalid character '\ufffd' - is there an invalid escape sequence somewhere? [v8.2001.0 try https://www.rsyslog.com/e/2207 ]\nJan 26 12:10:27 ubuntu rsyslogd: warning: ~ action is deprecated, consider using the 'stop' statement instead [v8.2001.0 try https://www.rsyslog.com/e/2307 ]\n\nRather, you should replace it with the word stop. So if you still want to log all ufw messages in rsyslog to a seperate ufw.log file then you should add the following to your /etc/rsyslog.d/50-default.conf file:\n:msg, contains, \u201cUFW\u201d -/var/log/ufw.log\n& stop\n\ninstead of\n:msg, contains, \u201cUFW\u201d -/var/log/ufw.log\n& ~\n",
        "url": "https://serverfault.com/questions/516838/where-are-the-logs-for-ufw-located-on-ubuntu-server"
    },
    {
        "title": "Creating a virtual machine in VirtualBox from a physical one",
        "question": "Is there any way to create a virtual machine that you can use in VirtualBox from a physical installation that you have? For instance, if I have Windows XP installed on a physical computer and want to have a virtual version of that machine on a different computer. This would save a ton of time by not having to reinstall and reconfigure the whole OS. \nI would think there would be issues with Microsoft's licensing. But even if it's not possible with Windows would it be possible to take a physical Linux machine and create a VirtualBox version of that? Does any other desktop virtualization software provide this feature?",
        "top_answer": "Windows is a bit different, see How to migrate existing Windows installations to VirtualBox for a guide. \nFrom memory you can use VMware's converter and VirtualBox will read VMDK files.\nFor Linux, if you want the easy solution, boot a live CD,\ndd if=/dev/sda1 of=/path/to/images/sda1.img bs=1024\n\nDo that for every partition mounted in /etc/fstab of your machine, and then setup those images in VirtualBox.",
        "url": "https://serverfault.com/questions/33603/creating-a-virtual-machine-in-virtualbox-from-a-physical-one"
    },
    {
        "title": "PowerShell script, showing commands run",
        "question": "I am playing around with PowerShell scripts and they're working great.  However, I am wondering if there is any way to also show all the commands that were run, just as if you were manually typing them in yourself.  This would be similar to \"echo on\" in batch files.  I looked at the PowerShell command-line arguments, the cmdlets, but I didn't find anything obvious.  Thanks!",
        "top_answer": "The following command will output each line of script to Write-Debug-\nSet-PSDebug -Trace 1\n\nFrom man Set-PSDebug\n\nWhen the Trace parameter is set to 1, each line of script is traced as\n  it is executed. When the parameter is set to 2, variable assignments,\n  function calls, and script calls are also traced. If the Step\n  parameter is specified, you are prompted before each line of the\n  script is executed.\n",
        "url": "https://serverfault.com/questions/102098/powershell-script-showing-commands-run"
    },
    {
        "title": "What is causing Access Denied when using the aws cli to download from Amazon S3?",
        "question": "I'm really flailing around in AWS trying to figure out what I'm missing here. I'd like to make it so that an IAM user can download files from an S3 bucket - without just making the files totally public - but I'm getting access denied. If anyone can spot what's off I'll be stoked.\nWhat I've done so far:\n\nCreated a user called my-user (for sake of example)\nGenerated access keys for the user and put them in ~/.aws on an EC2 instance\nCreated a bucket policy that I'd hoped grants access for my-user\nRan the command aws s3 cp --profile my-user s3://my-bucket/thing.zip .\n\nBucket policy:\n{\n  \"Id\": \"Policy1384791162970\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1384791151633\",\n      \"Action\": [\n        \"s3:GetObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::my-bucket/*\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::111122223333:user/my-user\"\n      }\n    }\n  ]\n}\n\nThe result is A client error (AccessDenied) occurred: Access Denied although I can download using the same command and the default (root account?) access keys.\nI've tried adding a user policy as well. While I don't know why it would be necessary I thought it wouldn't hurt, so I attached this to my-user.\n{\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1384889624746\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::my-bucket/*\"\n    }\n  ]\n}\n\nSame results.",
        "top_answer": "I was struggling with this, too, but I found an answer over here https://stackoverflow.com/a/17162973/1750869 that helped resolve this issue for me. Reposting answer below.\n\nYou don't have to open permissions to everyone. Use the below Bucket policies on source and destination for copying from a bucket in one account to another using an IAM user \nBucket to Copy from \u2013 SourceBucket\nBucket to Copy to \u2013 DestinationBucket\nSource AWS Account ID - XXXX\u2013XXXX-XXXX\nSource IAM User - src\u2013iam-user\nThe below policy means \u2013 the IAM user - XXXX\u2013XXXX-XXXX:src\u2013iam-user has s3:ListBucket and s3:GetObject privileges on SourceBucket/*  and s3:ListBucket and s3:PutObject privileges on DestinationBucket/*\nOn the SourceBucket the policy should be like:\n{\n\"Id\": \"Policy1357935677554\",\n\"Statement\": [\n    {\n        \"Sid\": \"Stmt1357935647218\",\n        \"Action\": [\n            \"s3:ListBucket\"\n        ],\n        \"Effect\": \"Allow\",\n        \"Resource\": \"arn:aws:s3:::SourceBucket\",\n        \"Principal\": {\"AWS\": \"arn:aws:iam::XXXXXXXXXXXX:user/src\u2013iam-user\"}\n    },\n    {\n        \"Sid\": \"Stmt1357935676138\",\n        \"Action\": [\"s3:GetObject\"],\n        \"Effect\": \"Allow\",\n        \"Resource\": \"arn:aws:s3::: SourceBucket/*\",\n        \"Principal\": {\"AWS\": \"arn:aws:iam::XXXXXXXXXXXX:user/src\u2013iam-user\"}\n   }\n]\n}\n\nOn the DestinationBucket the policy should be:\n{\n\"Id\": \"Policy1357935677554\",\n\"Statement\": [\n    {\n        \"Sid\": \"Stmt1357935647218\",\n        \"Action\": [\n            \"s3:ListBucket\"\n        ],\n        \"Effect\": \"Allow\",\n        \"Resource\": \"arn:aws:s3::: DestinationBucket\",\n        \"Principal\": {\"AWS\": \"arn:aws:iam::XXXXXXXXXXXX:user/src\u2013iam-user\"}\n    },\n    {\n        \"Sid\": \"Stmt1357935676138\",\n        \"Action\": [\"s3:PutObject\"],\n        \"Effect\": \"Allow\",\n        \"Resource\": \"arn:aws:s3::: DestinationBucket/*\",\n        \"Principal\": {\"AWS\": \"arn:aws:iam::XXXXXXXXXXXX:user/src\u2013iam-user\"}\n   }\n]\n}\n\ncommand to be run is s3cmd cp s3://SourceBucket/File1 s3://DestinationBucket/File1",
        "url": "https://serverfault.com/questions/556077/what-is-causing-access-denied-when-using-the-aws-cli-to-download-from-amazon-s3"
    },
    {
        "title": "How Often Should I Update our Linux Server?",
        "question": "I am responsible for managing both our production server (mail, web, database are all on one server) and our test server. Both are built on Debian. However as I am very new to system administration, I have only been installing updates as I come across things that have to be updated so that I can have newer features and get bug fixes. Its a pretty ad hoc process right now, and I'd like to make it less so.\nSo I am wondering how people who know what they're doing handle this? How often do you perform upgrades on your servers? Is the upgrade process different between test and production? Do you always upgrade any test servers first? And do you do a full update of all software, or do you just install selected updates?",
        "top_answer": "I run apt-get update -qq; apt-get upgrade -duyq daily.  This will check for updates, but not do them automatically.\nThen I can run the upgrades manually while I am watching, and can correct anything that might go wrong.\nBesides the security concerns of maintaining a patched system, I find that if I leave it too long between patches, I end up with a whole bunch of packages that want to be upgraded, and that scares me a-lot more than just upgrading one or two every week or so.  Therefore I tend to run my upgrades weekly, or if they are high priority, daily.  This has the added advantage of knowing which package broke your system (ie. if you're only upgrading a couple at a time)\nI always upgrade less critical systems first.  I also have a \"rollback plan\" in place in case I can't fix the system.  (since most of our servers are virtual, this rollback plan usually consists of taking a snapshot before the upgrade that I can revert to if necessary)\nThat being said, I think an upgrade has broken something only once or twice in the past 4 years, and that was on a highly customized system - so you don't have to be TOO paranoid :)",
        "url": "https://serverfault.com/questions/9490/how-often-should-i-update-our-linux-server"
    },
    {
        "title": "What does \"Connection: close\" mean when used in the response message?",
        "question": "When the client uses the Connection: close header in the request message, this means that it wants the server to close the connection after sending the response message.\nI thought that this header is only used in the request messages, but I have noticed that it is also used in the response messages. For example:\n\nWhat does this header means when used in the response message?\nI think that it means that the server will close the connection after sending the response the message (even if the client has used the Connection: keep-alive header in its request message). Am I correct?",
        "top_answer": "Yes, this is correct. The server can just say \"I don't support your keep-alive request and will just close the connection when I am finished\".\nFrom RFC 2616, Section 14.10:\n\nHTTP/1.1 defines the \"close\" connection option for the sender to\nsignal that the connection will be closed after completion of the\nresponse. For example,\n   Connection: close\n\nin either the request or the response header fields indicates that\nthe connection SHOULD NOT be considered `persistent' (section 8.1)\nafter the current request/response is complete.\nHTTP/1.1 applications that do not support persistent connections\nMUST    include the \"close\" connection option in every message.\n",
        "url": "https://serverfault.com/questions/790197/what-does-connection-close-mean-when-used-in-the-response-message"
    },
    {
        "title": "Hidden Features of Linux",
        "question": "Following in the spirit of Hidden Features of PowerShell and various others on Stack Overflow, what Linux commands or combinations of commands do you find essential in your work?\nAlso See:\nHidden Features of MySQL\nHidden Features of PowerShell \nHidden features of Oracle Database\nHidden Features of Windows 2008\nHidden Features of Solaris/OpenSolaris\nHidden Features of SQL Server\nHidden Features of IIS (6.0 / 7.0)",
        "top_answer": "To get the ball going, I find screen to be essential:\n\nWhen screen is called, it creates a\n  single window with a shell in it (or\n  the specified command) and then gets\n  out of your way so that you can use\n  the program as you normally would.\n  Then, at any time, you can create new\n  (full-screen) windows with other\n  programs in them (including more\n  shells), kill the current window, view\n  a list of the active windows, turn\n  output logging on and off, copy text\n  between windows, view the scrollback\n  history, switch between windows, etc.\n  All windows run their programs\n  completely independent of each other.\n  Programs continue to run when their\n  window is currently not visible and\n  even when the whole screen session is\n  detached from the users terminal.\n",
        "url": "https://serverfault.com/questions/2817/hidden-features-of-linux"
    },
    {
        "title": "What does 'nice' mean on CPU utilization graphs?",
        "question": "On Ubuntu server load graphs I see 4 types of CPU consumption: User, System, Nice and Idle. \nWhat does Nice type mean?",
        "top_answer": "It is the CPU scheduling priority, higher vales (+19) mean lower priority, and lower values (-20) mean higher priority (inverse relationship). man 2 getpriority will give you lots of details.  You can set the nice value when launching a process with the nice command and then change it with the renice command. Only the superuser (root) can specify a priority increase of a process.",
        "url": "https://serverfault.com/questions/116950/what-does-nice-mean-on-cpu-utilization-graphs"
    },
    {
        "title": "What is the recommended size for a Linux /boot partition?",
        "question": "What is the recommended size for a Linux /boot partition?\nAnd is it safe to not have a /boot partition?\nI see some servers don't have a /boot partition while some servers have a 128 MB /boot partition. I am a little confused. Is /boot partition necessary? If it is, how large should it be?",
        "top_answer": "These days, 100 Megabytes or 200 Megabytes is the norm. \nYou do not need to have a /boot partition. However, it's good to have for flexibility reasons (LVM, encryption, BIOS limitations).\nEdit:\nThe recommended size has been increased to 300MB-500MB.\nAlso see: https://superuser.com/questions/66015/installing-ubuntu-do-i-really-need-a-boot-parition",
        "url": "https://serverfault.com/questions/334663/what-is-the-recommended-size-for-a-linux-boot-partition"
    },
    {
        "title": "How to prevent \"ps\" reporting its own process?",
        "question": "$ ps | grep django\n28006 ttys004    0:01.12 /usr/bin/python bin/django celeryd --beat\n51393 ttys005    0:01.45 /usr/bin/python bin/django celeryd -l INFO\n51472 ttys005    0:01.29 /usr/bin/python bin/django celeryd -l INFO\n51510 ttys005    0:01.89 /usr/bin/python bin/django celeryd -l INFO\n51801 ttys005    0:01.83 /usr/bin/python bin/django celeryd -l INFO\n53470 ttys005    0:03.97 /usr/bin/python bin/django celeryd -l INFO\n53780 ttys005    0:00.00 grep django\n\nIs there a way to prevent the last process (that is, the grep that was started at the same time as my ps command) being reported?\n(I started trying to come up with a regex that would match the literal but not match itself, but that seemed, um, not the right approach...)",
        "top_answer": "My answer is a variation on the typical answer for searching for \"foobar\" in a ps listing. The argument of \"-A\" \"ps\" is more portable than \"aux\", I believe, but this change is irrelevant to the answer. The typical answer looks like this:\n$ ps -A -ww | grep [f]oobar\n\nInstead I use this pattern:\n$ ps -A -ww | grep [^]]foobar\n\nThe main advantage is that it's easier to write scripts based on this patterns because you simply concatenate a static string [^]] with whatever pattern you are looking for. You don't need to strip off the first letter of the string then insert it between the square braces and then concatenate the that back together again. When scripting in shell it's easier to simply stick [^]] in front of the pattern you were lookginfor looking for. String slicing in Bash is an ugly thing, so my variation avoids that. This variation says show the lines where the pattern matches WITHOUT a leading right-square-bracket ]. Since the search pattern to exclude a square bracket actually adds the square bracket to the pattern then it will never match itself.\nSo you could write a portable psgrep command as follows. Here, I make some allowance for differences between Linux, OS X BSD, and others. This adds the column headers from ps, provides a more custom ps format that suits my needs betters, and displays processes listing extra, extra wide so that none of the command-line arguments are missed. Well, most are not missed. Java being Java, it often does things in the worst possible way, so you some java services will run past the maximum allowed length of arguments that the process table will keep track of. I believe this is 1024 characters. The command-lone length allowed to start a process is much longer, but the kernel process table doesn't bother to keep track of anything over 1K in length. Once the command is started the command-name and argument list isn't needed against, so what gets stored in the process table is just informational.\npsgrep ()\n{\n    pattern=[^]]${1};\n    case \"$(uname -s)\" in\n        Darwin)\n            ps -A -ww -o pid,ppid,nice,pri,pcpu,pmem,etime,user,wchan,stat,command | grep -i -e \"^[[:space:]]*PID\" -e ${pattern}\n        ;;\n        Linux)\n            ps -A -ww -o pid,ppid,tid,nice,pri,pcpu,pmem,etime,user,wchan:20,stat,command | grep -i -e \"^[[:space:]]*PID\" -e ${pattern}\n        ;;\n        *)  # other UNIX flavors get a minimalist version.\n            ps -A -ww | grep -i -e ${pattern}\n        ;;\n    esac\n}\n",
        "url": "https://serverfault.com/questions/367921/how-to-prevent-ps-reporting-its-own-process"
    },
    {
        "title": "How to extinguish a small fire in a server rack to minimize damage to surrounding equipment?",
        "question": "Suppose I have a rack with several servers and other stuff. One of servers overheats severely and either starts smoking or catches fire while there's a serviceman nearby.\nIf anything similar happens in an apartment and there's a fire extinguisher nearby using the latter promptly often lets extinguish the fire very fast but in case of server rack improper extinguishing may lead to unneeded extra damage to surrounding equipment.\nTo clarify, I'm talking about a really small fire that one can try to extinguish without risking their life - like grab a nearby extinguisher, discharge it and get the fire extinguished in say ten fifteen seconds.\nWhat's the strategy to extinguish a small local fire in a server rack? What type of extinguisher is to be used? How to minimize damage to surrounding equipment?",
        "top_answer": "Definitely extinguish the fire without regard for the rest of the servers on the same rack.  Any properly designed data center will have backup servers at another location for the applications to fall back on.  It is better to lose a rack of servers than to lose a life or the whole data center.",
        "url": "https://serverfault.com/questions/324549/how-to-extinguish-a-small-fire-in-a-server-rack-to-minimize-damage-to-surroundin"
    },
    {
        "title": "How to pass password to scp command used in bash script? [duplicate]",
        "question": "\nPossible Duplicate:\nConnect through SSH and type in password automatically, without using a public key \n\nI have a bash script that makes dump of DB then copies file from one server to another but it always asks for password before connection.\nscp file.tar.gz [email\u00a0protected]:/backup\n\nIs there a way to pass password directly into script ?",
        "top_answer": "Use the tool sshpass\nsshpass -p 'password' scp file.tar.gz [email\u00a0protected]:/backup \n",
        "url": "https://serverfault.com/questions/318474/how-to-pass-password-to-scp-command-used-in-bash-script"
    },
    {
        "title": "Run a shell script as a different user",
        "question": "What's a good way of running a shell script as a different user. I'm using Debian etch, and I know which user I want to impersonate.\nIf I was doing it manually, I would do:\nsu postgres\n./backup_db.sh /tmp/test\nexit\n\nSince I want to automate the process, I need a way to run backup_db.sh as postgres (inheriting the environment, etc)\nThanks!",
        "top_answer": "To run your script as another user as one command, run:\n/bin/su -c \"/path/to/backup_db.sh /tmp/test\" - postgres\n\nBreaking it down:\n /bin/su : switch user\n -c \"/path/to...\" : command to run\n - : option to su, make it a login session (source profile for the user)\n postgres : user to become\n\nI recommend always using full paths in scripts like this - you can't always guarantee that you'll be in the right directory when you su (maybe someone changed the homedir on you, who knows).  I also always use the full path to su (/bin/su) because I'm paranoid.  It's possible someone can edit your path and cause you to use a compromised version of su.",
        "url": "https://serverfault.com/questions/44400/run-a-shell-script-as-a-different-user"
    },
    {
        "title": "Windows command prompt: how do I get the output of a command into a environment variable?",
        "question": "I want to have an environment variable that contains the day of week in cmd.exe.\nWhen I run this command I get the result I want.\nC:\\Users\\tisc> powershell (get-date).dayofweek\nFriday\n\nHere I'm trying to store the result in an environment variable.\nC:\\Users\\tisc> set dow = powershell (get-date).dayofweek\n\nBut when I try to get it I don't get the string as I wanted.\nC:\\Users\\tisc> set dow\nDoW=0\ndow = powershell (get-date).dayofweek\n\nMy goal is to use the variable in a batch file for some backup scripts.",
        "top_answer": "You can use something like:\n$env:DOW = \"foo\"\n",
        "url": "https://serverfault.com/questions/349585/windows-command-prompt-how-do-i-get-the-output-of-a-command-into-a-environment"
    },
    {
        "title": "Tool to test a user account and password (test login)",
        "question": "Yeah, I can fire up a VM or remote into something and try the password...I know...but is there a tool or script that will simulate a login just enough to confirm or deny that the password is correct?\nScenario:\nA server service account's password is \"forgotten\"...but we think we know what it is.  I'd like to pass the credentials to something and have it kick back with \"correct password\" or \"incorrect password\".\nI even thought about a drive mapping script with that user account and password being passed to see if it mapped the drive successfully or not but got lost in the logic of making it work correctly...something like:\n-Script asks for username via msgbox\n-script asks for password via msgbox\n-script tries to map a drive to a common share that everyone has access to\n-script unmaps drive if successful\n-script returns popup msgbox stating \"Correct Password\" or else \"Incorrect Password\"\nAny help is appreciated...you'd think this would be a rare occurrence not requiring a tool to support it but...well....",
        "top_answer": "runas /u:yourdomain\\a_test_user notepad.exe\n\nThe utility will prompt for the password, if the right password has been provided, notepad will launch, if not it will produce error 1326: the username or password is incorrect",
        "url": "https://serverfault.com/questions/67706/tool-to-test-a-user-account-and-password-test-login"
    },
    {
        "title": "Bash find command verbose output",
        "question": "Is there way to tell the bash find command to output what it is doing (verbose mode)?\nFor example for the command:\nfind /media/1Tb/videos -maxdepth 1 -type d -mtime +7 -exec rm -rf {} \\; to output:\nFound /media/1Tb/videos/102, executing rm -rf /media/1Tb/videos/102\n...\n",
        "top_answer": "You could concoct something with -printf, but the easiest is just to tack on -print on the end. This will show what was successfully deleted.",
        "url": "https://serverfault.com/questions/319362/bash-find-command-verbose-output"
    },
    {
        "title": "Multiple \"ServerName\" per VHost?",
        "question": "In Apache2 is it possible to set multiple ServerNames in one VHost?\nI want to setup a \"wiki\" vhost for an internal wiki.\nMy network has a \".lan\" suffix.  How do I get Apache to answer both \"wiki\" and \"wiki.lan\" on the same vhost?",
        "top_answer": "Use both the ServerName and ServerAlias directives in your virtualhost definition. You would do something like:\n<VirtualHost *:80>\n\n    Servername wiki.lan \n    ServerAlias wiki\n\n    [...]\n\n</Virtualhost>\n\nSee Apache Docs \u2013 ServerAlias Directive.",
        "url": "https://serverfault.com/questions/294423/multiple-servername-per-vhost"
    },
    {
        "title": "Force SSH to use a specific shell",
        "question": "Is there any way to force SSH to use a particular shell on the remote end, regardless of what the user's default shell is?\nI've tried solutions akin to:\nssh host.domain.com /bin/bash -c 'complicated, multi-line command'\n\nbut unfortunately the default shell on the remote end is responsible for parsing the \"complicated, multi-line command\" part, and I'm having difficulty escaping it sufficiently to work both for Bash and C shell users.",
        "top_answer": "I don't believe this is possible, at least with openssh-based systems. If you have the ability, a better solution might be to sftp up a shell-script file, and then execute it with the method you posted. It would have the advantage of minimizing the amount of escaping needed, but would leave a file behind that would have to be removed (perhaps as the last step of the script).",
        "url": "https://serverfault.com/questions/162018/force-ssh-to-use-a-specific-shell"
    },
    {
        "title": "debian packages version convention",
        "question": "I'm using debian/Ubuntu, and get confused about versions of packages.\nWhen using dpkg -l command, I get:\nii  vim                                 2:7.3.429-2ubuntu2.1                Vi IMproved - enhanced vi editor\nii  vim-common                          2:7.3.429-2ubuntu2.1                Vi IMproved - Common files\nii  vim-runtime                         2:7.3.429-2ubuntu2.1                Vi IMproved - Runtime files\nii  vim-tiny                            2:7.3.429-2ubuntu2.1                Vi IMproved - enhanced vi editor - compact version\nii  virt-what                           1.11-1                              detect if we are running in a virtual machine\nii  w3m                                 0.5.3-5ubuntu1                      WWW browsable pager with excellent tables/frames support\nii  watershed                           6                                   reduce superfluous executions of idempotent command\nii  wget                                1.13.4-2ubuntu1                     retrieves files from the web\nii  whiptail                            0.52.11-2ubuntu10                   Displays user-friendly dialog boxes from shell scripts\nii  whoopsie                            0.1.33                              Ubuntu crash database submission daemon\nii  wimlib9                             1.5.0-1~webupd8~precise             Library to extract, create, modify, and mount WIM files\nii  wimtools                            1.5.0-1~webupd8~precise             Tools to extract, create, modify, and mount WIM files\nii  wireless-tools                      30~pre9-5ubuntu2                    Tools for manipulating Linux Wireless Extensions\nii  wpasupplicant                       0.7.3-6ubuntu2.1                    client support for WPA and WPA2 (IEEE 802.11i)\nii  x11-common                          1:7.6+12ubuntu2                     X Window System (X.Org) infrastructure\nii  x11-utils                           7.6+4ubuntu0.1                      X11 utilities\nii  xauth                               1:1.0.6-1                           X authentication utility\nii  xbitmaps                            1.1.1-1                             Base X bitmaps\nii  xclip                               0.12-1                              command line interface to X selections\nii  xfonts-encodings                    1:1.0.4-1ubuntu1                    Encodings for X.Org fonts\nii  xfonts-utils                        1:7.6+1                             X Window System font utility programs\nii  xkb-data                            2.5-1ubuntu1.3                      X Keyboard Extension (XKB) configuration data\nii  xml-core                            0.13                                XML infrastructure and XML catalog file support\nrc  xpdf                                3.02-21build1                       Portable Document Format (PDF) reader\nii  xterm                               271-1ubuntu2.1                      X terminal emulator\nii  xz-lzma                             5.1.1alpha+20110809-3               XZ-format compression utilities - compatibility commands\nii  xz-utils                            5.1.1alpha+20110809-3               XZ-format compression utilities\nii  zabbix-agent                        1:1.8.11-1                          network monitoring solution - agent\nii  zlib1g                              1:1.2.3.4.dfsg-3ubuntu4             compression library - runtime\nii  zlib1g-dev                          1:1.2.3.4.dfsg-3ubuntu4             compression library - development\nii  zsh                                 4.3.17-1ubuntu1                     shell with lots of features\n\nThe third column is version, but it is all \"messed up\" in a way I can't understand. I mean, different packages use totally different naming specifications. \nHere are the major questions:\n\nWhy do some version numbers have ubuntu in them, and some not?\nWhat does all the special punctuation -~+ mean?\nWhat are alpha, build, and dfsg? Can I just use them casually?\nvim and other packages have 2:. What does that mean?\nHow does \"version comparison\" work, when version formats can be so different?\n\nCan anyone please explain this to me? Or where can I find an official document?\nThanks in advance.",
        "top_answer": "The Debian Policy Manual has this to say about the version field, which answers some parts of your question:\n\n[Format]1\n... The format is: [epoch:]upstream_version[-debian_revision]\nThe three components here are:\nepoch\nThis is a single (generally small) unsigned integer. It may be omitted, in which case zero is assumed.\nEpochs can help when the upstream version numbering scheme changes, but they must be used with care. You should not change the epoch, even in experimental, without getting consensus on debian-devel first.\nupstream_version\nThis is the main part of the version number. It is usually the version number of the original (\"upstream\") package from which the .deb file has been made, if this is applicable. Usually this will be in the same format as that specified by the upstream author(s); however, it may need to be reformatted to fit into the package management system's format and comparison scheme.\nThe comparison behavior of the package management system with respect to the upstream_version is described below. The upstream_version portion of the version number is mandatory.\nThe upstream_version must contain only alphanumerics [6] and the characters \".\" (full stop), \"+\" (plus), \"-\" (hyphen), \"~\" (tilde)2 and should start with a digit. If there is no debian_revision then hyphens are not allowed.\ndebian_revision\nThis part of the version number specifies the version of the Debian package based on the upstream version. It may contain only alphanumerics and the characters \"+\" (plus), \".\" (full stop), \"~\" (tilde) and is compared in the same way as the upstream_version is.\nIt is conventional to restart the debian_revision at 1 each time the upstream_version is increased.\nThe package management system will break the version number apart at the last hyphen in the string (if there is one) to determine the upstream_version and debian_revision. The absence of a debian_revision is equivalent to a debian_revision of 0.\nPresence of the debian_revision part indicates this package is a non-native package (see Source packages). Absence indicates the package is a native package.\n[Comparison]1\nWhen comparing two version numbers, first the epoch of each are compared, then the upstream_version if epoch is equal, and then debian_revision if upstream_version is also equal. epoch is compared numerically. The upstream_version and debian_revision parts are compared by the package management system using the following algorithm:\nThe strings are compared from left to right.\nFirst the initial part of each string consisting entirely of non-digit\ncharacters is determined. These two parts (one of which may be empty)\nare compared lexically. If a difference is found it is returned. The\nlexical comparison is a comparison of ASCII values modified so that\nall the letters sort earlier than all the non-letters and so that a\ntilde sorts before anything, even the end of a part. For example, the\nfollowing parts are in sorted order from earliest to latest: ~~,\n~~a, ~, the empty part, a. [7]\nThen the initial part of the remainder of each string which consists\nentirely of digit characters is determined. The numerical values of\nthese two parts are compared, and any difference found is returned as\nthe result of the comparison. For these purposes an empty string\n(which can only occur at the end of one or both version strings being\ncompared) counts as zero.\nThese two steps (comparing and removing initial non-digit strings and\ninitial digit strings) are repeated until a difference is found or\nboth strings are exhausted.\n5.6.12.1. Epochs should be used sparingly\nNote that the purpose of epochs is to cope with situations where the upstream version numbering scheme changes and to allow us to leave behind serious mistakes. If you think that increasing the epoch is the right solution, you should consult debian-devel and get consensus before doing so (even in experimental).\nEpochs should not be used when a package needs to be rolled back. In that case, use the +really convention: for example, if you uploaded 2.3-3 and now you need to go backwards to upstream 2.2, call your reverting upload something like 2.3+really2.2-1. Eventually, when we upload upstream 2.4, the +really part can go away.\nEpochs are also not intended to cope with version numbers containing strings of letters which the package management system cannot interpret (such as ALPHA or pre-), or with silly orderings. [8]\n\nFurther, from the Ubuntu Maintainers Handbook:\n\nThe ubuntu string then marks that whatever follows it is related to changes added in Ubuntu.\n\nThe alpha string doesn't seem to have any particular\nmeaning.\nThe buildX suffix is used by Ubuntu, e.g. to indicate no change rebuilds.\ndfsg refers to a package that has been modified\nfor compliance with the Debian Free Software Guidelines.\nFootnotes3\n\n1 Square brackets used as journalists do to indicate additions or significant rewrites.\n2 For historical completeness, the upstream_version wording changed from may to must and the \":\" (colon) was removed, the last revision allowing it is c. May 2017\n3 https://meta.stackoverflow.com/questions/258597/formatting-footnotes\n",
        "url": "https://serverfault.com/questions/604541/debian-packages-version-convention"
    },
    {
        "title": "Purpose of Debian \"sites-available\" and \"sites-enabled\" directories?",
        "question": "Can anyone tell me\u2014in a nutshell\u2014what the purpose of these two directories are in Debian?\n/etc/apache2/sites-enabled\n/etc/apache2/sites-available\n\nI notice that diffing sites-available/000-default and sites-enabled/default shows they are identical.\nWhat gives?",
        "top_answer": "sites-available contains the apache config files for each of your sites.  For example:\n<VirtualHost *:80>\n  ServerName site.mysite.com\n  ServerAdmin [email\u00a0protected]\n\n  DirectoryIndex index.php\n  DocumentRoot /home/user/public_html/site.mysite.com/public\n\n  LogLevel warn\n  ErrorLog /home/user/public_html/site.mysite.com/logs/error.log\n  CustomLog /home/user/public_html/site.mysite.com/logs/access.log combined\n</VirtualHost>\n\nWhen you want to add a new site (for example, site.mysite.com), you add it here, and use:\na2ensite site.mysite.com\n\nTo enable the site.  Once the site is enabled, a symlink to the config file is placed in the sites-enabled directory, indicating that the site is enabled.",
        "url": "https://serverfault.com/questions/83508/purpose-of-debian-sites-available-and-sites-enabled-directories"
    },
    {
        "title": "How to change my MySQL root password back to empty?",
        "question": "When I'm working locally, I don't really need to enter my password to access my database. I changed my root password when I first installed MySQL, but I don't know how to change my password back. What should I do?",
        "top_answer": "To change the root password to newpassword:\n mysqladmin -u root -p'oldpassword' password 'newpassword'\n\nTo change it so root doesn't require a password:\n mysqladmin -u root -p'oldpassword' password ''\n\nNote: I think it matters that there isn't a space between the -p and 'oldpassword' but I may be wrong about that",
        "url": "https://serverfault.com/questions/103412/how-to-change-my-mysql-root-password-back-to-empty"
    },
    {
        "title": "What is the ipv6 equivalent of 0.0.0.0/0",
        "question": "When describing IPv4 networks, I can use 0.0.0.0/0 or just 0/0 to specify all networks. What is the equivalent notation for IPv6?",
        "top_answer": "The IPv6 equivalent of IPv4's 0.0.0.0 is ::/0.",
        "url": "https://serverfault.com/questions/698369/what-is-the-ipv6-equivalent-of-0-0-0-0-0"
    },
    {
        "title": "What's the difference between 'useradd' and 'adduser'?",
        "question": "What's the difference between useradd and adduser? When/why should I prefer using one or the other?",
        "top_answer": "In the case of Debian and its related distros, adduser is a friendlier interactive frontend to useradd.",
        "url": "https://serverfault.com/questions/218993/whats-the-difference-between-useradd-and-adduser"
    },
    {
        "title": "Any difference between DOMAIN\\username and [email\u00a0protected]?",
        "question": "I'm trying to troubleshoot an obscure authentication error and need some background information.\n\nIs there any difference between how Windows (and programs like Outlook) process DOMAIN\\username and [email\u00a0protected]?\nWhat are the proper terms for these two username formats?\nEdit:  In particular, are there any differences in how Windows authenticates the two username formats?\n",
        "top_answer": "Assuming you have an Active Directory environment:\nI believe the backslash format DOMAIN\\USERNAME will search domain DOMAIN for a user object whose SAM Account Name is USERNAME.\nThe UPN format username@domain will search the forest for a user object whose User Principle Name is username@domain.\nNow, normally a user account with a SAM Account Name of USERNAME has a UPN of USERNAME@DOMAIN, so either format should locate the same account, at least provided the AD is fully functional.  If there are replication issues or you can't reach a global catalog, the backslash format might work in cases where the UPN format will fail.  There may also be (abnormal) conditions under which the reverse applies - perhaps if no domain controllers can be reached for the target domain, for example.\nHowever: you can also explicitly configure a user account to have a UPN whose username component is different from the SAM Account Name and whose domain component is different from the name of the domain.\nThe Account tab in Active Directory Users and Computers shows the UPN under the heading  \"User logon name\" and the SAM Account Name under the heading \"User logon name (pre-Windows 2000)\".  So if you are having trouble with particular users I would check that there aren't any discrepancies between these two values.\nNote: it is possible that additional searches are done if the search I describe above doesn't find the user account.  For example, perhaps the specified username is converted into the other format (in the obvious way) to see if that produces a match.  There must also be some procedure for finding accounts in trusted domains that are not in the forest.  I don't know where/whether the exact behaviour is documented.\nJust to further complicate troubleshooting, Windows clients will by default cache information about successful interactive logons, so that you may be able to log into the same client even if your user account information in the Active Directory is inaccessible.",
        "url": "https://serverfault.com/questions/371150/any-difference-between-domain-username-and-usernamedomain-local"
    },
    {
        "title": "nginx without server_name and using only static ip address?",
        "question": "this is my first web app deployment and am running into all sorts of issues.\nI am currently going for a nginx + gunicorn implementation for the Django app, but mostly this question relates to nginx configurations. For some context - nginx would receive connections and proxy to the gunicorn local server.\nin the nginx configurations, where it says server_name do I have to provide one? I don't plan to use domain names of any kind, just through my network's external ip (it is static) and the port number to listen to.\nMy desire is that when I access something like http://xxx.xxx.xxx.xxx:9050 I would be able to get the site.\nThe following is the sample code that I will base the configurations on for reference.\n   server {\n        listen   80;\n        server_name WHAT TO PUT HERE?;\n\n    root /path/to/test/hello;\n\n    location /media/ {\n        # if asset versioning is used\n        if ($query_string) {\n            expires max;\n        }\n    }\n    location /admin/media/ {\n        # this changes depending on your python version\n        root /path/to/test/lib/python2.6/site-packages/django/contrib;\n    }\n    location / {\n        proxy_pass_header Server;\n        proxy_set_header Host $http_host;\n        proxy_redirect off;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Scheme $scheme;\n        proxy_connect_timeout 10;\n        proxy_read_timeout 10;\n        proxy_pass http://localhost:8000/;\n    }\n        # what to serve if upstream is not available or crashes\n        error_page 500 502 503 504 /media/50x.html;\n     }\n",
        "top_answer": "server_name defaults to an empty string, which is fine; you can exclude it completely.\nAnother common approach for the \"I don't want to put a name on this\" need is to use server_name _;\nYour http://xxx.xxx.xxx.xxx:9050 URL won't work with this config, though; you're only listening on port 80.  You'd need to add a listen 9050; as well.",
        "url": "https://serverfault.com/questions/351559/nginx-without-server-name-and-using-only-static-ip-address"
    },
    {
        "title": "How to [politely?] tell software vendor they don't know what they're talking about",
        "question": "Not a technical question, but a valid one nonetheless.  Scenario:\nHP ProLiant DL380 Gen 8 with 2 x 8-core Xeon E5-2667 CPUs and 256GB RAM running ESXi 5.5.  Eight VMs for a given vendor's system. Four VMs for test, four VMs for production. The four servers in each environment perform different functions, e.g.: web server, main app server, OLAP DB server and SQL DB server.\nCPU shares configured to stop the test environment from impacting production. All storage on SAN.\nWe've had some queries regarding performance, and the vendor insists that we need to give the production system more memory and vCPUs. However, we can clearly see from vCenter that the existing allocations aren't being touched, e.g.: a monthly view of CPU utilization on the main application server hovers around 8%, with the odd spike up to 30%. The spikes tend to coincide with the backup software kicking in.\nSimilar story on RAM - the highest utilization figure across the servers is ~35%.\nSo, we've been doing some digging, using Process Monitor (Microsoft SysInternals) and Wireshark, and our recommendation to the vendor is that they do some TNS tuning in the first instance. However, this is besides the point. \nMy question is: how do we get them to acknowledge that the VMware statistics that we've sent them are evidence enough that more RAM/vCPU won't help?\n--- UPDATE 12/07/2014 ---\nInteresting week.  Our IT management have said that we should make the change to the VM allocations, and we're now waiting for some downtime from the business users.  Strangely, the business users are the ones saying that certain aspects of the app are running slowly (compared to what, I don't know), but they're going to \"let us know\" when we can take the system down (grumble, grumble!).\nAs an aside, the \"slow\" aspect of the system is apparently not the HTTP(S) element, i.e.: the \"thin app\" used by most of the users.  It sounds like it's the \"fat client\" installs, used by the main finance bods, that is apparently \"slow\".  This means that we're now considering the client and the client-server interaction in our investigations.\nAs the initial purpose of the question was to seek assistance as to whether to go down the \"poke it\" route, or just make the change, and we're now making the change, I'll close it using longneck's answer.\nThank you all for your input; as usual, serverfault has been more than just a forum - it's kind of like a psychologist's couch as well :-)",
        "top_answer": "I suggest that you make the adjustments they have requested. Then benchmark the performance to show them that it made no difference. You could even go so far to benchmark it with LESS memory and vCPU to make your point.\nAlso, \"We're paying you to support the software with actual solutions, not guesswork.\"",
        "url": "https://serverfault.com/questions/611050/how-to-politely-tell-software-vendor-they-dont-know-what-theyre-talking-abo"
    },
    {
        "title": "How can I check /dev/xvda1?",
        "question": "On login to EC2 (Ubuntu) instance, I see \n*** /dev/xvda1 should be checked for errors ***\n\nI can't fsck /dev/xvda1 because it is mounted, and sudo umount /dev/xvda1 fails because it is in use.  lsof shows \njbd2/xvda  172               root  cwd       DIR              202,1     4096          2 /\njbd2/xvda  172               root  rtd       DIR              202,1     4096          2 /\njbd2/xvda  172               root  txt   unknown                                        /proc/172/exe\n\nand kill -SIGKILL 172 is ineffective.\nWhat to do?",
        "top_answer": "Most Linuxes these days should perform a forced fsck at boot time when the file /forcefsck is present on the system. \nIf you are at liberty to reboot the VM,  run\ntouch /forcefsck\n\nThen reboot at your convenience ",
        "url": "https://serverfault.com/questions/627371/how-can-i-check-dev-xvda1"
    },
    {
        "title": "Connecting to HTTPS with netcat (nc) [closed]",
        "question": "I'm working on a homework assignment for my college course.\nThe task is to fetch web pages on HTTPS using nc (netcat).\nTo fetch a page over HTTP, I can simply do the following:\ncat request.txt | nc -w 5 <someserver> 80\n\nIn request.txt I have an HTTP 1.1 request\nGET / HTTP/1.1\nHost: <someserver>\n\nNow... This works perfectly fine. The challenge is, however - to fetch a web page that uses HTTPS?\nI get a page certificate like this. And this is the point at which I'm currently stuck\nopenssl s_client -connect <someserver>:443\n",
        "top_answer": "nc doesn't do https.  openssl s_client is as close as you'll get.  Do something like this:\n$ cat request.txt | openssl s_client -connect server:443\n",
        "url": "https://serverfault.com/questions/102032/connecting-to-https-with-netcat-nc"
    },
    {
        "title": "How to configure vsftpd to work with passive mode",
        "question": "Whenever I install vsftpd on centos, I only setup the jail environment for the users and rest is default configuration of vsftpd. I create user and try to connect with filezila ftp client, but I could not connect with passive mode. I always change the transfer settings to active mode to successfully connect to the ftp server otherwise I get     \n Error: Failed to retrieve directory listing\n\nSo is there a way to change any directive in vsftp.conf file and we can connect with passive mode to the server?",
        "top_answer": "To configure passive mode for vsftpd you need to set some parameters in vsftpd.conf.\npasv_enable=Yes\npasv_max_port=10100\npasv_min_port=10090\n\nThis enables passive mode and restricts it to using the eleven ports for data connections. This is useful as you need to open these ports on your firewall.\niptables -I INPUT -p tcp --destination-port 10090:10100 -j ACCEPT\n\nIf after testing this all works then save the state of your firewall with\nservice iptables save\n\nwhich will update the /etc/sysconfig/iptables file.\nTo do this is CentOS 7 you have to use the new firewalld, not iptables: \nFind your zone:\n# firewall-cmd --get-active-zones\npublic\n  interfaces: eth0\n\nMy zone is 'public', so I set my zone to public, add the port range, and after that we reload:\n# firewall-cmd --permanent --zone=public --add-port=10090-10100/tcp\n# firewall-cmd --reload\n\nWhat happens when you make a connection\n\nYour client makes a connection to the vsftpd server on port 21.\nThe sever responds to the client telling it which port to connect to from the range specified above.\nThe client makes a data connection on the specified port and the session continues.\n\nThere is a great explanation of the different ftp modes here.",
        "url": "https://serverfault.com/questions/421161/how-to-configure-vsftpd-to-work-with-passive-mode"
    },
    {
        "title": "How to know if a machine is an EC2 instance",
        "question": "I would like to run some scripts on hosts which are EC2 instances but I don't know how to be sure that the host is really an EC2 instance.\nI have made some tests, but this is not sufficient:\n\nTest that binary ec2_userdata is available (but this will not always be true)\nTest availability of \"http://169.254.169.254/latest/meta-data\" (but will this be always true ? and what is this \"magical IP\" ?)\n",
        "top_answer": "First, I felt the need to post a new answer because of the following subtle problems with the existing answers, and after receiving a question about my comment on @qwertzguy's answer. Here are the problems with the current answers:\n\nThe accepted answer from @MatthieuCerda definitely does not work reliably, at least not on any VPC instances I checked against. (On my instances, I get a VPC name for hostname -d, which is used for internal DNS, not anything with \"amazonaws.com\" in it.)\nThe highest-voted answer from @qwertzguy does not work on new m5 or c5 instances, which do not have this file. Amazon neglects to document this behavior change AFAIK, although the doc page on this subject does say \"... If /sys/hypervisor/uuid exists ...\".  I asked AWS support whether this change was intentional, see below \u2020.\nThe answer from @Jer does not necessarily work everywhere because the instance-data.ec2.internal DNS lookup may not work. On an Ubuntu EC2 VPC instance I just tested on, I see:\n$ curl http://instance-data.ec2.internal\ncurl: (6) Could not resolve host: instance-data.ec2.internal\n\nwhich would cause code relying on this method to falsely conclude it is not on EC2!\nThe answer to use dmidecode from @tamale may work, but relies on you a.) having dmidecode available on your instance, and b.) having root or sudo password-less ability from within your code.\nThe answer to check /sys/devices/virtual/dmi/id/bios_version from @spkane is dangerously misleading! I checked one Ubuntu 14.04 m5 instance, and got a bios_version of 1.0. This file is not documented at all on Amazon's doc, so I would really not rely on it.\nThe first part of the answer from @Chris-Montanaro to check an unreliable 3rd-party URL and use whois on the result is problematic on several levels. Note the URL suggested in that answer is a 404 page right now! Even if you did find a 3rd-party service that did work, it would be comparatively very slow (compared to checking a file locally) and possibly run into rate-limiting issues or network issues, or possibly your EC2 instance doesn't even have outside network access.\nThe second suggestion in the answer from @Chris-Montanaro to check http://169.254.169.254/ is a little better, but another commenter notes that other cloud providers make this instance metadata URL available, so you have to be careful to avoid false positives. Also it will still be much slower than a local file, I have seen this check be especially slow (several seconds to return) on heavily loaded instances. Also, you should remember to pass a -m or --max-time argument to curl to avoid it hanging for a very long time, especially on a non-EC2 instance where this address may lead to nowhere and hang (as in @algal's answer).\n\nAlso, I don't see that anyone has mentioned Amazon's documented fallback of checking for the (possible) file /sys/devices/virtual/dmi/id/product_uuid.\nWho knew that determining whether you are running on EC2 could be so complicated?! OK, now that we have (most) of the problems with listed approaches listed, here is a suggested bash snippet to check whether you are running on EC2. I think this should work generally on almost any Linux instances, Windows instances are an exercise for the reader.\n#!/bin/bash\n\n# This first, simple check will work for many older instance types.\nif [ -f /sys/hypervisor/uuid ]; then\n  # File should be readable by non-root users.\n  if [ `head -c 3 /sys/hypervisor/uuid` == \"ec2\" ]; then\n    echo yes\n  else\n    echo no\n  fi\n\n# This check will work on newer m5/c5 instances, but only if you have root!\nelif [ -r /sys/devices/virtual/dmi/id/product_uuid ]; then\n  # If the file exists AND is readable by us, we can rely on it.\n  if [ `head -c 3 /sys/devices/virtual/dmi/id/product_uuid` == \"EC2\" ]; then\n    echo yes\n  else\n    echo no\n  fi\n\nelse\n  # Fallback check of http://169.254.169.254/. If we wanted to be REALLY\n  # authoritative, we could follow Amazon's suggestions for cryptographically\n  # verifying their signature, see here:\n  #    https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html\n  # but this is almost certainly overkill for this purpose (and the above\n  # checks of \"EC2\" prefixes have a higher false positive potential, anyway).\n  if $(curl -s -m 5 http://169.254.169.254/latest/dynamic/instance-identity/document | grep -q availabilityZone) ; then\n    echo yes\n  else\n    echo no\n  fi\n\nfi\n\nObviously, you could expand this with even more fallback checks, and include paranoia about handling e.g. a false positive from /sys/hypervisor/uuid happening to start with \"ec2\" by chance and so on. But this is a good-enough solution for illustration purposes and probably nearly all non-pathological use-cases.\n[\u2020] Got back this explanation from AWS support about the change for c5/m5 instances:\n\nThe C5 and M5 instances use a new hypervisor stack and the associated kernel drivers do not create files in sysfs (which is mounted at /sys) as the Xen drivers used by the other/older instance types do. The best way to detect whether the operating system is running on an EC2 instance is to account for the different possibilities listed in the documentation you linked.\n",
        "url": "https://serverfault.com/questions/462903/how-to-know-if-a-machine-is-an-ec2-instance"
    },
    {
        "title": "How can I hide the output of a shell application in Linux?",
        "question": "How can I hide the screen output (printf) of a shell application in Linux?",
        "top_answer": "You can redirect the output of any program so that it won't be seen.\n$ program > /dev/null\n\nThis will redirect the standard output - you'll still see any errors\n$ program &> /dev/null\n\nThis will redirect all output, including errors.",
        "url": "https://serverfault.com/questions/41964/how-can-i-hide-the-output-of-a-shell-application-in-linux"
    },
    {
        "title": "I accidentaly forbid SSH connection to a remote server... What's next?",
        "question": "Let's say it again, we all make mistakes, and I have just made one.\nA brief history: I was doing some stuff on a VPS (Debian) I'm renting, when I noticed some strange behaviour. Using the netstat command I saw an non-authorized connection through SSH. I didn't know what to do, so I decided to close his connection using iptables:\niptables -A INPUT -p tcp --dport ssh -s IP -j DROP\n\nBut I am tired, and I wrote\niptables -A INPUT -p tcp --dport ssh -j DROP\n\nand I kicked myself (and everyone else) out...\nHow do I fix this?",
        "top_answer": "There are several alternatives:\n\nSee if they have IPMI / \"KVM\" / console access to the server which lets you control it as if you had a physical keyboard plugged into it.\nIf they don't offer that, see if you can boot the VM to a recovery linux CD (some providers offer this) and then correct the firewall rules that way and then boot it like normal.\nIf you don't have console access, before you boot to recovery or attach the volume to another VM (as in the Amazon case, credit user3550767's answer), you can try Ankh2054's answer of rebooting first if you haven't saved the rules (likely the case since you kicked yourself out before you had a chance to save).  Use the control panel or ask someone to power cycle it using a non-graceful reset / poweroff (aka hard reboot or hard shutdown) in case the init script saves the rules automatically when gracefully rebooting (credit @jfalcon, @joshudson).\nWeigh the drawbacks of this (such as data being written during reboot may be lost and filesystem check may be required on boot so longer boot up time, though that delay may be less than booting to recovery).\n",
        "url": "https://serverfault.com/questions/684688/i-accidentaly-forbid-ssh-connection-to-a-remote-server-whats-next"
    },
    {
        "title": "iptables Tips & Tricks [closed]",
        "question": "I'm sure Linux sysadmins are quite familiar with iptables, the userland interface to the netfilter packet-filtering framework.\nNow, this \"Question\" is meant to be a Community Wiki for collecting together various bits-n-pieces of iptables wisdom. Nothing is too common or too obscure. Post anything you know that would help others make the most of iptables.",
        "top_answer": "Using whitelist and blacklist with iptables\n#!/bin/bash\n\nWHITELIST=/whitelist.txt\nBLACKLIST=/blacklist.txt\n\n#THIS WILL CLEAR ALL EXISTING RULES!\necho 'Clearing all rules'\niptables -F\n\n#\n## Whitelist\n#\n\nfor x in `grep -v ^# $WHITELIST | awk '{print $1}'`; do\n        echo \"Permitting $x...\"\n        $IPTABLES -A INPUT -t filter -s $x -j ACCEPT\ndone\n\n#\n## Blacklist\n#\n\nfor x in `grep -v ^# $BLACKLIST | awk '{print $1}'`; do\n        echo \"Denying $x...\"\n        $IPTABLES -A INPUT -t filter -s $x -j DROP\ndone\n\nScript to open ports\n#!/bin/bash\nALLOWEDTCP=\"80 3128 3784\"\nALLOWEDUDP=\"3128 3784\"\n\n#\n## Permitted Ports\n#\n\nfor port in $ALLOWEDTCP; do\n       echo \"Accepting port TCP $port...\"\n       $IPTABLES -A INPUT -t filter -p tcp --dport $port -j ACCEPT\ndone\n\nfor port in $ALLOWEDUDP; do\n        echo \"Accepting port UDP $port...\"\n        $IPTABLES -A INPUT -t filter -p udp --dport $port -j ACCEPT\ndone\n\nBlocking portscan\n# Attempt to block portscans\n# Anyone who tried to portscan us is locked out for an entire day.\niptables -A INPUT   -m recent --name portscan --rcheck --seconds 86400 -j DROP\niptables -A FORWARD -m recent --name portscan --rcheck --seconds 86400 -j DROP\n\n# Once the day has passed, remove them from the portscan list\niptables -A INPUT   -m recent --name portscan --remove\niptables -A FORWARD -m recent --name portscan --remove\n\n# These rules add scanners to the portscan list, and log the attempt.\niptables -A INPUT   -p tcp -m tcp --dport 139 -m recent --name portscan --set -j LOG --log-prefix \"Portscan:\"\niptables -A INPUT   -p tcp -m tcp --dport 139 -m recent --name portscan --set -j DROP\n\niptables -A FORWARD -p tcp -m tcp --dport 139 -m recent --name portscan --set -j LOG --log-prefix \"Portscan:\"\niptables -A FORWARD -p tcp -m tcp --dport 139 -m recent --name portscan --set -j DROP\n\nSpoofed/Invalid packets\n# Reject spoofed packets\n# These adresses are mostly used for LAN's, so if these would come to a WAN-only server, drop them.\niptables -A INPUT -s 10.0.0.0/8 -j DROP\niptables -A INPUT -s 169.254.0.0/16 -j DROP\niptables -A INPUT -s 172.16.0.0/12 -j DROP\niptables -A INPUT -s 127.0.0.0/8 -j DROP\n\n#Multicast-adresses.\niptables -A INPUT -s 224.0.0.0/4 -j DROP\niptables -A INPUT -d 224.0.0.0/4 -j DROP\niptables -A INPUT -s 240.0.0.0/5 -j DROP\niptables -A INPUT -d 240.0.0.0/5 -j DROP\niptables -A INPUT -s 0.0.0.0/8 -j DROP\niptables -A INPUT -d 0.0.0.0/8 -j DROP\niptables -A INPUT -d 239.255.255.0/24 -j DROP\niptables -A INPUT -d 255.255.255.255 -j DROP\n\n# Drop all invalid packets\niptables -A INPUT -m state --state INVALID -j DROP\niptables -A FORWARD -m state --state INVALID -j DROP\niptables -A OUTPUT -m state --state INVALID -j DROP\n\nBlock Smurf attacks\n# Stop smurf attacks\niptables -A INPUT -p icmp -m icmp --icmp-type address-mask-request -j DROP\niptables -A INPUT -p icmp -m icmp --icmp-type timestamp-request -j DROP\niptables -A INPUT -p icmp -m icmp -j DROP\n\n# Drop excessive RST packets to avoid smurf attacks\niptables -A INPUT -p tcp -m tcp --tcp-flags RST RST -m limit --limit 2/second --limit-burst 2 -j ACCEPT\n\nBlock ICMP (aka ping)\n# Don't allow pings through\niptables -A INPUT -p icmp -m icmp --icmp-type 8 -j DROP\n",
        "url": "https://serverfault.com/questions/245711/iptables-tips-tricks"
    },
    {
        "title": "How to run cron job on a specific hour every day?",
        "question": "What do I need to write in crontab to execute a script at 3pm every day?",
        "top_answer": "You are looking for something like this (via crontab -e):\n0 15 * * * your.command.goes.here\n\n15 is the hour and 0 is the minute that the script is run.  Day of month, month, and day of week get wildcards so that the script gets run daily.",
        "url": "https://serverfault.com/questions/258489/how-to-run-cron-job-on-a-specific-hour-every-day"
    },
    {
        "title": "How to Modify a Cronjob Email Subject",
        "question": "I am running a half dozen different cron jobs from my hosting at Hostmonster.com.  When a cronjob has been executed I receive an email with the output of the script.\nThe email comes in the format of:\nFrom: Cron Daemon\nSubject: Cron  /ramdisk/bin/php5 -c /home5/username/scheduled/optimize_mysql.bash\n\nThe problem with this is that the subject of the email makes it very hard to read which cronjob the email is pertaining to.\nIs there a way to modify the subject of a cronjob email so that it's easier to read?\nFor example:\n\nFrom: Cron Daemon\nSubject: Optimize MySQL Database\n",
        "top_answer": "On my systems (most Debian) all output, from a script/program called as a crontab-entry, is sent by email to the account@localhost who initiated the cron. \nThese emails have a subject like yours. \nIf you want to receive an email, write a script that has no output on its own. But instead put all output in a textfile. \nAnd with\nmail -s 'your subject' adress@where < textfile\n\nyou receive it the way you want.",
        "url": "https://serverfault.com/questions/56497/how-to-modify-a-cronjob-email-subject"
    },
    {
        "title": "Where does UFW (uncomplicated firewall) save command-line rules to?",
        "question": "You add a rule like this:\nufw allow 22/tcp\n\nThe rule is saved, and is applied even after reboot. But it's not written anywhere in /etc/ufw. Where is it saved to? (Ubuntu, using ufw as pre-installed.)",
        "top_answer": "In my Ubuntu 11 server, the firewall rules are saved in /lib/ufw/user.rules",
        "url": "https://serverfault.com/questions/475468/where-does-ufw-uncomplicated-firewall-save-command-line-rules-to"
    },
    {
        "title": "What are the advantages of tape drives?",
        "question": "IBM still develop and sell tape drives today. The capacity of them seems to be on a par with today's hard drives, but the search time and transfer rate are both significantly lower than that of hard drives.\nSo when is tape drives preferable to hard drives (or SSDs) today?",
        "top_answer": "For me, the single biggest argument in favour of tape is that doubling your storage capacity is cheap.  That is, to go from 1TB of HDD storage to 2TB is the same as going from nothing to that first TB.  With tape, you pay a large premium for the drive, but storage after that is comparitively cheap.  You don't have to have lengthy budget meetings about increasing the size of the storage NAS by 15TB, you just order another box of LTO5s.\n(Chopper makes a valid point about compulsory labels, but tape labels are in a standard format, and there are free software solutions to printing your own onto label stock.)\nTapes are much easier to ship, and easier to store, than HDD and HDD-like media.  They're more resistant to shocks, and their temperature tolerances are higher.\nThey also benefit from the existence of autoloaders.  This allows you to spread a large dump over multiple storage containers, which means you don't have to worry about how to break up your backups.  While it's perfectly possible to make an autoloader for HDD-type media, I've never seen one, and I suspect the lack of standardisation in physical package size will make it difficult to bring one to market at a reasonable price.\nYour point about transfer rates is valid, but in the context of backups it's of minimal import.  The time required to back up a 1TB file system to anything is large enough that you shouldn't be doing it on a live file system; and if you're dumping a snapshot to tape, who cares if it takes an extra hour or two?  Search times are an equally minor concern, because all decent backup software maintains indices, so one can generally go straight to the relevant portion of the tape to restore a file.\nEdit: after an incident earlier this week, one more advantage of tape has struck me most forcefully.  A client got infected with ransomware, which promptly encrypted several hundred gigabytes of their main corporate file server.  Online backups are all very well, but any system that can write to those backups can rewrite or erase them as well - even if you would rather it hadn't.  That certainly doesn't argue against all HDD-based backup systems, but it is a weakness in the simple \"let's just have a big NAS and do all our backups there via cron\" approach.\nMy client has tape storage, by the way - so apart from a couple of lost days, no harm was done.",
        "url": "https://serverfault.com/questions/550763/what-are-the-advantages-of-tape-drives"
    },
    {
        "title": "rsync - Exclude files that are over a certain size?",
        "question": "I am doing a backup of my desktop to a remote machine. I'm basically doing rsync -a ~ example.com:backup/ However there are loads of large files, e.g. wikipedia dumps etc. Most of the files I care a lot about a small, such as firefox cookie files, or .bashrc. Is there some invocation to rsync that will exclude files that are over a certain size? That way I could copy all files that are less than 10MB first, then do all files. That way I can do a fast backup of the most important files, then a longer backup of everything else.",
        "top_answer": "There is a max-size option:\n--max-size=SIZE         don't transfer any file larger than SIZE\n\nSo:\n# rsync -rv --max-size=1.5m root@tss01:/tmp/dm\n\nWill send only files less than 1.5m.\nRegarding sizes from man:\nThe suffixes are as follows: \"K\" (or \"KiB\") is  a  kibibyte  (1024),  \"M\"  (or  \"MiB\")  is  a  mebibyte (1024*1024),  and \"G\" (or \"GiB\") is a gibibyte (1024*1024*1024).  If you want the multiplier to be 1000 instead of 1024, use \"KB\", \"MB\", or  \"GB\".   (Note:  lower-case  is  also  accepted  for  all  values.) Finally, if  the suffix ends in either \"+1\" or \"-1\", the value will be offset by one byte in the indicated direction*",
        "url": "https://serverfault.com/questions/105206/rsync-exclude-files-that-are-over-a-certain-size"
    },
    {
        "title": "Can an SSL certificate be on a single line in a file (no line breaks)?",
        "question": "SSL certificates by default have line breaks after 67 characters. I'm trying to create SSL certificate files using Chef. Essentially I want to create the entire certificate file from a string variable without any line breaks. I've tried this a few times to no avail (Apache complains about not being able to find certificate). \nI don't see why line breaks in an SSL cert would be necessary. Any ideas if it's possible to have a cert without any line breaks in the file?",
        "top_answer": "No, the certificate won't be handled properly without the line breaks - software will fail to parse it.\nIf you're trying to pass it in a string, why not just include them in it? (\\n)",
        "url": "https://serverfault.com/questions/466683/can-an-ssl-certificate-be-on-a-single-line-in-a-file-no-line-breaks"
    },
    {
        "title": "Do Apache config files support block commenting?",
        "question": "You can use # to comment out individual lines.\nIs there a syntax for commenting out entire blocks?\nI've tired surrounding the block (specifically a <Directory> block) with <IfModule asdfasdf>...</IfModule>, but that didn't work.",
        "top_answer": "I came across this post from a Google search for \"Apache block comment\".  Later, I discovered a non-perl, non-import solution from Apache's core documentation (although I'm sure this is very non-intended practice).  From the core documentation for Apache 2.0 http://httpd.apache.org/docs/2.0/mod/core.html, you can see that the tag <IfDefine> will handily ignore statements when the parameter you specify does not exist:\n<IfDefine IgnoreBlockComment>\n...\n</IfDefine>\n\nSo that'll successfully \"comment\" out the statements in between.",
        "url": "https://serverfault.com/questions/345848/do-apache-config-files-support-block-commenting"
    },
    {
        "title": "What user do scripts in the cron folders run as? (i.e. cron.daily, cron.hourly, etc)",
        "question": "If I put a script in /etc/cron.daily on CentOS what user will it run as? Do they all run as root or as the owner?",
        "top_answer": "They all run as root.  If you need otherwise, use su in the script or add a crontab entry to the user's crontab (man crontab) or the system-wide crontab (whose location I couldn't tell you on CentOS).",
        "url": "https://serverfault.com/questions/247043/what-user-do-scripts-in-the-cron-folders-run-as-i-e-cron-daily-cron-hourly"
    },
    {
        "title": "Mount Remote CIFS/SMB Share as a Folder not a Drive Letter",
        "question": "Is there any way to mount a remote CIFS/SMB/SAMBA share as a folder/directory and not as a drive letter.  For example, I want this map:\n\\\\Server\\ShareName -> C:\\Folder\\ShareName\nInstead of the usual map like this:\n\\\\Server\\ShareName -> Z:\\\nThe server is Linux/Samba and the client is Windows 7 Professional 64-bit.  The closest I've found is being able to mount a local volume as a subfolder using the Windows disk manager, but it doesn't appear to handle remote CIFS shares (see http://support.microsoft.com/kb/307889).",
        "top_answer": "Just to map a network share directory you would use this command:\nnet use \\\\Server\\ShareName\\Directory\nThis mapping would:\n\nnot be persistent\nwould have to be established and authenticated at user login\nyou would access the share using the UNC path, and not a local drive letter\n\nIf you want to access the network share through a location on your local C: drive, you'll want to set up a symbolic link:\nmklink /d  C:\\Folder\\ShareName \\\\Server\\ShareName\\Directory\n\nNow when you navigate to C:\\Folder\\Share you'll see the contents of \\\\\\Server\\Sharename\\Directory.  You'll still need to provide authentication for the resource with something like net use (or just be logged into a domain account on a domain system that has access) otherwise the link will probably error out angrily.",
        "url": "https://serverfault.com/questions/105633/mount-remote-cifs-smb-share-as-a-folder-not-a-drive-letter"
    },
    {
        "title": "Mapping UID and GID of local user to the mounted NFS share",
        "question": "I have a server with NFSv4.\nI am mounting contents of the home folder of remote user to local host.\nAble to read and write contents, but when I am checking ownership of files at the mounted volume from the local host, they all belongs to corresponding remote user and group (512).\nIs there any way to make it look like they belong to the local user and group (1000) on the local host?\n/etc/exports on remote host (IP is 192.168.1.110)\n/home/user512 192.168.1.142(rw,sync,all_squash,anonuid=512,anongid=512)\n\n/etc/fstab on local host (IP is 192.168.1.142)\n192.168.1.110:/home/user512    /home/localuser/projects/project512   nfs    rw,hard,intr,rsize=32768,wsize=32768    0       0\n",
        "top_answer": "This is what idmapping is suppose to do. First of all, enable is on the client and server:\n# echo N > /sys/module/nfsd/parameters/nfs4_disable_idmapping\n\nclean idmap cache and restart idmap daemon:\n# nfsidmap -c\n# service rpcidmapd restart\n\nNow on server and the client will send instead of numeric IDs string principals like [email\u00a0protected]. You need to have bob account on the both hosts - client and server.\nNevertheless, the numeric ID's can be different.",
        "url": "https://serverfault.com/questions/514118/mapping-uid-and-gid-of-local-user-to-the-mounted-nfs-share"
    },
    {
        "title": "Proxy HTTPS requests to a HTTP backend with NGINX",
        "question": "I have nginx configured to be my externally visible webserver which talks to a backend over HTTP. \nThe scenario I want to achieve is:\n\nClient makes HTTP request to nginx which is redirect to the same URL but over HTTPS\nnginx proxies request over HTTP to the backend\nnginx receives response from backend over HTTP.\nnginx passes this back to the client over HTTPS\n\nMy current config (where backend is configured correctly) is:\n\nserver {\n        listen       80;\n        server_name  localhost;\n\n        location ~ .* {\n            proxy_pass http://backend;\n            proxy_redirect http://backend https://$host;\n            proxy_set_header Host $host;\n            }\n        }\n\nMy problem is the response to the client (step 4) is sent over HTTP not HTTPS. Any ideas?",
        "top_answer": "The type of proxy you are trying to set up is called a reverse proxy.  A quick search for reverse proxy nginx got me this page:\nhttp://intranation.com/entries/2008/09/using-nginx-reverse-proxy/\nIn addition to adding some useful features like an X-Forwarded-For header (which will give your app visibility into the actual source IP), it specifically does:\nproxy_redirect off\n\nGood luck! :)",
        "url": "https://serverfault.com/questions/145383/proxy-https-requests-to-a-http-backend-with-nginx"
    },
    {
        "title": "Windows 2008: WinSXS directory growing uncontrollably, blocking server",
        "question": "I run a (remotely hosted) virtual Server with Windows 2008 Server for a client. \nInitially, it had 10 GB of space. \nDuring the course of a few weeks - during which nothing was done on the machine except normal work using a web-basedt icket system - , Windows began to fill up its infamous \"winsxs\" directory so much that in the end, the hard disk was full and we had to order another 5 GB.\nNow, three weeks later, these 5 GB have been consumed by winsxs as well, and again I can't work on the machine. Winsxs is now 8 GB big, the rest of the windows directory 5 GB.\nI have found various sources on the web that describe the same problem. Apparently, Windows 2008 stores all language versions for all DLLs it downloads in the normal updating process. Just deleting stuff there is described as mortally dangerous as it contains vital components. I have not found any kind of tool or instructions to identify and remove those files that are no longer needed.\nWhat can I do? \nIs this normal behaviour and if it is, how do other servers with equally limited space manage? Is there something I can turn off or on?\nOf the pre-defined server roles, only \"File services\" (or whatever it's called in english, it's a swiss server) is activated. In addition, I have installed Apache, mySQL, and Subversion. Automatic updates are activated.\nEdit: The problem persists.\nNote: I am aware that the WinSXS directory consist mainly of symlinks and that users often panic looking at its size. Still, Out of 15 GB of space, I have 1.5 MB used by programs and data, and nothing left. I'm glad I can even access the damn machine. *I have already freed up 1 GB of data, which was filled by the windows Windows within 24 hours. It's like in a horror movie. \nWhat I have tried:\n\nInstalling SP2 (which comes with compcln.exe) is not an option, as the disk space is not enough for even that. \nThere is no vsp1clean.exe on the machine, probably because SP1 has already been merged into the system. In fact, there exists no file named *cln.exe anywhere.\nThere are no shadow copies. Shadow copies are not active.\nAs far as I can tell, there are no system restore points active.\nThe only server role activated is \"file server\".\nThe standard \"cleanup\" function (right-click on C: drive) is offering me a baffling 2 MB in trash contents and temporary internet files.\nUsing one of the \"cleanup winsxs\" scripts around is not an option for me, they all look too shady. I can't find anything directly from Microsoft addressing this issue.\n",
        "top_answer": "The WinSxS directory doesn't take up nearly the space reported by Explorer since it uses Hard Links to physical files, not actual files.  Explorer just has issues reporting the size of hard links.\nThis article on Disk Space (referenced here\nhttp://aspoc.net/archives/2008/11/20/winsxs-disk-space-and-windows-7/) has a great explanation on the WinSxS directory.\nAs to your actual disk usage problem - You can try to run COMPCLN.EXE to see if you can clean up any old service pack and hot fix files, which should help quite a bit.  I would also look at any logging directories to see if theres something else going on.",
        "url": "https://serverfault.com/questions/79485/windows-2008-winsxs-directory-growing-uncontrollably-blocking-server"
    },
    {
        "title": "Best Practise and Solutions for Sharing Passwords [closed]",
        "question": "We have various passwords that need to be known to more than one person in our company.  For example, the admin password to our internet routers, the password for our web-host, and also a few \"non-IT\" passwords like safe codes.\nCurrently, we use an ad hoc system of \"standard passwords\" for low-value systems, and verbal sharing of passwords for more important/potentially damaging systems.  I think most people would agree that this is not a good system.\nWhat we would like is a software solution for storing \"shared\" passwords, with access for each limited to the people who actually need it.  Ideally, this would prompt, or enforce, periodic password changes.  It should also be able to indicate who has access to a particular password (e.g., who knows the root password for server XYZ?)\nCan you suggest any software solutions for storing and sharing passwords?  Is there anything particular to be wary of?\nWhat is the common practise in small-medium sized companies for this?",
        "top_answer": "I face this problem every time I go to a new startup.  First thing I do is make a couple of \"Password safes\" with a program like this one (or one of its derivatives):\nhttp://passwordsafe.sourceforge.net/\nSet strong combinations and throw them up on a network share.  Segment by area of responsibility... central infrastructure, production servers, dev/QA, etc.\nOnce there's enough momentum, and assuming I have the proper Windows environment dependencies, I like to move everyone to this:\nhttp://www.clickstudios.com.au/passwordstate.html\nIt has features for both shared and personal credentials.",
        "url": "https://serverfault.com/questions/10285/best-practise-and-solutions-for-sharing-passwords"
    },
    {
        "title": "How do view older journalctl logs (after a rotation maybe?)",
        "question": "I am running docker on ubuntu 16.04 and would like to view the logs. However, I am unable to view logs after what I am guessing is some sort of rotation or the logs grow to a certain size.\nI have not made any changes to my journald.conf, so I am using defaults there. There are containers running so the docker log outputs quite a lot of data.\nExamples of what I am seeing:\nsystemctl docker status confirms service has been active:\nsince Thu 2016-10-13 18:56:28 UTC\n\nHowever, when I run something like: \njournalctl -u docker.service --since \"2016-10-13 22:00\"\n\nThe only output I get is:\n-- Logs begin at Fri 2016-10-14 01:18:49 UTC, end at Fri 2016-10-14 16:18:25 UTC. --\n\nI can view the logs in that range as expected.\nMy question is: why can I not view older logs with journalctl, and how can I fix this issue so I can view the logs?",
        "top_answer": "It could be because you are trying to review the journal since the last boot, which seems likely to be the case inside a docker image. \nOn Ubuntu 16.04, the journal storage defaults to being in-memory. You can change the default to be persistent by opening /etc/systemd/journald.conf and changing the Storage= line from auto to persistent.  You may need to restart journald by systemctl restart systemd-journald after the config file edit. \nI think the journal should be persistent-by-default, so I opened a bug about that.",
        "url": "https://serverfault.com/questions/809093/how-do-view-older-journalctl-logs-after-a-rotation-maybe"
    },
    {
        "title": "\"reboot\" or \"shutdown -r now\": what restart command is safer?",
        "question": "We have in our organization around ~500 RedHat Linux machines.\nOn all the machines we installed applications and services under /etc/init.d, and oracle RAC servers. We intend to perform yum updates on all machines and after that take a reboot.\nSo I was wondering what command is safer:\nreboot\n\nor\nshutdown -r now\n",
        "top_answer": "For Red Hat systems, there is no functional difference between reboot and shutdown -r now. \nDo whatever is easier for you.",
        "url": "https://serverfault.com/questions/787144/reboot-or-shutdown-r-now-what-restart-command-is-safer"
    },
    {
        "title": "Linux command line utility to resolve host names using /etc/hosts first",
        "question": "There are several command line utilities to resolve host names (host, dig, nslookup), however they all use nameservers exclusively, while applications in general look in /etc/hosts first (using gethostbyname I believe).\nIs there a command line utility to resolve host names that behaves like a usual application, thus looking in /etc/hosts first and only then asking a nameserver?\n(I am aware that it would probably be like 3 lines of c, but I need it inside of a somewhat portable shell script.)",
        "top_answer": "This is easily achieved with getent:\ngetent hosts 127.0.0.1\n\ngetent will do lookups for any type of data configured in nsswitch.conf.",
        "url": "https://serverfault.com/questions/303716/linux-command-line-utility-to-resolve-host-names-using-etc-hosts-first"
    },
    {
        "title": "How to monitor a windows log file in real time? [closed]",
        "question": "On windows how can I easily monitor a log file and see updates to the file in real time?\nBasically, same functionality like tail -f log_file on Unix systems\nPlatform: Windows XP/2003/2008 server\n[Update] this is quite handy for a quick monitoring(thanks to Ckarras's answer)\n\nQuick screen shot of PowerShell type -wait (type is an alias for get-content)\n",
        "top_answer": "I've been using BareTailPro for awhile, and have been very pleased.  It hasn't been updated in awhile, but it still meets my needs.  There's also a free version.\nHere's some of the features:\n\nView files of any size (> 2GB)\nConfigurable highlighting\nMonitor multiple files simultaneously\nHigh-performance search algorithm\nRegular expression text search\nFilter tail mode (include or exclude lines)\n",
        "url": "https://serverfault.com/questions/1845/how-to-monitor-a-windows-log-file-in-real-time"
    },
    {
        "title": "The corporate benefits of using MSI files",
        "question": "What are the advantages of using .msi files over regular setup.exe files? \nI have the impression that deployment is easier on machines where users have few permissions, but not sure about the details.\nWhat features does msiexec.exe have that makes deployment more easy than using setup.exe scenarios?\nAny tips or tricks when deploying .msi applications?",
        "top_answer": "Just a few benefits:\n\nCan be advertised (so that on demand installation could take place). \nLike advertisement, features can be installed as soon as the user tries to use them.\nState management is maintained so Windows Installer provides a way to let administrators see if an application is installed on a machine.\nAbility to roll back if an installation fails.\n\nI think to when I'm deploying software in an enterprise setting: deploying software via MSI is almost enjoyable. In contrast, I almost always find myself dreading deploying software when it's in another container.\nFor some additional info on manipulating MSI installations, type msiexec into the Run dialog.",
        "url": "https://serverfault.com/questions/11670/the-corporate-benefits-of-using-msi-files"
    },
    {
        "title": "How to remove Private Key Password from pkcs12 container?",
        "question": "\nI extracted certificate using Chrome's SSL/export command.\nThen provided it as input to openvpn - in the config for openvpn:\npkcs12 \"path/to/pkcs12_container\"\nWhen calling openvpn ~/openvp_config it asks for a password for private key (wich I entered when exporting using Chrome):\nEnter Private Key Password:...\nI want to remove this password request.\n\nThe question: how to remove the password for private key from pkcs12?\nThat is, create pkcs12 file which doesn't require a password.\n(seems that I already somehow did this a year ago, and now forgot it.damn.)",
        "top_answer": "It can be achieved by various openssl calls.\n\nPASSWORD is your current password\nYourPKCSFile is the file you want to convert\nNewPKCSWithoutPassphraseFile is the target file for the PKCS12 without passphrase\n\nFirst, extract the certificate:\n$ openssl pkcs12 -clcerts -nokeys -legacy -in \"YourPKCSFile\" \\\n      -out certificate.crt -password pass:PASSWORD -passin pass:PASSWORD\n\nSecond, the CA key:\n$ openssl pkcs12 -cacerts -nokeys -legacy -in \"YourPKCSFile\" \\\n      -out ca-cert.ca -password pass:PASSWORD -passin pass:PASSWORD\n\nNow, the private key:\n$ openssl pkcs12 -nocerts -legacy -in \"YourPKCSFile\" \\\n      -out private.key -password pass:PASSWORD -passin pass:PASSWORD \\\n      -passout pass:TemporaryPassword\n\nNow remove the passphrase:\n$ openssl rsa -legacy -in private.key -out \"NewKeyFile.key\" \\\n      -passin pass:TemporaryPassword\n\nPut things together for the new PKCS-File:\n$ cat \"NewKeyFile.key\"  \\\n      \"certificate.crt\" \\\n      \"ca-cert.ca\" > PEM.pem\n\nAnd create the new file:\n$ openssl pkcs12 -export -nodes -CAfile ca-cert.ca \\\n      -in PEM.pem -out \"NewPKCSWithoutPassphraseFile\"\n\nNow you have a new PKCS12 key file without passphrase on the private key part.",
        "url": "https://serverfault.com/questions/515833/how-to-remove-private-key-password-from-pkcs12-container"
    },
    {
        "title": "How to redirect root and only root via htaccess?",
        "question": "I want to redirect only my root to another url, but maintain all the /sub/directories where they belong (and redirect)\nexample:\nmysite.com/1 redirects to somewhere\nmysite.com/admin opens a page\ni want mysite.com/ to redirect to mysecondsite.com and only this with a 301 redirect using htaccess",
        "top_answer": "Try this:\nRewriteEngine on\nRewriteCond %{HTTP_HOST} mysite\\.com [NC]\nRewriteCond %{REQUEST_URI} ^/$\nRewriterule ^(.*)$ http://mysecondsite.com/ [L,R=301]\n\nIf you don't need to check for the old domain (for example, if the directory where your .htaccess is placed is only used by the old domain) you can remove the second line.",
        "url": "https://serverfault.com/questions/58762/how-to-redirect-root-and-only-root-via-htaccess"
    },
    {
        "title": "How can I find current SSH protocol version of the current connection?",
        "question": "I connect to a Linux machine (CentOS 6.4) using PuTTY. Except from fact that I can set PuTTY to only use one type of protocol, how can I find the current SSH connection's version (SSH1 or SSH2)?",
        "top_answer": "Once you are in you say:\nssh -v localhost\n\nit will tell you the exact version of the server.",
        "url": "https://serverfault.com/questions/545555/how-can-i-find-current-ssh-protocol-version-of-the-current-connection"
    },
    {
        "title": "Can I make rsync output only the summary?",
        "question": "I use rsync to backup a directory which is very big, containing many sub-directories and files, so I don't want to see the \"incremental file list\". I just want to know the summary in the end. If I use the argument -q, nothing is output at all. Can I make rsync output only the summary?",
        "top_answer": "Use the following:\nrsync -vr src/ dest/ | sed '0,/^$/d'\n\nExplanation: rsync is run in verbose mode using the -v flag. It outputs a detailed file list, an empty line and the summary. Now sed is used to take advantage of the fact that the summary is separated by an empty line. Everything up to the first empty line is not printed to stdout. ^$ matches an empty line and d prevents it from being output.",
        "url": "https://serverfault.com/questions/526726/can-i-make-rsync-output-only-the-summary"
    },
    {
        "title": "How do you recover you RDS master user username?",
        "question": "Resetting the RDS master user's password is simple enough, but how do you find your master users username?",
        "top_answer": "The master user name can be recovered with the rds-describe-db-instances command.\nIf you are using aws cli-2, command shall become :\naws rds describe-db-instances --region ap-south-1\n",
        "url": "https://serverfault.com/questions/276541/how-do-you-recover-you-rds-master-user-username"
    },
    {
        "title": "loginctl enable-linger/disable-linger ... but reading linger-status?",
        "question": "I know how to enable/disable lingering with loginctl.\nBut up to now I found no way to query the status of a user.\nI want to know: Is lingering enable for user foo?\nHow can I access this information?",
        "top_answer": "You can show a list of lingering users with\nls /var/lib/systemd/linger\n\nbecause\nloginctl enable-linger $USER\nloginctl disable-linger $USER\n\ndo the equivalent of\ntouch /var/lib/systemd/linger/$USER\nrm /var/lib/systemd/linger/$USER\n",
        "url": "https://serverfault.com/questions/846441/loginctl-enable-linger-disable-linger-but-reading-linger-status"
    },
    {
        "title": "Moving Servers Within The Same Building",
        "question": "Here's my scenario: I'm a developer that inherited (unbeknownst to me) three servers located within my office. I also inherited the job of being the admin of the servers with a distinct lack of server administration knowledge and google / ServerFault as a reference point. Luckily, I've never actually had to come into contact physically with the machines or address any issues as they've always 'just worked'.\nAll three machines are located within the same data room and serve the following purpose:\nMachine1 - IIS 8.0 hosting a number of internal applications\nMachine2 - SQL Server 2008 R2 data store for the internal applications\nMachine3 - SQL Server 2008 R2 mirror store of Machine2\nAll three have external hard drives connected that complete back ups frequently.\nI've been informed that all three need to move from one data room to another within the same premises. I wont be completing the physical moving of the hardware, that'll be handled by a competent mover.\nApart from completing a full back up of each, what considerations do I need to make prior to hypothetically flicking the power switch and watching my world move?\nI'm aware that it's far from ideal having all three located in the same room / premises but that's past the scope of this question.",
        "top_answer": "Genuinely interesting question, well asked :)\nThere's a few things you need to check before this move, some easy, some hard.\nPower - check that the new room has not only the right amount of power outlets but that they're the right type - as in physical connector type and if the current location allows for different power phases per server to protect against single phase failure then I'd strongly urge you to replicate that also in the new location.\nCooling - you need to check that there won't be an immediate or gradual build-up of heat that will lead to overheating and potential server shutdown. You can usually look up the maximum power (in watts) or heat (in BTUs) that each server can draw from the manufacturers website - let your building manager know this and get a written confirmation from them stating that the cooling in that location will cope.\nNetworking - this is the hard one - not only does the same number of ports need to be replicated between old and new location but so does their type, speed and most importantly configuration. This last point is the key - there was a time when almost all ports in a network were pretty much equal - I'm old enough to remember those times! but these days the number of port configurations and the place in the network that any one port can be in is astronomical, you need to make sure that your network people replicated EVERYTHING to be identical from old to new - again get this in writing as this isn't easy. If something goes wrong with this move I'd put money it'll be on the network ports not being identical, it happens all the time.\n'Other connections' - do you know if your servers have any other connections than power and networking? perhaps they have Fibre-Channel links to shared storage, KVM links to a shared management screen - again if they do you need to replicate these identically.\nOther than that feel free to come back here with any more specific questions, and I hope the move goes well.",
        "url": "https://serverfault.com/questions/798298/moving-servers-within-the-same-building"
    },
    {
        "title": "adding password to .ssh/config [closed]",
        "question": "I'm using ubuntu 12.04. I'm using ssh for connecting to many servers daily, so I put their parameters in .ssh/config file; like this :\nHost server1\nUser tux\nPort 2202\nHostName xxx.x.xx.x\n\nI know we should use key-pair ensure security, however sometimes we can't add public key into the remote machine (e.g. a public SSH server which accepting password and execute a specific command, or an user without a home directory).\nSo, is there a way to put passwords in this file, for each connection? So when the server asks for a password, the terminal puts its pass and send it to the server, so I need not type the password each time.",
        "top_answer": "No, There is no method to specify or provide on the command line the password in a non-interactive manner for ssh authentication using a openssh built-in mechanism. At least not one what I know of. You could hardcode your password into expect script but it is not a good solution either. \nYou definitely would want to use keypairs for passwordless authentication as Michael stated, in the end private key is pretty much a big password in the file.",
        "url": "https://serverfault.com/questions/535028/adding-password-to-ssh-config"
    },
    {
        "title": "Why do I need to purchase an SSL certificate when I can generate one locally?",
        "question": "I am having trouble understanding why we need to purchase SSL certificates when we can generate them locally using openSSL. What is the difference between the certificate I purchase and a test certificate I generate locally? Is it just a big scam?",
        "top_answer": "One word - trust. The SSL certificate from a provider that your browser trusts means that they have at least done basic verification to say that you are who you say you are.\nOtherwise I could make my own certificates for google.com or yourbank.com and pretend to be them.\nPaid certificates do not provide any extra level of encryption over self signed (usually). But a self signed certificate will cause the browser to throw an error.\nYes parts of SSL are a scam (a verisign certificate vs a geotrust where verisign are up to 100x more expensive) but not all of it.\nIf this is all internal stuff, then there is no need for a paid certificate as you can employ your own trust methods (e.g. Do nothing, or perhaps just fingerprint checking).",
        "url": "https://serverfault.com/questions/503513/why-do-i-need-to-purchase-an-ssl-certificate-when-i-can-generate-one-locally"
    },
    {
        "title": "How to Chown a directory recursively including hidden files or directories",
        "question": "Seems like chown with the recursive flag will not work on hidden directories or files. Is there any simple workaround for that?",
        "top_answer": "I'm pretty sure the -R flag does work - it always has for me anyway. What won't work, and what tripped me up early in my command line usage, is using * in a directory with hidden files/directories. So doing \n$ chown -R /home/user/*\n\nwill not do the hidden files and directories. However if you follow it with\n$ chown -R /home/user/.[^.]*\n\nthen you will do all the hidden files, (but not . or .. as /home/user/.* would do). Having said all that, I would expect\n$ chown -R /home/user\n\nto get all the hidden files and directories inside /home/user - though that will of course also change the permissions of the directory itself, which might not be what you intended.",
        "url": "https://serverfault.com/questions/156437/how-to-chown-a-directory-recursively-including-hidden-files-or-directories"
    },
    {
        "title": "How can I run mongod in the background on unix (mac osx)?",
        "question": "I would like to run mongod in the background as an always present sort of thing. What would be the best way to do this? Kind of like the way I can run MySQL on startup and it's just always there running in the background. Maybe it's just some bash scripts, but it would be nice to hear if there is a better way. If it is just bash - what would that look like? Thanks.",
        "top_answer": "The MongoDB daemon (mongod) has a command-line option to run the server in the background...\n--fork\n\nThis command-line option requires that you also specify a file to log messages to (since it can not use the current console). An example of this command looks like:\nmongod --fork --logpath /var/log/mongod.log\n\nYou could put this into an /etc/init.d/mongod bash script file. And then to have the service run at startup, create the standard symbolic links (S## & K##) inside of /etc/rc#.d/. Here is a tutorial that explains this process in more detail. Scroll down to the section titled \"Init Script Activation\". This also has the added benefit of being able to execute commands like...\nservice mongod status\nservice mongod start\nservice mongod stop\n",
        "url": "https://serverfault.com/questions/157705/how-can-i-run-mongod-in-the-background-on-unix-mac-osx"
    },
    {
        "title": "Should I use semicolons at the end of each PowerShell statement? [closed]",
        "question": "I've done a fair bit of programming in C#, but then I've also written a lot of T-SQL scripts. C# requires semicolons, and T-SQL and PowerShell they're optional. What do you do for PowerShell? Why? My gut feel is to include semicolons but I don't know why.",
        "top_answer": "Powershell primarily uses new lines as statement seperators, but semicolons may be used for multiple statements on a single line.",
        "url": "https://serverfault.com/questions/443344/should-i-use-semicolons-at-the-end-of-each-powershell-statement"
    },
    {
        "title": "systemd service automatic restart after StartLimitInterval",
        "question": "I want my systemd service to be automatically restarted on failure.  Additionally I want to rate limit the restarts.  I want to allow maximum of 3 restarts within 90 seconds duration.  Hence I have done the following configuration.\n[Service]  \nRestart=always  \nStartLimitInterval=90  \nStartLimitBurst=3\n\nNow the service is restarted on failure.  After 3 Quick failures/restarts it is not restarting anymore as expected.  Now I expected the systemd to start the service after the timeout (StartLimitInterval).  But the systemd is not automatically starting the service after the timeout(90sec), if I manually restart the service after the timeout it is working.  But I want the systemd to automatically start the service after the StartLimitInterval.  Please let me know on how to achieve this feature.",
        "top_answer": "To have a service restart 3 times at 90 second intervals include the following lines in your systemd service file:\n[Unit]\nStartLimitIntervalSec=400\nStartLimitBurst=3\n[Service]\nRestart=always\nRestartSec=90\n\nBefore systemd-230 it was called just StartLimitInterval:\n[Unit]\nStartLimitInterval=400\nStartLimitBurst=3\n[Service]\nRestart=always\nRestartSec=90\n\nThis worked worked for me for a service that runs a script using Type=idle. Note that StartLimitIntervalSec must be greater than RestartSec * StartLimitBurst otherwise the service will be restarted indefinitely.\nIt took me some time with a lot of trial and error to work out how systemd uses these options, which suggests that systemd isn't as well documented as one would hope.  These options effectively provide the retry cycle time and maximum retries that I was looking for.\nReferences: https://manpages.debian.org/testing/systemd/systemd.unit.5.en.html for Unit section\nhttps://manpages.debian.org/testing/systemd/systemd.exec.5.en.html for Service section",
        "url": "https://serverfault.com/questions/736624/systemd-service-automatic-restart-after-startlimitinterval"
    },
    {
        "title": "how to use xauth to run graphical application via other user on linux",
        "question": "My regular user account is, let's say, user1. I created separate user2 for some x application that i would like to run while being logged into x as user1 but in a way that will prevent it from read/write access to user1 data. I thought that i could use xauth and sudo/su to user2 from user1 to run this application. How do i do this? I'm not sure how to configure xauth.",
        "top_answer": "To use xauth selectively, as user1 run:\nxauth list|grep `uname -n`\n\nThis prints the hexkey authorization entries for you . You could have different displays associated with those hosts as well.\nAs user2 set your display (assuming default case):\nDISPLAY=:0; export DISPLAY\n\nThen run:\nxauth add $DISPLAY . hexkey\n\nNote the dot after the $DISPLAY and before the hexkey. \nWhen access is no longer needed, as user2 you can run:\nxauth remove $DISPLAY\n",
        "url": "https://serverfault.com/questions/51005/how-to-use-xauth-to-run-graphical-application-via-other-user-on-linux"
    },
    {
        "title": "How to give username/password to git clone in a script, but not store credentials in .git/config",
        "question": "I am cloning a git repository in a script like this:\ngit clone https://user:[email\u00a0protected]/name/.git\n\nThis works, but my username and my password! are now stored in the origin url in .git/config.\nHow can I prevent this, but still do this in a script (so no manual password input)?",
        "top_answer": "The method that I use is to actually use a git pull instead of a clone. The script would look like:\nmkdir repo\ncd repo\ngit init\ngit config user.email \"email\"\ngit config user.name \"user\"\ngit pull https://user:[email\u00a0protected]/name/repo.git master\n\nThis will not store your username or password in .git/config. However, unless other steps are taken, the plaintext username and password will be visible while the process is running from commands that show current processes (e.g. ps).\nAs brought up in the comments, since this method is using HTTPS you must URL-encode any special characters that may appear in your password as well.\nOne further suggestion I would make (if you can't use ssh) is to actually use an OAuth token instead of plaintext username/password as it is slightly more secure. You can generate an OAuth token from your profile settings: https://github.com/settings/tokens.\nThen using that token the pull command would be\ngit pull https://$OAUTH_TOKEN:[email\u00a0protected]/name/repo.git master\n",
        "url": "https://serverfault.com/questions/815043/how-to-give-username-password-to-git-clone-in-a-script-but-not-store-credential"
    },
    {
        "title": "ssh on windows - Corrupted MAC on input",
        "question": "I've installed OpenSSH on Windows.\nI can successfully connect to my remote server via ssh with Putty from this Windows machine.\nBut when opening a PowerShell, and trying\nssh [email\u00a0protected]\n\nI've got the error:\ndebug3: send packet: type 5                                                                    \nCorrupted MAC on input.                                                                        \nssh_dispatch_run_fatal: Connection to 1.2.3.4 port 22: message authentication code incorrect\n\nWhen looking on my remote server in the secure logs, I've got:\nDec  7 03:20:22 allo-01 sshd[10102]: Connection from 4.3.2.1 port 49869 on 1.2.3.4 port 22\nDec  7 03:20:23 allo-01 sshd[10102]: Connection reset by 4.3.2.1 port 49869 [preauth]\n\nDo you know what's wrong? Why my ssh command from openssl on windows behave differently from PuTTY?",
        "top_answer": "Raoul's answer to his own question is correct. I ran into the same issue and adding the correct algorithm name after the -m option works (in my case the option was -m hmac-sha2-512 to connect from PowerShell to a machine running Ubuntu 18.04).\nI wasn't sure which algorithm to use, but you can list all the available ones by running:\nssh -Q mac\n\nI selected one at random, tried it and the remote server returned saying that algorithm wasn't supported, but it handily told me which one's were so that I could amend my command. Using this command I could then ssh into the remote machine:\nssh -m hmac-sha2-512 <user_name>@<remote_address>\n\nIf you need to use scp too, the parameter is different:\nscp -o MACs=hmac-sha2-512 <and the rest of your scp command>\n\nYou can add this to you ~/.ssh/config also:\nHost name\n    Hostname <fqdn>\n    IdentityFile ~/.ssh/<key-file>\n    User <username>\n    MACs hmac-sha2-512\n",
        "url": "https://serverfault.com/questions/994646/ssh-on-windows-corrupted-mac-on-input"
    },
    {
        "title": "iptables port redirect not working for localhost",
        "question": "I want to redirect all traffic from port 443 to the internal port 8080. I'm using this config for iptables:\niptables -t nat -I PREROUTING --source 0/0 --destination 0/0 -p tcp \\\n         --dport 443 -j REDIRECT --to-ports 8080\n\nThis works for all external clients. But if I'm trying to access the port 443 from the same maschine I'll get a connection refused error.\nwget https://localhost\n\nHow can I extend the iptables rule to redirect local traffic too?",
        "top_answer": "PREROUTING isn't used by the loopback interface, you need to also add an OUTPUT rule:\niptables -t nat -I PREROUTING -p tcp --dport 443 -j REDIRECT --to-ports 8080\niptables -t nat -I OUTPUT -p tcp -o lo --dport 443 -j REDIRECT --to-ports 8080\n",
        "url": "https://serverfault.com/questions/211536/iptables-port-redirect-not-working-for-localhost"
    },
    {
        "title": "Does bigger capacity SSD have longer life due to wear leveling?",
        "question": "I have been told that you can get a longer lifespan of an SSD if you buy a bigger capacity SSD. The reasoning goes that newer SSDs have wear leveling and thus should sustain the same amount of writing whether you spread this writing on the (logical) disk or not. And if you get an SSD that is twice the size of what you need, then you have twice the capacity to do wear leveling on.\nIs there any truth to that?",
        "top_answer": "This is true, and it was one of the key motivation to backing the switch from SLC (fast and durable flash cells, but small capacity) to MLC (slower and less durable flash cells, but bigger capacity). To give you some ballpark numbers (on old 34nm tech):\n\nSLC drive: 100K P/E cycles (program-erase cycles), 100 GB in size, 10 DWPD (Drive Writes Per Day) x 5y, total 1825 TBW (TeraBytes Written);\nMLC drive: 30K P/E cycles, 200 GB in size, 3 DWPD x 5y, total 1095 TBW.\n\nAs you can see, while the MLC drive as less than 1/3 the P/E endurance, due to its bigger size, its total endurance (in Terabyte Written) is 60% of the SLC drive (rather than the expected 30%). An even higher endurance can be achieved with sufficient overprovisioning, bringing relative parity between the two disks.\nThat said, SSDs rarely die due to NAND wear. Rather, controller and FLT (flash translation layer) bugs are what kill, or brick, flash-based solid state drives. Choosing an SSD, I would put a priority on these things:\n\ncapacity: as space is never enough, do not underestimate your needs. Bigger disks are (often) also faster than smaller ones, due to more NAND chips available;\npower loss protection: if used for synchronous writes, be sure to buy a disk with powerloss protected writeback caches;\nvendor track record: if used for enterprise workloads, do not buy \"no-name\" SSD or \"game oriented\" models. Rather, go with a know and reliable vendor, as Intel, Samsung, and Micron/Crucial.\n",
        "url": "https://serverfault.com/questions/883697/does-bigger-capacity-ssd-have-longer-life-due-to-wear-leveling"
    },
    {
        "title": "iptables -L pretty slow. Is this normal?",
        "question": "Quick question but Gooling has not revealed an answer. When I do iptables -L, it seems to lag on displaying items in where I have limited the source to internal ips 192.168.0.0/24\nThe whole listing takes about 30 seconds to display. \nI just want to know: Does this affect the speed of my incoming connections or is this simply a side effect of having all these ranges within my iptables rules?\nThanks!",
        "top_answer": "Include the -n option so it doesn't try to use DNS to resolve names for every ip address, network and port.  Then it will be fast.",
        "url": "https://serverfault.com/questions/85602/iptables-l-pretty-slow-is-this-normal"
    },
    {
        "title": "Multiple EC2 security groups - permissive or restrictive?",
        "question": "What happens when I assign multiple security groups to an instance?  Is it permissive in the sense that the traffic is allowed in if any one of the security groups allows it.  OR is it restrictive in the sense that every security group must allow the traffic in for it to be passed in?\nFor example, lets say I have a class of instances that will only ever talk to other instances in the same account.  I also have a class of instances that will only accept traffic via HTTP (port 80).\nIs it possible to restrict access to internal instances and only via HTTP by creating and applying two security groups:\n\nAn \"internal\" security group.  Allow all traffic in from other members of that security group on all ports for all transports (TCP, UDP, ICMP)\nCreate an \"http\" security group.  Allow all traffic into port 80 via TCP from any source.\n\nOR am I forced to create a single security group that allows traffic from port 80 where the source is itself?",
        "top_answer": "If an instance has multiple security groups, it has the sum of all rules in the various groups.\n\nFor example, lets say I have a class of instances that will only ever talk to other instances in the same account. I also have a class of instances that will only accept traffic via http (port 80).\n\nThis is a perfect situation for AWS Virtual Private Cloud. Put the internal instances in private subnets, and the public-facing instances in public subnets.",
        "url": "https://serverfault.com/questions/483938/multiple-ec2-security-groups-permissive-or-restrictive"
    },
    {
        "title": "What are SPF records, and how do I configure them?",
        "question": "\nThis is a canonical question about setting up SPF records.  \n\nI have an office with many computers that share a single external ip (I'm unsure if the address is static or dynamic). Each computer connects to our mail server via IMAP using outlook. Email is sent and received by those computers, and some users send and receive email on their mobile phones as well.\nI am using http://wizard.easyspf.com/ to generate an SPF record and I'm unsure about some of the fields in the wizard, specifically:\n\nEnter any other domains who may send or relay mail for this domain\nEnter any IP addresses in CIDR format for netblocks that originate or relay mail for this domain\nEnter any other hosts which can send or relay mail for this domain\nHow stringent should SPF-aware MTA's treat this?\n\nthe first few questions i'm fairly certain about... hope i have given enough info.",
        "top_answer": "SPF records detail which servers are allowed to send mail for your domain.  \nQuestions 1-3 really summarise the whole point of SPF: You're supposed to be listing the addresses of all the servers that are authorised to send mail coming from your domain.\nIf you don't have an exhaustive list at this time, it's generally not a good idea to set up an SPF record.  Also a domain can only have one SPF record, so you'll need to combine all the information into a single record. \nThe individual questions really just help break the list down for you.\n\nasks you for other domains whose mail servers may relay mail from you; if you have eg a secondary MX server at mail-relay.example.org, and that is the main mail server (MX record) for the domain example.org, then you should enter mx:example.org.  Your SPF record should include your own domain's MX record under nearly all circumstances (mx).\nasks you for your ip netblocks.  If you have colocated servers at 1.2.3.0/28, and your office address space is 6.7.8.0/22, enter ip4:1.2.3.0/28 ip4:6.7.8.0/22.  IPv6 space should be added as eg ip6:2a01:9900:0:4::/64.\nif (eg) you also have a machine off in someone else's office that has to be allowed to send mail from your domain, enter that as well, with eg a:mail.remote.example.com.\n\nYour mobile phone users are problematic.  If they send email by connecting to your mail server using eg SMTP AUTH, and sending through that server, then you've dealt with them by listing the mail server's address in (2).  If they send email by just connecting to whatever mail server the 3G/HSDPA provider's offering, then you can't do SPF meaningfully until you have rearchitected your email infrastructure so that you do control every point from which email purporting to be from you hits the internet.\nQuestion 4 is a bit different, and asks what recipients should do with email that claims to be from your domain that doesn't come from one of the systems listed above.  There are several legal responses, but the only interesting ones are ~all (soft fail) and -all (hard fail).  ?all (no answer) is as useless as ~all (qv), and +all is an abomination.\n~all is the simple choice; it tells people that you've listed a bunch of systems who are authorized to send mail from you, but that you're not committing to that list being exhaustive, so mail from your domain coming from other systems might still be legal.  I urge you not to do that.  Not only does it make SPF completely pointless, but some mail admins on SF deliberately configure their SPF receivers to treat ~all as the badge of a spammer.  If you're not going to do -all, don't bother with SPF at all.\n-all is the useful choice; it tells people that you've listed the systems that are allowed to send email from you, and that no other system is authorized to do so, so they are OK to reject emails from systems not listed in your SPF record.  This is the point of SPF, but you have to be sure that you have listed all the hosts that are authorized to originate or relay mail from you before you activate it.\nGoogle is known to advise that\n\nPublishing an SPF record that uses -all instead of ~all may result in\n  delivery problems.\n\nwell, yes, it may; that is the whole point of SPF.  We cannot know for sure why google gives this advice, but I strongly suspect that it's to prevent sysadmins who don't know exactly whence their email originates from causing themselves delivery problems.  If you don't know where all your email comes from, don't use SPF.  If you're going use SPF, list all the places it comes from, and tell the world you're confident in that list, with -all.\nNote that none of this is binding on a recipient's server; the fact that you advertise an SPF record in no way obliges anyone else to honour it. It is up to the admins of any given mail server what email they choose to accept or reject.  What I think SPF does do is allow you to disclaim any further responsibility for email that claimed to be from your domain, but wasn't. Any mail admin coming to you complaining that your domain is sending them spam when they haven't bothered to check the SPF record you advertise that would have told them that the email should be rejected can fairly be sent away with a flea in their ear.\n\nSince this answer has been canonicalised, I'd better say a few words about include and redirect.  The latter is simpler; if your SPF record, say for example.com, says redirect=example.org, then example.org's SPF record replaces your own. example.org is also substituted for your domain in those look-ups (eg, if example.org's record includes the mx mechanism, the MX lookup should be done on example.org, not on your own domain).\ninclude is widely misunderstood, and as the standard's authors note \"the name 'include' was poorly chosen\".  If your SPF record includes example.org's record, then example.org's record should be examined by a recipient to see if it gives any reason (including +all) to accept your email.  If it does, your mail should pass.  If it doesn't, the recipient should continue to process your SPF record until landing on your all mechanism.  Thus, -all, or indeed any other use of all except +all, in an included record, has no effect on the result of processing.\nFor more information on SPF records http://www.openspf.org is an excellent resource.\n\nPlease don't take this the wrong way, but if you get an SPF record wrong, you can stop a significant fraction of the internet from receiving email from you until you fix it.  Your questions suggest you might not be completely au fait with what you're doing, and if that's the case, then you might want to consider getting professional assistance before you do something that stops you sending email to an awful lot of people.\nEdit: thank you for your kind words, they're much appreciated.\nSPF is primarily a technique to prevent joe-jobbing, but some people seem to have started to use it to try to detect spam.  Some of those may indeed attach a negative value to your having no SPF record at all, or an overbroad record (eg a:3.4.5.6/2 a:77.5.6.7/2 a:133.56.67.78/2 a:203.54.32.1/2, which rather sneakily equates to +all), but that's up to them and there's not much you can do about it.\nI personally think SPF is a good thing, and you should advertise a record if your current mail structure permits it, but it's very difficult to give an authoritative answer, valid for the entire internet, about how people are using a DNS record designed for a specific purpose, when they decide to use it for a different purpose.  All I can say with certainty is that if you do advertise an SPF record with a policy of -all, and you get it wrong, a lot of people will never see your mail.\nEdit 2: deleted pursuant to comments, and to keep the answer up-to-date.",
        "url": "https://serverfault.com/questions/369460/what-are-spf-records-and-how-do-i-configure-them"
    },
    {
        "title": "Unusual HEAD requests to nonsense URLs from Chrome",
        "question": "I have noticed unusual traffic coming from my workstation the last couple of days. I am seeing HEAD requests sent to random character URLs, usually three or four within a second, and they appear to be coming from my Chrome browser. The requests repeat only three or four times a day, but I have not identified a particular pattern. The URL characters are different for each request.\nHere is an example of the request as recorded by Fiddler 2:\nHEAD http://xqwvykjfei/ HTTP/1.1\nHost: xqwvykjfei\nProxy-Connection: keep-alive\nContent-Length: 0\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.13 (KHTML, like Gecko) Chrome/9.0.597.98 Safari/534.13\nAccept-Encoding: gzip,deflate,sdch\nAccept-Language: en-US,en;q=0.8\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3\n\nThe response to this request is as follows:\nHTTP/1.1 502 Fiddler - DNS Lookup Failed\nContent-Type: text/html\nConnection: close\nTimestamp: 08:15:45.283\n\nFiddler: DNS Lookup for xqwvykjfei failed. No such host is known\n\nI have been unable to find any information through Google searches related to this issue. I do not remember seeing this kind of traffic before late last week, but it may be that I just missed it before. The one modification I made to my system last week that was unusual was adding the Delicious add-in/extension to both IE and Chrome. I have since removed both of these, but am still seeing the traffic. I have run virus scan (Trend Micro) and HiJackThis looking for malicious code, but I have not found any. \nI would appreciate any help tracking down the source of the requests, so I can determine if they are benign, or indicative of a bigger problem. Thanks.",
        "top_answer": "This is actually legitimate behaviour.  Some ISPs improperly respond to DNS queries to non-existent domains with an A record to a page that they control, usually with advertising, as a \"did you mean?\" kind of thing, instead of passing NXDOMAIN as the RFC requires.  To combat this, Chrome makes several HEAD requests to domains which cannot exist to check how the DNS servers resolve them.  If they return A records, Chrome knows to perform a search query for the host instead of obeying the DNS record so that you are not affected by the ISPs improper behaviour. [1]",
        "url": "https://serverfault.com/questions/235307/unusual-head-requests-to-nonsense-urls-from-chrome"
    },
    {
        "title": "Given a debian source package - How do I install the build-deps?",
        "question": "I have a debian (well technically ubuntu) source package, i.e. the .dsc, the .tar.gz, etc., I want to build this. The dpkg-buildpackage fails, since I don't have all the build dependencies.\nNormally I'd use apt-get build-dep, but this package isn't in apt.\nIs there a 'clean', 'proper' way to install all the build dependencies, given a source package. I know I could just open the debian/control file,  but I'm curious if there's a 'proper' way.\nI have installed build-essential, and have all the normal compilers etc., this package requires extra software.",
        "top_answer": "Use mk-build-deps which is part of devscripts.",
        "url": "https://serverfault.com/questions/127625/given-a-debian-source-package-how-do-i-install-the-build-deps"
    },
    {
        "title": "Is virtual machine slower than the underlying physical machine?",
        "question": "This question is quite general, but most specifically I'm interested in knowing if virtual machine running Ubuntu Enterprise Cloud will be any slower than the same physical machine without any virtualization. How much (1%, 5%, 10%)?\nDid anyone measure performance difference of web server or db server (virtual VS physical)?\nIf it depends on configuration, let's imagine two quad core processors, 12 GB of memory and a bunch of SSD disks, running 64-bit ubuntu enterprise server. On top of that, just 1 virtual machine allowed to use all resources available.",
        "top_answer": "The typical experience for a general purpose server workload on a bare metal\\Type 1 Hypervisor is around 1-5% of CPU overhead and 5-10% Memory overhead, with some additional overhead that varies depending on overall IO load. That is pretty much consistent in my experience for modern Guest OS's running under VMware ESX\\ESXi, Microsoft Hyper-V and Xen where the underlying hardware has been appropriately designed. For 64 bit Server operating systems running on hardware that supports the most current cpu hardware virtualization extensions I would expect all Type 1 hypervisors to be heading for that 1% overhead number. KVM's maturity isn't quite up to Xen (or VMware) at this point but I see no reason to think that it would be noticeably worse than them for the example you describe.\nFor specific use cases though the overall\\aggregate \"performance\" of a virtual environment can exceed bare metal \\ discrete servers. Here's an example of a discussion on how a VMware Clustered implentation can be faster\\better\\cheaper than a bare metal Oracle RAC.  VMware's memory management techniques (especially transparent page sharing) can eliminate the memory overhead almost entirely if you have enough VM's that are similar enough. The important thing in all these cases is that the performance\\efficiency benefits that virtualization can deliver will only be realised if you are consolidating multiple VM's onto hosts, your example (1 VM on the host) will always be slower than bare metal to some degree.\nWhile this is all useful the real issues in terms of Server virtualization tend to be centered around management, high availability techniques and scalability. A 2-5% CPU performance margin is not as important as being able to scale efficiently to 20, 40 or however many VM's you need on each host. You can deal with the performance hit by selecting a slightly faster CPU as your baseline, or by adding more nodes in your clusters but if the host can't scale out the number of VM's it can run, or the environment is hard to manage or unreliable then its worthless from a server virtualization perspective. ",
        "url": "https://serverfault.com/questions/135431/is-virtual-machine-slower-than-the-underlying-physical-machine"
    },
    {
        "title": "find command default sorting order",
        "question": "what is the default sorting order for entries returned by the linux find command?\nFor example, if I issue \nfind . -type f -name '*mp3' \n\nand the output consists of multiple files across multiple sub-folders, what is the default order in which directories are listed? At the same time, what is the sorting order in which files within an individual directory are listed?\nSometimes it returns:\n./B/01.mp3\n./A/01.mp3\n./A/04.mp3\n./A/02.mp3\n\nSee how first the contents of directory B is listed, then that of directory A. At the same time within directory A, files are listed in a funny order.",
        "top_answer": "find will be traversing the directory tree in the order items are stored within the directory entries. This will (mostly) be consistent from run to run, on the same machine and will essentially be \"file/directory creation order\" if there have been no deletes. \nHowever, some file systems will re-order directory entries as part of compaction operations or when the size of the entry needs to be expanded, so there's always a small chance the \"raw\" order will change over time. If you want a consistent order, feed the output through an extra sorting stage.",
        "url": "https://serverfault.com/questions/181787/find-command-default-sorting-order"
    },
    {
        "title": "When do DNS queries use TCP instead of UDP? [duplicate]",
        "question": "\nPossible Duplicate:\nIs it true that a nameserver have to answer queries over TCP? \n\nI know DNS uses UDP for most of its queries, but in what circumstances will it use TCP instead?",
        "top_answer": "DNS uses TCP when the size of the request or the response is greater than a single packet such as with responses that have many records or many IPv6 responses or most DNSSEC responses.\nThe maximum size was originally 512 bytes but there is an extension to the DNS protocol that allows clients to indicate that they can handle UDP responses of up to 4096 bytes.\nDNSSEC responses are usually larger than the maximum  UDP size.\nTransfer requests are usually larger than the maximum UDP size and hence will also be done over TCP.",
        "url": "https://serverfault.com/questions/404840/when-do-dns-queries-use-tcp-instead-of-udp"
    },
    {
        "title": "Soft limit vs Hard limit?",
        "question": "Can anyone explain in layman's terms what the difference between soft and hard limit is?\nShould I set my soft and hard limit to be the same? Or should soft be significantly lower? Does the system benefit either way?",
        "top_answer": "The hard limit is the ceiling for the soft limit. The soft limit is what is actually enforced for a session or process. This allows the administrator (or user) to set the hard limit to the maximum usage they wish to allow. Other users and processes can then use the soft limit to self-limit their resource usage to even lower levels if they so desire.",
        "url": "https://serverfault.com/questions/265155/soft-limit-vs-hard-limit"
    },
    {
        "title": "How do I get the MD5 of a file on Windows?",
        "question": "I need to check the MD5 of a few files on Windows. Any recommendations on either a command line or an explorer-plugin utility?",
        "top_answer": "Single file: Look the answer below me.\nAll .jpg files in current directory:\nforfiles /s /m *.jpg /c \"cmd /c CertUtil -hashfile @path MD5\"",
        "url": "https://serverfault.com/questions/57529/how-do-i-get-the-md5-of-a-file-on-windows"
    },
    {
        "title": "How do I make Linux recognize a new SATA /dev/sda drive I hot swapped in without rebooting?",
        "question": "Hot swapping out a failed SATA /dev/sda drive worked fine, but when I went to swap in a new drive, it wasn't recognized:\n[root@fs-2 ~]# tail -18 /var/log/messages\nMay 5 16:54:35 fs-2 kernel: ata1: exception Emask 0x10 SAct 0x0 SErr 0x50000 action 0xe frozen\nMay 5 16:54:35 fs-2 kernel: ata1: SError: { PHYRdyChg CommWake }\nMay 5 16:54:40 fs-2 kernel: ata1: link is slow to respond, please be patient (ready=0)\nMay 5 16:54:45 fs-2 kernel: ata1: device not ready (errno=-16), forcing hardreset\nMay 5 16:54:45 fs-2 kernel: ata1: soft resetting link\nMay 5 16:54:50 fs-2 kernel: ata1: link is slow to respond, please be patient (ready=0)\nMay 5 16:54:55 fs-2 kernel: ata1: SRST failed (errno=-16)\nMay 5 16:54:55 fs-2 kernel: ata1: soft resetting link\nMay 5 16:55:00 fs-2 kernel: ata1: link is slow to respond, please be patient (ready=0)\nMay 5 16:55:05 fs-2 kernel: ata1: SRST failed (errno=-16)\nMay 5 16:55:05 fs-2 kernel: ata1: soft resetting link\nMay 5 16:55:10 fs-2 kernel: ata1: link is slow to respond, please be patient (ready=0)\nMay 5 16:55:40 fs-2 kernel: ata1: SRST failed (errno=-16)\nMay 5 16:55:40 fs-2 kernel: ata1: limiting SATA link speed to 1.5 Gbps\nMay 5 16:55:40 fs-2 kernel: ata1: soft resetting link\nMay 5 16:55:45 fs-2 kernel: ata1: SRST failed (errno=-16)\nMay 5 16:55:45 fs-2 kernel: ata1: reset failed, giving up\nMay 5 16:55:45 fs-2 kernel: ata1: EH complete\n\nI tried a couple things to make the server find the new /dev/sda, such as rescan-scsi-bus.sh but they didn't work:\n[root@fs-2 ~]# echo \"---\" > /sys/class/scsi_host/host0/scan\n-bash: echo: write error: Invalid argument\n[root@fs-2 ~]#\n[root@fs-2 ~]# /root/rescan-scsi-bus.sh -l\n[snip]\n0 new device(s) found.\n0 device(s) removed.\n[root@fs-2 ~]#\n[root@fs-2 ~]# ls /dev/sda\nls: /dev/sda: No such file or directory\n\nI ended up rebooting the server.  /dev/sda was recognized, I fixed the software RAID, and everything is fine now.  But for next time, how can I make Linux recognize a new SATA drive I have hot swapped in without rebooting?\nThe operating system in question is RHEL5.3:\n[root@fs-2 ~]# cat /etc/redhat-release\nRed Hat Enterprise Linux Server release 5.3 (Tikanga)\n\nThe hard drive is a Seagate Barracuda ES.2 SATA 3.0-Gb/s 500-GB, model ST3500320NS.\nHere is the lscpi output:\n[root@fs-2 ~]# lspci\n00:00.0 RAM memory: nVidia Corporation MCP55 Memory Controller (rev a2)\n00:01.0 ISA bridge: nVidia Corporation MCP55 LPC Bridge (rev a3)\n00:01.1 SMBus: nVidia Corporation MCP55 SMBus (rev a3)\n00:02.0 USB Controller: nVidia Corporation MCP55 USB Controller (rev a1)\n00:02.1 USB Controller: nVidia Corporation MCP55 USB Controller (rev a2)\n00:04.0 IDE interface: nVidia Corporation MCP55 IDE (rev a1)\n00:05.0 IDE interface: nVidia Corporation MCP55 SATA Controller (rev a3)\n00:05.1 IDE interface: nVidia Corporation MCP55 SATA Controller (rev a3)\n00:05.2 IDE interface: nVidia Corporation MCP55 SATA Controller (rev a3)\n00:06.0 PCI bridge: nVidia Corporation MCP55 PCI bridge (rev a2)\n00:08.0 Bridge: nVidia Corporation MCP55 Ethernet (rev a3)\n00:09.0 Bridge: nVidia Corporation MCP55 Ethernet (rev a3)\n00:0a.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:0b.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:0c.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:0d.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:0e.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:0f.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:18.0 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] HyperTransport Technology Configuration\n00:18.1 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] Address Map\n00:18.2 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] DRAM Controller\n00:18.3 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] Miscellaneous Control\n00:19.0 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] HyperTransport Technology Configuration\n00:19.1 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] Address Map\n00:19.2 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] DRAM Controller\n00:19.3 Host bridge: Advanced Micro Devices [AMD] K8 [Athlon64/Opteron] Miscellaneous Control\n03:00.0 VGA compatible controller: Matrox Graphics, Inc. MGA G200e [Pilot] ServerEngines (SEP1) (rev 02)\n04:00.0 PCI bridge: NEC Corporation uPD720400 PCI Express - PCI/PCI-X Bridge (rev 06)\n04:00.1 PCI bridge: NEC Corporation uPD720400 PCI Express - PCI/PCI-X Bridge (rev 06)\n\n\nUpdate: In perhaps a dozen cases, we've been forced to reboot servers because hot swap hasn't \"just worked.\"  Thanks for the answers to look more into the SATA controller.  I've included the lspci output for the problematic system above (hostname: fs-2).  I could still use some help understanding what exactly isn't supported hardware-wise in terms of hot swap for that system.  Please let me know what other output besides lspci might be useful.\nThe good news is that hot swap \"just worked\" today on one of our servers (hostname: www-1), which is very rare for us.  Here is the lspci output:\n[root@www-1 ~]# lspci\n00:00.0 RAM memory: nVidia Corporation MCP55 Memory Controller (rev a2)\n00:01.0 ISA bridge: nVidia Corporation MCP55 LPC Bridge (rev a3)\n00:01.1 SMBus: nVidia Corporation MCP55 SMBus (rev a3)\n00:02.0 USB Controller: nVidia Corporation MCP55 USB Controller (rev a1)\n00:02.1 USB Controller: nVidia Corporation MCP55 USB Controller (rev a2)\n00:04.0 IDE interface: nVidia Corporation MCP55 IDE (rev a1)\n00:05.0 IDE interface: nVidia Corporation MCP55 SATA Controller (rev a3)\n00:05.1 IDE interface: nVidia Corporation MCP55 SATA Controller (rev a3)\n00:05.2 IDE interface: nVidia Corporation MCP55 SATA Controller (rev a3)\n00:06.0 PCI bridge: nVidia Corporation MCP55 PCI bridge (rev a2)\n00:08.0 Bridge: nVidia Corporation MCP55 Ethernet (rev a3)\n00:09.0 Bridge: nVidia Corporation MCP55 Ethernet (rev a3)\n00:0b.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:0c.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:0f.0 PCI bridge: nVidia Corporation MCP55 PCI Express bridge (rev a3)\n00:18.0 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] HyperTransport Configuration\n00:18.1 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] Address Map\n00:18.2 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] DRAM Controller\n00:18.3 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] Miscellaneous Control\n00:18.4 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] Link Control\n00:19.0 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] HyperTransport Configuration\n00:19.1 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] Address Map\n00:19.2 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] DRAM Controller\n00:19.3 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] Miscellaneous Control\n00:19.4 Host bridge: Advanced Micro Devices [AMD] K10 [Opteron, Athlon64, Sempron] Link Control\n03:00.0 VGA compatible controller: Matrox Graphics, Inc. MGA G200e [Pilot] ServerEngines (SEP1) (rev 02)\n04:00.0 PCI bridge: NEC Corporation uPD720400 PCI Express - PCI/PCI-X Bridge (rev 06)\n04:00.1 PCI bridge: NEC Corporation uPD720400 PCI Express - PCI/PCI-X Bridge (rev 06)\n09:00.0 SCSI storage controller: LSI Logic / Symbios Logic SAS1064ET PCI-Express Fusion-MPT SAS (rev 04)\n",
        "top_answer": "If your SATA controller supports hot swap, it should \"just work(tm).\"\nTo force a rescan on a SCSI BUS (each SATA port shows as a SCSI BUS) and find new drives, you will use:\necho \"0 0 0\" >/sys/class/scsi_host/host<n>/scan\n\nOn the above, < n > is the BUS number.",
        "url": "https://serverfault.com/questions/5336/how-do-i-make-linux-recognize-a-new-sata-dev-sda-drive-i-hot-swapped-in-without"
    },
    {
        "title": "How do I know I'm running inside a linux \"screen\" or not?",
        "question": "The \"screen\" refers to a program mentioned in How to reconnect to a disconnected ssh session . That is a good facility.\nBut there is a question I'd really like to know. How do I know whether I'm running inside a \"screen\"? The difference is: \n\nIf yes, I know I can safely close current terminal window, e.g., close a PuTTY window, without losing my shell(Bash etc) session.\nIf no, I know I have to take care of any pending works before I close the terminal window.\n\nBetter, I'd like this status to be displayed in PS1 prompt so that I can see it any time automatically.",
        "top_answer": "(Stolen from \"How can I tell whether I'm in a screen?\" over on StackOverflow and authored by user jho. P.S. You can't vote for a duplicate across StackExchange sites.)\nCheck $STY. If it's null, you're on a \"real\" terminal. If it contains anything, it's the name of the screen you're in. \nIf you are not in screen:\neric@dev ~ $ echo $STY\neric@dev ~ $ \n\nIf you are in screen:\neric@dev ~ $ echo $STY\n2026.pts-0.ip-10-0-1-71\n\nIf you use tmux instead of screen, also check $TMUX. To add this to your prompt, add the following to your ~/.bashrc:\nif [ -n \"$STY\" ]; then export PS1=\"(screen) $PS1\"; fi\nif [ -n \"$TMUX\" ]; then export PS1=\"(tmux) $PS1\"; fi\n",
        "url": "https://serverfault.com/questions/377221/how-do-i-know-im-running-inside-a-linux-screen-or-not"
    },
    {
        "title": "Favorite rsync tips and tricks",
        "question": "The more I use rsync the more I realise that it's a swiss army knife of file transfer. There are so many options. I recently found out that you can go --remove-source-files and it'll delete a file from the source when it's been copied, which makes it a bit more of a move, rather than copy programme. :)\nWhat are you favorite little rsync tips and tricks?",
        "top_answer": "Try to use rsync version 3 if you have to sync many files! V3 builds its file list incrementally and is much faster and uses less memory than version 2.\nDepending on your platform this can make quite a difference. On OSX version 2.6.3 would take more than one hour or crash trying to build an index of 5 million files while the version 3.0.2 I compiled started copying right away.",
        "url": "https://serverfault.com/questions/45083/favorite-rsync-tips-and-tricks"
    },
    {
        "title": "How to determine AWS security group dependencies?",
        "question": "Amazon EC2 won't let me delete a security group, complaining that the group still has dependencies. How Can I find what those dependencies are?\naws ec2 describe-security-groups doesn't say.",
        "top_answer": "Paste the security group ID in the \"Network Interfaces\" section of EC2. This will find usage across EC2, EB, RDS, ELB.\nCLI: aws ec2 describe-network-interfaces --filters Name=group-id,Values=sg-123abc45",
        "url": "https://serverfault.com/questions/546012/how-to-determine-aws-security-group-dependencies"
    },
    {
        "title": "Can I create SSH to tunnel HTTP through server like it was proxy?",
        "question": "Say I have a server and client. I need to create connection from client to a website through server like it was proxy.\nIs it possible to do this using a SSH tunel, or do I have to install some proxy service to the server?",
        "top_answer": "You can do this using ssh\nssh -L 80:remotehost:80 user@myserver\n\nYou will have a tunnel from your local port 80 to the remotehost port 80 then. This does not have to be the same as myserver.\nTo make that transparent you should add an entry to the hosts file. If you don't do that vhosts will not work.\nIf you want a SOCKS-proxy connection you could also use\nssh -D 5000 user@myserver\n\nThis will create a SOCKS-proxy on localhost port 5000 which routes all requests through myserver.",
        "url": "https://serverfault.com/questions/78351/can-i-create-ssh-to-tunnel-http-through-server-like-it-was-proxy"
    }
]